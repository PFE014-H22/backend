Id,AcceptedAnswerId,CreationDate,Title,Body,Tags,AnswerBody,Params
1093115,1094826,2009-07-07 15:44:46,Atomic transactions in key-value stores,"<p>Please excuse any mistakes in terminology.  In particular, I am using relational database terms.</p>

<p>There are a number of persistent key-value stores, including <a href=""http://couchdb.apache.org/"" rel=""noreferrer"" title=""CouchDB"">CouchDB</a> and <a href=""http://incubator.apache.org/cassandra/"" rel=""noreferrer"" title=""Cassandra"">Cassandra</a>, along with plenty of other projects.</p>

<p>A typical argument against them is that they do not generally permit atomic transactions across multiple rows or tables.  I wonder if there's a general approach would would solve this issue.</p>

<p>Take for example the situation of a set of bank accounts.  How do we move money from one bank account to another?  If each bank account is a row, we want to update two rows as part of the same transaction, reducing the value in one and increasing the value in another.</p>

<p>One obvious approach is to have a separate table which describes transactions.  Then, moving money from one bank account to another consists of simply inserting a new row into this table.  We do not store the current balances of either of the two bank accounts and instead rely on summing up all the appropriate rows in the transactions table.  It is easy to imagine that this would be far too much work, however; a bank may have millions of transactions a day and an individual bank account may quickly have several thousand 'transactions' associated with it.</p>

<p>A number (all?) of key-value stores will 'roll back' an action if the underlying data has changed since you last grabbed it.  Possibly this could be used to simulate atomic transactions, then, as you could then indicate that a particular field is locked.  There are some obvious issues with this approach.</p>

<p>Any other ideas?  It is entirely possible that my approach is simply incorrect and I have not yet wrapped my brain around the new way of thinking.</p>
",<couchdb><transactions><cassandra><key-value>,"<p>If, taking your example, you want to atomically update the value in a <em>single</em> document (row in relational terminology), you can do so in CouchDB. You will get a conflict error when you try to commit the change if an other contending client has updated the same document since you read it. You will then have to read the new value, update and re-try the commit. There is an indeterminate (possibly infinite if there is a <em>lot</em> of contention) number of times you may have to repeat this process, but you are guaranteed to have a document in the database with an atomically updated balance if your commit ever succeeds.</p>

<p>If you need to update two balances (i.e. a transfer from one account to an other), then you need to use a separate transaction document (effectively another table where rows are transactions) that stores the amount and the two accounts (in and out). This is a common bookkeeping practice, by the way. Since CouchDB computes views only as needed, it is actually still very efficient to compute the current amount in an account from the transactions that list that account. In CouchDB, you would use a map function that emitted the account number as key and the amount of the transaction (positive for incoming, negative for outgoing). Your reduce function would simply sum the values for each key, emitting the same key and total sum. You could then use a view with group=True to get the account balances, keyed by account number.</p>
",['table']
1623399,1670457,2009-10-26 06:46:46,storing massive ordered time series data in bigtable derivatives,"<p>I am trying to figure out exactly what these new fangled data stores such as bigtable, hbase and cassandra really are.</p>

<p>I work with massive amounts of stock market data, billions of rows of price/quote data that can add up to 100s of gigabytes every day (although these text files often compress by at least an order of magnitude).  This data is basically a handful of numbers, two or three short strings and a timestamp (usually millisecond level).  If I had to pick a unique identifier for each row, I would have to pick the whole row (since an exchange may generate multiple values for the same symbol in the same millisecond).</p>

<p>I suppose the simplest way to map this data to bigtable (I'm including its derivatives) is by symbol name and date (which may return a very large time series, more than million data points isn't unheard of).  From reading their descriptions, it looks like multiple keys can be used with these systems.  I'm also assuming that decimal numbers are not good candidates for keys.</p>

<p>Some of these systems (Cassandra, for example) claims to be able to do range queries.  Would I be able to efficiently query, say, all values for MSFT, for a given day, between 11:00 am and 1:30 pm ?</p>

<p>What if I want to search across ALL symbols for a given day, and request all symbols that have a price between $10 and $10.25 (so I'm searching the values, and want keys returned as a result)?</p>

<p>What if I want to get two times series, subtract one from the other, and return the two times series and their result, will I have to do his logic in my own program?</p>

<p>Reading relevant papers seems to show that these systems are not a very good fit for massive time series systems.  However, if systems such as google maps are based on them, I think time series should work as well.  For example, think of time as the x-axis, prices as y-axis and symbols as named locations--all of a sudden it looks like bigtable should be the ideal store for time series (if the whole earth can be stored, retrieved, zoomed and annotated, stock market data should be trivial). </p>

<p>Can some expert point me in the right direction or clear up any misunderstandings.</p>

<p>Thanks</p>
",<cassandra><finance><hbase><bigtable><time-series>,"<p><strong>I am not an expert</strong> yet, but I've been playing with Cassandra for a few days now, and I have some answers for you:</p>

<ol>
<li>Don't worry about amount of data, it's irrelevant with systems like Cassandra, if you have $$$ for a large hardware cluster.</li>
</ol>

<blockquote>
  <p>Some of these systems (Cassandra, for example) claims to be able to do range queries. Would I be able to efficiently query, say, all values for MSFT, for a given day, between 11:00 am and 1:30 pm ?</p>
</blockquote>

<p>Cassandra is very useful when you know how to work with keys. It can swift through keys very quickly. So to search for MSFT between 11:00 and 1:30pm, you'd have to key your rows like this:</p>

<p>MSFT-timestamp, GOOG-timestamp , ..etc
Then you can tell Cassandra to find all keys that start with MSFT-now and end with MSFT-now+1hour.</p>

<blockquote>
  <p>What if I want to search across ALL symbols for a given day, and request all symbols that have a price between $10 and $10.25 (so I'm searching the values, and want keys returned as a result)?</p>
</blockquote>

<p>I am not an expert, but so far I realized that Cassandra doesn't' search by values at all. So if you want to do the above, you will have to make another table dedicated just to this problem and design your schema to fit the case. But it won't be much different from what I described above. It's all about naming your keys and columns. Cassandra can find them very quickly!</p>

<blockquote>
  <p>What if I want to get two times series, subtract one from the other, and return the two times series and their result, will I have to do his logic in my own program?</p>
</blockquote>

<p>Correct, all logic is done inside your program. This is not MySQL. This is just a storage engine. (But I am sure the next versions will offer these sort of things)</p>

<p>Please remember, that I am a novice at this, if I am wrong, feel free to correct me.</p>
",['table']
2573106,2608981,2010-04-04 00:08:51,What are the alternative ways to model M:M relations in Cassandra?,"<p>Consider a M:M relation that needs to be represented in a Cassandra data store.</p>

<p>What M:M modeling options are available? For each alternative, when is it to prefer? What M:M modeling choices have you made in your Cassandra powered projects?</p>
",<database><cassandra><nosql><referential-integrity>,"<p>Instead of using a join table the way you would with an rdbms, you would have one ColumnFamily containing a row for each X and a list of Ys associated with it, then a CF containing a row for each Y and a list of each X associated with it.</p>

<p>If it turns out you don't really care about querying one of those directions then only keep the CF that you do care about.</p>
",['table']
2577113,2577223,2010-04-05 05:55:35,MySQL app with Cassandra in mind,"<p>I am planning to make a web app which will <em>eventually</em> use <a href=""http://cassandra.apache.org/"" rel=""nofollow noreferrer"">Cassandra</a>. But now that I don't have it and don't have server(s) to run it from, I'd like to start by using MySQL. </p>

<p>The question is, is it possible to structure data and queries in such a way that it would be fairly easy to later port it to Cassandra? Main idea, I guess, would be writing MySQL queries that would be possible to change to same Cassandra queries.</p>

<p>Maybe you have a better suggestion?</p>
",<mysql><cassandra>,"<p>Well, for the easiest transition, you probably just need to use MySQL as a key-value store. Cassandra allows a little more complexity than straight key-value (basically, it allows sub-keys), but that's going to be tricky to do in MySQL without making your eventual conversion efforts a lot harder.</p>

<p>So to use MySQL as a key-value store, that means you just create one table with two columns, <code>key</code> (which should be the primary key on the table) and <code>value</code>. <code>value</code> is probably going to need to be of type <code>text</code>, if you intend to store large amounts of data in it, but <code>key</code> could just be a <code>varchar(255)</code>, unless you need large keys for some reason.</p>

<p>Then you just store any data under a key, and retrieve it back with its key, that's it. That'll be very simple to convert to Cassandra later.</p>

<p>Out of curiosity, why are you planning on using Cassandra?</p>
",['table']
2581465,2581853,2010-04-05 22:04:57,Cassandra instead of MySQL for social networking app,"<p>I am in the middle of building a new app which will have very similar features to Facebook and although obviously it wont ever have to deal with the likes of 400,000,000 million users it will still be used by a substantial user base and most of them will demand it run very very quickly.</p>

<p>I have extensive experience with MySQL but a social app offers complexities which MySQL is not well suited too. I know Facebook, Twitter etc have moved towards Cassandra for a lot of their data but I am not sure how far to go with it.</p>

<p>For example would you store such things as user data - username, passwords, addresses etc in Cassandra? Would you store e-mails, comments, status updates etc in Cassandra? I have also read alot that something like neo4j is much better for representing the friend relationships used by social apps as it is a graph database. I am only just starting down the NoSQL route so any guidance is greatly appreciated.</p>

<p>Would anyone be able to advise me on this? I hope I am not being too general!</p>
",<mysql><social-networking><cassandra><neo4j>,"<blockquote>
  <p>For example would you store such things as user data - username, passwords, addresses etc in Cassandra? </p>
</blockquote>

<p>No, since it does not guarantee consistency. Cassandra is <em>eventually consistent</em>. Surely there shouldn't be concurrency on a certain user account's data, but I wouldn't want to bet on it. You might not need consistency on your fulltext search, your message inbox, etc. but you want consistency in anything that is security-related.</p>

<blockquote>
  <p>I have also read alot that something like neo4j is much better for representing the friend relationships used by social apps as it is a graph database.</p>
</blockquote>

<p>I'm a big fan of the right tool for the right job. I haven't used neo4j but I've been using db4o (which is an object database) and find it very helpful. It makes development easier to use a tool that natively supports your needs. Since you need graphs and working with graphs in SQL is a pain, I'd recommend to give it a look, and evaluate whether it fits your specific needs. </p>

<p>Mixing databases sounds like a good idea to me as long as the choice is natural (i.e. the respective database is helpful with the specific jobs, a  graph databases for graphs, a table for tables, ACID databases for anything that needs transaction safety, etc...). </p>
",['table']
3138738,3139627,2010-06-29 07:39:11,is there any trick to do wildcards search on apache cassandra?,"<p>i need to do something like this on apache cassandra,
SELECT * FROM mytable where address = ""%indonesia%""</p>

<p>any idea how to do it on cassandra?</p>
",<database><nosql><cassandra>,"<p>Its not supported out of the box. You must maintain your own indices. </p>

<p>I would recommend to use ""Supercolumn index"" or use a order preserving partitioner (e.g. org.apache.cassandra.dht.OrderPreservingPartioner) in conjunction with range queries. </p>

<p>Take a look at <a href=""http://www.slideshare.net/benjaminblack/cassandra-basics-indexing"" rel=""nofollow noreferrer"">the slides</a> from Benjamin Black's excellent talk about cassandra and index</p>
",['partitioner']
3232206,3232997,2010-07-12 20:56:25,"Sequential Row IDs in Column Oriented DBs (HBase, Cassandra)?","<p>I've seen two contradictory pieces of advice when it comes to designing row IDs in HBase, (specifically, but I think it applies to Cassandra as well.)</p>

<ol>
<li>Group keys that you'll be aggregating together often to take advantage of data locality. (White, Hadoop: The Definitive Guide and I recall seeing it on the HBase site, but can't find it...)</li>
<li>Spread keys around so that work can be distributed across multiple machines (<a href=""http://www.scribd.com/doc/31652181/Twitter-Pig-and-HBase-For-Bay-Area-Hadoop-User-Group-May-2010"" rel=""nofollow noreferrer"">Twitter, Pig, and HBase at Twitter</a> slide 14)</li>
</ol>

<p>I'm guessing which one is optimal can depend on your use case, but does anyone have any experience with either strategy?</p>
",<nosql><cassandra><hbase><column-oriented>,"<p>In HBase, a table is partitioned into regions by dividing up the key space, which is sorted lexicographically.  Each region of the table belongs to a single region server, so all reads and writes are handled by that server (which allows for a strong consistency guarantee).  This means that if all of your reads or writes are concentrated on a small range of your keyspace, that you will only be able to scale to what a single region server can handle.  For example, if your data is a time series and keyed by the timestamp, then all writes are going to the last region in the table, and you will be constrained to writing at the rate that a single server can handle.</p>

<p>On the other hand, if you can choose your keys such that any given query only needs to scan a small range of rows, but that the overall set of reads and writes are spread across your keyspace, then the total load will be distributed and scale nicely, but you can still enjoy the locality benefits for your query.</p>
",['table']
3489329,3489358,2010-08-15 21:23:48,Question about DB of social networking websites,"<p>Like facebook, everyone has messages on the wall. Are all messages stored in only one table? And it will only display messages belong to the user's ID when loading the page? And if it is then there will be so many rows in that table. Is there any limit of the rows in one table?</p>

<p>Or every user has one table only store their own messages?</p>
",<mysql><database><nosql><cassandra>,"<blockquote>
  <p>Or every user has one table only store
  their own messages?</p>
</blockquote>

<p>That's not a good idea, you could end up with lots of tables. Lots of rows are better.</p>

<p>I would simply use a ""table"" that pairs <code>message_id</code> with <code>user_id</code> with proper indexes to quickly find messages attributed to a specific user.</p>

<blockquote>
  <p>Is there any limit of the rows in one
  table?</p>
</blockquote>

<p>Quote from <a href=""http://dev.mysql.com/doc/refman/5.1/en/features.html"" rel=""nofollow noreferrer"">MySQL 5.1 Features</a>:</p>

<blockquote>
  <p>Support for large databases. We use
  MySQL Server with databases that
  contain 50 million records. We also
  know of users who use MySQL Server
  with 200,000 tables and about
  5,000,000,000 rows.</p>
</blockquote>
",['table']
4419499,4421601,2010-12-11 23:15:19,MySQL and NoSQL: Help me to choose the right one,"<p>There is a big database, 1,000,000,000 rows, called threads (these threads actually exist, I'm not making things harder just because of I enjoy it). Threads has only a few stuff in it, to make things faster: (int id, string hash, int replycount, int dateline (timestamp), int forumid, string title)</p>

<p>Query:</p>

<p><code>select * from thread where forumid = 100 and replycount &gt; 1 order by dateline desc limit 10000, 100</code></p>

<p>Since that there are 1G of records it's quite a slow query. So I thought, let's split this 1G of records in as many tables as many forums(category) I have! That is almost perfect. Having many tables I have less record to search around and it's really faster. The query now becomes:</p>

<p><code>select * from thread_{forum_id} where replycount &gt; 1 order by dateline desc limit 10000, 100</code></p>

<p>This is really faster with 99% of the forums (category) since that most of those have only a few of topics (100k-1M). However because there are some with about 10M of records, some query are still to slow (0.1/.2 seconds, to much for my app!, <strong><em>I'm already using indexes!</em></strong>).</p>

<p>I don't know how to improve this using MySQL. Is there a way?</p>

<p>For this project I will use 10 Servers (12GB ram, 4x7200rpm hard disk on software raid 10, quad core)</p>

<p>The idea was to simply split the databases among the servers, but with the problem explained above that is still not enought.</p>

<p>If I install cassandra on these 10 servers (by supposing I find the time to make it works as it is supposed to) should I be suppose to have a performance boost?</p>

<p><em><strong>What should I do? Keep working with MySQL with distributed database on multiple machines or build a cassandra cluster?</em></strong></p>

<p>I was asked to post what are the indexes, here they are:</p>

<pre><code>mysql&gt; show index in thread;
PRIMARY id
forumid
dateline
replycount
</code></pre>

<p>Select explain:</p>

<pre><code>mysql&gt; explain SELECT * FROM thread WHERE forumid = 655 AND visible = 1 AND open &lt;&gt; 10 ORDER BY dateline ASC LIMIT 268000, 250;
+----+-------------+--------+------+---------------+---------+---------+-------------+--------+-----------------------------+
| id | select_type | table  | type | possible_keys | key     | key_len | ref         | rows   | Extra                       |
+----+-------------+--------+------+---------------+---------+---------+-------------+--------+-----------------------------+
|  1 | SIMPLE      | thread | ref  | forumid       | forumid | 4       | const,const | 221575 | Using where; Using filesort | 
+----+-------------+--------+------+---------------+---------+---------+-------------+--------+-----------------------------+
</code></pre>
",<php><mysql><nosql><cassandra>,"<p>You should read the following and learn a little bit about the advantages of a well designed innodb table and how best to use clustered indexes - only available with innodb !</p>

<p><a href=""http://dev.mysql.com/doc/refman/5.0/en/innodb-index-types.html"" rel=""noreferrer"">http://dev.mysql.com/doc/refman/5.0/en/innodb-index-types.html</a></p>

<p><a href=""http://www.xaprb.com/blog/2006/07/04/how-to-exploit-mysql-index-optimizations/"" rel=""noreferrer"">http://www.xaprb.com/blog/2006/07/04/how-to-exploit-mysql-index-optimizations/</a></p>

<p>then design your system something along the lines of the following simplified example:</p>

<h2>Example schema (simplified)</h2>

<p>The important features are that the tables use the innodb engine and the primary key for the threads table is no longer a single auto_incrementing key but a composite <strong>clustered</strong> key based on a combination of forum_id and thread_id. e.g.</p>

<pre><code>threads - primary key (forum_id, thread_id)

forum_id    thread_id
========    =========
1                   1
1                   2
1                   3
1                 ...
1             2058300  
2                   1
2                   2
2                   3
2                  ...
2              2352141
...
</code></pre>

<p>Each forum row includes a counter called next_thread_id (unsigned int) which is maintained by a trigger and increments every time a thread is added to a given forum. This also means we can store 4 billion threads per forum rather than 4 billion threads in total if using a single auto_increment primary key for thread_id.</p>

<pre><code>forum_id    title   next_thread_id
========    =====   ==============
1          forum 1        2058300
2          forum 2        2352141
3          forum 3        2482805
4          forum 4        3740957
...
64        forum 64       3243097
65        forum 65      15000000 -- ooh a big one
66        forum 66       5038900
67        forum 67       4449764
...
247      forum 247            0 -- still loading data for half the forums !
248      forum 248            0
249      forum 249            0
250      forum 250            0
</code></pre>

<p>The disadvantage of using a composite key is that you can no longer just select a thread by a single key value as follows:</p>

<pre><code>select * from threads where thread_id = y;
</code></pre>

<p>you have to do:</p>

<pre><code>select * from threads where forum_id = x and thread_id = y;
</code></pre>

<p>However, your application code should be aware of which forum a user is browsing so it's not exactly difficult to implement - store the currently viewed forum_id in a session variable or hidden form field etc...</p>

<p>Here's the simplified schema:</p>

<pre><code>drop table if exists forums;
create table forums
(
forum_id smallint unsigned not null auto_increment primary key,
title varchar(255) unique not null,
next_thread_id int unsigned not null default 0 -- count of threads in each forum
)engine=innodb;


drop table if exists threads;
create table threads
(
forum_id smallint unsigned not null,
thread_id int unsigned not null default 0,
reply_count int unsigned not null default 0,
hash char(32) not null,
created_date datetime not null,
primary key (forum_id, thread_id, reply_count) -- composite clustered index
)engine=innodb;

delimiter #

create trigger threads_before_ins_trig before insert on threads
for each row
begin
declare v_id int unsigned default 0;

  select next_thread_id + 1 into v_id from forums where forum_id = new.forum_id;
  set new.thread_id = v_id;
  update forums set next_thread_id = v_id where forum_id = new.forum_id;
end#

delimiter ;
</code></pre>

<p>You may have noticed I've included reply_count as part of the primary key which is a bit strange as (forum_id, thread_id) composite is unique in itself. This is just an index optimisation which saves some I/O when queries that use reply_count are executed. Please refer to the 2 links above for further info on this.</p>

<h2>Example queries</h2>

<p>I'm still loading data into my example tables and so far I have a loaded approx. 500 million rows (half as many as your system). When the load process is complete I should expect to have approx:</p>

<pre><code>250 forums * 5 million threads = 1250 000 000 (1.2 billion rows)
</code></pre>

<p>I've deliberately made some of the forums contain more than 5 million threads for example, forum 65 has 15 million threads:</p>

<pre><code>forum_id    title   next_thread_id
========    =====   ==============
65        forum 65      15000000 -- ooh a big one
</code></pre>

<h2>Query runtimes</h2>

<pre><code>select sum(next_thread_id) from forums;

sum(next_thread_id)
===================
539,155,433 (500 million threads so far and still growing...)
</code></pre>

<p>under innodb summing the next_thread_ids to give a total thread count is much faster than the usual:</p>

<pre><code>select count(*) from threads;
</code></pre>

<p>How many threads does forum 65 have:</p>

<pre><code>select next_thread_id from forums where forum_id = 65

next_thread_id
==============
15,000,000 (15 million)
</code></pre>

<p>again this is faster than the usual:</p>

<pre><code>select count(*) from threads where forum_id = 65
</code></pre>

<p>Ok now we know we have about 500 million threads so far and forum 65 has 15 million threads - let's see how the schema performs :)</p>

<pre><code>select forum_id, thread_id from threads where forum_id = 65 and reply_count &gt; 64 order by thread_id desc limit 32;

runtime = 0.022 secs

select forum_id, thread_id from threads where forum_id = 65 and reply_count &gt; 1 order by thread_id desc limit 10000, 100;

runtime = 0.027 secs
</code></pre>

<p>Looks pretty performant to me - so that's a single table with 500+ million rows (and growing) with a query that covers 15 million rows in 0.02 seconds (while under load !)</p>

<h2>Further optimisations</h2>

<p>These would include:</p>

<ul>
<li><p>partitioning by range </p></li>
<li><p>sharding</p></li>
<li><p>throwing money and hardware at it</p></li>
</ul>

<p>etc...</p>

<p>hope you find this answer helpful :)</p>
",['table']
5100782,5102518,2011-02-24 05:35:31,cassandra upgrade from 0.6 to 0.7.2,"<p>I followed the instructions in NEWS.txt to upgrade cassandra 0.6 to 0.7.2. 
The instructions are: 
    The process to upgrade is: 
    1) run ""nodetool drain"" on <em>each</em> 0.6 node.  When drain finishes (log 
       message ""Node is drained"" appears), stop the process. 
    2) Convert your storage-conf.xml to the new cassandra.yaml using 
       ""bin/config-converter"". 
    3) Rename any of your keyspace or column family names that do not adhere 
       to the '^\w+' regex convention. 
    4) Start up your cluster with the 0.7 version. 
    5) Initialize your Keyspace and ColumnFamily definitions using 
       ""bin/schematool   import"".  <em>You only need to do 
       this to one node</em>. </p>

<p>I did the first three steps. drain node, stop cassandra 0.6, convert old storage-conf.xml to cassandra.yaml. 
I start cassandra 0.7.2 using: ""bin/cassandra -f"". But it always complains the following errors. I am wondering whether I followed the right instructions. If so, how could i fix this problem?</p>

<p>""Fatal configuration error 
org.apache.cassandra.config.ConfigurationException: saved_caches_directory missing"" </p>
",<upgrade><cassandra>,"<p>Default location for saved_caches_directory is /var/lib/cassandra/saved_caches (From <a href=""http://wiki.apache.org/cassandra/StorageConfiguration"" rel=""nofollow"">wiki</a>). Try to create that manually (dont forget user permissions)</p>
",['saved_caches_directory']
5221115,5226189,2011-03-07 14:54:37,"Has anyone used HBase, Tornado, Cassandra or HipHop with Drupal?","<p>I have a Drupal application that has a very large recordset, almost half a million nodes (475,181). Even simple joins are becoming too time consuming (3-10s), and we are becoming increasingly dependent on Memcached. I wonder if anyone has used the same technologies that Facebook is using with Drupal.</p>
",<php><drupal><cassandra><hbase><hiphop>,"<p>To answer your question: Not yet, but there is significant work being done on HipHop and a nosql database named MongoDB. More on those below. </p>

<p>As others have mentioned I would first make absolutely sure that your database tables are optimized properly, indexed and that your database has sufficient resources. If your database is choking on under 500k rows it's likely that there is something wrong. (We have over 150k node, about 600k rows in our node_revisions table and because the nodes have multiple images over a million rows in some of our cck tables.) I'm no MySQL expert and was able to get the database query times to under a hundred milleseconsds for most of our queries.) 
Here are the steps I would take before looking into Switching to a different db engine. (from easiest to hardest)</p>

<ol>
<li>look for places that you need indexes. The most likely places are tables that have nid, vid or where there are a lot of queries that use a field value in the where, and any of the columns in your joins. This is where I got 80% of the improvement. Adding too many indexes can slow down inserts and deletes, but they are very simple to add, and remove.</li>
<li>Consider switching to INNODB it's almost always faster than the standard drupal 6 myisam. This is a super easy change. </li>
<li>Confirm that your my.cnf is set up properly. If you haven't changed it, it's probably wrong. The settings that ship with most distributions of MySQL are very, very conservative, and probably wrong for your environment. </li>
<li>Add RAM, if you have switched to INNODB adding RAM can make a huge difference in your performance, even without MySQL loves RAM.</li>
<li>Switch to pressflow if you can. It fixes a number of things that slow down regular drupal. </li>
<li>If you are still having problems look to moving away from CCK and created your own content types.</li>
<li>Review and optemize your views queries. Views isn't terribly efficient and you can sometimes make  very big headway by modifying the queries. </li>
</ol>

<p><a href=""http://2bits.com/drupal-performance/presentation-34-million-page-views-day-92-million-month-one-server-and-drupal.html"" rel=""nofollow"">Here</a> is a great article on how much can be achieved with drupal and 1 single server.</p>

<p>If none of that works here's a few links regarding some of the newer haute technogogies that people are playing with. The most promising is MongoDB, and if you have the development resources to use it on a project I envy you. (It's still a little new new and unpolished for the small shop we are, but I can't wait to sink my teeth into it.)</p>

<p><a href=""http://fourkitchens.com/blog/2010/02/03/making-drupal-pressflow-more-mundane"" rel=""nofollow"">drupal and hiphop</a></p>

<p><a href=""http://drupal.org/project/mongodb"" rel=""nofollow"">drupal and Mongodb</a> - there is also a great talk in the sessions from drupalcon SF (last year) </p>
",['table']
5534713,5539325,2011-04-04 04:57:55,Cassandra startup problem: Attempt to assign id to existing column family,"<p>I'm using <strong>cassandra 0.7.4</strong> on centos5.5 x86_64 with jdk-1.6.0_24 64-Bit.
 When I restart it , it throw out:</p>

<hr>

<pre><code>ERROR 11:37:32,009 Exception encountered during startup.
java.io.IOError: org.apache.cassandra.config.ConfigurationException: Attempt to assign id to existing column family.
    at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:476)
    at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:138)
    at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:314)
    at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
Caused by: org.apache.cassandra.config.ConfigurationException: Attempt to assign id to existing column family.
    at org.apache.cassandra.config.CFMetaData.map(CFMetaData.java:223)
    at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:472)
    ... 3 more    
</code></pre>

<hr>

<p>I try to location the problem: when I delete the file of the system keyspace ,It can restart <strong>sucess</strong>!
 So I think this problem is cause by system Keyspace,even at the CF <strong>Scheam</strong>.</p>

<p>Then I build a new test environment, I know this proble is cause by this opeartion</p>

<pre><code>update keyspace system with replication_factor=3;
</code></pre>

<p>But now how can i repair it ?!
  There are many data on this cluster,and I <strong><em>couldn't</em></strong> lose data.</p>

<p>I have already do <code>update keyspace system with replication_factor=1;</code> ,but the problem <em>still exist</em>.
 I try to use nodetool to repair after or befor flush, all no effect.</p>

<p>How can I restart cassandra <strong>without lose data</strong> ? Who can help me?</p>
",<cassandra><startup>,"<p>You should never modify the system keyspace unless you really, really know what you are doing. (If you have to ask, you don't. :)</p>

<p>So, the answer is: don't do that.</p>

<p>To recover, you should set initial_token in cassandra.yaml to your node's current token (which you can see with ""nodetool ring""), then delete the system keyspace and restart. Then you'll need to recreate your columnfamily definitions, but your data will not be affected.</p>
",['initial_token']
5616532,5656076,2011-04-11 03:39:13,Need Cassandra schema,"<p>HI 
Just started to investifate Cassandra and have a bit confusion.
Could you suggest schema for following:</p>

<p>Schema: <code>email, city, items1[], items2[]</code></p>

<p>Input: <code>cityId, item1, item2</code></p>

<p>I need: </p>

<pre><code>select email 

where city=cityId 

and item1 is NOT in items1[] 

and item2 is NOT in items2[]
</code></pre>

<p>Is it possible?</p>
",<nosql><cassandra><schema-design>,"<p>You may need to give a bit more detail to get a thorough answer to this one. </p>

<p>In Cassandra 0.7 onwards, you can use a secondary index to select rows according to column value (i.e. select rows where city=cityId). You will need to enable this in your Cassandra schema by setting ""index_type: KEYS"" in the column metadata.</p>

<p>See <a href=""http://www.datastax.com/dev/blog/whats-new-cassandra-07-secondary-indexes"" rel=""nofollow"">http://www.datastax.com/dev/blog/whats-new-cassandra-07-secondary-indexes</a> for more details.</p>

<p>Cassandra does not provide negation, so your ""NOT in items1[]"" conditions may need to be tested by the client, not by the Cassandra nodes. How many possible values are there for your items1 and items2 (just a handful, or thousands?).</p>

<p>You will probably need to set up a column family purely for answering this specific type of query, i.e. a lookup table with some kind of compound key based on item1 and item2. However, this may be challenging to maintain server-side if there are many potential values of item1 and item2, and expensive to retrieve client-side!</p>
",['table']
5914735,5915094,2011-05-06 17:08:19,Selective get in cassandra faster than normal get?,"<p>I'd like to know if this:</p>

<pre><code>$column_family-&gt;get('row_key', $columns=array('name1', 'name2'));
</code></pre>

<p>Is faster then the more flexible get i now use:</p>

<pre><code>$column_family-&gt;get('row_key');
</code></pre>

<p>Method 1 is harder to implement of course but will it give less load/bandwidth/delay?</p>
",<php><database><nosql><cassandra><phpcassa>,"<p>First one is faster, especially if you work with large tables that contain plenty of columns. </p>

<p>Even you have just two columns called <code>name1</code> and <code>name2</code>, specifying their names should avoid extracting column names from table structure on MySQL side. So it should be faster than using <code>*</code> selector.</p>

<p>However, test your results using <strong><a href=""http://php.net/manual/en/function.microtime.php"" rel=""nofollow"">microtime()</a></strong> in PHP against large tables and you'll see what I'm talking about. Of course, if you have 20+ columns in table and you want to extract them all it's easier to put <code>*</code> than listing all those column-names but in terms of speed, listing columns is bit quicker.</p>

<p>The best way to check out this conclusion, is to test it by yourself.</p>
",['table']
6116201,6117172,2011-05-24 20:06:03,Maximum key size in Cassandra,"<p>I'm completely new to using cassandra.. is there a maximum key size and would that ever impact performance?</p>

<p>Thanks!</p>
",<cassandra>,"<p><a href=""http://en.wikipedia.org/wiki/Apache_Cassandra"" rel=""nofollow"">http://en.wikipedia.org/wiki/Apache_Cassandra</a> claims (apparently incorrectly!) that:</p>

<blockquote>
  <p>The row key in a table is a string
  with no size restrictions, although
  typically 16 to 36 bytes long</p>
</blockquote>

<p>See also:</p>

<p><a href=""http://cassandra-user-incubator-apache-org.3065146.n2.nabble.com/value-size-is-there-a-suggested-limit-td4959690.html"" rel=""nofollow"">http://cassandra-user-incubator-apache-org.3065146.n2.nabble.com/value-size-is-there-a-suggested-limit-td4959690.html</a> which suggests that there is <em>some</em> limit.</p>

<p>Clearly, very large keys could have <em>some</em> network performance impact if they need to be sent over the Thrift RPC interface - and they would cost storage. I'd suggest you try a quick benchmark to see what impact it has for your data.</p>

<p>One way to deal with this might be to pre-hash your keys and just use the hash value as the key, though this won't suit all use cases. </p>
",['table']
6165091,6165119,2011-05-28 23:53:15,Cassandra write performance vs Releational Databases,"<p>I am trying to grasp some performance differences between Cassandra and relational databases.</p>

<p>From what I have read, Cassandra's write performance remains constant regardless of data volume. By write performance, I am assuming this implies both new rows being added as well as existing rows being replaced on a key match (like an update in the relational world). Is that assumption correct?</p>

<p>Also, from what I understand about relational databases updates get slower when tables/partitions become larger. This is because a full table scan must be performed to locate the row, or an index lookup needs to be performed and both of these things will take longer as the table or partition grows. So updates take perpetually longer based on the data volume of the table/partition?</p>

<p>When new data is inserted to a relational database, I know any indexes need to to have the new data but there is no lookup involved correct? So will inserts also become perpetually slower as data volume increases or stay constant with relational databases?</p>

<p>Thanks for any tips</p>
",<relational-database><cassandra>,"<p>They will become slower if the table has indexes. Not only the data must be written, but the index must be updated too. Inserting in a table that has no indexes <em>and</em> no constraints is lightning fast, because no checks need to be done. The record can just be written at the end of the table space.</p>
",['table']
6292734,6307553,2011-06-09 12:25:02,Fetching key range with common prefix in Cassandra,"<p>I want to fetch all rows having a common prefix using hector API. I played with RangeSuperSlicesQuery a bit but didn't find a way to get it working properly. Does key range parameters work with wild cards etc? </p>

<p>Update: I used ByteOrderedPartitioner instead of RandomPartitioner and it works fine with that. Is this the expected behavior?</p>
",<cassandra><hector>,"<p>Yes, that's the expected behavior. In RandomPartitioner, rows are stored in the order of the MD5 hash of their keys, so to get a meaningful range of keys, you need to use an order preserving partitioner like ByteOrderedPartitioner.</p>

<p>However, there are downsides to using <a href=""http://ria101.wordpress.com/2010/02/22/cassandra-randompartitioner-vs-orderpreservingpartitioner/"" rel=""noreferrer"">ByteOrderedPartitioner or OrderPreservingPartitioner</a> that you can usually avoid with a slightly different data model and RandomPartitioner.  </p>
",['partitioner']
6358035,6361754,2011-06-15 13:00:15,Efficient retrieval of column families,"<p>Recently I've come up against efficient retrieval of several columns from single row in single column family. Currently, I am using Pelops as Cassandra API. The question is what to do if I want to get columns from several ranges. It would be easy if I could get columns from the family according to few slices at once, but I can't. </p>

<p>For example I have a family with enourmous number of columns. Some of them have a common prefix, let's say ""group/xxx"", where xxx is an identifier. There are also a couple of columns named for example ""a"", ""b"", ""c"". Now, I want to get these columns together, so I have to define two slices and call getColumnsFromRow twice. </p>

<p>How to solve this problem in terms of efficiency? Does Cassandra somehow cache a column family which was recently retrieved and calling getColumnsFromRow for the second time will not make searching it again?</p>
",<performance><cassandra>,"<p>Because you have rolled your own compound column names, you basically have to issue multiple get_slice calls. </p>

<p>This is not a terribly big deal efficiency wise since these columns are in the same row and, if you chose your comparator correctly, should be a single disk seek. Subsequent queries to this same row should hit this portion of the table in the OS's disk cache (OS level, nothing to do with Cassandra). </p>

<p>Row caching was designed for small rows where the entire contents are accessed frequently (like a serialized object or similar). They will actually impose a substantial amount of memory pressure for large rows like this. I would recommend leaving row cache disabled for this CF. </p>

<p>If you find you need to, you can do some additional tweaking via making the following adjustments:
- turn down read_repair_chance 
- enable 'result pinning': <a href=""https://github.com/apache/cassandra/blob/cassandra-0.7.0/conf/cassandra.yaml#L229-236"" rel=""nofollow"">https://github.com/apache/cassandra/blob/cassandra-0.7.0/conf/cassandra.yaml#L229-236</a></p>

<p>This will let your 0S'S file system cache work more efficiently since the same hosts will be handling the same queries, and the subsequent slices will be operating on sections of the row ideally in the same SSTable and thus in FS cache. </p>

<p>(Shameless plug - but actually quite helpful in these situations) Also, consider a free download OpsCenter (http://www.datastax.com/opscenter), and watch the metrics for the column family as you experiment with the different options. This will give you an idea of the most efficient way to structure your queries specifically for your data. </p>
",['table']
6433842,6436258,2011-06-22 01:14:33,"Cassandra 0.8 = What kind of ""Row Count"" features are provided?","<p>are ""Row Counts"" (in a CF) in Cassandra meanwhile supported for </p>

<p>a) RAndomPartitioner ?</p>

<p>b) OrderPreservingPartitioner?</p>

<p><a href=""http://www.datastax.com/dev/blog/whats-new-in-cassandra-0-8-part-2-counters"" rel=""nofollow noreferrer"">http://www.datastax.com/dev/blog/whats-new-in-cassandra-0-8-part-2-counters</a> implies this is easily possible? Quote: "" “counting,” we mean here to provide an atomic increment operation in a single column value, as opposed to counting the number of columns in a row, or rows in a column family, both of which were already supported.""</p>

<p>Two years ago it was defenitely not supported for RP:
<a href=""https://stackoverflow.com/questions/1951843/row-count-of-a-column-family-in-cassandra"">Row count of a column family in Cassandra</a></p>

<p>Furthermoe even with OrderPreservingPartitioner, it was(??) a very heavy Operation (as far as I understood i have to retrieve all objects, this is/was not only a lightweight count operation to the row-count, but rather read also all data (rows?) ?)</p>

<p>Update: I am absolutely aware of, that the new counting feature is completely different to row-counts. But the text above implies row-counts are also easily possible and supported quote ""...both of which are supported...""? Is this marketing language meaning it is only possible as an extremely heaving operation using get_range_slice? Or is there something new that I am completly missing, that does this lightweight for both partitioniers? </p>

<p>Thanks</p>

<p>Markus</p>
",<cassandra>,"<p>Counters and counting the number of rows / columns are two different topics.  </p>

<p><a href=""http://cassandra-user-incubator-apache-org.3065146.n2.nabble.com/Count-rows-td5420889.html"" rel=""nofollow"">http://cassandra-user-incubator-apache-org.3065146.n2.nabble.com/Count-rows-td5420889.html</a></p>

<p>I would suggest, as you add new rows to a column family, simply increment +1 a counter CF/row/key and you wont have to page through all of the rows (as the link above says, what if you have billions?) -- This also allows you to not care which partitioner you use ... </p>
",['partitioner']
6668360,6669172,2011-07-12 17:23:50,why is scaling writes to a relational database virtually impossible?,"<p>From Cassandra's presentation slides (slide 2) <a href=""http://www.slideshare.net/jbellis/cassandra-open-source-bigtable-dynamo"" rel=""noreferrer"">link 1</a>, <a href=""http://assets.en.oreilly.com/1/event/27/Cassandra_%20Open%20Source%20Bigtable%20+%20Dynamo%20Presentation.pdf"" rel=""noreferrer"">alternate link</a>:</p>

<blockquote>
  <p>scaling writes to a relational database is virtually impossible</p>
</blockquote>

<p>I cannot understand this statement. Because when I shard my database, I am scaling writes isn't it? And they seem to claim against that.. does anyone know why isn't sharding a database scaling writes?</p>
",<mysql><sql-server><database><nosql><cassandra>,"<p>The slowness of physical disk subsystems is usually the single greatest challenge to overcome when trying to scale a database to service a very large number of concurrent writers. But it is not ""virtually impossible"" to optimize writes to a relational database. It can be done. Yet there is a trade-off: when you optimize writes, selects of large subsets of logically related data usually are slower.</p>

<p>The writes of the primary data to disk and the rebalancing of index trees can be disk-intensive.  The maintenance of clustered indexes, whereby rows that belong logically together are stored physically contiguous on disk, is also disk-intensive. Such indexes make selects (reads) quicker while slowing writes. A heavily indexed table does not scale well therefore, and the lower the cardinality of the index, the less well it scales.</p>

<p>One optimization aimed at improving the speed of concurrent writers is to use sparse tables with hashed primary keys and minimal indexing. This approach eliminates the need for an index on the primary key value and permits an immediate seek to the disk location where a row lives, 'immediate' in the sense that the intermediary of an index read is not required.  The hashed primary key algorithm returns the physical address of the row using the primary key value itself-- a simple computation that requires no disk access.</p>

<p>The sparse table is exactly the opposite of storing logically related data so they are physically contiguous. In a sparse table, writers do not step on each others toes, so to speak. Writes are like raindrops falling on a large field not like a crowd of people on a subway platform trying to step into the train through a few open doors. The sparse table helps to eliminate write bottlenecks.</p>

<p>However, because logically related data are not physically contiguous, but scattered, the act of gathering all rows in a certain zipcode, say, is expensive. This sparse-table hashed-pk optimization is therefore optimal only when the predominant activity is the insertion of records, the update of individual records, and the lookup of data relating to a single entity at a time rather than to a large set of entities, as in, say, an order-entry system.  A company that sold merchandise on TV and had to service tens of thousands of simultaneous callers placing orders would be well served by a system that used sparse tables with hashed primary keys. A national security database that relied upon linked lists would also be well served by this approach. Many social networking applications could also use it to advantage.</p>
",['table']
6909826,6910417,2011-08-02 09:22:05,Sort by key in Cassandra,"<p>Let's assume I have a keyspace with a column family that stores user objects and the key of these objects is the username.</p>

<p>How can I use Hector to get a list of users sorted by username?</p>

<p>I tried to use a RangeSlicesQuery, paging works fine with this query, but the results are not sorted in any way.</p>

<p>I'm an absolute Cassandra beginner, can anyone point me to a simple example that shows how to sort a column family by key? Please ask if you need more details on my efforts.</p>

<p>Edit:</p>

<p>The result was not sorted because I used the default RandomPartitioner instead of the OrderPreseveringPartitioner in cassandra.yaml.</p>

<p>Probably it's better not to rely on the sorting by key but to use a secondary index.</p>
",<sorting><cassandra><hector>,"<p>Quoting <a href=""https://rads.stackoverflow.com/amzn/click/com/1449390412"" rel=""nofollow noreferrer"" rel=""nofollow noreferrer"">Cassandra - The Definitive Guide</a></p>

<blockquote>
  <p>Column names are stored in sorted order according to the value of compare_with. Rows,
  on the other hand, are stored in an order defined by the partitioner (for example,
  with RandomPartitioner, they are in random order, etc.)</p>
</blockquote>

<p>I guess you are using <code>RandomPartitioner</code> which </p>

<blockquote>
  <p>... return data in an essentially random order. </p>
</blockquote>

<p>You should probably use <code>OrderPreservingPartitioner (OPP)</code> where</p>

<blockquote>
  <p>Rows are therefore stored
  by  key  order,  aligning  the  physical  structure  of  the  data  with  your  sort  order.</p>
</blockquote>

<p>Be aware of inefficiency of OPP.</p>

<hr>

<p>(edit on Mar 07, 2014)<br>
<strong>Important:</strong></p>

<p>This answer is very old now.</p>

<p>It is a system-wide setting. You can set in <code>cassandra.yaml</code>. See <a href=""http://www.datastax.com/docs/1.1/configuration/node_configuration#partitioner"" rel=""nofollow noreferrer"">this doc</a>. Again, OPP is highly discouraged. This document is for version 1.1, and you can see <em>it is deprecated</em>. It is likely that it is removed from latest version. If you do want to use OPP, you may want to revisit the architecture the architecture.</p>
",['partitioner']
6932874,6959018,2011-08-03 20:28:57,Brisk cassandra TimeUUIDType,"<p>I used brisk. The cassandra column family automatically maps to Hive tables.<br>
However, if data type is timeuuid in column family, it is unreadable in Hive tables.</p>

<p>For example, I used following command to create an external table in hive to map column family. </p>

<pre><code>Hive &gt; create external table A (rowkey string, column_name string, value string) 
     &gt; STORED BY 'org.apache.hadoop.hive.cassandra.CassandraStorageHandler'
     &gt; WITH SERDEPROPERTIES (
     &gt; ""cassandra.columns.mapping"" = "":key,:column,:value"");  
</code></pre>

<p>If column name is TimeUUIDType in cassandra, it becomes unreadable in the Hive table.  </p>

<p>For example, a row in cassandra column family looks like:  </p>

<pre><code>RowKey: 2d36a254bb04272b120aaf79d70a3578  
        =&gt; (column=29139210-b6dc-11df-8c64-f315e3a329d6, value={""event_id"":101},timestamp=1283464254261)
</code></pre>

<p>Where column name is TimeUUIDType.  </p>

<p>In hive table, it looks like the following row:</p>

<pre><code> 2d36a254bb04272b120aaf79d70a3578    t��ߒ4��!��   {""event_id"":101}
</code></pre>

<p>So, column name is unreadable in Hive table.</p>
",<cassandra><hive><read-unread><brisk>,"<p>This is a known issue with the automatic table mapping. For best results with a timeUUIDType, turn the auto-mapping feature off in $brisk_home/resources/hive/hive-site.xml:
""cassandra.autoCreateHiveSchema""</p>

<p>and create the table in hive manually.</p>
",['table']
7237271,7251868,2011-08-29 23:46:12,Large scale data processing Hbase vs Cassandra,"<p>I am nearly landed at Cassandra after my research on large scale data storage solutions. But its generally said that Hbase is better solution for large scale data processing and analysis. </p>

<p>While both are same key/value storage and both are/can run (Cassandra recently) Hadoop layer then what makes Hadoop a better candidate when processing/analysis is required on large data.</p>

<p>I also found good details about both at
<a href=""http://ria101.wordpress.com/2010/02/24/hbase-vs-cassandra-why-we-moved/"">http://ria101.wordpress.com/2010/02/24/hbase-vs-cassandra-why-we-moved/</a> </p>

<p>but I'm still looking for concrete advantages of Hbase.</p>

<p>While I am more convinced about Cassandra because its simplicity for adding nodes and seamless replication and no point of failure features. And it also keeps secondary index feature so its a good plus.</p>
",<nosql><hadoop><cassandra><hbase><data-processing>,"<p>Trying to determine which is best for you really depends on what you are going to use it for, they each have their advantages and without any more details it becomes more of a religious war. That post you referenced is also more than a year old and both have gone through many changes since then. Please also keep in mind I am not familiar with the more recent Cassandra developments.</p>

<p>Having said that, I'll paraphrase HBase committer Andrew Purtell and add some of my own experiences:</p>

<ul>
<li><p>HBase is in larger production environments (1000 nodes) although that is still in the ballpark of Cassandra's ~400 node installs so its really a marginal difference.</p></li>
<li><p>HBase and Cassandra both supports replication between clusters/datacenters. I believe HBase's exposes more to the user so it appears more complicated but then you also get more flexibility.</p></li>
<li><p>If strong consistency is what your application needs then HBase is likely a better fit. It is designed from the ground up to be consistent. For example it allows for simpler implementation of atomic counters (I think Cassandra just got them) as well as Check and Put operations.</p></li>
<li><p>Write performance is great, from what I understand that was one of the reasons Facebook went with HBase for their messenger.</p></li>
<li><p>I'm not sure of the current state of Cassandra's ordered partitioner, but in the past it required manual rebalancing. HBase handles that for you if you want. The ordered partitioner is important for Hadoop style processing.</p></li>
<li><p>Cassandra and HBase are both complex, Cassandra just hides it better. HBase exposes it more via using HDFS for its storage, if you look at the codebase Cassandra is just as layered. If you compare the Dynamo and Bigtable papers you can see that Cassandra's theory of operation is actually more complex.</p></li>
<li><p>HBase has more unit tests FWIW.</p></li>
<li><p>All Cassandra RPC is Thrift, HBase has a Thrift, REST and native Java. The Thrift and REST do only offer a subset of the total client API but if you want pure speed the native Java client is there.</p></li>
<li><p>There are advantages to both peer to peer and master to slave. The master - slave setup generally makes it easier to debug and reduces quite a bit of complexity.</p></li>
<li><p>HBase is not tied to only traditional HDFS, you can change out your underlying storage depending on your needs. <a href=""http://www.mapr.com/"" rel=""noreferrer"">MapR</a> looks quite interesting and I have heard good things although I have not used it myself.</p></li>
</ul>
",['partitioner']
7578609,7587875,2011-09-28 05:11:18,"cassandra secondary index return results in lexical rowkey order, even with RandomPartitioner?","<p>As far as I understand, a Cassandra secondary index is stored as an internal CF, where the rowkeys are the values within the index, and the columns are rowkeys back to the original CF being indexed.</p>

<p>Is it possible to have the columns of the index store the original CF rowkey values? Then, since columns within the index row are sorted, a query for a particular value in the index theoretically could return rowkeys in sorted value order.</p>

<p>This is how I would do it if I was to manually maintain my own index CF (I'd have my manual index CF sort its columns as strings), I'm curious if the same can be done with built-in secondary indexes.</p>

<hr>

<p>A hopefully clarifying example... I have 5 rows with 2 columns each (<code>identifier</code> is to easily distinguish the rows, <code>birth_date</code> is being indexed), each row with a UTF8 key (in this case a single char string):</p>

<pre><code>[default@demo] create column family users with comparator=UTF8Type
...     and column_metadata=
...     [{column_name: identifier, validation_class: LongType}
...     ,{column_name: birth_date, validation_class: LongType, index_type: KEYS}];
86518c00-e9f7-11e0-0000-242d50cf1fde
Waiting for schema agreement...
... schemas agree across the cluster
[default@demo] set users['a']['identifier'] = 1;
Value inserted.
[default@demo] set users['a']['birth_date'] = 1975;
Value inserted.
[default@demo] set users['c']['identifier'] = 3;
Value inserted.
[default@demo] set users['c']['birth_date'] = 1975;
Value inserted.
[default@demo] set users['b']['identifier'] = 2;
Value inserted.
[default@demo] set users['b']['birth_date'] = 1975;
Value inserted.
[default@demo] set users['x']['identifier'] = 5;
Value inserted.
[default@demo] set users['x']['birth_date'] = 1975;
Value inserted.
[default@demo] set users['f']['identifier'] = 4;
Value inserted.
[default@demo] set users['f']['birth_date'] = 1975;
Value inserted.
</code></pre>

<p>Now when I make an index query, I get the users rows back in what appears to be reverse order of their rowkeys' md5 hashes (looking at the <code>identifier</code>, the result order is x,b,f,c,a):</p>

<pre><code>[default@demo] get users where birth_date = 1975;
-------------------
RowKey: ff
=&gt; (column=birth_date, value=1975, timestamp=1317231030507000)
=&gt; (column=identifier, value=5, timestamp=1317231030504000)
-------------------
RowKey: 0b
=&gt; (column=birth_date, value=1975, timestamp=1317231030502000)
=&gt; (column=identifier, value=2, timestamp=1317231030500000)
-------------------
RowKey: 0f
=&gt; (column=birth_date, value=1975, timestamp=1317231031992000)
=&gt; (column=identifier, value=4, timestamp=1317231030509000)
-------------------
RowKey: 0c
=&gt; (column=birth_date, value=1975, timestamp=1317231030498000)
=&gt; (column=identifier, value=3, timestamp=1317231030494000)
-------------------
RowKey: 0a
=&gt; (column=birth_date, value=1975, timestamp=1317231030491000)
=&gt; (column=identifier, value=1, timestamp=1317231030476000)

5 Rows Returned.
</code></pre>

<p>My question is, is there a way to have the internal index CF use 'a', 'b', 'c', 'f', 'x' as its column names, so that when I make an index query, I get back the users rows in lexical rowkey order.</p>
",<cassandra>,"<p>The reason you can't do this is, the index ordering has to match the partitioner ordering, or you couldn't ""page"" through resultsets across multiple nodes (without having to do scatter/gather for each query, anyway).</p>

<p>We do have <a href=""https://issues.apache.org/jira/browse/CASSANDRA-1599"" rel=""nofollow"">https://issues.apache.org/jira/browse/CASSANDRA-1599</a> open to allow custom ordering, so you should watch that issue for updates.</p>
",['partitioner']
7951310,7951579,2011-10-31 08:29:31,How to implement client authentication solution with NoSQL (Cassandra)?,"<p>I am currently thinking of how to implement an authentication for a web application with a NoSQL solution. The problem I encounter hereby is that in most of the NoSQL solutions (e.g. Cassandra, MongoDB) have probably delayed writes. For example we write on node A but it is not guaranteed that the write is appearing on node B at the same time. This is logical with the approaches behind the NoSQL solutions.</p>

<p>Now one idea would be that you do no secondary reads (so everything goes over a master). This would probably work in MongoDB (where you actually have a master) but not in Cassandra (where all nodes are equal). But our application runs at several independent points all over the world, so we need multi master capability.</p>

<p>At the moment I am not aware of a solution with Cassandra where I could update data and be sure that subsequent reads (to all of the nodes) do have the change. So how could one build an authentication on top of those NoSQL solutions where the authentication request (read) could appear on several nodes in parallel?</p>

<p>Thanks for your help!</p>
",<mongodb><cassandra><nosql>,"<p>With respects to Apache Cassandra:</p>

<ul>
<li><a href=""http://wiki.apache.org/cassandra/API#ConsistencyLevel"" rel=""nofollow"">http://wiki.apache.org/cassandra/API#ConsistencyLevel</a></li>
</ul>

<blockquote>
  <p>The ConsistencyLevel is an enum that controls both read and write behavior based on  in your schema definition. The different consistency levels have different meanings, depending on if you're doing a write or read operation. Note that if W + R > ReplicationFactor, where W is the number of nodes to block for on write, and R the number to block for on reads, you will have strongly consistent behavior; that is, readers will always see the most recent write. Of these, the most interesting is to do QUORUM reads and writes, which gives you consistency while still allowing availability in the face of node failures up to half of ReplicationFactor. Of course if latency is more important than consistency then you can use lower values for either or both. </p>
</blockquote>

<p>This is managed on the application side.  To your question specifically, it comes down to how you design your Cassandra implementation, replication factor across the Cassandra nodes and how your application behaves on read/writes.</p>

<blockquote>
  <p>Write</p>
</blockquote>

<ul>
<li>ANY:  Ensure that the write has been written to at least 1 node, including HintedHandoff recipients.</li>
<li>ONE: Ensure that the write has been written to at least 1 replica's commit log and memory table before responding to the client.</li>
<li>QUORUM: Ensure that the write has been written to N / 2 + 1 replicas before responding to the client.</li>
<li>LOCAL_QUORUM: Ensure that the write has been written to  / 2 + 1 nodes, within the local datacenter (requires NetworkTopologyStrategy)</li>
<li>EACH_QUORUM: Ensure that the write has been written to  / 2 + 1 nodes in each datacenter (requires NetworkTopologyStrategy)</li>
<li>ALL: Ensure that the write is written to all N replicas before responding to the client. Any unresponsive replicas will fail the operation.</li>
</ul>

<blockquote>
  <p>Read</p>
</blockquote>

<ul>
<li>ANY: Not supported. You probably want ONE instead.</li>
<li>ONE: Will return the record returned by the first replica to respond. A consistency check is always done in a background thread to fix any consistency issues when ConsistencyLevel.ONE is used. This means subsequent calls will have correct data even if the initial read gets an older value. (This is called ReadRepair)</li>
<li>QUORUM: Will query all replicas and return the record with the most recent timestamp once it has at least a majority of replicas (N / 2 + 1) reported. Again, the remaining replicas will be checked in the background.</li>
<li>LOCAL_QUORUM: Returns the record with the most recent timestamp once a majority of replicas within the local datacenter have replied.</li>
<li>EACH_QUORUM: Returns the record with the most recent timestamp once a majority of replicas within each datacenter have replied.</li>
<li>ALL: Will query all replicas and return the record with the most recent timestamp once all replicas have replied. Any unresponsive replicas will fail the operation. </li>
</ul>
",['table']
8154332,50285066,2011-11-16 15:39:50,Cassandra - unique constraint on row key,"<p>I would like to know whenever it is possible in Cassandra to specify unique constrain on row key. Something similar to SQL Server's <code>ADD CONSTRAINT myConstrain UNIQUE (ROW_PK)</code></p>

<p>In case of insert with already existing row key, the existing data will be not overwritten, but I receive kind of exception or response that update cannot be performed due to constrain violation.</p>

<p>Maybe there is a workaround for this problem - there are counters which updates seams to be atomic.</p>
",<cassandra>,"<p>Cassandra - a unique constraint can be implemented with the help of a primary key constrain. You need to put all the columns as primary key, those you want to be unique. Cassandra will tackle the rest on it own.</p>

<pre><code>CREATE TABLE users (firstname text, lastname text, age int, 
email text, city text, PRIMARY KEY (firstname, lastname));
</code></pre>

<p>It means Cassandra will not insert two different rows in this <code>users</code> table when <code>firstname</code> and <code>lastname</code> are the same.</p>
",['table']
8793430,8834951,2012-01-09 18:41:04,"For Cassandra, how should the historical data be taken care of?","<p>Say I have a dozen CF/SCF and have the write traffic keeps coming (might have spikes). Over the time, a few of them will growing much faster (due to their own nature) than others and data table could be huge. At that stage, should they still be sitting on the same disk as the other CF/SCF? what if the disk is almost full due to the large amount of store data? or should we consider introducing additional CF/SCF for storing historical data?</p>

<p>In general, what's the best practices that we need follow to take care of the historical data? </p>
",<cassandra><database-backups><historical-db>,"<p>The size of the CF isn't really the issue, as the keys are replicated and spread based on the # of nodes, the token selection per node, the partitioner selected and the replication strategy -- all configurable for a keyspace.  </p>
",['partitioner']
9063113,9087991,2012-01-30 11:27:21,Can Cassandra partition tables based on date/timestamp?,"<p>I have a very big table with many columns. Values in some of the columns change rarely, and as its bad to store all of this data in a single table, I would like to partition the table into many tables based on timestamp. That is, for one timestamp, one table is made, while querying all these tables should give the abstraction of one single table. That is, the query should be only executed on the required tables  (based on the time range query) and all these results should be merged.</p>

<p>Thus I need two functionalities:</p>

<ul>
<li>Automatic sparse implementation</li>
<li>Storage as multiple table based on timestamp and abstraction of hitting a single table</li>
</ul>

<p>Which tool is best suited for this purpose? Would Cassandra be suitable?</p>
",<database><nosql><cassandra><sparse-matrix>,"<p>Conventional SQL databases such as <a href=""http://www.postgresql.org/about/"" rel=""nofollow"">PostgreSQL</a> can handle several TB (maximum theoretical table size is 32TB). Some can handle much larger volumes of data, though this generally requires partioning the data around a cluster of machines.</p>

<p>10 columns is not very many - PostgreSQL has a maximum of 250-16600 columns per table depending on the column type. Indexing on time is provided, so there should be no need to partition by timestamp, given that you still need to query the data (i.e. you are not archiving old data).</p>

<p>Cassandra can handle much larger volumes of data than this, but typically one would use multiple nodes in a cluster to share the load and provide replication. The typical advice seems to be to use one node per TB if the system is heavily read/write loaded, or more (2-3 TB ?) if it is lightly loaded.</p>

<p>Cassandra doesn't use tables as such. It has column families, which contain rows of sparse columns (<a href=""http://wiki.apache.org/cassandra/CassandraLimitations"" rel=""nofollow"">up to 2 billion per row</a>). Again, partitioning of data should not be required, in general - you can store a vast number of rows in a single column family (under the hood, they are partitioned across your nodes, and further partitioned into files called SSTables).</p>

<p>Whether Cassandra is suitable depends somewhat on the types of queries you want to make. Cassandra does not providie flexible SQL queries, so you need to structure the data to suit the queries.</p>
",['table']
9777284,9881796,2012-03-19 20:18:19,Run analytics on huge MySQL database,"<p>I have a MySQL database with a few (five to be precise) huge tables. It is essentially a star topology based data warehouse. The table sizes range  from 700GB (fact table) to 1GB and whole database goes upto 1 terabyte. Now I have been given  a task of running analytics on these tables which might even include joins. 
A simple analytical query on this database can be ""find number of smokers per state and display it in descending order"" this requirement could be converted in a simple query like </p>

<pre><code>select state, count(smokingStatus) as smokers 
from abc 
having smokingstatus='current smoker' 
group by state....
</code></pre>

<p>This query (and many other of same nature) takes a lot of time to execute on this database, time taken is in order of tens of hours. </p>

<p>This database is also heavily used for insertion which means every few minutes there are thousands of rows getting added. </p>

<p>In such a scenario how can I tackle this querying problem? 
I have looked in Cassandra which seemed easy to implement but I am not sure if it is going to be as easy for running analytical queries on the database especially when I have to use ""where clause and group by construct""</p>

<p>Have Also looked into Hadoop but I am not sure how can I implement RDBMS type queries. I am not too sure if I want to right away invest in getting at least three machines for name-node, zookeeper and data-nodes!! Above all our company prefers windows based solutions. </p>

<p>I have also thought  of pre-computing all the data in a simpler summary tables but that limits my ability to run different kinds of queries. </p>

<p>Are there any other ideas which I can implement?</p>

<h2><strong>EDIT</strong></h2>

<p>Following is the mysql environment setup</p>

<p>1) master-slave setup
2) master for inserts/updates
3) slave for reads and running stored procedures 
4) all tables are innodb with files per table
5) indexes on string as well as int columns.</p>

<p>Pre-calculating values is an option but since requirements for this kind of ad-hoc aggregated values keeps changing.</p>
",<mysql><hadoop><cassandra><analytics>,"<p>Looking at this from the position of attempting to make MySQL work better rather than positing an entirely new architectural system:</p>

<p>Firstly, verify what's really happening.  EXPLAIN the queries which are causing issues, rather than guessing what's going on.</p>

<p>Having said that, I'm going to guess as to what's going on since I don't have the query plans. I'm guessing that (a) your indexes aren't being used correctly and you're getting a bunch of avoidable table scans, (b) your DB servers are tuned for OLTP, not analytical queries, (c) writing data while reading is causing things to slow down greatly, (d) working with strings just sucks and (e) you've got some inefficient queries with horrible joins (everyone has some of these).</p>

<p>To improve things, I'd investigate the following (in roughly this order):</p>

<ul>
<li><p>Check the query plans, make sure the existing indexes are being used correctly - look at the table scans, make sure the queries actually make sense.</p></li>
<li><p>Move the analytical queries off the OLTP system - the tunings required for fast inserts and short queries are very different to those for the sorts of queries which potentially read most of a large table.  This might mean having another analytic-only slave, with a different config (and possibly table types - I'm not sure what the state of the art with MySQL is right now).</p></li>
<li><p>Move the strings out of the fact table - rather than having the smoking status column with string values of (say) 'current smoker', 'recently quit', 'quit 1+ years', 'never smoked', push these values out to another table, and have the integer keys in the fact table (this will help the sizes of the indexes too).</p></li>
<li><p>Stop the tables from being updated while the queries are running - if the indexes are moving while the query is running I can't see good things happening.  It's (luckily) been a long time since I cared about MySQL replication, so I can't remember if you can batch up the writes to the analytical query slave without too much drama.</p></li>
<li><p>If you get to this point without solving the performance issues, <em>then</em> it's time to think about moving off MySQL. I'd look at Infobright first - it's open source/$$ &amp; based on MySQL, so it's probably the easiest to put into your existing system (make sure the data is going to the InfoBright DB, then point your analytical queries to the Infobright server, keep the rest of the system as it is, job done), or if Vertica ever releases its Community Edition.  Hadoop+Hive has a lot of moving parts - its pretty cool (and great on the resume), but if it's only going to be used for the analytic portion of you system it may take more care &amp; feeding than other options.</p></li>
</ul>
",['table']
9886996,9890346,2012-03-27 09:43:02,Need suggestion on Cassandra Keyspaces sample,"<p>I have been trying out Cassandra and need some help in understanding a few issues. I am new to cassandra and I am not sure of translating a MySQL DB to Cassandra would lead me to pitfalls which due to say inexperience or limited knowledge of cassandra. So I hope I can get the useful information from experienced cassandra users/developers.</p>

<p>Below are sample keyspaces I have created. I would like to know any sort of drawback in the design if someone from their experience can point out.</p>

<pre><code>create keyspace Students with placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy' and strategy_options = {replication_factor:1};
use Students;
create column family StudentID with column_type = 'Super' and comparator = 'UTF8Type' and subcomparator = 'UTF8Type' and default_validation_class = 'UTF8Type' and column_metadata = 
[{column_name : 'First Name', validation_class : UTF8Type}, 
{column_name : 'Last Name', validation_class : UTF8Type}, 
{column_name : 'Subjects', validation_class : UTF8Type}, 
{column_name : 'Class', validation_class : UTF8Type}];


 set StudentID[utf8('1968')]['00001']['First Name'] = 'Mark';
 set StudentID[utf8('1968')]['00001']['Last Name'] = 'Myers';
 set StudentID[utf8('1968')]['00001']['Subjects'] = 'Maths, Chemistry';
 set StudentID[utf8('1968')]['00001']['Class'] = '10th grade';


create keyspace Teachers with placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy' and strategy_options = {replication_factor:1};
use Teachers;
create column family TeacherID with column_type = 'Super' and comparator = 'UTF8Type' and subcomparator = 'UTF8Type' and default_validation_class = 'UTF8Type' and column_metadata = 
[{column_name : 'First Name', validation_class : UTF8Type}, 
{column_name : 'Last Name', validation_class : UTF8Type}, 
{column_name : 'Subjects', validation_class : UTF8Type}, 
{column_name : 'Class', validation_class : UTF8Type}];

set TeacherID[utf8('777')]['234-333']['First Name'] = 'Mark';
set TeacherID[utf8('777')]['234-333']['Last Name'] = 'Myers';
set TeacherID[utf8('777')]['234-333']['Subjects'] = 'Maths, Chemistry,physics';
set TeacherID[utf8('777')]['234-333']['Class'] = '10th grade, 11th grade, 9th grade';



create keyspace Subjects with placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy' and strategy_options = {replication_factor:1};
use Subjects;
create column family SubjectNames with default_validation_class = 'UTF8Type' and comparator = 'UTF8Type' and column_metadata = 
[{column_name : 'Names1', validation_class : UTF8Type}];


set SubjectNames[utf8('Current')]['Name1']= 'maths';
set SubjectNames[utf8('Current')]['Name2']= 'physics';
set SubjectNames[utf8('Current')]['Name3']= 'chemistry';
set SubjectNames[utf8('Current')]['Name4']= 'CS';
</code></pre>

<p>Three keyspaces - Students, Teachers and Subjects. 
I would definitely need some relationship amongst these keyspaces and would also require querying data.
e.g.</p>

<ul>
<li>I would query for students with a certain subject and/or class</li>
<li>A teacher with certain class</li>
<li>List all subjects taken up by a certain student from a certain class.</li>
</ul>

<p>From what I know, I would definitely need to create secondary indices to make the queries work. That is, retrieving data on certain clauses. </p>

<p>What I know if I am correct</p>

<ul>
<li>We donot have a 'like' clause in cassandra</li>
<li>For each value for a column (the very last key-value pair), the value must be broken up. That is to individual words. Say, I want to get a list of subjects so each subject must reside in a distinct column associated to it. I cannot query column values that are like ""subjectA,subjectB"" instead I would break it up to SubjectA and SubjectB and put them in different columns. </li>
</ul>

<p>Below are the keyspaces.</p>

<p><img src=""https://i.stack.imgur.com/re86f.png"" alt=""students"">
<img src=""https://i.stack.imgur.com/nCJeI.png"" alt=""subject"">
<img src=""https://i.stack.imgur.com/rDPjP.png"" alt=""teachers""></p>
",<database><cassandra>,"<p>First off, is Cassandra the right tool for the job you're trying to do? Cassandra does an excellent job dealing with distributed, loosely coupled, data sets that need high speed reading and writing capability but it starts getting clunky when you want to enforce a relational model on top of it, hence my question. <strong>If you have a highly relational dataset,</strong> like the example you showed here, where the emphasis rests on determining relationships between information, then <strong>MySQL would be a better tool than Cassandra</strong>.</p>
<p>I think you are confusing keyspaces as a 1-1 mapping with MySQL tables. <em>A keyspace would more directly correspond to a database instead of a table in the database.</em> Firstly, you might want to redesign your keyspace setup to put everything together, something like this:</p>
<pre><code>keyspace: School
Column Family: Student ; Row Key: StudentID ; Col1 = First Name, Col2 = Last Name, Col3 = subjects, Col4 = class.
</code></pre>
<p>Repeat for your other two column families--not sure if you need supers or not.</p>
<p>To do cross cutting retrievals, you would need to make a column family such as:</p>
<pre><code>Column Family: Class ; RowKey: ClassId (ie 10th Grade) ; col1= (TeacherId:TeacherId), Col2 = (StudentId:StudentId) 
</code></pre>
<p>to build a relationship column family between a specific class and all the people who belong to it.</p>
<p><strong>Breaking Up</strong><br />
Yes, you would need to break them up by subject and place them into their own column families. Caveat to this being you can use <a href=""http://www.datastax.com/dev/blog/whats-new-cassandra-07-secondary-indexes"" rel=""nofollow noreferrer"">secondary indices</a> (as of Cassandra .7) that allow you to perform more equality type queries such as:</p>
<pre><code>get users where birth_date = 1973;
</code></pre>
<p>Also refer to this <a href=""http://www.datastax.com/docs/0.8/ddl/indexes"" rel=""nofollow noreferrer"">document</a> regarding the use of Secondary Indices. Relevant quote being,</p>
<blockquote>
<p>Cassandra’s built-in secondary indexes are best for cases when many
rows contain the indexed value. The more unique values that exist in a
particular column, the more overhead you will have, on average, to
query and maintain the index. For example, suppose you had a user
table with a billion users and wanted to look up users by the state
they lived in. Many users will share the same column value for state
(such as CA, NY, TX, etc.). This would be a good candidate for a
secondary index. On the other hand, if you wanted to look up users by
their email address (a value that is typically unique for each user),
it may be more efficient to manually maintain a dynamic column family
as a form of an “index”. Even for columns containing unique data, it
is often fine performance-wise to use secondary indexes for
convenience, as long as the query volume to the indexed column family
is moderate and not under constant load.</p>
</blockquote>
<p>If you haven't already seen it, the <a href=""http://www.datastax.com/"" rel=""nofollow noreferrer"">DataStax</a> website will answer lots of your Cassandra questions, I highly recommend browsing through it if you're going to use Cassandra extensively.</p>
<p>In short, your two options are to decouple the items and create column families for each relationship you want to maintain OR to possibly use secondary indices depending on how you separate your data. I personally prefer the former method--despite the boilerplate--because I think it scales out better.</p>
",['table']
10407072,10407149,2012-05-02 02:49:50,Cassandra seed nodes and clients connecting to nodes,"<p>I'm a little confused about Cassandra seed nodes and how clients are meant to connect to the cluster. I can't seem to find this bit of information in the documentation.</p>

<p>Do the clients only contain a list of the seed node and each node delegates a new host for the client to connect to? Are seed nodes only really for node to node discovery, rather than a special node for clients?</p>

<p>Should each client use a small sample of random nodes in the DC to connect to?</p>

<p>Or, should each client use all the nodes in the DC?</p>
",<cassandra>,"<p>Answering my own question:</p>

<p><strong>Seeds</strong></p>

<p>From the <a href=""http://cassandra.apache.org/doc/latest/faq/index.html#what-are-seeds"" rel=""noreferrer"">FAQ</a>:</p>

<blockquote>
  <p>Seeds are used during startup to discover the cluster.</p>
</blockquote>

<p>Also from the <a href=""http://www.datastax.com/docs/1.0/cluster_architecture/gossip"" rel=""noreferrer"">DataStax documentation</a> on ""Gossip"":</p>

<blockquote>
  <p>The seed node designation has no purpose other than bootstrapping the gossip process
  for new nodes joining the cluster. Seed nodes are not a single
  point of failure, nor do they have any other special purpose in
  cluster operations beyond the bootstrapping of nodes.</p>
</blockquote>

<p>From these details it seems that a seed is nothing special to clients.</p>

<p><strong>Clients</strong></p>

<p>From the <a href=""http://www.datastax.com/docs/1.0/cluster_architecture/about_client_requests"" rel=""noreferrer"">DataStax documentation</a> on client requests:</p>

<blockquote>
  <p>All nodes in Cassandra are peers. A client read or write request can
  go to any node in the cluster. When a client connects to a node and
  issues a read or write request, that node serves as the coordinator
  for that particular client operation.</p>
  
  <p>The job of the coordinator is to act as a proxy between the client
  application and the nodes (or replicas) that own the data being
  requested. The coordinator determines which nodes in the ring should
  get the request based on the cluster configured partitioner and
  replica placement strategy.</p>
</blockquote>

<p>I gather that the pool of nodes that a client connects to can just be a handful of (random?) nodes in the DC to allow for potential failures.</p>
",['partitioner']
11014014,11019631,2012-06-13 11:34:47,get column position,"<p>In CassandraDB, using an ordered column family. I know you can get slices, but can you get the position. For example, in this datamodel I save scores like this:</p>

<pre><code>""Scores"":
{
   ""1000"": ""bob, lucas"",
   ""900"": ""tim""
   ""800"": ""mario""
}
</code></pre>

<p>Is it possible, knowing that the user has a score of ""900"" and his nick is ""tim"", to know that he is at position 2 of the ordered column family?</p>
",<nosql><cassandra>,"<p>Cassandra does not provide this functionality out of the box, but you could implement this yourself using three separate CFs.  Consider this scenario:</p>

<pre><code>""Scores"":
{
   ""1000"": ""bob, lucas""
   ""900"": ""tim""
   ""800"": ""mario""
}

""PlayerScores"":
{
   ""bob"": ""1000""
   ""lucas"": ""1000""
   ""tim"": ""900""
   ""mario"": ""800""
}

""ScoreTotals"":
{
   ""1000"": 
   {
      ""1000"":2
   }
   ""900"": 
   {
      ""900"":1
   }
   ""800"": 
   {
      ""800"":1
   }
}
</code></pre>

<p>The ScoreTotals CF would be used with counter columns to increment/decrement the value each time a user achieves that score.  If you have additional granularity in your scores (like 910 vs an even 900) you can consider the keys as buckets with column names as specific scores. PlayerScores obviously exists so you can query a score for a player.</p>

<p>You can then determine ranking by simply summing the totals of all scores greater than that of the player.  Since scores are stored in column names, you can use a standard slice query to get your range without needing to use an order-preserving partitioner (which has some negative side effects).</p>
",['partitioner']
11036928,11087803,2012-06-14 15:59:21,Cassandra CQL: How to select encoded value from column,"<p>I have inserted string and integer values into dynamic columns in a Cassandra Column Family. When I query for the values in CQL they are displayed as hex encoded bits.</p>

<p>Can I somehow tell the query to decode the value into a string or integer?</p>

<p>I also would be happy to do this in the CLI if that's easier. There I see you can specify <code>assume &lt;column_family&gt; validator as &lt;type&gt;;</code>, but that applies to all columns and they have different types, so I have to run the assumption and query many times.</p>

<p>(Note that the columns are dynamic, so I haven't specified the validator when creating the column family).</p>
",<cassandra>,"<p>You can use <code>ASSUME</code> in cqlsh like in cassandra-cli (although it only applies to printing values, not sending them, but that ought to be ok for you). You can also use it on a per-column basis, like:</p>

<pre><code>ASSUME &lt;column_family&gt; ('anchor:cnnsi.com') VALUES ARE text;
</code></pre>

<p>..although (a), I just tested it, and this functionality is broken in cassandra-1.1.1 and later. I posted a fix at <a href=""https://issues.apache.org/jira/browse/CASSANDRA-4352"" rel=""nofollow"">CASSANDRA-4352</a>. And (b), this probably isn't a very versatile or helpful solution for more than a few one-off uses. I'd strongly recommend using CQL 3 here, as CQL direct support for wide storage engine rows like this is deprecated. Your table here is certainly adaptable to an (easier to use) CQL 3 model, but I couldn't say exactly what it would be without knowing more about how you're using it.</p>
",['table']
11133770,11143578,2012-06-21 07:51:06,strange result when use Where filter in CQL cassandra,"<p>i have a column family use counter as create table command below: (KEY i use bigin to filter when query ).</p>

<pre><code>CREATE TABLE BannerCount (
KEY bigint PRIMARY KEY
) WITH
comment='' AND
comparator=text AND
read_repair_chance=0.100000 AND
gc_grace_seconds=864000 AND
default_validation=counter AND
min_compaction_threshold=4 AND
max_compaction_threshold=32 AND
replicate_on_write='true' AND
compaction_strategy_class='SizeTieredCompactionStrategy' AND
compression_parameters:sstable_compression='SnappyCompressor';
</code></pre>

<p>But when i insert data to this column family , and select using <code>Where</code> command to filter data
results i retrived very strange :( like that:</p>

<p>use Query: </p>

<pre><code>select count(1) From BannerCount where KEY &gt; -1

count
-------
71
</code></pre>

<p>use Query:</p>

<pre><code>select count(1) From BannerCount where KEY &gt; 0;
count
-------
3
</code></pre>

<p>use Query: </p>

<pre><code>select count(1) From BannerCount ;
count
-------
122
</code></pre>

<p>What happen with my query , who any tell me why i get that :( :(</p>
",<nosql><cassandra><cql><nosql-aggregation>,"<p>To understand the reason for this, you should understand Cassandra's data model. You're probably using <code>RandomPartitioner</code> here, so each of these KEY values in your table are being hashed to token values, so they get stored in a distributed way around your ring.</p>

<p>So finding all rows whose key has a higher value than X isn't the sort of query Cassandra is optimized for. You should probably be keying your rows on some other value, and then using either wide rows for your bigint values (since columns are sorted) or put them in a second column, and create an index on it.</p>

<p>To explain in a little more detail why your results seem strange: CQL 2 implicitly turns ""<code>KEY &gt;= X</code>"" into ""<code>token(KEY) &gt;= token(X)</code>"", so that a querier can iterate through all the rows in a somewhat-efficient way. So really, you're finding all the rows whose <em>hash</em> is greater than the <em>hash</em> of X. See <a href=""https://issues.apache.org/jira/browse/CASSANDRA-3771"" rel=""nofollow"">CASSANDRA-3771</a> for how that confusion is being resolved in CQL 3. That said, the proper fix for you is to structure your data according to the queries you expect to be running on it.</p>
",['table']
11243825,11247271,2012-06-28 11:40:29,Create two composite columns with cassandra-cli,"<p>My column family needs two composite columns, the key data type is BytesType.</p>

<p>Here is the definition of table using CQL:</p>

<pre><code>CREATE TABLE stats (
      gid          blob,
      period       int,
      tid          blob, 
      sum          int,
      uniques      blob,
      PRIMARY KEY(gid, period, tid)
     );
</code></pre>

<p>What I want to do is to create the column family but with Cassandra CLI.  Here is my shot.  </p>

<p>The structure of the first composite is: </p>

<pre><code>CompositeType(Int32Type, BytesType, AsciiType)
</code></pre>

<p>and it will holds an integer.</p>

<p>The structure of the second composite is:   </p>

<pre><code>CompositeType(Int32Type, BytesType)
</code></pre>

<p>and will holds BytesType.  </p>

<pre><code>create column family stats with comparator = 'CompositeType(Int32Type, BytesType, AsciiType)';
</code></pre>

<p>I'm not sure how to define the second composite column in create column family command.</p>

<p>Of course I'm assuming that the table created with CQL will generate two composite columns.</p>
",<nosql><cassandra>,"<p>You can only have one comparator on a column family in cassandra. This means you can also only have one type of composite column in column family. The table created by the CQL statement you used would actually use the first composite type comparator that you mention:</p>

<pre><code>CompositeType(Int32Type, BytesType, AsciiType)
</code></pre>

<p>That comparator can describe all of your schema because of the 'AsciiType' component at the end of your composite. This component of your column names will contain the literal string 'sum' or 'uniques', and the column value will match the type accordingly.</p>

<p>An example using a json style notation:</p>

<pre><code>&lt;bytes&gt; : {                               # a row key in your stats column family
    (100, &lt;bytes&gt;, ""sum"") : 100,          # one column in your row
    (100, &lt;bytes&gt;, ""uniques"") : &lt;bytes&gt;,  
    (200, &lt;bytes&gt;, ""sum"") : 200,
    (200, &lt;bytes&gt;, ""uniques"") : &lt;bytes&gt;
}
&lt;bytes&gt; : {                               # another row in the cf
    ...
}
</code></pre>
",['table']
11659001,11659796,2012-07-25 21:35:50,Cassandra-cli cant connect to remote cassandra server,"<p>I have a cassandra server running on a server(serv1). cassandra-cli can connect to it when run on serv1. However, when i try to connect to it through some other server(serv2), i get the following exception:</p>

<pre><code>org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused
    at org.apache.thrift.transport.TSocket.open(TSocket.java:183)
    at org.apache.thrift.transport.TFramedTransport.open(TFramedTransport.java:81)
    at org.apache.cassandra.cli.CliMain.connect(CliMain.java:80)
    at org.apache.cassandra.cli.CliMain.main(CliMain.java:256)
Caused by: java.net.ConnectException: Connection refused
    at java.net.PlainSocketImpl.socketConnect(Native Method)
    at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:351)
    at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:213)
    at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:200)
    at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
    at java.net.Socket.connect(Socket.java:529)
    at org.apache.thrift.transport.TSocket.open(TSocket.java:178)
    ... 3 more
Exception connecting to jckstore/9160. Reason: Connection refused.
</code></pre>

<p>I looked in cassandra.yaml and found that the property ""listen_address"" is configured to ""localhost"" and using 0.0.0.0 is severely discouraged. I tried to change localhost to serv2, ip address of serv1 but nothing worked. Even commenting out didnt help. </p>

<p>Is there a way i can make my cassandra server listen on all the ip's without using 0.0.0.0</p>
",<java><nosql><cassandra><thrift><thrift-protocol>,"<p>I was able to solve the problem as following:</p>

<ol>
<li>changing the rpc_address property in cassandra.yaml to 0.0.0.0 instead of localhost. </li>
<li>set the broadcast_rpc_address property in cassandra.yaml to a value other than 0.0.0.0</li>
</ol>

<p>Then I can access.</p>
","['broadcast_rpc_address', 'rpc_address']"
11707073,11716462,2012-07-29 06:03:24,how to efficiently manage cassandra initial token?,"<p>I'm new cassandra user. I know that there is initial token configuration and how to generate it.
The question is if I have an existen cluster with x nodes and I want to add additional node (one or more) should I reconfigure all the nodes to the new tokens (according to new  generated values)?  </p>

<p>Or is there more efficient way to manage this?</p>
",<cassandra>,"<p>If you're looking for what the best practices are for handling such tasks, take a look at this section of the Cassandra 1.0 docs dedicated to <a href=""http://www.datastax.com/docs/1.0/operations/cluster_management#calculating-tokens-for-the-new-nodes"">token strategy</a>.</p>

<p>Shortened version of your options, from the documentation:</p>

<blockquote>
  <ul>
  <li><strong>Add capacity by doubling the cluster size</strong> -- <em>[..]</em> nodes can keep their existing token assignments, and new nodes are assigned tokens that bisect (or trisect) the existing token ranges.</li>
  <li><strong>Recalculate new tokens for all nodes and move nodes</strong> -- <em>[..]</em> you will have to recalculate tokens for the entire cluster. Existing nodes will have to have their new tokens assigned using nodetool move.</li>
  <li><strong>Add one node at a time and leave initial_token empty</strong> -- <em>[..]</em> splits the token range of the heaviest loaded node and places the new node into the ring at that position. <em>[..]</em> not result in a perfectly balanced ring, but it will alleviate hot spots.
  link</li>
  </ul>
</blockquote>

<p>If you were seeking a management solution <a href=""https://github.com/netflix/priam"">Priam (from Netflix)</a> might be worth looking at. It's open source and Apache-licensed, but requires some amount of configuration and is probably only worth investing [time] in for larger clusters.</p>
",['initial_token']
11728611,11733167,2012-07-30 19:53:41,select compositetype keys in cassandra,"<p>So I've defined a column family that uses composite ids for the row keys. So say the composite key is <code>CompositeType(LongType,LongType)</code>. So I've tested storing items with this type and that works fine and <code>SELECT</code> works as expected too when I know the full key. But lets say I want all keys that have 0 as the first element and anything as the second. So far the only way that I can see to perform this query is as follows:</p>

<p>if I was all keys that are 0:* then I would do a CQL query for <code>key &gt;= 0:0 AND key &lt; 1:0</code> which works as long as there is an order preserving partitioner.</p>

<p>My questions are:</p>

<p>1) is this odd syntax only because I'm using a CQL driver (only option for nodejs aside from thrift)</p>

<p>2) is there any inefficiency with this type of query? essentially i'm using a composite key instead of super columns since those aren't supported in CQL. I have no problem dealing with this logic in the code as long as there is no limitations to using it like this. </p>
",<node.js><cassandra><cql><composite-types>,"<p>I would suggest you change your data model. Use RandomPartitioner and just have the first component as the row key. Push the second component into the column names, that is make your column names composites instead. </p>

<p>Since column names are always sorted, you can do easy slicing operations. For example, </p>

<p>a) When you know both the components, do a get slice on the row key(first component) and first component of the composite.</p>

<p>b) When you know just the first component, fetch the complete row for the row key(first component)</p>

<p>This is the approach CQL3 takes when you ask it to create a table with multiple primary keys. </p>
",['table']
11825783,11828376,2012-08-06 09:38:56,Cassandra and querying hierarchies,"<p>I am investigating Cassandra, but cannot find an answer in any documentation to the following.</p>

<p>I need to query ranges across a deep hierarchy. I have determined that the easiest way to represent the hierarchy is to have each level as a column. For example:</p>

<pre><code>Origin           Manufacturer    Price    ID
Europe.Germany   VW Group.Audi   20000    1
Europe.Germany   VW Group.Porshe 21000    2
Europe.Germany   BMW             19000    3
</code></pre>

<p>Here is a pseudo SQL example:</p>

<pre><code>SELECT ID FROM CompositeTable WHERE (Origin STARTS WITH 'Europe')
AND (Manufacturer STARTS WITH 'VW Group' AND IS NOT 'VW Group.Porshe' OR IS 'BMW')
AND (Price BETWEEN 18000 AND 22000)
</code></pre>

<p>Result:</p>

<pre><code>ID = [1, 3]
</code></pre>

<p>Can Cassandra perform this type of search across a composite index?</p>
",<nosql><cassandra><hierarchical-data><composite-key>,"<p>Composite Keys in Cassandra is a kind of <code>multiple-column index</code> in DBMS, where if you have index on c1, c2 and c3 in a table with columns from c1 to c6. DB will always try to use you index if the query is for <code>=, &gt; , &lt;, &gt;=, &lt;=</code> operation on (c1), (c1,c2) or (c1, c2, c3) but not for (c2), (c2, c3), (c3) or (c1, c3). The case is the same in cassandra but here you have multiple-column index on c1 to c6 [since the columns are sorted first based on c1 and the collision carries over to c2 and follows on]</p>
",['table']
11826143,11869585,2012-08-06 10:02:51,Hector - Insert row with composite key,"<p>Hi I want to insert into this kind of column family row with composite key:</p>

<pre><code>CREATE TABLE my_items (
user_id uuid,
item_id uuid,
description varchar,
 PRIMARY KEY (user_id, item_id));
</code></pre>

<p>So I try this:</p>

<pre><code>StringSerializer stringSerializer = StringSerializer.get();
    UUIDSerializer uuidSerializer = UUIDSerializer.get();
    CompositeSerializer compositeSerializer = CompositeSerializer.get();

    HColumn&lt;String, UUID&gt; hColumnObj_userID = HFactory.createColumn(""user_id"", userID, stringSerializer, uuidSerializer);
    HColumn&lt;String, UUID&gt; hColumnObj_itemID= HFactory.createColumn(""item_id"", itemID, stringSerializer, uuidSerializer);

    Mutator&lt;Composite&gt; mutator = HFactory.createMutator(
            repository.getKeyspace(),
            compositeSerializer);
    Composite colKey = new Composite();
    colKey.addComponent(userID, uuidSerializer);
    colKey.addComponent(itemID, uuidSerializer);

    mutator.addInsertion(colKey,
            ""my_items"", hColumnObj_userID);
    mutator.addInsertion(colKey,
            ""my_items"", hColumnObj_itemID);

    mutator.execute();
</code></pre>

<p>What's wrong with code above? I keep getting this error: ""InvalidRequestException(why:UUIDs must be exactly 16 bytes)"". And how can I insert data into column family that I describe above.</p>

<p>Cheers</p>
",<java><cassandra><hector>,"<p>It looks like Hector was expecting a Composite containing a UUID and a String and found only a string.</p>

<p>Before writing the Hector code you have to translate the create DDL into the actual storage pattern CQL uses.  In this case, even though you have two primary keys, only the first, user_id, is used as the row key.  That's always the case.  Any other primary keys (item_id in this case) are used to form composite column names for every column except the first primary key.  That means that when using Hector for your my_items column family you'll have to write two columns, one for item_ID and one for description.</p>

<p>The column name for the item_id value is a composite consisting of the values of primary keys 2...n  (item_id in this example) and a constant string name of the value (""item_id"").</p>

<p>The column name for the description value is also a composite of the item_id value and the name of the value (""description"").</p>

<p>If you wrote 3 CQL table rows, each with the same user_id but having different item_id values then you'd end up with a single column family row whose row key is the common user_id value and which has 6 columns, an item_id column and a description column for each of the 3 CQL table rows.</p>

<p>The code should look like this:</p>

<pre><code>import java.util.UUID;

import me.prettyprint.cassandra.serializers.CompositeSerializer;
import me.prettyprint.cassandra.serializers.IntegerSerializer;
import me.prettyprint.cassandra.serializers.StringSerializer;
import me.prettyprint.cassandra.serializers.UUIDSerializer;
import me.prettyprint.hector.api.Keyspace;
import me.prettyprint.hector.api.beans.Composite;
import me.prettyprint.hector.api.beans.HColumn;
import me.prettyprint.hector.api.beans.AbstractComposite.ComponentEquality;
import me.prettyprint.hector.api.factory.HFactory;
import me.prettyprint.hector.api.mutation.Mutator;

    // put this here to make it compile cleanly

    Keyspace keyspace = null;
    UUID userID = null;
    UUID itemID = null;
    String description = null;

            // Row key is user_id of type UUID

    Mutator&lt;UUID&gt; mutator = HFactory.createMutator(
            keyspace,
            UUIDSerializer.get());

        // write column for itemID.  
        // Column name is composite of itemID value and constant ""item_id""
        // Row key is value of userID

    Composite itemIdColumnName = new Composite();
    itemIdColumnName.addComponent(itemID    , UUIDSerializer.get());
    itemIdColumnName.addComponent(""item_id"" , StringSerializer.get());
        // HFactory.createColumn takes args: column name, column value, serializer for column name, serializer for column value
    HColumn&lt;Composite, UUID&gt; hColumnObj_itemID = HFactory.createColumn(itemIdColumnName, userID, new CompositeSerializer(), UUIDSerializer.get());
    mutator.addInsertion(userID, ""my_items"", hColumnObj_itemID);

    // write column for description.  
    // Column name is composite of itemID value and constant ""description""
    // Row key is value of userID

    Composite descriptionColumnName = new Composite();
    itemIdColumnName.addComponent(itemID    , UUIDSerializer.get());
    itemIdColumnName.addComponent(""description"" , StringSerializer.get());
    HColumn&lt;Composite, String&gt; hColumnObj_description = HFactory.createColumn(descriptionColumnName, description , new CompositeSerializer(), StringSerializer.get());
    mutator.addInsertion(userID, ""my_items"", hColumnObj_description);

    mutator.execute();
</code></pre>
",['table']
11832886,11889327,2012-08-06 17:16:07,Cassandra CQL method for paging through all rows,"<p>I want to programmatically examine all the rows in a large cassandra table, and was hoping to use CQL. I know I could do this with thrift, getting 10,000 (or so) rows at a time with multiget and handing the last retrieved key into to the next multiget call. But I have looked through all the documentation on CQL select, and there doesn't seem to be a way to do this. I have resorted to setting the select limit higher and higher, and setting the timeout higher and higher to match it.</p>

<p>Is there an undocumented way to hand in a starting point to CQL select, or do I just need to break down and rewrite my code using the thrift API? </p>
",<cassandra><thrift><cql>,"<p>Turns out greater than and less than have a very non-intuitive, but useful, behavior (at least in CQL2, I haven't check CQL3 yet). It actually compares the tokens not the key values. Here is an example:</p>

<pre><code>&gt; create table users (KEY varchar PRIMARY KEY, data varchar);
&gt; insert into users (KEY, 'data') values ('1', 'one');
&gt; insert into users (KEY, 'data') values ('2', 'two');
&gt; insert into users (KEY, 'data') values ('3', 'three');
&gt; insert into users (KEY, 'data') values ('4', 'four');
&gt; select * from users;
   3 | three
   2 |   two
   1 |   one
   4 |  four
&gt; select * from users LIMIT 1;
   3 | three
&gt; select * from users WHERE KEY &gt; '3' LIMIT 1;
   2 |  two
&gt; select * from users WHERE KEY &gt; '2' LIMIT 1;
   1 |  one
&gt; select * from users WHERE KEY &gt; '1' LIMIT 1;
   4 | four
</code></pre>
",['table']
12029827,12030029,2012-08-19 20:41:14,How to get groups of rows in MySQL and Cassandra,"<p>So I have a table that is currently in mysql, but will be transferred to a nosql system soon.  So I took out the normalization of the tables, and now there are duplicates of the data, but one of the ids changes in each row, while the rest of the data is constant.  All rows are connected through ID A.  ID B changes for each row, and the user ID is the same for all of the rows in ID A.</p>

<p>Now I need to grab 2 groups of rows using the user ID.  The number of ID B's is variable for every group of A though, so it could have variables number of rows all grouped together by each ID A.  So far I have just been displaying one group at a time so I have been selecting based on ID A, now I need to try and grab 2 sets by the user ID...</p>

<p>I can't seem to find a way to do this...although I don't know everything about sql.  How can I do this now on mysql?  and then on nosql when i move to the system in a bit?  Will be happy to answer any further questions.</p>
",<mysql><sql><database><nosql><cassandra>,"<p>I think you're saying that the rows have a composite key made up of two columns, id's A and B. On the assumption that I got that right here's how you'd do it in Cassandra (and there are two
approaches).</p>

<p>You could use CQL and declare your table to have two primary keys, A and B, in that order, along with any other columns in your original MySql table. </p>

<p>You could also create a column family whose row key is id A and which will have a column for every unique id B for that id A.  The name of the column will be the value of id B and the value of that column will be the value (or serialized values) of the remaining MySQL row values. Note that id B doesn't have to be a String value. For any given value of id A, this will result in a Cassandra column family row with as many columns as there unique id B values for that id A value. This is called the ""Dynamic Column Family Pattern"".</p>

<p>If you take the first approach, you basically end up doing the second approach under the covers (oversimplification alert).</p>
",['table']
12338616,12345898,2012-09-09 11:16:57,Creating composite COLUMNS (not keys) with CQL 3,"<p><a href=""https://issues.apache.org/jira/browse/CASSANDRA-2474"" rel=""nofollow"">This article</a> discusses possible ways CQL 3 could be used for creating composite columns in Cassandra 1.1. They are just ideas. Nothing is official, and the Datastax documentation doesn't cover this (only composite keys).</p>

<p>As I understand it, composite columns are a number of columns that together have only one value.</p>

<p>How do you create them with CQL?</p>

<p><strong>EDIT</strong></p>

<p>I will be using C# to interface into Cassandra. CQL looks straightforward to use, which is why I want to use it.</p>
",<c#><cassandra><composite><cql>,"<p>You've got a couple concepts confused, I think. Quite possibly this is the fault of the Datastax documentation; if you have any good suggestions for making it clearer after you have a better picture, I'll be glad to send them on.</p>

<p>The ""composite keys"" stuff in the Datastax docs <em>is</em> actually talking about composite Cassandra columns. The reason for the confusion is that rows in CQL 3 do not map directly to storage engine rows (what you work with when you use the thrift interface). ""Composite key"" in the context of a CQL table just means a primary key which consists of multiple columns, which <em>is implemented by</em> composite columns at the storage layer.</p>

<p><a href=""http://www.datastax.com/dev/blog/schema-in-cassandra-1-1"" rel=""nofollow noreferrer"">This article</a> is one of the better explanations as to how the mapping happens and why the CQL model is generally easier to think about.</p>

<p>With this sort of use, the first CQL column becomes being the storage engine partition key.</p>

<p>As of Cassandra 1.2 (in development), it's also possible to create composite storage engine keys using CQL, by putting extra parentheses in the PRIMARY KEY definition around the CQL columns that will be stored in the partition key (see <a href=""https://issues.apache.org/jira/browse/CASSANDRA-4179"" rel=""nofollow noreferrer"">CASSANDRA-4179</a>), but that's probably going to be the exception, not the rule.</p>
",['table']
12583996,12586258,2012-09-25 13:28:53,DataStax Cassandra File System - Fixed Width Text File - Hive Integration Issue,"<p>I'm trying to read a fixed width text file stored in Cassandra File System (CFS) using Hive. I'm able to query the file when I run from hive client. However, when I try to run from Hadoop Hive JDBC, It says table is not available or bad connection. Below are the steps I followed.</p>

<p><strong>Input file (employees.dat):</strong></p>

<pre><code>21736Ambalavanar              Thirugnanam              BOY-EAG       2005-05-091992-11-18
21737Anand                    Jeyamani                 BOY-AST       2005-05-091985-02-12
31123Muthukumar               Rajendran                BOY-EES       2009-08-121983-02-23
</code></pre>

<p><strong>Starting Hive Client</strong></p>

<pre><code>bash-3.2# dse hive;
Logging initialized using configuration in file:/etc/dse/hive/hive-log4j.properties
Hive history file=/tmp/root/hive_job_log_root_201209250900_157600446.txt
hive&gt; use HiveDB;
OK
Time taken: 1.149 seconds
</code></pre>

<p><strong>Creating Hive External Table pointing to fixed width format text file</strong></p>

<pre><code>hive&gt; CREATE EXTERNAL TABLE employees (empid STRING, firstname STRING, lastname STRING, dept STRING, dateofjoining STRING, dateofbirth STRING)
    &gt; ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.RegexSerDe'
    &gt; WITH SERDEPROPERTIES (""input.regex"" = ""(.{5})(.{25})(.{25})(.{15})(.{10})(.{10}).*"" )
    &gt; LOCATION 'cfs://hostname:9160/folder/';
OK
Time taken: 0.524 seconds
</code></pre>

<p><strong>Do a select * from table.</strong></p>

<pre><code>hive&gt; select * from employees;
OK
21736    Ambalavanar                     Thirugnanam                     BOY-EAG        2005-05-09      1992-11-18
21737    Anand                           Jeyamani                        BOY-AST        2005-05-09      1985-02-12
31123    Muthukumar                      Rajendran                       BOY-EES        2009-08-12      1983-02-23
Time taken: 0.698 seconds
</code></pre>

<p><strong>Do a select with specific fields from hive table throws permission error (first issue)</strong></p>

<pre><code>hive&gt; select empid, firstname from employees;
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
java.io.IOException: The ownership/permissions on the staging directory cfs:/tmp/hadoop-root/mapred/staging/root/.staging is not as expected. It is owned by root and permissions are rwxrwxrwx. The directory must be owned by the submitter root or by root and permissions must be rwx------
        at org.apache.hadoop.mapreduce.JobSubmissionFiles.getStagingDir(JobSubmissionFiles.java:108)
        at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:856)
        at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:850)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:416)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1093)
        at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:850)
        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:824)
        at org.apache.hadoop.hive.ql.exec.ExecDriver.execute(ExecDriver.java:452)
        at org.apache.hadoop.hive.ql.exec.MapRedTask.execute(MapRedTask.java:136)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:133)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:57)
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1332)
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1123)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:931)
        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:255)
        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:212)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403)
        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:671)
        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:554)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)
Job Submission failed with exception 'java.io.IOException(The ownership/permissions on the staging directory cfs:/tmp/hadoop-root/mapred/staging/root/.staging is not as expected. It is owned by root and permissions are rwxrwxrwx. The directory must be owned by the submitter root or by root and permissions must be rwx------)'
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MapRedTask
</code></pre>

<p>The second issue is, when I try to run the select * query from JDBC Hive driver (outside of dse/cassandra nodes), It says the table employees is not available. The external table created acts like a temporary table and it does not get persisted. When I use 'hive> show tables;', the employees table is not listed. Can anyone please help me figure out the problem?</p>
",<cassandra><hive>,"<p>I don't have an immediate answer for the first issue, but the second looks like its due to a known issue.</p>

<p>There is a bug in DSE 2.1 which drops external tables created from CFS files from the metastore when show tables is run. Only the table metadata is removed, the data remains in CFS so if you recreate the table definition you shouldn't have to reload it. Tables backed by Cassandra ColumnFamilies are notaffected by this bug. This has been fixed in the 2.2 release of DSE, which is due for release imminently.</p>

<p>I'm not familiar with the Hive JDBC driver, but if it issues a Show Tables command at any point, it could be triggering this bug. </p>
",['table']
12591660,12603885,2012-09-25 21:55:08,Cassandra Schema Design,"<p>I'm continuing exploring Cassandra and I would like to create Student &lt;=> Course relation which is similar to Many-to-Many on RDBMS. </p>

<p>In term of Queries I will use the following query; </p>

<ol>
<li>Retrieve all courses in which student enrolled. </li>
<li>Retrieve all students enrolled in specific course. </li>
</ol>

<p>Let's say that I create to Column Families. one for Course and another for Student.</p>

<pre><code>CREATE COLUMN FAMILY student with comparator = UTF8Type AND key_validation_class=UTF8Type and column_metadata=[ 
{column_name:firstname,validation_class:UTF8Type} 
{column_name:lastname,validation_class:UTF8Type}
{column_name:gender,validation_class:UTF8Type}];


CREATE COLUMN FAMILY course with comparator = UTF8Type AND key_validation_class=UTF8Type and column_metadata=[ 
{column_name:name,validation_class:UTF8Type} 
{column_name:description,validation_class:UTF8Type}
{column_name:lecturer,validation_class:UTF8Type}
{column_name:assistant,validation_class:UTF8Type}];
</code></pre>

<p>Now how should I move on? </p>

<p>Should I create third Column Family with <code>courseID:studentId</code> CompisiteKey? if yes, Can I use Hector to query by only one (left or right) Composite key component?</p>

<p>Please help. </p>

<p><strong>Update:</strong> </p>

<p>Following the suggestion I created the following Schema:</p>

<p>For Student:</p>

<pre><code>CREATE COLUMN FAMILY student with comparator = UTF8Type and key_validation_class=UTF8Type and default_validation_class=UTF8Type;
</code></pre>

<p>and then we will add some data:</p>

<pre><code>set student['student.1']['firstName']='Danny'
set student['student.1']['lastName']='Lesnik'
set student['student.1']['course.1']=''
set student['student.1']['course.2']='';
</code></pre>

<p>Create column Family for Course:</p>

<pre><code>CREATE COLUMN FAMILY course with comparator = UTF8Type and key_validation_class=UTF8Type and default_validation_class=UTF8Type;
</code></pre>

<p>add some data:</p>

<pre><code>set course['course.1']['name'] ='History'
set course['course.1']['description'] ='History Course'
set course['course.1']['name'] ='Algebra'
set course['course.1']['description'] ='Algebra Course'
</code></pre>

<p>and Finally Student In Course:</p>

<pre><code>CREATE COLUMN FAMILY StudentInCourse with comparator = UTF8Type and key_validation_class=UTF8Type and default_validation_class=UTF8Type;
</code></pre>

<p>add data:</p>

<pre><code>set StudentInCourse['studentIncourse.1']['student.1'] =''; 
set StudentInCourse['studentIncourse.2']['student.1'] =''; 
</code></pre>
",<nosql><cassandra><hector>,"<p>I defined a data model below but it is easier to decribe the object model first and then dive into the row model, so from PlayOrm's perspective you would have</p>

<pre><code>public class Student {
  @NoSqlId
  private String id;
  private String firstName;
  private String lastName;
  @ManyToMany
  private List&lt;Course&gt; courses = new ArrayList(); //constructing avoids nullpointers
}

public class Course {
  @NoSqlId
  private String id;
  private String name;
  private String description
  @ManyToOne
  private Lecturer lecturer;
  @ManyToMany
  private CursorToMany students = new CursorToManyImpl();
}
</code></pre>

<p>I could have used List in course but I was concerned I may get OutOfMemory if too many students take a course over years and years and years.  NOW, let's jump to what PlayOrm does and you can do something similar if you like</p>

<p>A single student row would look like so</p>

<pre><code>rowKey(the id in above entity) = firstName='dean',
lastName='hiller' courses.rowkey56=null, courses.78=null, courses.98=null, courses.101=null
</code></pre>

<p>This is the wide row where we have many columns with the name 'fieldname' and 'rowkey to actual course'</p>

<p>The Course row is a bit more interesting....because the user thinks loading al the Students for a single course could cause out of memory, he uses a cursor which only loads 500 at a time as you loop over it.</p>

<p>There are two rows backing the Course in this case that PlayOrm will have.  Sooo, let's take our user row above and he was in course rowkey56 so let's describe that course</p>

<pre><code>rowkey56 = name='coursename', description='somedesc', lecturer='rowkey89ToLecturer'
</code></pre>

<p>Then, there is another row in the some index table for the students(it is a very wide row so supports up to millions of students)</p>

<pre><code>indexrowForrowkey56InCourse = student34.56, student39.56, student.23.56.... 
into the millions of students
</code></pre>

<p>If you want a course to have more than millions of students though, then you need to think about partitioning whether you use playOrm or not.  PlayOrm does partitioning for you if you need though.</p>

<p>NOTE: If you don't know hibernate or JPA, when you load the above Student, it loads a proxy list so if you start looping over the courses, it then goes back to the noSQL store and loads the Courses so you don't have to ;).  </p>

<p>In the case of Course, it loads a proxy Lecturer that is not filled in until you access a property field like lecturer.getName().  If you call lecturer.getId(), it doesn't need to load the lecturer since it already has that from the Course row.</p>

<p>EDIT(more detail): PlayOrm has 3 index tables Decimal(stores double, float, etc and BigDecimal), Integer(long, short, etc and BigInteger and boolean), and String index tables.  When you use CursorToMany, it uses one of those tables depending on the FK type of key.  It also uses those tables for it's Scalable-SQL language.  The reason it uses a separate row on CursorToMany is just so clients don't get OutOfMemory on reading a row in as the toMany could have one million FK's in it in some cases.  CursorToMany then reads in batches from that index row.</p>

<p>later,
Dean</p>
",['table']
12858282,12885448,2012-10-12 11:40:52,Structuring cassandra database,"<p>I don't understand one thing about Cassandra. Say, I have similar website to Facebook, where people can share, like, comment, upload images and so on.</p>

<p>Now, let's say, I want to get all of the things my friends did:</p>

<ul>
<li>Username1 liked you comment</li>
<li>username 2 updated his profile picture</li>
</ul>

<p>And so on.</p>

<p>So after a lot of reading, I guess I would need to do is create new Column Family for each single thing, for example: <code>user_likes</code> <code>user_comments</code>, <code>user_shares</code>. Basically, anything you can think off, and even after I do that, I would still need to create secondary indexes for most of the columns just so I could search for data? And even so how would I know which users are my friends? Would I need to first get all of my friends id's and then search through all of those Column Families for each user id? </p>

<p><strong>EDIT</strong>
Ok so i did some more reading and now i understand things a little bit better, but i still can't really figure out how to structure my tables, so i will set a bounty and i want to get a clear example of how my tables should look like if i want to store and retrieve data in this kind of order:</p>

<ul>
<li>All</li>
<li>Likes</li>
<li>Comments</li>
<li>Favourites</li>
<li>Downloads</li>
<li>Shares</li>
<li>Messages</li>
</ul>

<p>So let's say i want to retrieve ten last uploaded files of all my friends or the people i follow, this is how it would look like:</p>

<p><code>John uploaded song AC/DC - Back in Black 10 mins ago</code></p>

<p>And every thing like comments and shares would be similar to that...</p>

<p>Now probably the biggest challenge would be to retrieve 10 last things of all categories together, so the list would be a mix of all the things... </p>

<p>Now i don't need an answer with a fully detailed tables, i just need some really clear example of how would i structure and retrieve data like i would do in <code>mysql</code> with <code>joins</code></p>
",<nosql><cassandra>,"<p>With sql, you structure your tables to normalize your data, and use indexes and joins to query.  With cassandra, you can't do that, so you structure your tables to serve your queries, which requires denormalization.</p>

<p>You want to query items which your friends uploaded, one way to do this is t have a single table per user, and write to this table whenever a friend of that user uploads something.</p>

<pre><code>friendUploads { #columm family
    userid { #column 
        timestamp-upload-id : null #key : no value
    }
 }
</code></pre>

<p>as an example,</p>

<pre><code>friendUploads {
    userA {
         12313-upload5 : null
         12512-upload6 : null
         13512-upload8 : null
    }
}

friendUploads {
    userB {
         11313-upload3 : null
         12512-upload6 : null
    }
}
</code></pre>

<p>Note that upload 6 is duplicated to two different columns, as whoever did upload6 is a friend of both User A and user B.</p>

<p>Now to query the friends upload display of a friend, do a getSlice with a limit of 10 on the userid column.  This will return you the first 10 items, sorted by key.</p>

<p>To put newest items first,  use a <a href=""http://thelastpickle.com/2011/10/03/Reverse-Comparators/"" rel=""noreferrer"">reverse comparator</a> that sorts larger timestamps before smaller timestamps.</p>

<p>The drawback to this code is that when User A uploads a song, you have to do N writes to update the friendUploads columns, where N is the number of people who are friends of user A.</p>

<p>For the value associated with each timestamp-upload-id key, you can store enough information to display the results (probably in a json blob), or you can store nothing, and fetch the upload information using the uploadid.</p>

<p>To avoid duplicating writes, you can use a structure like, </p>

<pre><code>userUploads { #columm family
    userid { #column 
        timestamp-upload-id : null #key : no value
    }
 }
</code></pre>

<p>This stores the uploads for a particular user.  Now when want to display the uploads of User B's friends, you have to do N queries, one for each friend of User B, and merge the result in your application.  This is slower to query, but faster to write.  </p>

<p>Most likely, if users can have thousands of friends, you would use the first scheme, and do more writes rather than more queries, as you can do the writes in the background after the user uploads, but the queries have to happen while the user is waiting.</p>

<p>As an example of denormalization, look at how many writes twitter rainbird does when a single <a href=""http://news.ycombinator.com/item?id=2180840"" rel=""noreferrer"">click occurs</a>.  Each write is used to support a single query.</p>
",['table']
13110363,13121756,2012-10-28 15:52:30,One to Many equivalent in Cassandra and data model optimization,"<p>I am modeling my database in Cassandra, coming from RDBMS. I want to know how can I create a one-to-many relationship which is embedded in the same Column Name and model my table to fit the following query needs.</p>

<p>For example:</p>

<pre><code>Boxes:{
  23442:{
    belongs_to_user: user1,
    box_title: 'the box title',
    items:{
      1: {
         name: 'itemname1',
         size: 44
      },
      2: {
        name: 'itemname2',
        size: 24
      }
    }
  },

 { ... }
}
</code></pre>

<p>I read that its preferable to use composite columns instead of super columns, so I need an example of the best way to implement this. My queries are like:</p>

<ul>
<li>Get items for box by Id</li>
<li>get top 20 boxes with their items (for displaying a range of boxes with their items on the page)</li>
<li>update items size by item id (increment size by a number)</li>
<li>get all boxes by userid (all boxes that belongs to a specific user)</li>
</ul>

<p>I am expecting lots of writes to change the size of each item in the box. I want to know the best way to implement it without the need to use super columns. Furthermore, I don't mind getting a solution that takes Cassandra 1.2 new features into account, because I will use that in production. </p>

<p>Thanks</p>
",<nosql><cassandra>,"<p>This particular model is somewhat challenging, for a number of reasons.</p>

<p>For example, with the box ID as a row key, querying for a range of boxes will require a range query in Cassandra (as opposed to a column slice), which means the use of an ordered partitioner.  An ordered partitioner is almost always a <em>Bad Idea</em>.</p>

<p>Another challenge comes from the need to increment the item size, as this calls for the use of a counter column family.  Counter column families store counter values <em>only</em>.</p>

<p>Setting aside the need for a range of box IDs for a moment, you could model this using multiple tables in CQL3 as follows:</p>

<pre><code>CREATE TABLE boxes (                                                                       
   id int PRIMARY KEY,                                                                 
   belongs_to_user text,                                                               
   box_title text,                                                                     
);
CREATE INDEX useridx on boxes (belongs_to_user);

CREATE TABLE box_items (                                                                   
   id int,                                                                             
   item int,                                                                           
   size counter,                                                                       
   PRIMARY KEY(id, item)                                                               
);

CREATE TABLE box_item_names (
    id int PRIMARY KEY,
    item int,
    name text
);

BEGIN BATCH
  INSERT INTO boxes (id, belongs_to_user, box_title) VALUES (23442, 'user1', 'the box title');
  INSERT INTO box_items (id, item, name) VALUES (23442, 1, 'itemname1');
  INSERT INTO box_items (id, item, name) VALUES (23442, 1, 'itemname2');
  UPDATE box_items SET size = size + 44 WHERE id = 23442 AND item = 1;                       
  UPDATE box_items SET size = size + 24 WHERE id = 23442 AND item = 2;
APPLY BATCH

-- Get items for box by ID                                                               
SELECT size FROM box_items WHERE id = 23442 AND item = 1;

-- Boxes by user ID
SELECT * FROM boxes WHERE belongs_to_user = 'user1';
</code></pre>

<p>It's important to note that the BATCH mutation above is both atomic, and isolated.</p>

<p>Technically speaking, you could also denormalize all of this into a single table.  For example:</p>

<pre><code>CREATE TABLE boxes (
   id int,
   belongs_to_user text,
   box_title text,
   item int,
   name text,
   size counter,
   PRIMARY KEY(id, item, belongs_to_user, box_title, name)
);

UPDATE boxes set size = item_size + 44 WHERE id = 23442 AND belongs_to_user = 'user1'
    AND box_title = 'the box title' AND name = 'itemname1' AND item = 1;

SELECT item, name, size FROM boxes WHERE id = 23442;
</code></pre>

<p><em>However, this provides no guarantees of correctness</em>.  For example, this model makes it possible for items of the same box to have different users, or titles.  And, since this makes <code>boxes</code> a counter column family, it limits how you can evolve the schema in the future.</p>
",['partitioner']
13219270,13270710,2012-11-04 14:09:40,Dynamically adding new nodes in Cassandra,"<p>Is it possible to add new hosts to a Cassandra cluster dynamically?</p>

<p>What I'm trying to do is set up a program that can:</p>

<ul>
<li>Set up a local version of the database for each user</li>
<li>Each user's machine will become part of the cluster (the machines will be hosts)</li>
<li>Data will be replicated across all the clusters</li>
</ul>

<p>Building a cluster of multiple hosts usually entails <a href=""http://www.datastax.com/docs/0.7/getting_started/configuring"" rel=""noreferrer"">configuring the cassandra.yaml</a> to store the seeds, listen_address and rpc_address of each host.</p>

<p>My idea is to edit these files through java and insert the new host addresses as required but making sure that data is accurate across each users's cassandra.yaml files would be challenging. </p>

<p>I'm wondering if someone has done something similar or has any advice on a better way to achieve this.</p>
",<cassandra>,"<p>Yes is possible. Look at <a href=""https://github.com/Netflix/Priam/wiki"" rel=""nofollow"">Netflix's Priam</a> for an complete example of a dynamic cassandra cluster management (but designed to work with Amazon EC2).</p>

<p>For rpc_address and listen_address, you can setup a startup script that configures the cassandra.yaml if it's not ok.</p>

<p>For seeds you can configure a custom seed provider. Look at the <a href=""https://github.com/Netflix/Priam/blob/master/priam-cass-extensions/src/main/java/com/netflix/priam/cassandra/extensions/NFSeedProvider.java"" rel=""nofollow"">seed provider used for Netflix's Priam</a> for some ideas how to implement it</p>

<p>The most difficult part will be managing the tokens assigned to each node in a efficient way. Cassandra 1.2 is around the corner and will include a feature called virtual nodes that, IMO, will work well in your case. See the <a href=""http://www.youtube.com/watch?v=GddZ3pXiDys"" rel=""nofollow"">Acunu presentation about it</a></p>
",['rpc_address']
13838990,13842356,2012-12-12 11:41:59,Implementing FIFO read in Cassandra,"<p>Given a Cassandra database, is there a mechanism for fetching records in a FIFO manner such that records can be read in the ascending order of their insertion time. I basically need to read N oldest rows in batches, process them and delete the batch once it is processed. </p>

<p>As far as my understanding goes, Columns are sorted by their type (as specified by CompareWith), and rows are sorted by their partitioner. </p>

<p>Can I use OrderPreservingPartitioner to sort my rows in the ascending order of insertion time? I am running Cassandra on a single node so I am not really worried about the distribution of keys. If OrderPreservingPartitioner can be used, how do I configure the sort criteria for my keys so that the records are maintained in the ascending order of insertion?</p>

<p>Alternately, does Hector provide a mechanism to always fetch rows such that the oldest rows are fetched first?</p>

<p><strong>Edit :</strong> </p>

<p>After reading rs_atl's post, I have some more doubts : </p>

<ol>
<li><p>If I have understood this correctly, I will create a column family with TimeUUIDType as the comparator. I will then have to use timestamps for column names. The immediate question that comes to my mind is how do I define the sort order for the column names as ascending or descending? Can I do this at column family creation time or I have to do this through the client API?</p></li>
<li><p>If I decide to use 'hours' as my shard interval i.e, if I append hours to my keys, how do I retrieve the row for the oldest hour? </p></li>
</ol>
",<java><sorting><cassandra><hector>,"<p>There are a number of things to consider when attempting such a solution with Cassandra:</p>

<ol>
<li>Always use RandomPartitioner, because you'll get hot spots if you don't.</li>
<li>Your keys should be buckets of time (like days or hours), so you can know them in advance for a given time period.</li>
<li>Your column names should be timestamps that sort in time order (either lexicographically or numerically).  This will allow you to query for ranges.</li>
<li>Make sure to use at least QUORUM (or LOCAL_QUORUM) reads and writes, so you don't end up with consistency issues.</li>
<li>You'll need to find a way in your app to make sure you don't process the same data more than once, because someone else could pick up the record between the time that you read it for processing and then delete it (i.e., it's not a like a queue).</li>
</ol>

<p>Hector doesn't determine ordering at all; this happens on insert and is based on the comparator you've chosen.  If you want a specific ordering you have to write the data that way (see point 3 above).</p>

<p>Regarding the additional information in your edit:</p>

<ol>
<li><p>I wouldn't use TimeUUIDType as your comparator, just a long value that's either the Unix epoch or a numeric representation of time in the form of YYYYMMDDxx to the level of precision you need.  You can decide at query time whether you want the values in normal (ascending) or reversed (descending) order.</p></li>
<li><p>You can ask for all keys and simply take the smallest one, which could work fine or be a terrible idea depending on how many you have and your latency requirements.  Alternatively (and certainly more efficient), you could actually write the oldest key somewhere (a file, another CF, in memory, whatever makes sense).</p></li>
</ol>
",['precision']
13882313,13883307,2012-12-14 16:09:30,Cassandra - Understanding Rack Concept on PropertyFileSnitch example,"<p>I am working on multi DC deployment and one thing is not clear to me - this is the rack concept interpretation from Cassandra perspective.</p>

<p>I can enforce replication order by defining proper key ranges. Why do I need to specify racks additionally in <code>cassandra-topology.properties</code> ? </p>

<p>Lets take as example Cassandra documentation: <a href=""http://www.datastax.com/docs/1.1/cluster_architecture/replication"" rel=""nofollow"">http://www.datastax.com/docs/1.1/cluster_architecture/replication</a></p>

<p>If I have replication factor 3, and my row key is stored on Node 1, than replicas will be stored on Node 2 and 3 - this is obvious when we look on ring structure, so... why do I need to duplicate this information in rack configuration?</p>
",<cassandra>,"<p>The rack configuration allows cassandra to optimize replica placement so you have better fault tolerance properties.  If you have all your replicas in rack 1, and that rack goes down, you'll lose the data.  If you tell Cassandra about your rack configuration it will keep replicas on different racks.</p>
",['rack']
14105992,14106468,2012-12-31 20:54:40,Hadoop and Cassandra processing rows in sorted order,"<p>I want to fill a Cassandra database with a list of strings that I then process using Hadoop. What I want to do it run through all the strings in order using a Hadoop cluster and record how much overlap there is between each string in order to find the Longest Common Substring. </p>

<p>My question is, will the InputFormat object allow me to read out the data in a sorted order or will my strings be read out ""randomly"" (according to how Cassandra decides to distribute them) throughout every machine in the cluster? Is the MapReduce process designed to process each row by itself w/out the intent of looking at two rows consecutively like I'm asking for?</p>
",<hadoop><cassandra>,"<p>First of all, the Mappers will read the data in whatever order they get it from the InputFormat. I'm not a Cassandra expert, but I don't expect that will be in sorted order.</p>

<p>If you want sorted order, you should use an identity mapper (one that does nothing) whose output key is the string itself. Then they will be sorted before passed to the reduce step. But it gets a little more complicated since you can have more than one reducer. With only one reducer, everything is globally sorted. With more than one, each reducer's input is sorted, but the input across reducers might not be sorted. That is, adjacent strings might not go to the same reducer. You would need a custom partitioner to handle that.</p>

<p>Lastly, you mentioned that you're doing longest common substring- are you looking for the longest substring among each pair of strings? Among consecutive pairs of strings? Among all strings? Each of these possibilities will affect how you need to structure your MapReduce job.</p>
",['partitioner']
14407468,14408108,2013-01-18 20:55:11,Cassandra CQL time range query,"<p>I have a Cassandra column family where I am storing a large number (hundreds of thousands) of events per month with timestamp (“Ymdhisu”) as the row key. It has multiple columns capturing some data for each event. I tried retrieving events data for a specific time range. For example for the month of Jan, I used the following CQL query:</p>

<p>a) Query between range Jan 1- Jan 15, 2013</p>

<blockquote>
  <p>select count(*) from Test where Key > 20130101070100000000 and Key &lt;
  20130115070100000000 limit 100000; Bad Request: Start key's md5 sorts
  after end key's md5. This is not allowed; you probably should not
  specify end key at all, under RandomPartitioner</p>
</blockquote>

<p>b) Query between range Jan 1- Jan 10, 2013</p>

<blockquote>
  <p>select count(*) from Test where Key > 20130101070100000000 and Key &lt;
  20130110070100000000 limit 100000; count - 73264</p>
</blockquote>

<p>c) Query between range Jan 1- Jan 2, 2013 </p>

<blockquote>
  <p>select count(*) from Test where Key > 20130101070100000000 and Key &lt;
  20130102070100000000 limit 100000; count - 78328</p>
</blockquote>

<p>It appears as though the range search simply is not working! The schema of my Columnfamily is:</p>

<pre><code>Create column family Test with comparator=UTF8Type and default_validation_class=UTF8Type and key_validation_class=UTF8Type AND compression_options={sstable_compression:SnappyCompressor, chunk_length_kb:64};
</code></pre>

<p>To extract data, what are the suggestions? Do I need to redefine my schema with key validation class as TimeUUID type? Is there any other way to query efficiently without changing the schema?
I am dealing with at least 100-200K rows of data monthly in this column family. If this schema does not work for this purpose, what would be an appropriate Cassandra schema to store and retrieve the kind of data described here?</p>
",<nosql><cassandra><bigdata><cql>,"<p>You can create secondary indexes such as ""Date"" and ""Month"", and store each event's Date and Month in those columns along with other data. When querying data, you can fetch all rows for specified months or days.</p>

<p>I dont think range query on Keys will work. Perhaps if you change your partitioner from RandomPartitioner to ByteOrderedPartitioner?</p>
",['partitioner']
14456720,14470984,2013-01-22 10:44:47,How to define dynamic column families in cassandra,"<p><a href=""https://stackoverflow.com/questions/8762377/how-to-define-dynamic-columns-in-a-column-family-in-cassandra"">Here</a> it is said, that no special effort is need to get a dynamic column family. But I always get an exception, when I try to set a value for an undefined column.</p>

<p>I created a column family like this:</p>

<pre><code>CREATE TABLE places (
   latitude double,
   longitude double,
   name text,
   tags text,
   PRIMARY KEY (latitude, longitude, name)
)
</code></pre>

<p>BTW: I had to define the tags column. Can somebody explain me why? Maybe because all other columns are part of the Index?</p>

<p>Now when inserting data like this:</p>

<pre><code>INSERT INTO places (""latitude"",""longitude"",""name"",""tags"") VALUES (49.797888,9.934771,'Test','foo,bar')
</code></pre>

<p>it works just fine! But when I try:</p>

<pre><code>INSERT INTO places (""latitude"",""longitude"",""name"",""tags"",""website"") VALUES (49.797888,9.934771,'Test','foo,bar','test.de')
</code></pre>

<p>I get following error:</p>

<pre><code>Bad Request: Unknown identifier website
text could not be lexed at line 1, char 21
</code></pre>

<p>Which changes are needed so I can dynamically add columns?</p>

<p>I am using Cassandra 1.1.9 with CQL3 with the cqlsh directly on a server.</p>
",<database><cassandra><cql><dynamic-columns>,"<p>CQL3 supports dynamic column family but you have to alter the table schema first</p>

<pre><code>ALTER TABLE places ADD website varchar;
</code></pre>

<p>Check out the <a href=""http://www.datastax.com/docs/1.2/cql_cli/using/alter"" rel=""noreferrer"">1.2 documentation</a> and <a href=""http://www.slideshare.net/yukim/cql3-in-depth"" rel=""noreferrer"">CQL in depth</a> slides</p>
",['table']
15660383,15665187,2013-03-27 13:44:00,In Cassandra database the Table created in CQL is not seen in CLI?,"<p>In Cassandra database the Table created in CQL is not seen in CLI in same column family is there any reference reason for the same ?</p>
",<cassandra><command-line-interface><cql>,"<p>This is because two diffrent protocols are used to create the column family. You are creating the 'table' using the CQL binary protocol and then you are trying to edit it using the THRIFT interface (through CLI), this is not allowed on purpose. From the Cassandra CTO: </p>

<blockquote>
  <p>This is by design. CLI and Thrift-based consumers in general do not know how to understand CQL3 metadata, and would do the Wrong Thing without realizing it.</p>
</blockquote>

<p>You can find the documented ""issue"" <a href=""https://issues.apache.org/jira/browse/CASSANDRA-5246"" rel=""nofollow"">here</a>.</p>

<p>If you wanted to access the data in the table you created you can use <a href=""http://www.datastax.com/docs/1.1/dml/using_cql#using-cql"" rel=""nofollow"">cqlsh</a>.</p>
",['table']
15857779,15859759,2013-04-07 01:07:27,commitLog and SSTables in Cassandra database,"<p>I recently started working with Cassandra database. I have installed <code>single node cluster</code> in my local box. And I am working with <code>Cassandra 1.2.3</code>.</p>

<p>I was reading the article on the internet and I found this line-</p>

<blockquote>
  <p>Cassandra writes are first written to a commit log (for durability),
  and then to an in-memory table structure called a memtable. A write is
  successful once it is written to the commit log and memory, so there
  is very minimal disk I/O at the time of write. Writes are batched in
  memory and periodically written to disk to a persistent table
  structure called an SSTable (sorted string table).</p>
</blockquote>

<p>So to understand the above lines, I wrote a simple program that will write to Cassandra Database using <code>Pelops client</code>. And I was able to insert the data in Cassandra database. </p>

<p>And now I am trying to see how my data was written into <code>commit log</code> and where that <code>commit log file</code> is? And also how <code>SSTables</code> is generated and where I can find that as well in my local box and what it contains also. </p>

<p>I wanted to see these two files so that I can understand more how Cassandra works behind the scenes.</p>

<p>In my cassandra.yaml file, I have something like this</p>

<pre><code># directories where Cassandra should store data on disk.
data_file_directories:
    - S:\Apache Cassandra\apache-cassandra-1.2.3\storage\data

# commit log
commitlog_directory: S:\Apache Cassandra\apache-cassandra-1.2.3\storage\commitlog

# saved caches
saved_caches_directory: S:\Apache Cassandra\apache-cassandra-1.2.3\storage\savedcaches
</code></pre>

<p>But when I opened commitLog, first of all it has lot of data so my notepad++ is not able to open it properly and if it gets opened, I cannot see properly because of some encoding or what. And in my data folder, I cannot find out anything?</p>

<p>Meaning this folder is empty for me-</p>

<pre><code>S:\Apache Cassandra\apache-cassandra-1.2.3\storage\data\my_keyspace\users
</code></pre>

<p>Is there anything I am missing here? Can anybody explain me how to read commitLog and SSTables files and where I can find these two files? And also what exactly happens behind the scenes whenever I am writing to Cassandra database.</p>

<p><strong>Updated:-</strong></p>

<p>Code I am using to insert into Cassandra Database-</p>

<pre><code>public class MyPelops {

    private static final Logger log = Logger.getLogger(MyPelops.class);

    public static void main(String[] args) throws Exception {


        // -------------------------------------------------------------
        // -- Nodes, Pool, Keyspace, Column Family ---------------------
        // -------------------------------------------------------------

        // A comma separated List of Nodes
        String NODES = ""localhost"";

        // Thrift Connection Pool
        String THRIFT_CONNECTION_POOL = ""Test Cluster"";

        // Keyspace
        String KEYSPACE = ""my_keyspace"";

        // Column Family
        String COLUMN_FAMILY = ""users"";

        // -------------------------------------------------------------
        // -- Cluster --------------------------------------------------
        // -------------------------------------------------------------

        Cluster cluster = new Cluster(NODES, 9160);

        Pelops.addPool(THRIFT_CONNECTION_POOL, cluster, KEYSPACE);

        // -------------------------------------------------------------
        // -- Mutator --------------------------------------------------
        // -------------------------------------------------------------

        Mutator mutator = Pelops.createMutator(THRIFT_CONNECTION_POOL);

        log.info(""- Write Column -"");

        mutator.writeColumn(
                COLUMN_FAMILY,
                ""Row1"",
                new Column().setName("" Name "".getBytes()).setValue("" Test One "".getBytes()).setTimestamp(new Date().getTime()));

        mutator.writeColumn(
                COLUMN_FAMILY,
                ""Row1"",
                new Column().setName("" Work "".getBytes()).setValue("" Engineer "".getBytes()).setTimestamp(new Date().getTime()));

        log.info(""- Execute -"");
        mutator.execute(ConsistencyLevel.ONE);

        // -------------------------------------------------------------
        // -- Selector -------------------------------------------------
        // -------------------------------------------------------------

        Selector selector = Pelops.createSelector(THRIFT_CONNECTION_POOL);

        int columnCount = selector.getColumnCount(COLUMN_FAMILY, ""Row1"",
                ConsistencyLevel.ONE);
        System.out.println(""- Column Count = "" + columnCount);

        List&lt;Column&gt; columnList = selector
                .getColumnsFromRow(COLUMN_FAMILY, ""Row1"",
                        Selector.newColumnsPredicateAll(true, 10),
                        ConsistencyLevel.ONE);
        System.out.println(""- Size of Column List = "" + columnList.size());

        for (Column column : columnList) {
            System.out.println(""- Column: ("" + new String(column.getName()) + "",""
                    + new String(column.getValue()) + "")"");
        }

        System.out.println(""- All Done. Exit -"");
        System.exit(0);
    }

}
</code></pre>

<p>Keyspace and Column family that I have created-</p>

<pre><code>create keyspace my_keyspace with placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy' and strategy_options = {replication_factor:1};
use my_keyspace;
create column family users with column_type = 'Standard' and comparator = 'UTF8Type';
</code></pre>
",<nosql><cassandra>,"<p>You are almost there in your understanding. However, missing some minute details.</p>

<p>So explaining things in a structured way, cassandra write operation life cycle is divided in these steps </p>

<ul>
<li>commitlog write </li>
<li>memtable write</li>
<li>sstable write</li>
</ul>

<p>Cassandra writes are first written to a commit log (for durability), and then to an in-memory table structure called a memtable. A write is said to successful once it is written to the commit log and memory, so there is very minimal disk I/O at the time of write. When ever the memtable runs out of space, i.e when the number of keys exceed certain limit (128 is default) or when it reaches the time duration (cluster clock), it is being stored into sstable, immutable space (This mechanism is called <strong>Flushing</strong>). Once writes are done on SSTable, then you can see the corresponding datas in the data folder, in your case its <code>S:\Apache Cassandra\apache-cassandra-1.2.3\storage\data</code>. Each SSTable composes of mainly 2 files - Index file and Data file</p>

<ul>
<li><p>Index file contains - Bloom filter and Key-Offset pairs  </p>

<ul>
<li>Bloom Filter: A Bloom filter, is a space-efficient probabilistic data structure that is used to test whether an element is a member of a set. False positives are possible, but false negatives are not. Cassandra uses bloom filters to save IO when performing a key lookup: each SSTable has a bloom filter associated with it that Cassandra checks before doing any disk seeks, making queries for keys that don't exist almost free</li>
<li>(Key, offset) pairs (points into data file)</li>
</ul></li>
<li><p>Data file contains the actual column data</p></li>
</ul>

<p>And regarding commitlog files, these are encrypted files maintained intrinsically by Cassandra, for which you are not able to see anything properly.  </p>

<p><strong>UPDATE:</strong> </p>

<p>Memtable is an in-memory cache with content stored as key/column (data are sorted by key). Each column-family has a separate Memtable and retrieve column data from the key. So now i hope you are in clear state of mind to understand the fact, why we can't locate them in our disk. </p>

<p>In your case your memtable is not full as memtable thresholds are not bleached yet resulting to no flushing. You can know more about MemtableThresholds <a href=""http://wiki.apache.org/cassandra/MemtableThresholds"">here</a> though it is recommended not to touch that Dial.</p>

<p><strong>SSTableStructure:</strong></p>

<ul>
<li>Your data folder
<ul>
<li>KEYSPACE
<ul>
<li>CF
<ul>
<li>CompressionInfo.db</li>
<li>Data.db</li>
<li>Filter.db</li>
<li>Index.db</li>
<li>Statistics.db</li>
<li>snapshots //if snapshots are taken</li>
</ul></li>
</ul></li>
</ul></li>
</ul>

<p>For more information  Refer <a href=""http://wiki.apache.org/cassandra/ArchitectureSSTable"">sstable</a> </p>
",['table']
15865710,15875434,2013-04-07 17:50:58,Cassandra RandomPartitioner on version 1.2.3,"<p>Im installing Cassandra 1.2.3 on debian using apt, I was previously using a tarball 1.1.7 install.  After install i'm changing the partitioner from Murmur3Partitioner to RandomPartitioner in cassandra.yaml as follows:</p>

<p>partitioner: org.apache.cassandra.dht.RandomPartitioner</p>

<p>Then on starting i'm seeing incompatible system keyspace errors as follows:</p>

<p>ERROR 18:22:11,465 Cannot open /var/lib/cassandra/data/system/schema_keyspaces/system-schema_keyspaces-ib-1; partitioner org.apache.cassandra.dht.Murmur3Partitioner does not match system partitioner org.apache.cassandra.dht.RandomPartitioner.  Note that the default partitioner starting with Cassandra 1.2 is Murmur3Partitioner, so you will need to edit that to match your old partitioner if upgrading.
Service exit with a return value of 1</p>

<p>How can I set the system keyspace to be RandomPartitioner? I have tried purging the data folder, apt-get remove, also apt-get purge then re-installing, changing to RandomPartitioner then starting cassandra but it is still failing.  I've also replicated this on my ubuntu desktop so im thinking im doing something wrong here.</p>

<p>Any help is appreciated!</p>

<p>Cheers</p>

<p>Sam</p>
",<cassandra>,"<p>The partitioner cannot be changed once Cassandra has started for the first time.  This error is showing that the data directory was initialized with Murmur3Partitioner but you're starting it using RandomPartitioner.</p>

<p>If you're trying to upgrade your data from your 1.1 install, Cassandra isn't reading from the right place.  Adjust your data directory to use your 1.1 directory and it should start with partitioner set to RandomPartitioner.</p>

<p>If you're trying to start with no data, stop Cassandra, remove /var/lib/cassandra/* and start it again.  Note you need to remove the commitlog directory as well as the data directory.</p>
",['partitioner']
15925549,22273466,2013-04-10 12:16:23,How does cassandra split keyspace data when multiple directories are configured?,"<p>I have configured three separate data directories in cassandra.yaml file as given below:</p>

<pre>
data_file_directories:
    - E:/Cassandra/data/var/lib/cassandra/data
    - K:/Cassandra/data/var/lib/cassandra/data
</pre>

<p>when I create keyspace and insert data my key space got created in both two directories and data got scattered. what I want to know is how cassandra splits the data between multiple directories?. And what is the rule behind this?</p>
",<cassandra>,"<p>You are using the JBOD feature of Cassandra when you add multiple entries under data_file_directories. Data is spread evenly over the configured drives proportionate to their available space. </p>

<p>This also let's you take advantage of the disk_failure_policy setting. You can read about the details here:
<a href=""http://www.datastax.com/dev/blog/handling-disk-failures-in-cassandra-1-2"" rel=""noreferrer"">http://www.datastax.com/dev/blog/handling-disk-failures-in-cassandra-1-2</a></p>

<p>In short, you can configure Cassandra to keep going, doing what it can if the disk becomes full or fails completely. This has advantages over RAID0 (where you would effectively have the same capacity as JBOD) in that you do not have to replace the whole data set from backup (or full repair) but just run a repair for the missing data. On the other hand, RAID0 provides higher throughput (depending how well you know how to tune RAID arrays to match filesystem and drive geometry). </p>

<p>If you have the resources for fault-tolerant/more performant RAID setup (like RAID10 for example), you may want to just use a single directory for simplicity. Most deployments are starting to lean towards the density route, using JBOD rather than systems-level tolerance though. </p>

<p>You can read about the thought process behind the development of this issue here:
<a href=""https://issues.apache.org/jira/browse/CASSANDRA-4292"" rel=""noreferrer"">https://issues.apache.org/jira/browse/CASSANDRA-4292</a></p>
",['disk_failure_policy']
15979170,15995723,2013-04-12 19:15:46,setting up cassandra multi node cluster: 'Nodes have the same token 0',"<p>I'm trying to set up a Cassandra multi node cluster in my computer just to test, but it seems not work... The Cassandra version is 1.1 and It runs on Ubuntu.</p>

<p>Fist of all, I've modified the cassandra.yaml file for each node as follows: </p>

<p><strong>node0</strong></p>

<ul>
<li>initial_token: 0</li>
<li>seeds: ""127.0.0.1""</li>
<li>listen_address: 127.0.0.1</li>
<li>rpc_address: 0.0.0.0 </li>
<li>endpoint_snitch: RackInferringSnitch</li>
</ul>

<p><strong>node1</strong></p>

<p>same as <em>node0</em> exept for:</p>

<ul>
<li>initial_token: 28356863910078205288614550619314017621 <em>(get using
cassandra token generator)</em> </li>
<li>listen_address: 127.0.0.2</li>
</ul>

<hr>

<p>After that, I've started first the seed node 127.0.0.1 and, once the node is up, I've started the other node 127.0.0.2. I've got the following:</p>

<p>[...]</p>

<p>INFO 06:09:27,146 Listening for thrift clients...</p>

<p>INFO 06:09:27,909 Node /127.0.0.1 is now part of the cluster</p>

<p>INFO 06:09:27,911 InetAddress /127.0.0.1 is now UP</p>

<p>INFO 06:09:27,913 Nodes /127.0.0.1 and /127.0.0.2 have the same token 0.  Ignoring /127.0.0.1</p>

<hr>

<p>Running <em>nodetool -h localhost ring</em> it shows:</p>

<p>Address: 127.0.0.2</p>

<p>DC:    datacenter1 </p>

<p>Rack:  rack1  </p>

<p>Status: Up</p>

<p>State:  Normal </p>

<p>Load:   11,21 KB    </p>

<p>Owns:   100,00%    </p>

<p>Token:  0</p>

<p>As you can see, only the information of the second node is showed owning 100% of the ring. Indeed, the token is initialized to 0 instead of to the value I defined at its cassandra.yaml file.</p>

<hr>

<p>The gossip Info is:</p>

<p>/127.0.0.2</p>

<p>LOAD:25559.0</p>

<p>STATUS:NORMAL,0</p>

<p>SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f</p>

<p>RELEASE_VERSION:1.1.6-SNAPSHOT</p>

<p>RPC_ADDRESS:0.0.0.0</p>

<p>/127.0.0.1</p>

<p>LOAD:29859.0</p>

<p>STATUS:NORMAL,0</p>

<p>SCHEMA:59adb24e-f3cd-3e02-97f0-5b395827453f</p>

<p>RELEASE_VERSION:1.1.6-SNAPSHOT</p>

<p>RPC_ADDRESS:0.0.0.0</p>

<hr>

<p>Does anyone know what is happening and how can I fix it?
Thank you so much in advance!!</p>
",<cassandra><installation><cluster-computing>,"<p>initial_token is only checked at first startup, when it is written to a system table.  Delete the system table files and restart.</p>
",['table']
15980936,16010641,2013-04-12 21:13:37,What ConsistencyLevel to use with Cassandra counter tables?,"<p>I have a table counting around 1000 page views per second. What  read and write ConsistencyLevel should I use with it? I am using the Cassandra Thrift client.</p>
",<cassandra>,"<p>Carlo has more or less the right idea. But you have to balance it with your use case.</p>

<p>I work in the game industry and we use cassandra for player data. It is quite heavily bound by the read-modify-write pattern which is not the strong suit of cassandra. But we also have some functionality that are Write heavy (thousands of writes for a few reads a day).</p>

<p>This is my opinion, based upon experience, of how you should use the consistency levels.</p>

<p>Write + Read at QUORUM means that before returning for both operations it will wait for a majority of nodes in the cluster to confirm the operation. It is the solution I use when Read and Writes are roughly at the same frequency. (Player data blob)</p>

<p>Write One + Read All is useful for something very write heavy. We use this for high scores for examples (write often read every 5 minutes for regenerating the high score table of the whole game)
 You could use Write Any if you do not care about the data that much (non critical logs comes to mind).</p>

<p>The only use case I could come up for the Write All + Read One would be messaging or feeds with periodical checks for updates. Chats and messaging seem a good fit for that since Cassandra does not have a subscription/push functionality to it.</p>

<p>Write &amp; Read ALL is a bad implementation. It IS a WASTE of resource as you will get the same consistency as if you were using one of the three set up I mentioned above.</p>

<p>A final note about Write ANY vs. Write ONE : ANY only confirms that anything in the cluster has received the mutation, but ONE confirms that it has been applied at least by one node. ANY is not safe as it could return without error even if all the nodes responsible for that mutation are down, or any other condition that could make the mutation fail after reception. It is also slightly quicker (I only use it as an async dump for logs that are not critical) that is its only advantage, but do not trust the response at 100%.</p>

<p>A good reference to study this subject about cassandra is <a href=""http://www.datastax.com/docs/1.2/dml/data_consistency"" rel=""nofollow"">http://www.datastax.com/docs/1.2/dml/data_consistency</a></p>
",['table']
16152160,16170352,2013-04-22 16:34:16,Multiple columns in Cassandra tables,"<p>I am wondering what happens when there are multiple Non-PK columns in a table. I've read this example:
<a href=""http://johnsanda.blogspot.co.uk/2012/10/why-i-am-ready-to-move-to-cql-for.html"" rel=""nofollow noreferrer"">http://johnsanda.blogspot.co.uk/2012/10/why-i-am-ready-to-move-to-cql-for.html</a></p>

<p>Which shows that with single column:</p>

<pre><code>CREATE TABLE raw_metrics (
schedule_id int,
time timestamp,
value double,
PRIMARY KEY (schedule_id, time)
);
</code></pre>

<p>We get:</p>

<p><img src=""https://3.bp.blogspot.com/-PIeVMTodi8U/UH48xDhMtVI/AAAAAAAAAPA/aitVqxDjJ0Y/s400/CQL+Raw+Metrics+Column+Family.png"" alt=""enter image description here""></p>

<p>Now I wonder what happens when we have two columns:</p>

<pre><code>CREATE TABLE raw_metrics (
schedule_id int,
time timestamp,
value1 double,
value2 int,
PRIMARY KEY (schedule_id, time)
);
</code></pre>

<p>Are we going to end up with something like:</p>

<pre><code>row key  columns...
123      1339707619:""value1"" | 1339707679:""value2"" | 1339707784:""value2""
...
</code></pre>

<p>or rather:</p>

<pre><code>row key  columns...
123      1339707619:""value1"":""value2"" | 1339707679:""value1"":""value2"" | 1339707784:""value1""""value2""
...
</code></pre>

<p>etc. I guess what I am asking is if this is going to be a <strong>sparse</strong> table given that I only insert ""value1"" or ""value2"" at a time.</p>

<p>In such situations if I want to store more columns (one per each type, eg. double, int, date, etc) would it be better perhaps to have separate tables rather than storing everything in a single table?</p>
",<cassandra><cql3>,"<p>This post might help in explaining what is happening when composite keys are created:
<a href=""https://stackoverflow.com/questions/13218868/cassandra-composite-columns-how-are-compositetypes-chosen?rq=1"">Cassandra Composite Columns - How are CompositeTypes chosen?</a></p>

<p>So essentially the table will look in the following way:</p>

<pre><code>row key  columns...
123      1339707619:""value1"" | 1339707679:""value2"" | 1339707784:""value2""
</code></pre>

<p>See also reference to secondary indexes:
<a href=""http://wiki.apache.org/cassandra/SecondaryIndexes"" rel=""nofollow noreferrer"">http://wiki.apache.org/cassandra/SecondaryIndexes</a></p>
",['table']
16204066,16234121,2013-04-24 23:40:12,Increasing of used space while rebalancing Cassandra cluster,"<p>Just an example: I have 2 Cassandra nodes, 1Gb data per each node, replication factor is 1. I use single column family with Leveled compaction with 100Mb sstable size, like this:</p>

<pre><code>create column family ColFamily with key_validation_class=UTF8Type 
  and compaction_strategy=LeveledCompactionStrategy 
  and compaction_strategy_options={sstable_size_in_mb: 100};
</code></pre>

<p>I want to add additional node. The data will be rebalanced across 3 nodes: ~0,667 Mb per node. Right?</p>

<p>But how the used space will be increased on each node while the process of rebalancing is being in progress? What will be the peak?</p>
",<cassandra>,"<p>Before Cassandra 1.2 and virtual nodes, you have to do the redistribution of data yourself after adding a new node.</p>

<p>If your two nodes are currently balanced i.e. have 50% of the ring each, then the tokens will be</p>

<pre><code>node1: 0
node2: 85070591730234615865843651857942052864
</code></pre>

<p>(or shifted, but I'll assume node1 has token 0).  The token for node2 is 2^127/2.  You want to end up with</p>

<pre><code>node1: 0
node2: 56713727820156410577229101238628035242
node3: 113427455640312821154458202477256070484
</code></pre>

<p>where the token for node2 is 2^127/3, and for node3 is (2^127/3)*2.  What you need to do is bootstrap node3 with initial_token set to the token above.  This copies data from node1, since node3's token precedes to node1's (the token ring is wrapped around).</p>

<p>Now node3 will have 1/6 of the data, node2 will still have 1/2 and node1 will store 1/2 but only be responsible for 1/3.  You could now run 'nodetool cleanup' on node1 to remove the data that it copied over to node3.  This will reduce node1's data to approx 677MB.</p>

<p>Now you need to move node2's token to its final place.  This copies data from node2 to node3, bringing node3 up to its quota of 1/3 of the data, approx 667 MB.  Now you can run 'nodetool cleanup' on node2 to remove the data it has just copied to node3.  Now the rebalancing is complete.</p>

<p>This means no node ever stores more than 1 GB of data during the rebalancing.</p>

<p>In general, if you had more nodes or higher replication factor, you can always do the rebalancing without increasing the data stored on any existing nodes if you run cleanup after each move on the node just moved.</p>

<p>Finally, if you had Cassandra 1.2 and virtual nodes, the tokens can be chosen randomly which gives even load as soon as you add a new node, with no need for any rebalancing (manual or automatic).  This is not only easier, it saves copying a constant fraction of your data around the cluster just to add one node.</p>
",['initial_token']
16301939,16318839,2013-04-30 14:06:30,How should I store a date interval in Cassandra?,"<p>I'm working on an application that stores sensor measurements. Sometimes, the sensors will send erroneous measurements (e.g. the measured value is out of bound). We do not want to persist each measurement error separately, but we want to persist statistics about these errors, such as the sensor id, the date of the first error, the date of the last error, and other infos like the number of successive errors, which I'll omit here...</p>

<p>Here is a simplified version of the ""ErrorStatistic"" class:</p>

<pre><code>package foo.bar.repository;

import org.joda.time.DateTime;

import javax.annotation.Nonnull;
import javax.annotation.Nullable;

import static com.google.common.base.Preconditions.checkNotNull;

public class ErrorStatistic {

    @Nonnull
    private final String sensorId;
    @Nonnull
    private final DateTime startDate;
    @Nullable
    private DateTime endDate;

    public ErrorStatistic(@Nonnull String sensorId, @Nonnull DateTime startDate) {
        this.sensorId = checkNotNull(sensorId);
        this.startDate = checkNotNull(startDate);
        this.endDate = null;
    }

    @Nonnull
    public String getSensorId() {
        return sensorId;
    }

    @Nonnull
    public DateTime getStartDate() {
        return startDate;
    }

    @Nullable
    public DateTime getEndDate() {
        return endDate;
    }

    public void setEndDate(@Nonnull DateTime endDate) {
        this.endDate = checkNotNull(endDate);
    }

}
</code></pre>

<p>I am currently persisting these ErrorStatistic using Hector as follows:</p>

<pre><code>private void persistErrorStatistic(ErrorStatistic errorStatistic) {
    Mutator&lt;String&gt; mutator = HFactory.createMutator(keyspace, StringSerializer.get());

    String rowKey = errorStatistic.getSensorId();
    String columnName = errorStatistic.getStartDate().toString(YYYY_MM_DD_FORMATTER);
    byte[] value = serialize(errorStatistic);

    HColumn&lt;String, byte[]&gt; column = HFactory.createColumn(columnName, value, StringSerializer.get(), BytesArraySerializer.get());
    mutator.addInsertion(rowKey, COLUMN_FAMILY, column);

    mutator.execute();
}

private static final DateTimeFormatter YYYY_MM_DD_FORMATTER = DateTimeFormat.forPattern(""yyyy-MM-dd"");
</code></pre>

<p>When we receive the first measurement in error, we create an ErrorStatistic with <code>sensorId</code> and <code>startDate</code> set, and a null <code>endDate</code>. This ErrorStatistic is kept in our in-memory model, and persisted in Cassandra.
We then update the ErrorStatistic in memory for the next measurements in error, until we receive a valid measurement, at which point the ErrorStatistic is persisted and removed from our in-memory model.</p>

<p>Cassandra thus contains ErrorStatistics with open-ended intervals (e.g. [2012-08-01T00:00Z|null]), and closed intervals (e.g. [2012-08-01T00:00Z|2013-01-12T10:23Z]).</p>

<p>I want to be able to query these ErrorStatistics by date.</p>

<p>For example, if I have these 3 error statistics:</p>

<pre><code>sensorId  = foo
startDate = 2012-08-01T00:00Z
endDate   = 2012-09-03T02:10Z

sensorId  = foo
startDate = 2012-10-04T03:12Z
endDate   = 2013-02-01T12:28Z

sensorId  = foo
startDate = 2013-03-05T23:22Z
endDate   = null
(this means we have not received a valid measurement since 2013-03-05)
</code></pre>

<p>If I query Cassandra with the date:</p>

<ul>
<li>2012-08-04T10:00Z --> it should return the first ErrorStatistic</li>
<li>2012-09-04T00:00Z --> it should return that there were no errors at this time</li>
<li>2014-01-03T00:00Z --> it should return the last ErrorStatistic (since it is open-ended)</li>
</ul>

<p>I am not sure how I should store and ""index"" these ErrorStatistic objects, to efficiently query them. I am quite new to Cassandra, and I might be missing something obvious.</p>

<hr>

<p>Edit: the following was added in response to Joost's suggestion that I should focus on the type of queries I am interested in.</p>

<p>I will have two types of query:</p>

<ul>
<li>The first, as you guessed, is to list all ErrorStatistics for a given sensor and time range. This seems relatively easy. The only problem I will have, is when an ErrorStatistics starts <em>before</em> the time range I'm interested in (e.g. I query all errors for the months of april, and I want my query to return ErrorStatistics[2012-03-29:2012-04-02] too...)</li>
<li>The second query seems harder. I want to find, for a given sensor and date, the ErrorStatistics whose interval contains the given date, or whose <code>startDate</code> precedes the given date, with a null <code>endDate</code> (this means that we are still receiving errors for this sensor). I don't know how to do this efficiently. I could just load up all ErrorStatistics for the given sensor, then check the intervals in Java... But I'd like to avoid this if possible. I guess I want Cassandra to start at a given date and look backward until it finds the first ErrorStatistics with a <code>startDate</code> that precedes the given date (if any), then load it and check in Java if its <code>endDate</code> is <code>null</code> or after the given date. But I have no idea if that's possible, and how efficient that would be.</li>
</ul>
",<java><nosql><cassandra>,"<p>The question you have to ask yourself is what questions you have towards the ErrorStatistics. Cassandra schema design typically starts with a 'Table per query' approach. Don't start with the data (entities) you have, but with your questions/queries. This is a different mindset than 'traditional' rdbms design, and I found it takes some time to get used to.</p>

<p>For example, do  you want to query the statistics per Sensor? Than a table with a composite key (sensor id, timeuuid) could  be a solution. Such a table allows for quick lookup per sensor id, sorting the results based on time.</p>

<p>If you want to query the sensor statistics based on time only, a (composite) key with a time unit may be of more help, possibly with sharding elements to better distribute the load over nodes. Note that there is catch: range queries on primary keys are not feasible using the Cassandra random or murmur partitioners. There are other partitioners, but they easily tend to uneven load distribution in your cluster.</p>

<p>In short, start with the answers you want, and then work 'backwards' to your table design. With a proper schema, your code will follow. </p>

<hr/>

<p><em>Addition (2013-9-5):</em> What is good to know is that Cassandra sorts data within the scope of a single partition key. That is something very useful. For example the measurements would be ordered by start_time in descending order (newest first) if you define a table as:</p>

<pre><code>create table SensorByDate
(
    sensor_id uuid,
    start_date datetime,
    end_date datetime,
    measurement int
    primary key (sensor_id, start_date)
)
with clustering order by (start_time DESC);
</code></pre>

<p>In this example the sensor_id is the partition key and determines the node this row is stored on. The start_date is the second item in the composite key and determines the sort order.</p>

<p>To get the first measurement after a certain start date in this table you could formulate a query like</p>

<pre><code>select * from SensorByDate 
where sensor_id = ? and start_date &lt; ? limit 1
</code></pre>
",['table']
16396542,16401485,2013-05-06 10:15:13,Unable to use cassandra counter column with cassandra gem,"<p>I have created a column family by following command using cqlsh:</p>

<pre><code>create table mystats (key PRIMARY KEY, count counter);
</code></pre>

<p>Now from the cqlsh I am able to increase the counter column but when I attempt to do this from cassandra gem, as adviced at:
<a href=""https://stackoverflow.com/questions/9307825/are-there-any-ruby-clients-for-cassandra-with-counters-and-supercolumn"">Are there any Ruby clients for Cassandra with counters and supercolumn?</a></p>

<p>So when I use:</p>

<pre><code>@stats.add(:mystats, 'randomkey', 1, 'count')
</code></pre>

<p>I get an error:</p>

<blockquote>
  <p>Cassandra::AccessError at /client Invalid column family ""mystats""</p>
</blockquote>

<p>When I further looked into it, I found that the error is raised from the gem:
in file: <a href=""https://github.com/twitter/cassandra/blob/master/lib/cassandra/columns.rb"" rel=""nofollow noreferrer"">https://github.com/twitter/cassandra/blob/master/lib/cassandra/columns.rb</a></p>

<pre><code>def column_family_property(column_family, key)
      cfdef = schema.cf_defs.find {|cfdef| cfdef.name == column_family }
      unless cfdef
        raise AccessError, ""Invalid column family \""#{column_family}\""""
      end
      cfdef.send(key)
    end
</code></pre>

<p>Can anyone please point out what I might be doing wrong here..</p>
",<ruby><cassandra><cql>,"<p>That client has not yet been updated to support CQL3.  I'm not aware of a CQL3-aware Ruby client, so your best bet is probably to create a Thrift-compatible table (TLDR add <code>WITH COMPACT STORAGE</code> to your table definition).</p>
",['table']
16453411,16471860,2013-05-09 02:32:35,What's the meaning of NodeDiscoveryType as TOKEN_AWARE in Astyanax client?,"<p>I found <code>TOKEN_AWARE</code> enum value in Astyanax client for Cassandra in <a href=""https://github.com/Netflix/astyanax/blob/master/astyanax-core/src/main/java/com/netflix/astyanax/connectionpool/NodeDiscoveryType.java"" rel=""nofollow"">com.netflix.astyanax.connectionpool.NodeDiscoveryType</a> and am trying to understand what it does?</p>

<pre><code>package com.netflix.astyanax.connectionpool;

public enum NodeDiscoveryType {
    /**
     * Discover nodes exclusively from doing a ring describe
     */
    RING_DESCRIBE,

    /**
     * Discover nodes exclusively from an external node discovery service
     */
    DISCOVERY_SERVICE,

    /**
     * Intersect ring describe and nodes from an external service. This solve
     * the multi-region ring describe problem where ring describe returns nodes
     * from other regions.
     */
    TOKEN_AWARE,

    /**
     * Use only nodes in the list of seeds
     */
    NONE
}
</code></pre>

<p>Suppose if I have 24 nodes <code>cross colo cluster</code> with 12 nodes in PHX <code>colo/datacenter</code> and 12 nodes in SLC <code>colo/datacenter</code>.</p>

<p>And I am connecting to Cassandra using Astyanax client as follows:</p>

<pre><code>private CassandraAstyanaxConnection() {
    context = new AstyanaxContext.Builder()
                .forCluster(ModelConstants.CLUSTER)
                .forKeyspace(ModelConstants.KEYSPACE)
    .withConnectionPoolConfiguration(new ConnectionPoolConfigurationImpl(""MyConnectionPool"")
        .setPort(9160)
        .setMaxConnsPerHost(40)
        .setSeeds(""cdb03.vip.phx.host.com:9160,cdb04.vip.phx.host.com:9160"")
    )
    .withAstyanaxConfiguration(new AstyanaxConfigurationImpl()      
        .setCqlVersion(""3.0.0"")
        .setTargetCassandraVersion(""1.2"")
        .setDiscoveryType(NodeDiscoveryType.TOKEN_AWARE))
    .withConnectionPoolMonitor(new CountingConnectionPoolMonitor())
    .buildKeyspace(ThriftFamilyFactory.getInstance());

    context.start();
    keyspace = context.getEntity();

    emp_cf = ColumnFamily.newColumnFamily(
        ModelConstants.COLUMN_FAMILY, 
        StringSerializer.get(), 
        StringSerializer.get());
}
</code></pre>

<p>Can anyone explain me what the difference between <code>TOKEN_AWARE</code> of <code>NodeDiscoveryType</code> vs <code>TOKEN_AWARE</code> of <code>ConnectionPoolType</code> is?</p>

<p>Thanks for the help.</p>

<p><strong>Updated Code</strong></p>

<p>Below is the code I am using so far after making changes-</p>

<pre><code>private CassandraAstyanaxConnection() {

    context = new AstyanaxContext.Builder()
    .forCluster(ModelConstants.CLUSTER)
    .forKeyspace(ModelConstants.KEYSPACE)
    .withConnectionPoolConfiguration(new ConnectionPoolConfigurationImpl(""MyConnectionPool"")
        .setPort(9160)
        .setMaxConnsPerHost(40)
        .setSeeds(""cdb03.vip.phx.host.com:9160,cdb04.vip.phx.host.com:9160"")
        .setLocalDatacenter(""phx"")
    )
    .withAstyanaxConfiguration(new AstyanaxConfigurationImpl()
        .setCqlVersion(""3.0.0"")
        .setTargetCassandraVersion(""1.2"")
        .setConnectionPoolType(ConnectionPoolType.TOKEN_AWARE))
    .withConnectionPoolMonitor(new CountingConnectionPoolMonitor())
    .buildKeyspace(ThriftFamilyFactory.getInstance());

    context.start();
    keyspace = context.getEntity();

    emp_cf = ColumnFamily.newColumnFamily(
        ModelConstants.COLUMN_FAMILY, 
        StringSerializer.get(), 
        StringSerializer.get());
}
</code></pre>

<p>You mentioned in your example that you will be using-</p>

<pre><code>    .setDiscoveryType(NodeDiscoveryType.RING_DESCRIBE)
    .setConnectionPoolType(ConnectionPoolType.TOKEN_AWARE)
</code></pre>

<p>these two together right? But I believe <code>TOKEN_AWARE ConnectionPoolType</code> by default uses <code>RING_DESCRIBE</code> so it doesn't make sense to add it again. Am I right?</p>

<p>Correct me if I am wrong?</p>
",<java><cassandra><astyanax>,"<p>When it comes to ""node discovery"" the relationship between TOKEN_AWARE for NodeDiscoveryType and TOKEN_AWARE for ConnectionPoolType is interrelated and somewhat confusing.</p>

<h2>NodeDiscoveryType is <a href=""https://github.com/Netflix/astyanax/blob/master/astyanax-cassandra/src/main/java/com/netflix/astyanax/AstyanaxContext.java#L91"">determined</a> as follows (and it -usually- isn't via setDiscoveryType()):</h2>

<ul>
<li><strong>If</strong> you've provided Seeds via setSeeds and ConnectionPoolType is
TOKEN_AWARE <strong>then</strong> NodeDiscoveryType is RING_DESCRIBE.</li>
<li><strong>If</strong> you've provided Seeds via setSeeds and ConnectionPoolType is
anything other than TOKEN_AWARE <strong>then</strong> your configured setDiscoveryType will be used.  <em>This is the only case in which your configured NodeDiscoveryType (via setDiscoveryType) will be used.</em></li>
<li><strong>If</strong> you did not provide Seeds via setSeeds AND ConnectionPoolType is
TOKEN_AWARE <strong>then</strong> NodeDiscoveryType is TOKEN_AWARE. </li>
<li><strong>If</strong> you did not provide Seeds via setSeeds AND ConnectionPoolType is
anything other than TOKEN_AWARE <strong>then</strong> NodeDiscoveryType is
DISCOVERY_SERVICE.</li>
</ul>

<h2>Node Discovery</h2>

<p>Now that we've determined how NodeDiscoveryType is set, let's <a href=""https://github.com/Netflix/astyanax/blob/master/astyanax-cassandra/src/main/java/com/netflix/astyanax/AstyanaxContext.java#L142"">see</a> how it impacts actually discovering nodes.  Node discovery boils down to which implementation of HostSupplier (i.e. <code>Supplier&lt;List&lt;Host&gt;&gt;</code>) is used.</p>

<ul>
<li><strong>If</strong> NodeDiscoveryType (from above) is DISCOVERY_SERVICE <strong>then</strong> must use HostSupplier (via <code>withHostSupplier</code>).</li>
<li><strong>If</strong> NodeDiscoveryType (from above) is RING_DESCRIBE <strong>then</strong> use RingDescribeHostSupplier.</li>
<li><strong>If</strong> NodeDiscoveryType (from above) is TOKEN_AWARE and HostSupplier is set (via <code>withHostSupplier</code>) <strong>then</strong> use FilteringHostSupplier with RingDescribeHostSupplier.</li>
<li><strong>If</strong> NodeDiscoveryType (from above) is TOKEN_AWARE and no HostSupplier is set <strong>then</strong> use RingDescribeHostSupplier.</li>
</ul>

<h2>RingDescribe and using the local DC</h2>

<p>Based on the configuration you've supplied you'll end up with RingDescribeHostSupplier.  RingDescribeHostSupplier allows connections to all nodes in the ring unless you've specified a datacenter.  So, when setting up your AstyanaxContext using ConnectionPoolConfigurationImpl you might want to setLocalDatacenter with the desired DC. That will ensure that hosts from the other dc's are not in the connection pool and that your requests are local.</p>

<pre><code>.withConnectionPoolConfiguration(new ConnectionPoolConfigurationImpl(""MyConnectionPool"")
        .setPort(9160)
        .setMaxConnsPerHost(40)
        .setLocalDatacenter(""phx"")
        .setSeeds(""cdb03.vip.phx.host.com:9160,cdb04.vip.phx.host.com:9160"")
    )
</code></pre>

<h2>ConnectionPoolType</h2>

<p>You also might want to set ConnectionPoolType to TOKEN_AWARE.  When that value is left unset, it will default to ROUND_ROBIN (using the nodes from the node discovery work described above).  TOKEN_AWARE ConnectionPoolType will ""keep track of which hosts have which tokens and attempt to direct traffic intelligently"".  </p>

<p>I'd do something like this for Astyanax configuration, unless you are providing a HostSupplier.</p>

<pre><code>.withAstyanaxConfiguration(new AstyanaxConfigurationImpl()      
        .setDiscoveryType(NodeDiscoveryType.RING_DESCRIBE)
        .setConnectionPoolType(ConnectionPoolType.TOKEN_AWARE)
    )
</code></pre>

<h2>Pool Optimizations</h2>

<p>Another consideration would be optimizing the pool usage with Astyanax ""latency awareness"" on ConnectionPoolConfigurationImpl, but YMMV on the settings.  e.g. :</p>

<pre><code>.setLatencyScoreStrategy(new SmaLatencyScoreStrategyImpl(10000,10000,100,0.50))
// The constructor takes:
//  UpdateInterval: 10000 : Will resort hosts per token partition every 10 seconds
//  ResetInterval: 10000 : Will clear the latency every 10 seconds
//  WindowSize: 100 : Uses last 100 latency samples
//  BadnessThreshold: 0.50 : Will sort hosts if a host is more than 100% 
</code></pre>

<p>See Astyanax <a href=""https://github.com/Netflix/astyanax/wiki/Configuration"">Configuration</a></p>

<h2>TLDR;</h2>

<p>In summary, set NodeDiscoveryType to RING_DESCRIBE (if you aren't using a HostSupplier) and ConnectionPoolType to TOKEN_AWARE.  Additionally, use setLocalDatacenter to keep requests local to the dc and consider the latency awareness settings.</p>
",['dc']
16532566,16547891,2013-05-13 23:03:28,How to insert a datetime into a Cassandra 1.2 timestamp column,"<p><strong>IMPORTANT</strong>
If you are dealing with this problem today, use the new cassandra-driver from datastax (i.e. import cassandra) since it solves most of this common problems and don't use the old cql driver anymore, it is obsolete! This question is old from before the new driver was even in development and we had to use an incomplete old library called cql (import cql &lt;-- don't use this anymore, move to the new driver).</p>

<p><strong>Intro</strong>
I'm using the python library cql to access a Cassandra 1.2 database. In the database I have a table with a timestamp column and in my Python code I have a datetime to be inserted in the column. Example as follows:</p>

<p><strong>Table</strong></p>

<pre><code>CREATE TABLE test (
     id text PRIMARY KEY,
     last_sent timestamp
);
</code></pre>

<p><strong>The code</strong></p>

<pre><code>import cql
import datetime
...
cql_statement = ""update test set last_sent = :last_sent where id =:id""
rename_dict = {}
rename_dict['id'] = 'someid'
rename_dict['last_sent'] = datetime.datetime.now()
cursor.execute (cql_statement, rename_dict)
</code></pre>

<p><strong>The problem</strong></p>

<p>When I execute the code the actual cql statement executed is like this:</p>

<pre><code>update test set last_sent =2013-05-13 15:12:51 where id = 'someid'
</code></pre>

<p>Then it fails with an error</p>

<pre><code> Bad Request: line 1:XX missing EOF at '-05'
</code></pre>

<p>The problem seems to be that the cql library is not escaping ('') or converting the datetime before running the query.</p>

<p><strong>The question</strong>
What is the correct way of doing this without manually escaping the date and be able to store a full timestamp with more precision into a cassandra timestamp column?</p>

<p>Thanks in advance!</p>
",<python><cassandra><cql>,"<p>Has abhi already stated this can be done using the milliseconds since epoch as a long value from cqlsh, now we need to make it work in the Python code.</p>

<p>When using the cql library this conversion (from datetime to milliseconds since epoch) is not happening so in order to make the update work and still have the precision you need to convert the datetime to milliseconds since epoch.</p>

<p><strong>Source</strong>
Using this useful question: <a href=""https://stackoverflow.com/questions/6999726/python-getting-millis-since-epoch-from-datetime"">Getting millis since epoch from datetime</a> , in particular this functions(note the little change I made):</p>

<p><strong>The solution</strong></p>

<pre><code>import datetime

def unix_time(dt):
    epoch = datetime.datetime.utcfromtimestamp(0)
    delta = dt - epoch
    return delta.total_seconds()

def unix_time_millis(dt):
    return long(unix_time(dt) * 1000.0)
</code></pre>

<p>For this example the code would be:</p>

<pre><code>cql_statement = ""update test set last_sent = :last_sent where id =:id""
rename_dict = {}
rename_dict['id'] = 'someid'
rename_dict['last_sent'] = unix_time_millis(datetime.datetime.now())
cursor.execute (cql_statement, rename_dict)
</code></pre>

<p>You can convert the datetime to a long value containing the number of milliseconds since epoch and that's all, the update is transformed to an equivalent form using a long value for the timestamp.</p>

<p>Hope it helps somebody else</p>
",['precision']
16549833,16585376,2013-05-14 18:01:54,Cassandra Commit and Recovery on a Single Node,"<p>I am a newbie to Cassandra - I have been searching for information related to commits and crash recovery in Cassandra on a single node. And, hoping someone can clarify the details.</p>

<p>I am testing Cassandra - so, set it up on a single node. I am using stresstool on datastax to insert millions of rows. What happens if there is an electrical failure or system shutdown? Will all the data that was in Cassandra's memory get written to disk upon Cassandra restart (I guess commitlog acts as intermediary)? How long is this process?</p>

<p>Thanks!</p>
",<crash><nosql><cassandra><recovery><datastax-enterprise>,"<p>Cassandra's commit log gives Cassandra durable writes.  When you write to Cassandra, the write is appended to the commit log before the write is acknowledged to the client.  This means every write that the client receives a successful response for is guaranteed to be written to the commit log.  The write is also made to the current memtable, which will eventually be written to disk as an SSTable when large enough.  This could be a long time after the write is made.</p>

<p>However, the commit log is not immediately synced to disk for performance reasons.  The default is periodic mode (set by the commitlog_sync param in cassandra.yaml) with a period of 10 seconds (set by commitlog_sync_period_in_ms in cassandra.yaml).  This means the commit log is synced to disk every 10 seconds.  With this behaviour you could lose up to 10 seconds of writes if the server loses power.  If you had multiple nodes in your cluster and used a replication factor of greater than one you would need to lose power to multiple nodes within 10 seconds to lose any data.</p>

<p>If this risk window isn't acceptable, you can use batch mode for the commit log.  This mode won't acknowledge writes to the client until the commit log has been synced to disk.  The time window is set by commitlog_sync_batch_window_in_ms, default is 50 ms.  This will significantly increase your write latency and probably decrease the throughput as well so only use this if the cost of losing a few acknowledged writes is high.  It is especially important to store your commit log on a separate drive when using this mode.</p>

<p>In the event that your server loses power, on startup Cassandra replays the commit log to rebuild its memtable.  This process will take seconds (possibly minutes) on very write heavy servers.</p>

<p>If you want to ensure that the data in the memtables is written to disk you can run 'nodetool flush' (this operates per node).  This will create a new SSTable and delete the commit logs referring to data in the memtables flushed.</p>
",['commitlog_sync']
16765495,16769128,2013-05-27 02:05:07,Cassandra and using cql to INSERT a column,"<p>Trying to understand some fundamentals in Cassandra, I was under the impression that one of the advantages a developer can take in designing a data model is by dynamically adding columns to a row identified by a key. That means I can model my data so that if it makes sense, a key can be something such as a user_id from a relational database, and I can for example, create arbitrary amounts of columns that relate to that user. </p>

<p>What I'm not understanding is why there is so much emphasis to predefined columns in CLQ examples, particularly in the CREATE TABLE/COLUMNFAMILY examples:</p>

<pre><code>CREATE TABLE emp (
  empID int,
  deptID int,
  first_name varchar,
  last_name varchar,
  PRIMARY KEY (empID, deptID)
);
</code></pre>

<p>Wouldn't this type of model make more sense to just stuff into a relational database? What if I don't know my column name until runtime and need to dynamically create it? Do I have to use ALTER TABLE to add a new column to the row using CLQ? The particular app use-case I have in mind I would just need a key identifier and arbitrary column names where the column name might include a timestamp+variable_identifier.</p>

<p>Is Cassandra the right tool for that? Are the predefined columns in documentation nothing more than an example? How does one add a dynamic column name with an existing column family/table?</p>
",<database><cassandra>,"<blockquote>
  <p>Do I have to use ALTER TABLE to add a new column to the row using CLQ?</p>
</blockquote>

<p>Yes, the schema must be defined before you can insert into 'new columns'.
However you could define 1 column that is a collection of data. Look at the 'tag' example in datastax's <a href=""http://www.datastax.com/dev/blog/thrift-to-cql3"">'thrift to cql upgrade'</a> blog under <strong>mixing dynamic and static columns</strong>.</p>

<blockquote>
  <p>How does one add a dynamic column name with an existing column family/table?</p>
</blockquote>

<p>In CQL you have to first alter the structure of the table (column family) using the <code>ALTER</code> keyword. My guess is that this is to ensure that column families contain the specified columns eliminating the chance of a column being added by mistake (better data quality).</p>

<blockquote>
  <p>Is Cassandra the right tool for that?</p>
</blockquote>

<p>I think it is, but if you need to add columns on-the-fly without specifying schema altering statements then you should probably look into thrift based APIs which can do that, but just a friendly warning, datastax advise that new applications use CQL.</p>
",['table']
16794374,16893463,2013-05-28 14:15:20,Setting cassandra column as set<text> gives InvalidRequestException,"<p>I am new to the cassandra world. I created a cassandra table using cqlsh like the following:</p>

<pre><code>CREATE TABLE ""userRecommendations"" (uid text PRIMARY KEY, app set&lt;text&gt;);
</code></pre>

<p>Now, when I use the cassandra-cli, and do a <code>describe keySpace;</code>, I get</p>

<pre><code>[default@keySpace] describe userRecommendations;
ColumnFamily: userRecommendations
  Key Validation Class: org.apache.cassandra.db.marshal.UTF8Type
  Default column value validator: org.apache.cassandra.db.marshal.BytesType
  Columns sorted by: org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.ColumnToCollectionType(617070:org.apache.cassandra.db.marshal.SetType(org.apache.cassandra.db.marshal.UTF8Type)))
  GC grace seconds: 0
  Compaction min/max thresholds: 0/0
  Read repair chance: 0.0
  DC Local Read repair chance: 0.0
  Populate IO Cache on flush: false
  Replicate on write: false
  Caching: keys_only
  Bloom Filter FP chance: default
  Built indexes: []
  Compaction Strategy: null
null
</code></pre>

<p>Then, if I did a GET on the column family I get an Exception.</p>

<pre><code>[default@UserInfo] get userRecommendations[utf8('aparna')][utf8('app')];
Not enough bytes to read value of component 0
InvalidRequestException(why:Not enough bytes to read value of component 0)
    at org.apache.cassandra.thrift.Cassandra$get_result.read(Cassandra.java:6592)
    at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
    at org.apache.cassandra.thrift.Cassandra$Client.recv_get(Cassandra.java:556)
    at org.apache.cassandra.thrift.Cassandra$Client.get(Cassandra.java:541)
    at org.apache.cassandra.cli.CliClient.executeGet(CliClient.java:723)
    at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:210)
    at org.apache.cassandra.cli.CliMain.processStatementInteractive(CliMain.java:210)
    at org.apache.cassandra.cli.CliMain.main(CliMain.java:337)
</code></pre>

<p>What is it that I am doing wrong?</p>

<p>Thanks much in advance!</p>
",<cassandra><cassandra-cli>,"<blockquote>
  <p>What is it that I am doing wrong?</p>
</blockquote>

<p>You are creating a column family using CQL and then trying to access it from the CLI. This doesn't work by design and cannot be done, if you however tried accessing the column family from a java driver that supports CQL, you wouldn't have a problem.</p>

<p>If you want support with thrift for a cql created table try adding the <code>WITH COMPACT STORAGE</code>  statement to your column family declaration:</p>

<pre><code>CREATE TABLE ""userRecommendations"" (
  uid text PRIMARY KEY,
  app set&lt;text&gt;
) WITH COMPACT STORAGE;
</code></pre>
",['table']
16813018,16814122,2013-05-29 11:46:37,need to search data using part of composite row key in cassandra,"<p>I am new to Cassandra and just playing around with it. I have created a Column <code>family</code> having <strong>composite key</strong> and <strong>composite column</strong>. Following is the script for same:</p>

<pre><code>create column family TestCompositeKey with key_validation_class='CompositeType(UTF8Type, TimeUUIDType)' and comparator='CompositeType(UTF8Type, UTF8Type, UTF8Type, UTF8Type)' and default_validation_class='UTF8Type';
</code></pre>

<p>After inserting data in the column family using Hector following is the view I am getting on CLI:</p>

<pre><code>RowKey: AB:e9a87550-c84b-11e2-8236-180373b60c1a
=&gt; (column=0007:TAR:PUB:BD_2013_01_11_0125094813, value=TESTSEARCH, timestamp=1369823914277000)
</code></pre>

<p>Now I want to search for data just by 'AB' given in row key as second part of key will be dynamic. It works fine when I give complete row key. Please tell me how can this be done. I am supplying search criteria on column too along with specifying key.</p>

<p>Thanks
Harish Kumar</p>
",<cassandra>,"<p>You can't do this (efficiently, at least): to lookup by row key you need the whole key.  In general, using TimeUUIDs as row keys should be avoided, unless you have some other table acting as an index to retrieve TimeUUIDs for a query.</p>

<p>If you want to lookup just by the first component of the key you should move the second component to the column composite and just have a single component as the row key.  The definition would be</p>

<pre><code>create column family TestCompositeKey with key_validation_class='UTF8Type' and comparator='CompositeType(TimeUUIDType, UTF8Type, UTF8Type, UTF8Type, UTF8Type)' and default_validation_class='UTF8Type';
</code></pre>

<p>If you used the CQL3 definition:</p>

<pre><code>CREATE TABLE TestCompositeKey (
    a varchar,
    b timeuuid varchar,
    c varchar,
    d varchar,
    e varchar,
    f varchar,
    PRIMARY KEY (a, b, c, d, e, f)
);
</code></pre>

<p>you would get essentially the same schema as I described.  The row key (partition key in CQL language) is a, and the column names are a composite of b:c:d:e:f.</p>
",['table']
16870502,16871984,2013-06-01 08:00:05,How to connect Cassandra using Java class,"<p>I am doing this  to connect cassandra.But my code is returning an error.. Here is my code</p>
<pre><code>public class CassandraConnection {

public static void main(String[] args) {
    String serverIp = &quot;166.78.10.41&quot;;
    String keyspace = &quot;gamma&quot;;
    CassandraConnection connection;

    Cluster cluster = Cluster.builder()
            .addContactPoints(serverIp)
            .build();

    Session session = cluster.connect(keyspace);


    String cqlStatement = &quot;SELECT * FROM TestCF&quot;;
    for (Row row : session.execute(cqlStatement)) {
        System.out.println(row.toString());
    }

}
}
</code></pre>
<p>this is the error log ..</p>
<blockquote>
<p>Failed to execute goal on project CassandraConnection: Could not resolve dependencies for project com.mycompany:CassandraConnection:jar:1.0-SNAPSHOT: The following artifacts could not be resolved: org.specs2:scalaz-effect_2.11.0-SNAPSHOT:jar:7.0.1-SNAPSHOT, org.scalaz:scalaz-effect_2.9.3:jar:7.1.0-SNAPSHOT: Could not find artifact org.specs2:scalaz-effect_2.11.0-SNAPSHOT:jar:7.0.1-SNAPSHOT -&gt; [Help 1]</p>
<p>To see the full stack trace of the errors, re-run Maven with the -e switch.
Re-run Maven using the -X switch to enable full debug logging.</p>
<p>For more information about the errors and possible solutions, please read the following articles: [Help 1] <a href=""http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException"" rel=""noreferrer"">http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException</a></p>
</blockquote>
",<java><cassandra>,"<p>Did you do any research on the matter?</p>
<h2>Picking a driver</h2>
<p>You need a way to communicate with cassandra, best option is to use a high level API. You have a wide range of choices here but when we look at it from a high level prespective there are really two choices.</p>
<ol>
<li><strong>CQL based drivers</strong> - Higher level abstraction of what thrift does. Also the newer tool, companies providing support / documentation for cassandra recommend that new cassandra applications are CQL based.</li>
<li><strong>Thrift based drives</strong> - Have access to low level storage, so its easier to get things wrong.</li>
</ol>
<p>I'll use <a href=""https://github.com/datastax/java-driver"" rel=""noreferrer"">datastax's CQL driver</a>.</p>
<p>Download and build the driver from datastax's github repo <strong>OR</strong> use maven and add the following dependencies:</p>
<pre class=""lang-xml prettyprint-override""><code>&lt;dependency&gt;
  &lt;groupId&gt;com.datastax.cassandra&lt;/groupId&gt;
  &lt;artifactId&gt;cassandra-driver-core&lt;/artifactId&gt;
  &lt;version&gt;2.1.3&lt;/version&gt;
&lt;/dependency&gt;

&lt;dependency&gt;
  &lt;groupId&gt;com.datastax.cassandra&lt;/groupId&gt;
  &lt;artifactId&gt;cassandra-driver-mapping&lt;/artifactId&gt;
  &lt;version&gt;2.1.2&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<p>Picking maven is a good idea, as it will manage all your dependencies for you, but if you dont use maven, at least you will learn about managing jars and reading through stack-traces.</p>
<br/>
<h2>Code</h2>
<p>The <a href=""http://www.datastax.com/doc-source/developer/java-driver/#quick_start/qsQuickstart_c.html"" rel=""noreferrer"">driver's documentation</a> is coming along nicely. If you get stuck read through it, the documentation contains lots of examples.</p>
<p>I'll use the following two variables throughout the examples.</p>
<pre><code>String serverIP = &quot;127.0.0.1&quot;;
String keyspace = &quot;system&quot;;

Cluster cluster = Cluster.builder()
  .addContactPoints(serverIP)
  .build();

Session session = cluster.connect(keyspace);

// you are now connected to the cluster, congrats!
</code></pre>
<p><strong>Read</strong></p>
<pre><code>String cqlStatement = &quot;SELECT * FROM local&quot;;
for (Row row : session.execute(cqlStatement)) {
  System.out.println(row.toString());
}
</code></pre>
<p><strong>Create/Update/Delete</strong></p>
<pre><code>// for all three it works the same way (as a note the 'system' keyspace cant 
// be modified by users so below im using a keyspace name 'exampkeyspace' and
// a table (or columnfamily) called users

String cqlStatementC = &quot;INSERT INTO exampkeyspace.users (username, password) &quot; + 
                      &quot;VALUES ('Serenity', 'fa3dfQefx')&quot;;

String cqlStatementU = &quot;UPDATE exampkeyspace.users &quot; +
                      &quot;SET password = 'zzaEcvAf32hla',&quot; +
                      &quot;WHERE username = 'Serenity';&quot;;

String cqlStatementD = &quot;DELETE FROM exampkeyspace.users &quot; + 
                      &quot;WHERE username = 'Serenity';&quot;;

session.execute(cqlStatementC); // interchangeable, put any of the statements u wish.
</code></pre>
<br/>
<h2>Other useful code</h2>
<p><strong>Creating a Keyspace</strong></p>
<pre><code>String cqlStatement = &quot;CREATE KEYSPACE exampkeyspace WITH &quot; + 
  &quot;replication = {'class':'SimpleStrategy','replication_factor':1}&quot;;

session.execute(cqlStatement);
</code></pre>
<p><strong>Creating a ColumnFamily (aka table)</strong></p>
<pre><code>// based on the above keyspace, we would change the cluster and session as follows:
Cluster cluster = Cluster.builder()
  .addContactPoints(serverIP)
  .build();
Session session = cluster.connect(&quot;exampkeyspace&quot;);

String cqlStatement = &quot;CREATE TABLE users (&quot; + 
                      &quot; username varchar PRIMARY KEY,&quot; + 
                      &quot; password varchar &quot; + 
                      &quot;);&quot;;

session.execute(cqlStatement);
</code></pre>
",['table']
16922951,16927739,2013-06-04 16:27:26,Get last record in Cassandra,"<p>Have a table with about 20 million rows in Cassandra. </p>

<p>The table is ordered by a <code>primary_key</code> column, which is a string. We are using 'ByteOrderedPartitioner', so the rows are ordered by the <code>primary_key</code> and not a hash of the <code>primary_key</code> column.</p>

<p>What is a good way to get the very last record in the table?</p>

<p>Thanks so much!</p>
",<cassandra>,"<p>If for ""very last record"" you mean the one ordered as last I don't think you can do it like a ""GET"", you have to scan rows. The best you can do, afaik, is select a good range to scan (good start key) according to your primary key.</p>

<p>From datastax docs:</p>

<blockquote>
  <p>""Using the ordered partitioner allows ordered scans by primary key.
  This means you can scan rows as though you were moving a cursor
  through a traditional index. For example, if your application has user
  names as the row key, you can scan rows for users whose names fall
  between Jake and Joe. This type of query is not possible using
  randomly partitioned row keys because the keys are stored in the order
  of their MD5 hash (not sequentially).""</p>
</blockquote>

<p>If you find better solution let me know.</p>

<p>Regards,
Carlo</p>
",['partitioner']
16951532,16958558,2013-06-05 23:33:20,Cassandra pagination: How to use get_slice to query a Cassandra 1.2 database from Python using the cql library,"<p>I have a Cassandra 1.2 cluster and I'm using it from Python using the cql library. Now I need to implement some paging functionality that seems pretty straightforward using get_slice, but I can't find any documentation on how to use something like this from the cql library:</p>

<pre><code>get_slice(""key"" : table_key,
      ""column_parent"" : {""column_family"" : ""MyColumnFamily""},
      ""predicate"" :
       { ""slice_range"" : 
 { ""start"" : ""SomeStartID"", 
 ""end"" : ""Z"", 
 ""reverse"" : ""false"", 
 ""count : ""100"" }
 } )
</code></pre>

<p>I've seen this type of syntax on random documentation for get_slice, and it doesn't look like CQL 3 syntax, how can I run this type of queries from Python to a Cassandra 1.2 cluster?, Is this the current way of using get_slice or there is a new syntax or CQL 3 alternative?</p>

<p>Thanks in advance!</p>
",<python><cassandra><cql3>,"<p>You can do paging in much the same way: set a limit and start at a column name greater than the previous one received.  As an example, I created a table test1 in keyspace ks1:</p>

<pre><code>CREATE TABLE test1 (
  a text,
  b text,
  PRIMARY KEY (a, b)
)
</code></pre>

<p>Here a is my row key and b is the column name.  I then inserted 12 records with a=a and b from a to l.  So</p>

<pre><code>cqlsh:ks1&gt; select * from test1;

 a | b
---+---
 a | a
 a | b
 a | c
 a | d
 a | e
 a | f
 a | g
 a | h
 a | i
 a | j
 a | k
 a | l
</code></pre>

<p>Then I paged with this python using the CQL driver:</p>

<pre><code>import cql
con = cql.connect('localhost', keyspace='ks1', cql_version='3.0.0')
cursor = con.cursor()
last = """"
while last != None:
    cursor.execute(""select * from test1 where a=:a and b&gt;:b limit 5"", {""a"": ""a"", ""b"": last})
    last = None
    for row in cursor:
        print row
        last = row[1]
</code></pre>

<p>which pages in batches of 5.  The output is:</p>

<pre><code>[u'a', u'a']
[u'a', u'b']
[u'a', u'c']
[u'a', u'd']
[u'a', u'e']
[u'a', u'f']
[u'a', u'g']
[u'a', u'h']
[u'a', u'i']
[u'a', u'j']
[u'a', u'k']
[u'a', u'l']
</code></pre>
",['table']
17046341,17049100,2013-06-11 14:15:34,Cassandra running out of memory for cql queries,"<p>We have a 32 node Cassandra cluster with around 100Gb per node using Murmur3 partitioner. It has time series data and we have build secondary indexes on two columns to perform range queries. Currently, the cluster is stable with all the data bulk loaded and all the secondary indexes rebuilt. The issue occurs when we are performing range queries  using cql client or hector, just the query for count of rows takes a huge amount of time and it most cases causes nodes to fail due to memory issues. The nodes have 8gb memory, Cassandra MAX Heap is allotted to 4 GB. Has anyone else faced such an issue ? Is there a better way to do count queries ?</p>
",<cassandra><cql><cql3>,"<p>I've had similar issues and most often this can be solved by redesigning the schema bearing in mind the queries that you plan to execute against the data in Cassandra. For a timeseries data it is better to have wide tables with granularity depending on your queries. If your query requires data at a granularity of 1 hour, then it is best to have a wide table with all timestamped data points stored within a single row for every hour so you can get all the required data for 1 hour by reading just 1 row.</p>

<p>Since you say the data is bulk loaded, I am assuming that you may have put all the data into a single table which is why the get_count query is taking an enormous amount of time. We have a a cluster with 8GB RAM but have set the heap size to 3 GB because at 4GB, the RAM utilization is almost always at 8GB [full utilization].</p>
",['table']
17270821,17272429,2013-06-24 08:15:16,"Syntax error at position 7: unexpected ""*"" for `Select * FROM mytable;`","<p>I write because I've a problem with cassandra; after have imported the data from pentaho as show here
<a href=""http://wiki.pentaho.com/display/BAD/Write+Data+To+Cassandra"" rel=""nofollow"">http://wiki.pentaho.com/display/BAD/Write+Data+To+Cassandra</a></p>

<p>when I try to execute the query
Select * FROM mytable;</p>

<p>cassandre give me an error message
Syntax error at position 7: unexpected ""*"" for Select * FROM mytable;.</p>

<p>and don't show the results of query.Why? what does it mean that error?</p>
",<cassandra>,"<blockquote>
  <p>the step that i make are the follow:</p>
  
  <ol>
  <li>start cassandra cli utility;</li>
  <li>use keyspace added from pentaho; (use tpc_h);</li>
  <li>select to show the data added (Select * FROM mytable;)</li>
  </ol>
</blockquote>

<p>The cassandra-cli does not support any CQL version. It has its own <a href=""http://www.datastax.com/docs/1.2/cql_cli/using_cli"" rel=""nofollow noreferrer"">syntax</a> which you can find on datastax's website. </p>

<p>Just for clarity, in cql to select everything from a table (aka column-family) called <em>mytable</em> stored in a keyspace called <em>myks</em> you would use:</p>

<pre><code>SELECT * FROM myks.mytable;
</code></pre>

<p>The equivalent in cassandra-cli would *roughly be :</p>

<pre><code>USE myks;
LIST mytable;
</code></pre>

<p><br/>
***** In the cli you are limited to selecting the first 100 rows. If this is a problem you can use the <code>limit</code> clause to specify how many rows you want:</p>

<pre><code>LIST mytable limit 10000;
</code></pre>

<p>As for this:</p>

<blockquote>
  <p>in cassandra i have read that isn't possible make the join such as sql, ther isn't a shortcut to issue this disadvantage</p>
</blockquote>

<p>There is a reason why joins don't exist in Cassandra, its for the same reason that C* isn't ACID compliant, it sacrifices that functionality for it's amazing performance and scalability, so it's not a disadvantage, you just need to <a href=""http://en.wikipedia.org/wiki/Denormalization"" rel=""nofollow noreferrer"">re-think your model</a> if you need joins. Also take a look at <a href=""https://stackoverflow.com/questions/17248232/how-to-do-a-join-queries-with-2-or-more-tables-in-cassandra-cql"">this question / answer</a>.</p>
",['table']
17361245,17361602,2013-06-28 09:16:06,new column added to columnfamily in cassandra-cli is not visible from cqlsh (cql3),"<p>I defined a table in cql3. Describing it returns the following:</p>

<blockquote>
  <p>CREATE TABLE results ( <br/>
    id uuid PRIMARY KEY,<br/>
    casename text,<br/>
    result text,<br/>
    run int,<br/>
    user text<br/>
  ) WITH ...</p>
</blockquote>

<p>I try to use the fact that this is should not limit the columnfamily and add a new column-value pair:</p>

<pre><code>insert into results (id, sogginess) values(3d6f63a4-c1e0-4dd8-b043-8e0754122f23, 'high') ;
</code></pre>

<p>But this doesn't work. IF I use cassandra-cli, I can add the value of sogginess with the standars set syntax and the row is persisted normally. Even though, switching back to cql3, the select * query does not return this new column and neither describe shows the new column sogginess.</p>

<p>This means that you can't add arbitrary data to the rows in cql3 and you're bound by the schema defined? If not, how can I do it?</p>
",<cassandra><cql3><cassandra-cli><cqlsh>,"<p><strong>TL;DR;</strong><br/>
No, but you can use collections: <code>phonenumbers set&lt;text&gt;</code>
<br/><br/></p>

<p>CQL 3 expects all columns to be defined in the metadata, so yes you are bound to a schema. </p>

<blockquote>
  <p>If not, how can I do it?</p>
</blockquote>

<p>Well, you cant. What you can do is use collections and structure your data model. Adding arbitrary columns via thrift is considered to be <em>bad practice</em>. My advice is read datastax's article on <a href=""http://www.datastax.com/dev/blog/does-cql-support-dynamic-columns-wide-rows"" rel=""nofollow noreferrer"">wide row support in CQL3</a>. Also these questions are related (if not duplicate...) so take a look at them:</p>

<ol>
<li><a href=""https://stackoverflow.com/questions/12792040/does-cql3-require-a-schema-for-cassandra-now"">Does cql3 require a schema for cassandra now</a></li>
<li><a href=""https://stackoverflow.com/questions/14720472/create-a-table-in-cassandra-1-2-with-cql3-where-column-names-will-be-created-at/14729968#14729968"">Create a table in Cassandra 1.2 with CQL3 where column names will be created at runtime</a></li>
</ol>
",['table']
17514757,17522756,2013-07-07 18:02:17,Apache Cassandra. Advantage and disadvantage of Secondary Index,"<p>I have read, that Secondary Index in Cassandra is quite useless feature. Indeed, it makes writing to DB much more slower, you can find value only by exact index and you need to make requests to all servers in claster to find value by index. Can anyone tell me about benifit, that will be the reason to use Secondary Index?</p>
",<indexing><nosql><cassandra>,"<p>Querying becomes more flexible when you add secondary indexes to table columns. You can add indexed columns to the <code>WHERE</code> clause of a <code>SELECT</code>.</p>

<p><strong>When to use secondary indexes</strong><br/>
You want to query on a column that isn't the primary key and isn't part of a composite key. The column you want to be querying on has few unique values (what I mean by this is, say you have a column Town, that is a good choice for secondary indexing because lots of people will be form the same town, date of birth however will not be such a good choice).</p>

<p><strong>When to avoid secondary indexes</strong><br/>
Try not using secondary indexes on columns contain a high count of unique values and that will produce few results.</p>

<p>As always, check out the documentation:</p>

<ul>
<li><a href=""http://www.datastax.com/docs/1.1/ddl/indexes"">About Indexes in Cassandra</a></li>
<li><a href=""http://wiki.apache.org/cassandra/SecondaryIndexes"">FAQ for Secondary Indexes</a></li>
</ul>
",['table']
17620651,17620911,2013-07-12 17:28:06,Python/Django: Create table with two primary_key using cql engine,"<p>I want create a table with two primary_key by it's django model as below:</p>

<pre><code>class UserView(Model):
    email= columns.Text(primary_key=True)
    entryLink= columns.Text(primary_key=True)
    date= columns.Date(default=datetime.date.today())
</code></pre>

<p>but when I want create table as below:</p>

<pre><code>&gt;&gt;&gt; from cqlengine import connection
&gt;&gt;&gt; from cqlengine.management import create_table
&gt;&gt;&gt; from MainAPP.models import UserView
&gt;&gt;&gt; connection.setup(['127.0.0.1:9160'])
&gt;&gt;&gt; create_table(UserView)
</code></pre>

<p>I see this error:</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;console&gt;"", line 1, in &lt;module&gt;
  File ""H:\Web-Programming\Python\Project\Prexter\Virtual-Environment\Lib\site-packages\cqlengine\management.py"", line 97, in create_table
execute(qs)
  File ""H:\Web-Programming\Python\Project\Prexter\Virtual-Environment\Lib\site-packages\cqlengine\connection.py"", line 172, in execute
return connection_pool.execute(query, params)
  File ""H:\Web-Programming\Python\Project\Prexter\Virtual-Environment\Lib\site-packages\cqlengine\connection.py"", line 164, in execute
    raise CQLEngineException(unicode(ex))
CQLEngineException: Bad Request: Missing CLUSTERING ORDER for column entryLink
</code></pre>

<p>When I remove <strong>primary_key</strong> property from <strong>entryLink</strong> field, I have no error! but I want define entryLink as a primary_key! What is my mistake?</p>
",<django><django-models><cassandra><primary-key><cql3>,"<p>A database table cannot have 2 primary key. If you are looking for a Composite Primary Key, <a href=""https://code.djangoproject.com/wiki/MultipleColumnPrimaryKeys#Multi-ColumnPrimaryKeysupport"" rel=""nofollow noreferrer"">Django does not support that yet</a>. </p>

<p>Now, what you <em>might</em> be looking for is <code>unique=True</code> (a candidate key).</p>

<pre><code>class UserView(Model):
    email= columns.Text(primary_key=True)
    entryLink= columns.Text(unique=True)
    date= columns.Date(default=datetime.date.today())
</code></pre>

<p>You can also read <a href=""https://stackoverflow.com/questions/217945/can-i-have-multiple-primary-keys-in-a-single-table"">this post for a better understanding</a></p>
",['table']
17631473,17642791,2013-07-13 15:14:58,Cassandra 1.2: Unable to complete request: one or more nodes were unavailable,"<p>I'm having trouble setting up a new Cassandra cluster. I've set up a 3 node cluster in EC2 (Zone: eu-west-1b). When I try to insert a record into a new table I receive this error message:</p>

<pre><code>cqlsh:test&gt; insert into mytest (id, value) values(1,100);
Unable to complete request: one or more nodes were unavailable.
</code></pre>

<p>I've confirmed that the 3 nodes are up and running:</p>

<pre><code>nodetool status
UN  ***.***.***.***  68.1 KB    256     33.2%  bbf1c5e9-ac68-41a1-81a8-00c7877c4eac  rack1
UN  ***.***.***.***  81.95 KB   256     34.1%  e118e3a7-2486-4c08-8ba1-d337888ff59c  rack1
UN  ***.***.***.***   68.12 KB   256     32.7%  041cb88e-df21-4640-b7ac-7a87fd38dae6  rack1
</code></pre>

<p>The commands I used to create the keyspace and table are:</p>

<pre><code>create keyspace test with replication ={'class':'NetworkTopologyStrategy', 'eu-west-1b': 2};
use test;
create table mytest (id int primary key, value int);
insert into mytest (id, value) values(1,100);
</code></pre>

<p>Each node can see the keyspace - I used CQLSH and ran descibe keyspace and got this output from each node:</p>

<pre><code>CREATE KEYSPACE test WITH replication = {
  'class': 'NetworkTopologyStrategy',
  'eu-west-1b': '2'
};

USE test;

CREATE TABLE mytest (
  id int PRIMARY KEY,
  value int
) WITH
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'SnappyCompressor'};
</code></pre>
",<amazon-ec2><cassandra>,"<p>I finally tracked down the problem - I had set the endpoint_snitch to Ec2Snitch, but the default Datastax was set beneath the comments (which I hadn't noticed). I commented out the DS snitch, restarted the dse service on all nodes and ran nodetool repair on each node and the problem went away.</p>
",['endpoint_snitch']
17705328,17711444,2013-07-17 16:29:29,How Cassandra stores multicolumn primary key (CQL),"<p>I have a little misunderstanding about composite row keys with CQL in Cassandra.
Let's say I have the following</p>

<pre><code>cqlsh:testcql&gt; CREATE TABLE Note (
           ... key int,
           ... user text,
           ... name text
           ... , PRIMARY KEY (key, user)
           ... );
cqlsh:testcql&gt; INSERT INTO Note (key, user, name) VALUES (1, 'user1', 'name1');
cqlsh:testcql&gt; INSERT INTO Note (key, user, name) VALUES (1, 'user2', 'name1');
cqlsh:testcql&gt;
cqlsh:testcql&gt; SELECT * FROM Note;

 key | user  | name
-----+-------+-------
   1 | user1 | name1
   1 | user2 | name1
</code></pre>

<p>How this data is stored? Are there 2 rows or one. </p>

<p>If two then how it is possible to have more than one row with the same key?
If one then having records with key=1 and user from ""user1"" to ""user1000"" does it mean it will have one row with key=1 and 1000 columns containing names for each user?</p>

<p>Can someone explain what's going on on the background? Thanks.</p>
",<cassandra><cql>,"<p>So, after diging a bit more and <a href=""http://www.planetcassandra.org/blog/post/composite-keys-in-apache---cassandra"" rel=""nofollow noreferrer"">reading an article</a> suggested by <a href=""https://stackoverflow.com/users/1159472/lyuben-todorov"">Lyuben Todorov</a> (thank you) I found the answer to my question.</p>

<p>Cassandra stores data in data structures called rows which is totally different than relational databases. Rows have a unique key. </p>

<p>Now, what's happening in my example... In table <code>Note</code> I have a composite key defined as <code>PRIMARY KEY (key, user)</code>. Only the first element of this key acts as a row key and it's called partition key. Internally the rest of this key is used to build a composite columns. </p>

<p>In my example </p>

<pre><code> key | user  | name
-----+-------+-------
   1 | user1 | name1
   1 | user2 | name1
</code></pre>

<p>This will be represented in Cassandra in one row as</p>

<pre><code>-------------------------------------
|   | user1:name    | user2:name    |
| 1 |--------------------------------
|   | name1         | name1         |
-------------------------------------
</code></pre>

<p>Having know that it's clear that it's not a good idea to add any column with huge amount of unique values (and growing) to the composite key because it will be stored in one row. Even worse if you have multiple columns like this in a composite primary key. </p>

<p><strong>Update</strong>: Later I found <a href=""http://planetcassandra.org/blog/post/primary-keys-in-cql/"" rel=""nofollow noreferrer"">this blog post by Aaron Morton</a> than explains the same in more details.</p>
",['table']
17719316,17723938,2013-07-18 09:16:41,Cassandra: How to model column family to perform intersect-like queries,"<p>Basically I have three attributes: <code>partId</code>, <code>measurementDef</code> and <code>value</code>.
Each part (partId) consists of multiple measures (value) of a certain type (measurementDef).</p>

<p>Formatted as a tree it would look something like this:</p>

<pre><code>-part 1
  |- measurementDef 1 -&gt; 15,86
  |- measurementDef 2 -&gt; 19,54
-part 2
  |- measurementDef 1 -&gt; 21,21
  |- measurementDef 3 -&gt; 65,54
  |- measurementDef 4 -&gt; 12,54
-part 3
   ...
</code></pre>

<p>Now my question is: How should I model my column family to do something like this:</p>

<pre><code>SELECT partId
FROM &lt;table&gt;
WHERE measurementDef = xxx AND value &gt; 10
INTERSECT
SELECT partId
FROM &lt;table&gt;
WHERE measurementDef = yyy AND value &lt; 50
</code></pre>

<p>In other words: I want to find all parts, whose value for measurementDef xxx is higher 10 and whose value for measurementDef yyy is lower 50.</p>
",<database-design><cassandra><intersect><cql3>,"<p>AFAIK, there is no modelling approach to make intersection within single query. I suggest to use following table design:</p>

<pre><code>create table mdefparts(
    mdef int,
    value float,
    parts set&lt;uuid&gt;,
    primary key(mdef, value)
);
</code></pre>

<p>Then use queries:</p>

<pre><code>select parts from mdefparts where mdef=XXX and value &gt; 10;
select parts from mdefparts where mdef=YYY and value &lt; 50;
</code></pre>

<p>Then join all sets from the first query into one set (say, set1). </p>

<p>Join all sets from the second query into set2. </p>

<p>Then just intersect set1 and set2.</p>
",['table']
17857918,17874169,2013-07-25 12:22:36,Secondary index in cassandra,"<p>In my application I have lists which have items in them, they would look like that</p>

<pre><code>1. list uuid: b1d19224-ebcc-4f69-a98e-4096a4b28121
 1. item
 2. item
 3. item

2. list uuid: 54b17b3a-5d83-4aec-9e7e-16bff1ba336b
 1. item
</code></pre>

<p>Those items are indexed by there numbers. What I would like to do is add items to those lists, but not just at the end of the list but sometimes also after a specific item for example after the first item.</p>

<p>The way I thought of doing that is by giving those items a unique id looking like that: <code>(uuid of list).(number of item)</code> for example <code>b1d19224-ebcc-4f69-a98e-4096a4b28121.1</code>. So every time I would like to add a new item it's either I would add it to the end of the list or after some item giving the rest of the items after that new an index+1 for example <code>(uuid of list).(number+1)</code>.</p>

<ul>
<li>Is there another way of accomplishing that, or should I do it like that?</li>
</ul>
",<cassandra>,"<p>If you want to insert your items in your lists sorted on the unique item number, you should use CQL3 based composite primary keyed column family.   </p>

<pre><code>     create table list (
         partkey varchar,
         item_num int,
         id varchar,
         data varchar,
         PRIMARY KEY (partkey, item_num)
    ) with clustering order by (item_num desc);
</code></pre>

<p>Where the first part of primary key would server as the partition key and the second one serves as the sorting value. Have a look at the following link :  </p>

<p><a href=""http://rollerweblogger.org/roller/entry/composite_keys_in_cassandra"" rel=""nofollow"">http://rollerweblogger.org/roller/entry/composite_keys_in_cassandra</a><a href=""http://rollerweblogger.org/roller/entry/composite_keys_in_cassandra"" rel=""nofollow""></a></p>
",['table']
18082930,18092262,2013-08-06 14:24:36,Cassandra List Column Names,"<p>I'm storing photos in a list cql3 column. I can query the list easily from cql3 but I also need to understand how the Cassandra storage model deals with lists to be able to use the JMX bulkLoad service to get my data into Cassandra. If I insert some test data into a list like this:</p>

<pre><code>insert into dat.lgr (id, photos) values (0, [0xaa, 0xbb]);
</code></pre>

<p>The resulting data, when queried with the cli looks like this:</p>

<pre><code>=&gt; (column=photos:2fce75c0fe9811e2ab248b7126053a99, value=aa, timestamp=1375794036508000)
=&gt; (column=photos:2fce75c1fe9811e2ab248b7126053a99, value=bb, timestamp=1375794036508000)
</code></pre>

<p>So it looks like Cassandra is actually storing a column for each element in the list, identified by a composite column name consisting of the collection name and an unknown hex number. The number is likely a 64 bit hash, or two 32 bit hashes appended together. But what's been hashed? I've looked through the source code but found nothing. Any help appreciated.</p>
",<cassandra>,"<p>I'd suggest that column names for list items are UUIDs. At least both these values represent valid date ""Tuesday, August 6, 2013 1:00:36 PM GMT"" (try """"2fce75c0-fe98-11e2-ab24-8b7126053a99"" in <a href=""http://www.famkruithof.net/uuid/uuidgen"" rel=""nofollow"">http://www.famkruithof.net/uuid/uuidgen</a> for example).</p>

<p>It's easy to verify - just truncate the table and repeat the same statement. You would get completely different column names for the same data if my guess is correct. </p>
",['table']
18106043,18106777,2013-08-07 14:25:38,Is there a way to discover Cassandra CQL table structure?,"<p>Let's say I use CQL to define this table.</p>

<pre><code>CREATE TABLE songs (
    id uuid PRIMARY KEY, 
    title text,
    album text, 
    artist text, 
    tags set&lt;text&gt;, 
    data blob);
</code></pre>

<p>How can other developers (or myself after a few weeks) (re)discover the layout of this table?</p>

<p>I'm thinking of an equivalent to the MySQL <code>DESCRIBE {tablename}</code> command.</p>

<p><strong>[EDIT]</strong></p>

<p>I see there is a <code>DESCRIBE</code> method in Cassandra's command line interface (CLI), but upon using it, it states that it doesn't include information on CQL tables in its results.</p>
",<cassandra><cql>,"<p>You should try the <code>cqlsh</code> tool which will show you exactly what you want:</p>

<pre><code>lyubent@vm: ~$ ./cqlsh 
cqlsh&gt; use system;
cqlsh&gt; describe columnfamily local;

CREATE TABLE local (
  key text PRIMARY KEY,
  bootstrapped text,
  cluster_name text,
  cql_version text,
  data_center text,
  gossip_generation int,
  host_id uuid,
  partitioner text,
  rack text,
  release_version text,
  schema_version uuid,
  thrift_version text,
  tokens set&lt;text&gt;,
  truncated_at map&lt;uuid, blob&gt;
) WITH
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='information about the local node' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=0 AND
  read_repair_chance=0.000000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'SnappyCompressor'};
</code></pre>

<p><strong>EDIT</strong><br/>
Although great at the time the blog i linked is ood. To run cqlsh in windows:</p>

<ul>
<li>first install python 2.7.x (not python 3!)
<a href=""http://www.python.org/getit/""><strong>download</strong></a><br/> </li>
<li>Add python to your path (as a new environment variable)</li>
<li><p>Run the setup by navigating to
<code>C:\dir\to\cassandra\pylib</code> in a cmd prompt and executing the below
line:</p>

<pre><code>python setup.py install
</code></pre></li>
</ul>

<p>GZ. Now you have cqlsh on windows.</p>
","['cluster_name', 'partitioner', 'rack']"
18112384,18113076,2013-08-07 19:41:45,How to rename table in Cassandra CQL3,"<p>I'm trying to rename table created via CQLSH. 
E.g. rename table ""AAA"" to ""BBB"". Can't find any command to do so. Any ideas?</p>

<p>Using [cqlsh 3.1.6 | Cassandra 1.2.8 | CQL spec 3.0.0 | Thrift protocol 19.36.0]</p>
",<cassandra><cql3>,"<p>I don't believe you can rename tables or keyspaces, there's no CQL3 operation to do it, and nothing in the old Thirft interfaces either, if I remember correctly.</p>

<p>One reason why you can't is that it would be an extremely hard thing for Cassandra to do due to its distributed nature, the change can't be done atomically so the cluster would be in an inconsistent state, and most likely updates would be lost. It's similar to creating and dropping tables, but in those cases it's expected that updates will be lost if they're issued before the table is created or after it has been dropped.</p>

<p>The only way that I know of to do what you ask is to create the new table and move all the data from the old to the new, then drop the old table. There might be a way to do it without moving the data, but it would probably require you to stop the cluster and change the name of all directories and files belonging to the table, and also change the metadata in the <code>system.schema_columnfamilies</code> table (but I'm not sure you can even do that).</p>
",['table']
18158441,18160006,2013-08-10 03:44:18,"Cassandra: Design Data Model for User, Roles and Permissions","<p>I have a requirement of designing Data Model in cassandra for User, Role, Organization and permissions.</p>

<ol>
<li>Each Organization can have Users and Roles </li>
<li>Each User can belong to number of Roles </li>
<li>Each Role can have number of Users and Permissions.</li>
</ol>

<p>So, based on the above design requirement, following will be my queries:</p>

<ol>
<li>For an Organization get all Users / Roles</li>
<li>For a User get all Roles</li>
<li>For a Role get all Users / Permissions</li>
</ol>

<p>Can anybody please help me in designing the data model for the above requirement.</p>
",<cassandra><cql3>,"<p>Cassandra does not have foreign key relationships like a relational database does (see <a href=""http://www.datastax.com/documentation/cassandra/1.2/webhelp/index.html?pagename=docs&amp;version=1.2&amp;file=ddl/index#cassandra/ddl/ddl_anatomy_table_c.html#concept_ds_qqw_1dy_zj"">here</a> and <a href=""http://www.datastax.com/docs/1.0/ddl/data_model_planning#denormalize-to-optimize"">here</a>), which means you cannot join multiple column families to satisfy a given query request.<br>
Here are the two possible solutions:</p>

<hr>

<p><strong>Solution 1: denormalize organizatios and users.</strong></p>

<p>Simply create a <code>Users</code> table (i.e., a denormalized table) similar to the following:</p>

<pre><code>create table Users (
        ... organization ascii,
        ... uid int primary key,
        ... role set&lt;ascii&gt;,
        ... permission set&lt;ascii&gt;);
</code></pre>

<p>and create an index for organization to allow querying on non-key column:</p>

<pre><code>create index demo_users_organization on users (organization);
</code></pre>

<p>This can satisfy the first two of your requirements:</p>

<p><em>Query 1 --- For an organization, get all users / roles:</em></p>

<pre><code>cqlsh:demo&gt; select * from users where organization='stack overflow';

 uid | organization   | permission                               | role
-----+----------------+------------------------------------------+-----------------------------
   1 | stack overflow |                     {down-vote, up-vote} |                  {end user}
   2 | stack overflow |         {close-vote, down-vote, up-vote} |       {end user, moderator}
   3 | stack overflow | {close-vote, down-vote, reboot, up-vote} | {end user, moderator, root}
</code></pre>

<p><em>Query 2 --- For a user get all roles</em></p>

<pre><code>cqlsh:demo&gt; select role from users where uid = 2;

 role
-----------------------
 {end user, moderator}
</code></pre>

<p>However, since index on collections are not yet supported, this denormalized table cannot handle your third requirement:</p>

<pre><code>cqlsh:demo&gt; create index demo_users_role on users (role);
Bad Request: Indexes on collections are no yet supported
</code></pre>

<hr>

<p><strong>Solution 2: denormalize organizations, users, and roles.</strong></p>

<p>One work-around for Solution 1 is to further denormalize the user and role where each (user, role) pair has a row in the table:</p>

<pre><code>cqlsh:demo&gt; create table RoleUsers (
    ... uid int,
    ... organization ascii,
    ... role ascii,
    ... permission set&lt;ascii&gt;,
    ... primary key(uid, role));
</code></pre>

<p>and again, create an index for <code>organization</code>.</p>

<p>Here are the example rows:</p>

<pre><code> uid | role      | organization   | permission
-----+-----------+----------------+------------------------------------------
   1 |  end user | stack overflow |                     {down-vote, up-vote}
   2 |  end user | stack overflow |         {close-vote, down-vote, up-vote}
   2 | moderator | stack overflow |         {close-vote, down-vote, up-vote}
   3 |  end user | stack overflow | {close-vote, down-vote, reboot, up-vote}
   3 | moderator | stack overflow | {close-vote, down-vote, reboot, up-vote}
   3 |      root | stack overflow | {close-vote, down-vote, reboot, up-vote}
</code></pre>

<p>Now, you are able to perform the third query.</p>

<p><em>Query 3 --- For a Role get all Users / Permissions:</em></p>

<pre><code>cqlsh:demo&gt; select uid from roleusers where role='moderator' allow filtering;

 uid | permission
-----+------------------------------------------
   2 |         {close-vote, down-vote, up-vote}
   3 | {close-vote, down-vote, reboot, up-vote}
</code></pre>
",['table']
18194891,18201800,2013-08-12 19:14:32,Cassandra: Which schema for table-like mappings?,"<p>I have tried different approaches but I can't find a solution to my problem:
My data is table-like, meaning I have one data point (float) for each combination of inputs from a set of Strings:</p>

<pre><code>(a mapping of S × S → ℝ )
</code></pre>

<p>I want to model the schema so that I can do the following lookups:</p>

<ul>
<li>all pairs of strings with a value in a certain range</li>
<li>for a given input String, all Strings for which the mapped value is in certain range</li>
<li>for a given combination of input Strings the mapped value</li>
</ul>

<p>Since the mapping is symmetrical (<code>m(x,y) == m(y,x)</code> ), it would be great if I only had to store the<br>
<code>n*(n+1) / 2</code> unique values instead of the <code>n^2</code> total mappings.</p>

<p>What I have tried so far:</p>

<ol>
<li>S1+"" ""+S2 as row key and the value as column name</li>
<li>S1 as row key and a Composite key of [S2:value] as column name</li>
<li>S1 as row key, S2 as column name, value as column value.</li>
</ol>

<p>but unfortunately, all these approaches don't let me do all the queries I need.
Is this even possible in Cassandra?</p>
",<database-design><cassandra><schema>,"<p>Cassandra does not support your first query --- <em>all pairs of strings with a value in a certain range</em> --- since currently, Cassandra only allows range queries with at least one <code>EQ</code> on the <code>WHERE</code> clause.  However, your second and third queries is doable :)</p>

<hr>

<p><strong>Example</strong></p>

<p>Consider the following example:</p>

<pre><code>cqlsh:so&gt; desc table string_mappings;
CREATE TABLE string_mappings (
  s1 ascii,
  s2 ascii,
  value float,
  PRIMARY KEY (s1, s2, value)
) 
</code></pre>

<p>and we have the following tuples:</p>

<pre><code>cqlsh:so&gt; select * from string_mappings;

 s1    | s2    | value
-------+-------+-------
 hello | hello |     1
 hello | world |   0.2
 stack | hello |     0
 stack | stack |     1
 stack | world |     0
 world | world |     1
</code></pre>

<p>Your first query does not work as Cassandra currently not support range queries without an <code>EQ</code> on the <code>WHERE</code> clause:</p>

<pre><code>cqlsh:so&gt; select * from string_mappings where value&gt;0.5;
Bad Request: PRIMARY KEY part value cannot be restricted (preceding part s2 is either not restricted or by a non-EQ relation)
</code></pre>

<p>However, the following range query (your second query) is fine since it has an <code>EQ</code>:</p>

<pre><code>cqlsh:so&gt; select * from string_mappings where value &gt; 0.5 and s2='hello' allow filtering;

 s1    | s2    | value
-------+-------+-------
 hello | hello |     1
</code></pre>

<p>and remember to put the <code>ALLOW FILTERING</code> keyword, or you will get the following error:</p>

<pre><code>cqlsh:so&gt; select * from string_mappings where value &gt; 0.5 and s2='hello';
Bad Request: Cannot execute this query as it might involve data filtering and thus may have unpredictable performance. If you want to execute this query despite the performance unpredictability, use ALLOW FILTERING
</code></pre>

<p>Finally, your third query is also not a problem :)</p>

<pre><code>cqlsh:so&gt; select * from string_mappings where S1='hello' and S2='world';

 s1    | s2    | value
-------+-------+-------
 hello | world |   0.2
</code></pre>
",['table']
18268201,18289095,2013-08-16 07:38:10,Cassandra - CqlEngine - using collection,"<p>I want to know how I can work with collection in cqlengine
I can insert value to list but just one value so I can't append some value to my list
I want to do this:
In CQL3:    </p>

<pre><code>UPDATE users
SET top_places = [ 'the shire' ] + top_places WHERE user_id = 'frodo';
</code></pre>

<p>In CqlEngine:</p>

<pre><code>connection.setup(['127.0.0.1:9160'])
TestModel.create(id=1,field1 = [2])
</code></pre>

<p>this code will add 2 to my list but when I insert new value it replace by old value in list.</p>

<p>The only help in Cqlengine :
  <a href=""https://cqlengine.readthedocs.org/en/latest/topics/columns.html#collection-type-columns"" rel=""nofollow"">https://cqlengine.readthedocs.org/en/latest/topics/columns.html#collection-type-columns</a></p>

<p>And I want to know that how I can Read collection field by cqlengine.
Is it an dictionary in my django project? how I can use it?!!</p>

<p>Please help.
Thanks</p>
",<collections><cassandra><cql>,"<p>Looking at your example it's a list. </p>

<p>Given a table based on the Cassandra CQL documentation:</p>

<pre><code>CREATE TABLE plays (
    id text PRIMARY KEY,
    game text,
    players int,
    scores list&lt;int&gt;
)
</code></pre>

<p>You have to declare model like this:</p>

<pre><code>class Plays(Model):
        id = columns.Text(primary_key=True)
        game = columns.Text()
        players = columns.Integer()
        scores = columns.List(columns.Integer())
</code></pre>

<p>You can create a new entry like this (omitting the code how to connect):</p>

<pre><code>Plays.create(id = '123-afde', game = 'quake', players = 3, scores = [1, 2, 3])
</code></pre>

<p>Then to update the list of scores one does:</p>

<pre><code>play = Plays.objects.filter(id = '123-afde').get()
play.scores.append(20) # &lt;- this will add a new entry at the end of the list
play.save()            # &lt;- this will propagate the update to Cassandra - don't forget it
</code></pre>

<p>Now if you query your data with the CQL client you should see new values:</p>

<pre><code> id       | game  | players | scores
----------+-------+---------+---------------
 123-afde | quake |       3 | [1, 2, 3, 20]
</code></pre>

<p>To get the values in python you can simply use an index of an array:</p>

<pre><code>print ""Length is %(len)s and 3rd element is %(val)d"" %\
 { ""len"" : len(play.scores), ""val"": play.scores[2] }
</code></pre>
",['table']
18274007,18276936,2013-08-16 13:04:02,Select 2000 most recent log entries in cassandra table using CQL (Latest version),"<p>How do you query and filter by timeuuid, ie assuming you have a table with</p>

<pre><code>create table mystuff(uuid timeuuid primary key, stuff text);
</code></pre>

<p>ie how do you do:</p>

<pre><code>select uuid, unixTimestampOf(uuid), stuff
from mystuff
order by uuid desc
limit 2000
</code></pre>

<p>I also want to be able to fetch the next older 2000 and so on, but thats a different problem. The error is:</p>

<pre><code>Bad Request: ORDER BY is only supported when the partition key is restricted by an EQ or an IN.
</code></pre>

<p>and just in case it matters, the real table is actually this:</p>

<pre><code>CREATE TABLE audit_event (
  uuid timeuuid PRIMARY KEY,
  event_time bigint,
  ip text,
  level text,
  message text,
  person_uuid timeuuid
) WITH
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'SnappyCompressor'};
</code></pre>
",<cassandra><cql>,"<p>I would recommend that you design your table a bit differently. It would be rather hard to achieve what you're asking for with the design you have currently.</p>

<p>At the moment each of your entries in the <code>audit_event</code> table will receive another <code>uuid</code>, internally Cassandra will create many short rows. Querying for such rows is inefficient, and additionally they are ordered randomly (unless using  Byte Ordered Partitioner, which you should avoid <a href=""http://www.datastax.com/documentation/cassandra/1.2/webhelp/index.html#cassandra/architecture/architecturePlanningAntiPatterns_c.html"" rel=""noreferrer"">for good reasons</a>).</p>

<p>However Cassandra is pretty good at sorting columns. If (back to your example) you declared your table like this :</p>

<pre><code>CREATE TABLE mystuff(
  yymmddhh varchar, 
  created timeuuid,  
  stuff text, 
  PRIMARY KEY(yymmddhh, created)
);
</code></pre>

<p>Cassandra internally would create a row, where the key would be the hour of a day, column names would be the actual created timestamp and data would be the stuff. That would make it efficient to query.</p>

<p>Consider you have following data (to make it easier I won't go to 2k records, but the idea is the same):</p>

<pre><code>insert into mystuff(yymmddhh, created, stuff) VALUES ('13081615', now(), '90');
insert into mystuff(yymmddhh, created, stuff) VALUES ('13081615', now(), '91');
insert into mystuff(yymmddhh, created, stuff) VALUES ('13081615', now(), '92');
insert into mystuff(yymmddhh, created, stuff) VALUES ('13081615', now(), '93');
insert into mystuff(yymmddhh, created, stuff) VALUES ('13081615', now(), '94');
insert into mystuff(yymmddhh, created, stuff) VALUES ('13081616', now(), '95');
insert into mystuff(yymmddhh, created, stuff) VALUES ('13081616', now(), '96');
insert into mystuff(yymmddhh, created, stuff) VALUES ('13081616', now(), '97');
insert into mystuff(yymmddhh, created, stuff) VALUES ('13081616', now(), '98');
</code></pre>

<p>Now lets say that we want to select last two entries (let's a assume for the moment that we know that the ""latest"" row key to be '13081616'), you can do it by executing query like this:</p>

<pre><code>SELECT * FROM mystuff WHERE yymmddhh = '13081616' ORDER BY created DESC LIMIT 2 ;
</code></pre>

<p>which should give you something like this:</p>

<pre><code> yymmddhh | created                              | stuff
----------+--------------------------------------+-------
 13081616 | 547fe280-067e-11e3-8751-97db6b0653ce |    98
 13081616 | 547f4640-067e-11e3-8751-97db6b0653ce |    97
</code></pre>

<p>to get next 2 rows you have to take the last value from the <code>created</code> column and use it for the next query:</p>

<pre><code>SELECT * FROM mystuff WHERE  yymmddhh = '13081616' 
AND created &lt; 547f4640-067e-11e3-8751-97db6b0653ce 
ORDER BY created DESC LIMIT 2 ;
</code></pre>

<p>If you received less rows than expected you should change your row key to another hour.</p>

<h2>Row key handling / calculation</h2>

<p>For now I've assumed that we know the row key with which we want to query the data. If you log a lot of information I'd say that's not the problem - you can take just current time and issue a query with the hour set to what hour we have now. If we run out of rows we can subtract one hour and issue another query.</p>

<p>However if you don't know where your data lies, or if it's not distributed evenly, you can create metadata table, where you'd store the information about the row keys:</p>

<pre><code>CREATE TABLE mystuff_metadata(
  yyyy varchar, 
  yymmddhh varchar, 
  PRIMARY KEY(yyyy, yymmddhh)
) WITH COMPACT STORAGE;
</code></pre>

<p>The row keys would be organized by a year, so to get the latest row key from the current year you'd have to issue a query:</p>

<pre><code>SELECT yymmddhh 
FROM  mystuff_metadata where yyyy = '2013' 
ORDER BY yymmddhh DESC LIMIT 1;
</code></pre>

<p>Your audit software would have to make an entry to that table on start and later on each hour change (for example before inserting data to <code>mystuff</code>).</p>
",['table']
18280748,18288347,2013-08-16 19:28:42,Revoke cassandra accidental update,"<p>Is there a way to revoke a columnfamily update command? I tried to update a column but ended up with ""update columnfamily dev ; "" and now i see only the ids when I query. But the data seem to exist there if I run a nodetool status. I tried to restore a snapshot but even that did not help. </p>
",<cassandra><cql>,"<p>So if I get you correctly you've erased your column metadata and you now get something like this:</p>

<pre><code>cqlsh:test&gt; select * from user;

 uuid
--------------------------------------
 fd24b190-072d-11e3-a1c4-97db6b0653ce
 054a43d0-072e-11e3-a1c4-97db6b0653ce
 0aa71920-072e-11e3-a1c4-97db6b0653ce
 07fda400-072e-11e3-a1c4-97db6b0653ce
</code></pre>

<p>while you wanted something like this:</p>

<pre><code> uuid                                 | email                | name
--------------------------------------+----------------------+-------
 fd24b190-072d-11e3-a1c4-97db6b0653ce | user0@somedomain.com | User0
 054a43d0-072e-11e3-a1c4-97db6b0653ce | user1@somedomain.com | User1
 0aa71920-072e-11e3-a1c4-97db6b0653ce | user3@somedomain.com | User3
 07fda400-072e-11e3-a1c4-97db6b0653ce | user2@somedomain.com | User2
</code></pre>

<p>You can get the data back by adding the information about the columns.</p>

<p>Given the original table was defined like this:</p>

<pre><code>CREATE TABLE user(
  uuid timeuuid PRIMARY KEY, 
  name varchar, 
  email varchar
);
</code></pre>

<p>You can add missing column information using CQL:</p>

<pre><code>cqlsh:test&gt; ALTER TABLE user ADD email varchar;                  
cqlsh:test&gt; ALTER TABLE user ADD name varchar;
</code></pre>
",['table']
18312977,18313314,2013-08-19 11:45:04,How to structure friend relation in cassandra,"<p>I have a user table in Cassandra currently. </p>

<p>I want To create a friend relation table but i am not sure what is the best way to do it.
First i thought put like key as current user and column as friend but than i wanted to achieve the functionality of sending friend request so i think there should be another object called accepted which takes care of that two persons are real friends or not or eather the other person accepted the friend request or not.
I think it will be Many to Many table. For example in a friendship relation.</p>

<pre><code>Friendship: {
John Dow: {
10: Mark Seldon,
8: Julian Hendrix,
...
},
Mark Seldon: {
9: John Dow,
...
},
...
}
</code></pre>

<p>Any directions is appreciated.</p>
",<cassandra><cassandra-0.7>,"<p>That seems to be possible by creating a new table ""friends"", where you could (for example) use a ""composite key"" {userid, friendid}, in CQL:</p>

<pre><code>CREATE TABLE friends(    
userid varchar,
friendid varchar,
PRIMARY KEY (userid, friendid)
);
</code></pre>

<p>which would define a ""friendship"" relation between two users (both with an entry in the Users table)... This would allow you to query several things:</p>

<ul>
<li>get all friend of ""userid""</li>
<li>check if userid and ""user x"" (with a given friendid) exist</li>
</ul>

<p>This would also mean that whenever you store a relationship, you need to create two ""entries"" in the table: {user1, user2} and {user2, user1}...</p>
",['table']
18398987,20334000,2013-08-23 09:07:18,How to reset a lost Cassandra admin user's password?,"<p>I have full access to the Cassandra installation files and a PasswordAuthenticator configured in <code>cassandra.yaml</code>. What do I have to do to reset admin user's password that has been lost, while keeping the existing databases intact?</p>
",<cassandra><cassandra-2.0>,"<p>Solved with the following steps:</p>

<ol>
<li>Change authenticator in <code>cassandra.yaml</code> to AllowAllAuthenticator and restart Cassandra</li>
<li><code>cqlsh</code></li>
<li><code>update system_auth.credentials set salted_hash='$2a$10$vbfmLdkQdUz3Rmw.fF7Ygu6GuphqHndpJKTvElqAciUJ4SZ3pwquu' where username='cassandra';</code></li>
<li>Exit <code>cqlsh</code></li>
<li>Change authenticator back to PasswordAuthenticator and restart Cassandra</li>
</ol>

<p>Now you can log in with</p>

<pre><code>cqlsh -u cassandra -p cassandra
</code></pre>

<p>and change the password to something else.</p>
",['authenticator']
18421668,18441094,2013-08-24 18:16:47,Alter cassandra column family primary key using cassandra-cli or CQL,"<p>I am using Cassandra 1.2.5. After creating a column family in Cassandra using cassandra-cli, is it possible to modify the primary key on the column family using either cassandra-cli or CQL? </p>

<p>Specifically, I currently have the following table (from CQL):</p>

<pre><code>CREATE TABLE ""table1"" (
  key blob,
  column1 blob,
  value blob,
  PRIMARY KEY (key, column1)
);
</code></pre>

<p>I would like the table to be as follows, without having to drop and recreate the table:</p>

<pre><code>CREATE TABLE ""table1"" (
  key blob,
  column1 blob,
  value blob,
  PRIMARY KEY (key)
);
</code></pre>

<p>Is this possible through either cassandra-cli or CQL?</p>
",<cassandra><cql><cql3><cassandra-cli><cqlsh>,"<p>The primary keys directly determine how and where cassandra stores the data contained in a table (column family). The primary key consists of partition key and clustering key (optional). </p>

<blockquote>
  <p>The partition key determines which node stores the data. It is responsible for data distribution across the nodes. The additional columns determine per-partition clustering (see <a href=""http://docs.datastax.com/en/cql/3.1/cql/ddl/ddl_compound_keys_c.html"" rel=""nofollow"">compound key documentation</a>).</p>
</blockquote>

<p>So changing the primary key will always require all data to be migrated. I do not think that either cqlsh or cassandra-cli have a command for this (as of 2015)..</p>
",['table']
18422846,18443608,2013-08-24 20:31:56,How to get the data from the Cassandra every 15 minutes but return me only the information that got changed?,"<p>I have a Column family in Cassandra in which I am going to store something like this-</p>

<pre><code>BundleName    |     Version
----------------------------
FrameworkBundle    1.0.0
BundleA            1.0.0
BundleB            1.0.0
BundleC            1.0.0
BundleD            1.0.0
</code></pre>

<p>I am using Astyanax client to retrieve the data from Cassandra database. I am going to have some method which will retrieve the data from Cassandra-</p>

<pre><code>public Map&lt;String, String&gt; getFromDatabase() {

    // 1) For the first time, return me everything in the map
    // 2) Second time, it should return me only the the change if there is any bundle version change

}
</code></pre>

<p>Now this method should return me everything as the Map, something like this-</p>

<pre><code>Key as FrameworkBundle and Value as 1.0.0
Key as BundleA and Value as 1.0.0
Key as BundleB and Value as 1.0.0
....
And for other Bundles like above
</code></pre>

<p>Now what I need is-</p>

<ol>
<li>For the first time when I am running my application, it should return me everything in the map just like above.</li>
<li>And I have a background thread that will check the Cassandra database every 15 minutes to see if there are new versions of the bundles or not. And if there are any new version of any bundle then just return me that Bundle Name and its new version and if there are no changes in any of the version, then don't return me anything second time. And this same process will happen every 15 minutes then.</li>
</ol>

<p>Meaning only the first time, I want to return everything otherwise, I don't want to return anything unless there is any change in the bundle version.</p>

<p>I am not sure whether Cassandra can provide directly the information on this without writing some sort of logic to get the information I need.</p>

<p>What's the best and efficient way to do this thing out in the Cassandra? I dont want to retrieve all the data from Cassandra database every 15 minutes and then do some sort of logic to find out which bundle version got changed..</p>
",<java><nosql><cassandra><astyanax>,"<p>Well, cassandra is something like a key/value store, so in order to make this happen, you need a sensible row key. You always need the row key when you submit a (column range) query. Neither bundle name nor version are a very good row key since you need to know them in advance. Do you have some kind of application categorization or other feature that you could use for partitioning? </p>

<p>For instance, if you had application type id (commercial, open source, private...) as another field, you could easily create a table where your clustering/column key is a timestamp. Your row key could be your application type id. Whenever there is a new version, insert the version number to application / timestamp. Then, do a range query using the timestamp.</p>

<pre><code>  CREATE TABLE Bundles (
    bundle varchar,
    type varchar,
    ts timeuuid,
    version varchar,
    PRIMARY KEY (type, ts)
   );
</code></pre>

<p><strong>If you run for the first time and want to know all new releases, you run:</strong></p>

<pre><code>cqlsh:test&gt; SELECT * FROM Bundles WHERE 
    ...        type = 'OSS' and
    ...        ts &lt; maxTimeuuid('2013-08-27 09:00:00');

(empty resultset)
</code></pre>

<p>Since there have been no inserts so far.</p>

<p><strong>Then, you (or some other process) inserts a new release.</strong> 
Assume you have a couple of software categories, named ""type"" and type is ""Frameworks"" or ""Open Source"" or whatever fits your use case, you could insert data like this:</p>

<pre><code>cqlsh:test&gt; INSERT INTO Bundles (bundle, type, ts, version) 
 VALUES ('SomeFramwork', 'OSS', now(), '0.1.0a');
</code></pre>

<p>This stores a new column (under the column key value of now()) in the partition 1 (for type, our sharding key).</p>

<p><strong>Fifteen mintues later, if you want to know all new releases over the last 15 minutes, you run:</strong></p>

<pre><code>    cqlsh:test&gt; SELECT type, dateOf(ts), bundle, version FROM Bundles WHERE
     type = 'OSS' and
     ts &gt; minTimeuuid('2013-08-27 09:00:00')
     and ts &lt; maxTimeuuid('2013-08-27 09:15:00');

     type | dateOf(ts)               | bundle       | version
    ------+--------------------------+--------------+---------
      OSS | 2013-08-27 09:14:27+0200 | SomeFramwork |  0.1.0a
</code></pre>

<p>You would need a query for each type. The TimeUUD type would guarantee that inserts remain collision free.</p>

<p>If you are worried about rows getting too long (>2 billion), you could use buckets to limit row length.</p>

<p>To insert in Astyanax using cql3 queries, you can use</p>

<pre><code>    keyspace.prepareQuery(CF_BUNDLES).withCql(cql).execute();
</code></pre>

<p>where cql is your cql query and CF_BUNDLES is an instance of ColumnFamily.</p>

<p>To fetch data using the cql query defined above in Astyanax you can use</p>

<pre><code>    CqlResult&lt;String, String&gt; result = keyspace
    .prepareQuery(CF_BUNDLES).withCql(cql).execute()
    .getResult();
</code></pre>

<p>which enables you to iterate over the results.</p>
",['table']
18560587,18562209,2013-09-01 16:44:23,cassandra migration from mysql (design schema for sortable query),"<p>I have a database like that:</p>

<ul>
<li>images

<ul>
<li>id (int)</li>
<li>name (text)</li>
<li>image (blob)</li>
<li>create_date (datetime)</li>
<li>comment (text)</li>
<li>size (int)</li>
<li>view (int)</li>
</ul></li>
</ul>

<p>the table images contains jpg with meta information.
I've capability to sort in MySQL (by view, size and create_date)</p>

<p><strong>How to do the same with Cassandra?</strong></p>

<hr>

<p>I try some design like:
 - images
   - id (text)
   - name (text)
   - image (blob)</p>

<ul>
<li><p>image_by_size</p>

<ul>
<li>id_image (text)</li>
<li>size (int)</li>
</ul></li>
<li><p>image_by_view</p>

<ul>
<li>id_image (text)</li>
<li>view (int)</li>
</ul></li>
<li><p>image_by_create</p>

<ul>
<li>id_image (text)</li>
<li>create_date (timestamp)</li>
</ul></li>
</ul>

<p>but when i don't know how to order without know ""id"" before...</p>

<p>I read <a href=""https://stackoverflow.com/questions/18274007/select-2000-most-recent-log-entries-in-cassandra-table-using-cql-latest-version?answertab=votes#tab-top"">Select 2000 most recent log entries in cassandra table using CQL (Latest version)</a> but I don't know how to port this to my usage...</p>
",<mysql><cassandra><database-schema><cql>,"<p>One solution:</p>
<ul>
<li>image_by_size</li>
</ul>
<p>Table</p>
<pre><code>CREATE TABLE image_by_size
(
   rowkey text, // arbitrary text, it can be 'IMAGE_BY_SIZE' for example
   size int,
   id_image text,
   PRIMARY KEY (rowkey,size,id_image)
);
</code></pre>
<p>To list image by size:</p>
<pre><code> SELECT id_image FROM image_by_size WHERE rowkey='IMAGE_BY_SIZE' ORDER BY size DESC;
</code></pre>
<ul>
<li>image by view</li>
</ul>
<p>Table</p>
<pre><code>   CREATE TABLE image_by_view
    (
       rowkey text, // arbitrary text, it can be 'IMAGE_BY_VIEW' for example
       view int,
       id_image text,
       PRIMARY KEY (rowkey,view,id_image)
    );
</code></pre>
<p>To list image by view:</p>
<pre><code>SELECT id_image FROM image_by_view WHERE rowkey='IMAGE_BY_VIEW' ORDER BY size DESC;
</code></pre>
<ul>
<li>image by create</li>
</ul>
<p>Table</p>
<pre><code>  CREATE TABLE image_by_create
    (
       rowkey text, // arbitrary text, it can be 'IMAGE_BY_CREATE_DATE' for example
       create_date timestamp,
       id_image text,
       PRIMARY KEY (rowkey,create_date,id_image)
    );
</code></pre>
<p>To list image by creation date:</p>
<pre><code> SELECT id_image FROM image_by_create WHERE rowkey='IMAGE_BY_CREATE_DATE' ORDER BY create_date DESC;
</code></pre>
<hr>
<p><strong>One table solution</strong></p>
<p>Since size, view and timestamp are numbers, it is possible to use only ONE table to index all this</p>
<pre><code>CREATE TABLE image_index
(
   index_type text, // 'IMAGE_BY_SIZE', 'IMAGE_BY_VIEW' or 'IMAGE_BY_CREATE_DATE'
   value bigint,
   id_image text,
   PRIMARY KEY (index_type,value,id_image)
);
</code></pre>
<p>To index image by size</p>
<pre><code>INSERT INTO image_index(index_type,value,id_image) VALUES('IMAGE_BY_SIZE',size_as_long,id_image);
</code></pre>
<p>To index image by view</p>
<pre><code>INSERT INTO image_index(index_type,value,id_image) VALUES('IMAGE_BY_VIEW',view_as_long,id_image);
</code></pre>
<p>To index image by create date</p>
<pre><code>INSERT INTO image_index(index_type,value,id_image) VALUES('IMAGE_BY_CREATE_DATE',create_timestamp_as_long,id_image);
</code></pre>
",['table']
18620231,18623229,2013-09-04 17:29:15,User point system in Cassandra,"<p>I want to design a system for user point. The main two points of this system are point and log.</p>

<p>I want to use Cassandra to store the data. Two reason:</p>

<ol>
<li>Cassandra provides counter feature which I can use to store the point.</li>
<li>The log of point changing may be too much I have to think about the scale of the storage. Cassandra can be easily scaled.</li>
</ol>

<p>The basic data sturcture:</p>

<pre><code>// row
name: user_id,
values: {
    point: {
        name: point,
        values: 1000
    },
    log: {
        name: log,
        values: {
            log_timestamp: {
                 name: timestamp,
                 values: xxxx
            },
            log_timestamp: {
                 name: timestamp,
                 values: xxxx
            },
            log_timestamp: {
                 name: timestamp,
                 values: xxxx
            },
            ……
        }
    }
}
</code></pre>

<p>My question is:</p>

<p>Is there any problem if the log is too many enough?</p>
",<cassandra><event-log><userpoints>,"<p>@lifei</p>

<p>What you want is somehow a table with:</p>

<ol>
<li>Row key (partition key) = userId</li>
<li>1 point counter</li>
<li>1 log collection with as many log_timestamp as possible</li>
</ol>

<p>It is indeed very easy to create a table like this:</p>

<pre><code>CREATE TABLE logs
(
  userId: int,
  log_timestamp long, //timestamp
  value text,
  PRIMARY KEY (userId,log_timestamp)
);
</code></pre>

<p>It is a clustered entity (wide row) with a maximum of 2 billions of log_timestamp/value couples.</p>

<p>For the point counter, unfortunately you have to create another column family to store it  because Cassandra does not allow mixing counter value with other types.</p>
",['table']
18635381,18636959,2013-09-05 11:42:35,Clustering Keys in Cassandra,"<p>On a given physical node, rows for a given partition key are stored in the order induced by the clustering keys, making the retrieval of rows in that clustering order particularly efficient. <a href=""http://cassandra.apache.org/doc/cql3/CQL.html#createTableStmt"">http://cassandra.apache.org/doc/cql3/CQL.html#createTableStmt</a> What kind of ordering is induced by clustering keys?</p>
",<cassandra><nosql>,"<p>Suppose your clustering keys are</p>

<pre><code>k1 t1, k2 t2, ..., kn tn
</code></pre>

<p>where ki is the ith key name and ti is the ith key type. Then the order data is stored in is lexicographic ordering where each dimension is compared using the comparator for that type.</p>

<p>So (a1, a2, ..., an) &lt; (b1, b2, ..., bn) if a1 &lt; b1 using t1 comparator, or a1=b1 and a2 &lt; b2 using t2 comparator, or (a1=b1 and a2=b2) and a3 &lt; b3 using t3 comparator, etc..</p>

<p>This means that it is efficient to find all rows with a certain k1=a, since the data is stored together.  But it is inefficient to find all rows with ki=x for i > 1.  In fact, such a query isn't allowed - the only clustering key constraints that are allowed specify zero or more clustering keys, starting from the first with none missing.</p>

<p>For example, consider the schema</p>

<pre><code>create table clustering (
    x text,
    k1 text,
    k2 int,
    k3 timestamp,
    y text,
    primary key (x, k1, k2, k3)
);
</code></pre>

<p>If you did the following inserts:</p>

<pre><code>insert into clustering (x, k1, k2, k3, y) values ('x', 'a', 1, '2013-09-10 14:00+0000', '1');
insert into clustering (x, k1, k2, k3, y) values ('x', 'b', 1, '2013-09-10 13:00+0000', '1');
insert into clustering (x, k1, k2, k3, y) values ('x', 'a', 2, '2013-09-10 13:00+0000', '1');
insert into clustering (x, k1, k2, k3, y) values ('x', 'b', 1, '2013-09-10 14:00+0000', '1');
</code></pre>

<p>then they are stored in this order on disk (the order <code>select * from clustering where x = 'x'</code> returns):</p>

<pre><code> x | k1 | k2 | k3                       | y
---+----+----+--------------------------+---
 x |  a |  1 | 2013-09-10 14:00:00+0000 | 1
 x |  a |  2 | 2013-09-10 13:00:00+0000 | 1
 x |  b |  1 | 2013-09-10 13:00:00+0000 | 1
 x |  b |  1 | 2013-09-10 14:00:00+0000 | 1
</code></pre>

<p><code>k1</code> ordering dominates, then <code>k2</code>, then <code>k3</code>.</p>
",['table']
18667556,18671633,2013-09-06 22:32:36,Using timestamp as column name in CQL,"<p>I'm using CQL to create a column family with timestamp as my column name in cassandra. Has anyone tried it before? I have looked through timeuuid functions in CQL syntax. But I'm not sure as to how to use any of them in the CREATE or UPDATE queries to create a column name with timestamp in it.</p>
",<timestamp><cassandra><cql>,"<p>You can use the <code>now()</code> function to generate a time UUID containing the current time.  For example:</p>

<pre><code>create table timeuuid_test (key timeuuid primary key, value text);
insert into timeuuid_test (key, value) VALUES (now(), 'hello');
insert into timeuuid_test (key, value) VALUES (now(), 'hello2');
select * from timeuuid_test;

 key                                  | value
--------------------------------------+--------
 0ad77930-179b-11e3-8f29-19ac47defd2c |  hello
 0c305270-179b-11e3-8f29-19ac47defd2c | hello2
</code></pre>
",['table']
18712650,18729891,2013-09-10 07:10:54,Cassandra: what is the correct configuration for EC2 multi-region?,"<p>What is the correct configuration for a mulit-region setup in EC2 instances?</p>

<p>What should the listen_address, broadcast_address, rpc_address and seed ip/addresses be to work?</p>

<p>When do you use public IP address and when do you use private IP addresses?</p>
",<amazon-ec2><cassandra>,"<p>According to the <a href=""http://www.datastax.com/documentation/cassandra/1.2/webhelp/index.html?pagename=docs&amp;version=1.2&amp;file=#cassandra/configuration/configCassandra_yaml_r.html"" rel=""noreferrer"">docs</a>:</p>

<p><code>broadcast_address</code>: (Default: <code>listen_address</code>) If your Cassandra cluster is deployed across multiple Amazon EC2 regions and you use the EC2MultiRegionSnitch, set the broadcast_address to public IP address of the node and the listen_address to the private IP.</p>

<p><code>listen_address</code>: (Default: localhost) The IP address or hostname that other Cassandra nodes use to connect to this node. If left unset, the hostname must resolve to the IP address of this node using/etc/hostname, /etc/hosts, or DNS. Do not specify 0.0.0.0.</p>

<p><code>rpc_address</code>: (Default: localhost) The listen address for client connections (Thrift remote procedure calls). </p>

<p><code>seed_provider</code>: (Default: org.apache.cassandra.locator.SimpleSeedProvider) A list of comma-delimited hosts (IP addresses) to use as contact points when a node joins a cluster. Cassandra also uses this list to learn the topology of the ring. When running multiple nodes, you must change the - seeds list from the default value (127.0.0.1). In multiple data-center clusters, the - seeds list should include at least one node from each data center (replication group)</p>

<p>Trying to summarize:</p>

<ol>
<li>the <code>rpc_address</code> is used for client connections and has nothing to do with multi-region EC2</li>
<li>the <code>listen_address</code> and <code>broadcast_address</code> are the 2 important options for multi-region EC2 configuration</li>
<li><p>in general when configuring any of these answer 2 questions: </p>

<ol>
<li>who is connecting? (another nodes? clients?)</li>
<li>what IPs are accessible? (is this network interface accessible to who is connecting?)</li>
</ol></li>
</ol>
","['broadcast_address', 'listen_address']"
18712967,18749576,2013-09-10 07:28:35,How to list all the available keyspaces in Cassandra?,"<p>I am newbie in Cassandra and trying to implement one toy application using Cassandra. I had created one keyspace and few column families in my Cassandra DB but I forgot the name of my cluster.</p>

<p>I am trying to find if there is any query which can list down all the available keyspaces.</p>

<p>Anybody knows such a query or command?</p>
",<cassandra><cassandra-cli>,"<p>If you want to do this outside of the <code>cqlsh</code> tool you can query the <code>schema_keyspaces</code> table in the <code>system</code> keyspace. There's also a table called <code>schema_columnfamilies</code> which contains information about all tables.</p>

<p>The <code>DESCRIBE</code> and <code>SHOW</code> commands only work in <code>cqlsh</code> and <code>cassandra-cli</code>.</p>
",['table']
18834982,18835257,2013-09-16 18:40:06,Cassandra 1.2: Is CQL preferred over Thrift Based Clients,"<p>I'm finally getting the hang of Cassandra, part of the issue was learning / respecting the differences between Thrift and CQL3.</p>

<p>Many of the tutorials I am finding online are for CQL3.  My question:  Is CQL3 truly the preferred method, and is Thrift being discouraged?  Reason I ask is I spent a couple of days trying to get what I needed through Pycassa which does not support Cassandra 1.2 and that is based on the Thrift model. </p>
",<cassandra><pycassa>,"<p><em>Is CQL3 truly the preferred method, and is Thrift being discouraged?</em></p>
<p>Short answer is <strong>yes</strong>.</p>
<p>Longer answer is:</p>
<p>CQL3 should be preferred for many reasons:</p>
<ol>
<li><p>platform-agnostic language: CQL3 looks like SQL and is easier to handle that pure Thrift API code.</p>
</li>
<li><p>Higher level of abstraction: for end-users, it's easier to deal with CQL3 to query data rather than juggling with low-level Thrift API, although some good higher abstraction frameworks exist (Hector for Java, Pycassa for Python ...=</p>
</li>
<li><p>Easier to administer for operational teams: when creating a new table or adding new <strong>referential</strong> data, it is easier to write CQL3 scripts that ops teams can understand, check and execute rather than cryptic cassandra-cli scripts (set cf[rowKey][columnName] = ...). I'm migrating all our cassandra-cli scripts to CQL3 because it's a pain in the ass to maintain them</p>
</li>
<li><p>Last but not least, CQL3 make life easier for third-party framework developers. I've developed <strong><a href=""https://github.com/doanduyhai/Achilles/wiki"" rel=""nofollow noreferrer"">Achilles</a></strong>, an open-source persistence manager for Cassandra. The Thrift version was painfull to implement, the CQL3 version was a piece of cake escpecially because it uses the Java Driver from Datastax</p>
</li>
</ol>
<p>That being said, CQL3 is no bed of roses either. Before <strong>leveraging the full power of the query language, you need to understand how Cassandra storage engine works</strong>. The language gives you the illusion that everything is easy and will work as SQL but the plain truth is no. There are some important semantics differences, especially when using the <strong>WHERE</strong> clause.</p>
",['table']
18842369,18850980,2013-09-17 06:05:10,Cassandra nodetool could not resolve '127.0.0.1': unknown host,"<p>I am very new to cassandra. Just started exploring.</p>

<p>I am running a single node cassandra server &amp; facing a problem in seeing status of the cassandra using nodetool command.</p>

<p>I have hostname configured on my VM as myMachineIP cass1 in /etc/hosts</p>

<p>and </p>

<p>I configured my cassandra_instal_path/conf/cassandra.yaml file with listen_address, rpc_address as localhost and clustername as casscluster</p>

<p>(also tried with my hostname which is cass1 as listen_address/rpc_address)</p>

<p>Not sure what is the reason why i am not able to get statususing nodetool command.</p>

<pre><code>$ nodetool

Cannot resolve '127.0.0.1': unknown host

$ nodetool -host 127.0.0.1

Cannot resolve '127.0.0.1': unknown host

$ nodetool -host cass1

Cannot resolve 'cass1': unknown host
</code></pre>

<p>But i am able to connect to cassandra-cli</p>

<p>console output:</p>

<pre><code>Connected to: ""casscluster"" on 127.0.0.1/9160
Welcome to Cassandra CLI version 1.2.8

Type 'help;' or '?' for help.
Type 'quit;' or 'exit;' to quit.
</code></pre>

<p>my /etc/hosts looks like:</p>

<pre><code>127.0.0.1       localhost.localdomain   localhost.localdomain   localhost4      localhost4.localdomain4 localhost       cass1

::1     localhost.localdomain   localhost.localdomain   localhost6      localhost6.localdomain6 localhost       cass1


[myMachineIP]  cass1
</code></pre>

<p>what could be the reason why i am not able to run nodetool?</p>

<p>Please help.</p>
",<cassandra><nodetool>,"<p>try setting actual IP address in  listen_address, rpc_address than localhost</p>
",['rpc_address']
18857187,18858681,2013-09-17 18:25:22,How Cassandra 2.0 distributes column family data if there is no primary key,"<p>I am learning cassandra 2.0</p>

<p>As per <a href=""http://www.datastax.com/documentation/cassandra/2.0/webhelp/index.html#cassandra/architecture/architectureDataDistributeAbout_c.html#concept_ds_r54_xhf_fk"" rel=""nofollow"">Documentation</a> cassandra distributes column family data across nodes(in cluster) based on primary key using hashing. But If my column family does not have any primary key, then how cassandra will distribute column family data accross node?</p>
",<cassandra>,"<p>All Cassandra tables (column families) must have a primary key.</p>

<p>Quoth the <a href=""http://cassandra.apache.org/doc/cql3/CQL.html"">CQL3 documentation</a>:</p>

<blockquote>
  <p>Within a table, a row is uniquely identified by its PRIMARY KEY (or more simply the key), and hence all table definitions must define a PRIMARY KEY (and only one)</p>
</blockquote>
",['table']
18872422,18874524,2013-09-18 12:35:02,RPC timeout error while exporting data from CQL,"<p>I am trying to export data from cassandra using CQL client. A column family has about 100000 rows in it. when i am copying dta into csv file using COPY TO command i get following rpc_time out error.</p>

<pre><code>copy mycolfamily to '/root/mycolfamily.csv'
Request did not complete within rpc_timeout.
</code></pre>

<p>I am running in:</p>

<p><code>[cqlsh 3.1.6 | Cassandra 1.2.8 | CQL spec 3.0.0 | Thrift protocol 19.36.0]</code> </p>

<p>How can I increase RPC timeout limit?</p>

<p>I tried adding <code>rpc_timeout_in_ms: 20000</code> (defalut is 10000) in my <code>conf/cassandra.yaml</code> file. but while restarting cassandra I get: </p>

<pre><code>[root@user ~]# null; Can't construct a java object for tag:yaml.org,2002:org.apache.cassandra.config.Config; exception=Cannot create property=rpc_timeout_in_ms for JavaBean=org.apache.cassandra.config.Config@71bfc4fc; Unable to find property 'rpc_timeout_in_ms' on class: org.apache.cassandra.config.Config
Invalid yaml; unable to start server.  See log for stacktrace.
</code></pre>
",<cassandra><cql3>,"<p>The <code>COPY</code> command currently does the same thing with <code>SELECT</code> with <code>LIMIT 99999999</code>. So, it will eventually goes to timeout while your data is growing. Here's the export function;</p>

<p><a href=""https://github.com/apache/cassandra/blob/trunk/bin/cqlsh#L1524"" rel=""nofollow"">https://github.com/apache/cassandra/blob/trunk/bin/cqlsh#L1524</a></p>

<p>I'm doing the same export on production. What I'm doing is the following;</p>

<ul>
<li>make select * from table where timeuuid = someTimeuuid limit 10000</li>
<li>write the result set to a csv file w/ >> mode</li>
<li>make the next selects with respect to the last timeuuid</li>
</ul>

<p>You can pipe command in cqlsh by the following cqlsh command</p>

<p><code>echo ""{$cql}"" | /usr/bin/cqlsh -u user -p password localhost 9160 &gt; file.csv</code></p>
",['table']
18877420,18877486,2013-09-18 16:22:48,Can I have different partitioners in a multiple datacenter configuration in cassandra?,"<p>Can I have RandomPartitioner in the cluster in datacenter1 and Murmur3Partitioner in the cluster in datacenter2?  </p>
",<cassandra><partitioner>,"<p>No, you need to have the same partitioner on all nodes in the cluster.</p>

<p>If you are asking this because you want a way of migrating from RandomPartitioner to Murmur3Partitioner then it won't work unfortunately.  I don't know of a method of moving to Murmur3Partitioner  on a live cluster, but the benefit is small so it is unlikely to be worth doing.</p>
",['partitioner']
19090855,19091104,2013-09-30 09:32:23,Cassandra write strategy,"<p>I want to write 1 billions rows with 2 connected nodes in Cassandra. I use 8 threads from the clients but I don't know whether I write only in one node or both to have the max performance?
Thanks</p>
",<cassandra>,"<p>It won't matter if you write to one node or both. Whichever node is receiving the updates (called the coordinator node) will partition the data based on the partitioner and distribute the necessary section of data to the other node. So whether the updates goes to 1 node or 2, the same network latency and processing will be carried out overall.</p>

<p>With 8 threads you should see good write performance, as Cassandra is optimized for a write heavy workload.</p>

<p>Here is a good treatment of <a href=""http://www.datastax.com/docs/1.0/cluster_architecture/about_client_requests"" rel=""nofollow"">client requests</a>.</p>
",['partitioner']
19143452,19145402,2013-10-02 17:49:55,Datastax Java driver to autodiscover all the nodes for specific datacenter in its connection pool?,"<p>I have recently started using <code>Cassandra</code> in our <code>Production environment</code>. We have a <code>24 node cluster</code> with <code>replication factor of 4</code>. Meaning <code>2 copies</code> will be there in <code>each datacenter</code>. So that means we have a single cross colo cluster with <code>24 nodes</code> which means <code>12 nodes in SLC colo</code> and <code>12 nodes in PHX colo</code>.</p>

<p>I am using <code>Astyanax client</code> currently to write the data in <code>Cassandra database</code>. And I know Astyanax client has this feature to autodiscover all the nodes in PHX colo or SLC colo of cassandra in its connection pooling but not all of the nodes.</p>

<p>In Astyanax we can use something like below - </p>

<pre><code>setLocalDatacenter(""DC1"")
</code></pre>

<p>Now we are planning to use Datastax Java driver. And I am not sure whether Datastax java driver has this feature or not to autodiscover all the cassandra nodes in its connection pool only for specific datacenter and not all the datacenters?</p>
",<java><cassandra><astyanax><datastax-java-driver>,"<p>The driver will discover all the nodes in your cluster, you want to change your load balancing policy in your client code. Specifically you want to use the dc aware load balancing policy.</p>

<p><a href=""http://www.datastax.com/drivers/java/apidocs/com/datastax/driver/core/policies/DCAwareRoundRobinPolicy.html"" rel=""nofollow"">http://www.datastax.com/drivers/java/apidocs/com/datastax/driver/core/policies/DCAwareRoundRobinPolicy.html</a></p>
",['dc']
19161940,19166638,2013-10-03 14:31:29,Astyanax Composite Keys in Cassandra,"<p>Im trying to create a schema that will enable me access rows with only part of the row_key.
For example the key is of the form user_id:machine_os:machine_arch</p>

<p>An example of a row key:   12242:""windows2000"":""x86""</p>

<p>From the documentation I could not understand whether this will enable me to query all rows that have userid=12242 or query all rows that have ""windows2000""</p>

<p>Is there any feasible way to achieve this ?</p>

<p>Thanks,</p>

<p>Yadid</p>
",<cassandra>,"<p>Alright, here is what is happening: based on your schema, you are effectively creating a column family with a <em>composite primary key</em> or a <em>composite rowkey</em>. What this means is, you will need to restrict each component of the composite key except the last one with a <em>strict equality relation</em>. The last component of the composite key can use inequality and the <code>IN</code> relation, but not the 1st and 2nd components.</p>

<p>Additionally, you <em>must</em> specify all three parts if you want to utilize any kind of filtering. This is necessary because without all parts of the partition key, the coordinator node will have no idea on which node in the cluster the data exists (remember, Cassandra uses the partition key to determine replicas and data placement).</p>

<p>Effectively, this means you can't do any of these:</p>

<pre><code>select * from datacf where user_id = 100012; # missing 2nd and 3rd key components
select * from datacf where user_id = 100012; and machine_arch = 'x86'; # missing 3rd key component
select * from datacf where machine_arch = 'x86'; # you have to specify the 1st
select * from datacf where user_id = 100012 and machine_arch in ('x86', 'x64'); # nope, still want 3rd
</code></pre>

<p>However, you will be able to run queries like this:</p>

<pre><code>select * from datacf where user_id = 100012 and machine_arch = 'x86'
   and machine_os = ""windows2000""; # yes! all 3 parts are there

select * from datacf where user_id = 100012 and machine_os = ""windows2000""
   and machine_arch in ('x86', 'x64'); # the last part of the key can use the 'IN' or other equality relations
</code></pre>

<p>To answer your initial question, with you existing data model, you will neither be able to query data with <code>userid = 12242</code> or query all rows that have ""windows2000"" as the <code>machine_os</code>.</p>

<p>If you can tell me exactly what kind of query you will be running, I can probably help in trying to design the table accordingly. Cassandra data models usually work better when looked at from the data retrieval perspective. Long story short- use only <code>user_id</code> as your primary key and use secondary indexes on other columns you want to query on.</p>
",['table']
19231778,19234190,2013-10-07 18:16:00,Cassandra CQL query check multiple values,"<p>How can I check if a non-primary key field's value is either 'A' or 'B' with a Cassandra CQL query? (I'm using Cassandra 2.0.1)</p>

<p>Here's the table definition:</p>

<pre><code>CREATE TABLE my_table (
  my_field text,
  my_field2 text,
  PRIMARY KEY (my_field)
);
</code></pre>

<p>I tried:</p>

<pre><code>1&gt; SELECT * FROM my_table WHERE my_field2 IN ('A', 'B');

2&gt; SELECT * FROM my_table WHERE my_field2 = 'A' OR my_field = 'B' ;
</code></pre>

<p>The first one failed with this messeage:</p>

<pre><code>Bad Request: IN predicates on non-primary-key columns (my_field2) is not yet supported
</code></pre>

<p>The second one failed because Cassandra CQL doesn't support OR keyword</p>

<p>I couldn't get this simple query working (with a pretty straight forward way). I'm pretty frustrated dealing with CQL queries in general. Is it because Cassandra is not mature enough and has really poor support with queries, or is it me who must change the way of thinking?</p>
",<cassandra><cql3>,"<p>This is the intentional functionality of cassandra.  You cannot query using a WHERE clause on columns that are not</p>

<ul>
<li>the partition key</li>
<li>part of a composite key</li>
</ul>

<p>This is because your data is partitioned around a ring of cassandra nodes.  You want to avoid having to ask the entire ring to return the answer to your query.  Ideally you want to be able to retrieve your data from a single node in the ring</p>

<p>Generally in cassandra you want to structure your table to match your queries as opposed to relational normalization.  So you have a few options to deal with this.</p>

<p>1) write your data to multiple tables to support various queries.  In your case you may want to create a second table as</p>

<pre><code>CREATE TABLE my_table (
  my_field2 text,
  my_field text,
  PRIMARY KEY (my_field2)
);
</code></pre>

<p>Then your first query will return correctly</p>

<p>2) Create your table with a composite key as</p>

<pre><code>CREATE TABLE my_table (
  my_field text,
  my_field2 text,
  PRIMARY KEY (my_field, my_field2)
);
</code></pre>

<p>With this method, if you do not specify a query value for my_field then you will need to append your query with a qualifier to tell cassandra that you really want to query the entire ring</p>

<pre><code>SELECT * FROM my_table WHERE my_field2 IN ('A', 'B') ALLOW FILTERING;
</code></pre>

<p>-edit-</p>

<p>you cannot use a secondary index to search for multiple values.  Per CQL documentation</p>

<p><a href=""http://www.datastax.com/documentation/cql/3.0/webhelp/cql/ddl/ddl_primary_index_c.html"" rel=""noreferrer"">http://www.datastax.com/documentation/cql/3.0/webhelp/cql/ddl/ddl_primary_index_c.html</a></p>

<p>""An index is a data structure that allows for fast, efficient lookup of data matching a given condition.""</p>

<p>So, you must give it one and only one value.</p>
",['table']
19254173,19284106,2013-10-08 17:17:30,cassandra Exception encountered during startup: index (1) must be less than size (1),"<p>I experienced a power outage which killed all 4 the nodes in my Cassandra cluster. I've brought back all the boxes, but now when I try to start Cassandra (method bin/Cassandra) I am getting the following message on 3 out of 4 after there is a bit of replaying the log.</p>

<pre><code>Exception encountered during startup: index (1) must be less than size (1)
 INFO 10:11:00,479 CFS(Keyspace='system', ColumnFamily='peers') liveRatio is 13.10204081632653 (just-counted was 13.10204081632653).  calculation took 18ms for 21 columns
</code></pre>

<p>(that's just the end, I can post more message if it helps)</p>

<p>The one that came up is running for reads only, I assume as I have replication factor 3 on all keyspaces so it can't work without the others.</p>

<p>Any idea's as to what to do to retrieve the situation.. I have quite a bit of valuable information in the database. </p>

<p>Cassandra v1.2.6,</p>

<p>CentOs v6.4</p>

<p><strong>Update:</strong></p>

<p>All three dead nodes seem to be failing during replay of the commit log.. I've bolded the line they get to. My initial thoughts are to remove that commit log and try restarting.. but I'm a little afraid of the consequences!!</p>

<pre><code> INFO 10:46:48,600 Replaying /var/lib/cassandra/commitlog/CommitLog-2-1379365208291.log, /var/lib/cassandra/commitlog/CommitLog-2-1379365208292.log, /var/lib/cassandra/commitlog/CommitLog-2-1379365208293.log, /var/lib/cassandra/commitlog/CommitLog-2-1379365208294.log, /var/lib/cassandra/commitlog/CommitLog-2-1379365208295.log, /var/lib/cassandra/commitlog/CommitLog-2-1379365208296.log, /var/lib/cassandra/commitlog/CommitLog-2-1379365208297.log, /var/lib/cassandra/commitlog/CommitLog-2-1379365208298.log, /var/lib/cassandra/commitlog/CommitLog-2-1379365208299.log, /var/lib/cassandra/commitlog/CommitLog-2-1379365208301.log, /var/lib/cassandra/commitlog/CommitLog-2-1379365208302.log, /var/lib/cassandra/commitlog/CommitLog-2-1379365208304.log, /var/lib/cassandra/commitlog/CommitLog-2-1379365208305.log, /var/lib/cassandra/commitlog/CommitLog-2-1379365208306.log, /var/lib/cassandra/commitlog/CommitLog-2-1379365208307.log, /var/lib/cassandra/commitlog/CommitLog-2-1379365208308.log, /var/lib/cassandra/commitlog/CommitLog-2-1379365208309.log, /var/lib/cassandra/commitlog/CommitLog-2-1379365208310.log, /var/lib/cassandra/commitlog/CommitLog-2-1379365208311.log, /var/lib/cassandra/commitlog/CommitLog-2-1381251642075.log, /var/lib/cassandra/commitlog/CommitLog-2-1381251731119.log
 INFO 10:46:48,614 Replaying /var/lib/cassandra/commitlog/CommitLog-2-1379365208291.log
 INFO 10:46:49,598 GC for ParNew: 605 ms for 2 collections, 50055912 used; max is 1046937600
 INFO 10:46:51,775 Finished reading /var/lib/cassandra/commitlog/CommitLog-2-1379365208291.log
 INFO 10:46:51,776 Replaying /var/lib/cassandra/commitlog/CommitLog-2-1379365208292.log
 INFO 10:46:53,995 Finished reading /var/lib/cassandra/commitlog/CommitLog-2-1379365208292.log
 INFO 10:46:53,995 Replaying /var/lib/cassandra/commitlog/CommitLog-2-1379365208293.log
 **INFO 10:46:54,087 Finished reading /var/lib/cassandra/commitlog/CommitLog-2-1379365208293.log**
ERROR 10:46:54,088 Exception encountered during startup
java.lang.IndexOutOfBoundsException: index (1) must be less than size (1)
    at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:305)
    at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:284)
    at com.google.common.collect.SingletonImmutableList.get(SingletonImmutableList.java:45)
    at org.apache.cassandra.db.marshal.CompositeType.getComparator(CompositeType.java:94)
    at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:76)
    at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:31)
    at java.util.TreeMap.compare(TreeMap.java:1188)
    at java.util.TreeMap.put(TreeMap.java:531)
    at org.apache.cassandra.db.TreeMapBackedSortedColumns.addColumn(TreeMapBackedSortedColumns.java:102)
    at org.apache.cassandra.db.TreeMapBackedSortedColumns.addColumn(TreeMapBackedSortedColumns.java:88)
    at org.apache.cassandra.db.AbstractColumnContainer.addColumn(AbstractColumnContainer.java:114)
    at org.apache.cassandra.db.AbstractColumnContainer.addColumn(AbstractColumnContainer.java:109)
    at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:101)
    at org.apache.cassandra.db.RowMutation$RowMutationSerializer.deserialize(RowMutation.java:397)
    at org.apache.cassandra.db.commitlog.CommitLogReplayer.recover(CommitLogReplayer.java:202)
    at org.apache.cassandra.db.commitlog.CommitLogReplayer.recover(CommitLogReplayer.java:97)
    at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:146)
    at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:126)
    at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:298)
    at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:441)
    at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:484)
java.lang.IndexOutOfBoundsException: index (1) must be less than size (1)
    at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:305)
    at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:284)
    at com.google.common.collect.SingletonImmutableList.get(SingletonImmutableList.java:45)
    at org.apache.cassandra.db.marshal.CompositeType.getComparator(CompositeType.java:94)
    at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:76)
    at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:31)
    at java.util.TreeMap.compare(TreeMap.java:1188)
    at java.util.TreeMap.put(TreeMap.java:531)
    at org.apache.cassandra.db.TreeMapBackedSortedColumns.addColumn(TreeMapBackedSortedColumns.java:102)
    at org.apache.cassandra.db.TreeMapBackedSortedColumns.addColumn(TreeMapBackedSortedColumns.java:88)
    at org.apache.cassandra.db.AbstractColumnContainer.addColumn(AbstractColumnContainer.java:114)
    at org.apache.cassandra.db.AbstractColumnContainer.addColumn(AbstractColumnContainer.java:109)
    at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:101)
    at org.apache.cassandra.db.RowMutation$RowMutationSerializer.deserialize(RowMutation.java:397)
    at org.apache.cassandra.db.commitlog.CommitLogReplayer.recover(CommitLogReplayer.java:202)
    at org.apache.cassandra.db.commitlog.CommitLogReplayer.recover(CommitLogReplayer.java:97)
    at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:146)
    at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:126)
    at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:298)
    at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:441)
    at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:484)
Exception encountered during startup: index (1) must be less than size (1)
 INFO 10:46:54,110 CFS(Keyspace='system', ColumnFamily='peers') liveRatio is 13.10204081632653 (just-counted was 13.10204081632653).  calculation took 19ms for 21 columns
</code></pre>
",<cassandra>,"<p>My best theory is that this is a possible consequence of dropping and re-creating a table with the same name (<a href=""https://issues.apache.org/jira/browse/CASSANDRA-5905"" rel=""nofollow"">https://issues.apache.org/jira/browse/CASSANDRA-5905</a>).  We are targeting a fix for 2.1 (<a href=""https://issues.apache.org/jira/browse/CASSANDRA-5202"" rel=""nofollow"">https://issues.apache.org/jira/browse/CASSANDRA-5202</a>); in the meantime, prefer TRUNCATE over drop/recreate.</p>
",['table']
19312341,19314738,2013-10-11 07:16:03,Insert into column family using Datastax java driver?,"<p>If I have a column family created like this in Cassandra as previously I was using Thrift based client..</p>

<pre><code>create column family USER
with comparator = 'UTF8Type'
and key_validation_class = 'UTF8Type'
and default_validation_class = 'BytesType'
</code></pre>

<p>Then can I insert into above column family by using the Datastax Java driver with asynchonous / batch writes capability?</p>

<p>I will be using INSERT statement to insert into above column family? Is that possible using Datastax Java driver?</p>

<p>I am in the impression that I can only insert into CQL based tables using Datastax Java driver not in the column family design tables...</p>
",<java><cassandra><datastax-java-driver>,"<p><strong>TL;DR</strong><br/>
Sort of, but it is better to create a cql3 based table and continue from there.</p>

<p>First off to get a clear impression of what's going on use the describe command in cqlsh:</p>

<pre><code>cqlsh&gt; describe COLUMNFAMILY ""USER"";

CREATE TABLE ""USER"" (
  key text,
  column1 text,
  value blob,
  PRIMARY KEY (key, column1)
) WITH COMPACT STORAGE AND
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  index_interval=128 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  default_time_to_live=0 AND
  speculative_retry='NONE' AND
  memtable_flush_period_in_ms=0 AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'LZ4Compressor'};
</code></pre>

<p>Then you can build insert statements using cqlh (i used cqlsh):</p>

<pre><code>cqlsh:test&gt; insert into test.""USER"" (key, column1, value) VALUES ('epickey', 'epic column 1 text', null);
</code></pre>

<p>If you do a select however...</p>

<pre><code>cqlsh:test&gt; SELECT * FROM test.""USER"" ;

(0 rows)
</code></pre>

<p>Once you've done that go back to the CLI:</p>

<pre><code>[default@unknown] use test;
Authenticated to keyspace: test
[default@test] list USER;
Using default limit of 100
Using default cell limit of 100
-------------------
RowKey: epickey

1 Row Returned.
Elapsed time: 260 msec(s).
</code></pre>

<p>The last tool I'll use is sstable2json (takes all your data from an sstable and ... turns it into a json representation)</p>

<p>For the above single insert of the USER cf i got this back:</p>

<pre><code>[
{""key"": ""657069636b6579"",""columns"": [[""epic column 1 text"",""5257c34b"",1381483339315000,""d""]]}
]
</code></pre>

<p>So the data is there, but you just dont really have access to it over cql.</p>

<p><strong>Note</strong> This is all done using C* 2.0 and cqlsh 4.0 (cql3) </p>
",['table']
19374143,19997995,2013-10-15 05:53:22,Get rows using first component of composite key using hector client in Cassandra,"<p>I'm using composite datatype in rowkey, column family is as below</p>

<pre><code>create column family CompositeTest
with comparator = 'UTF8Type'
and key_validation_class = 'CompositeType(UTF8Type,UTF8Type)'
and default_validation_class = 'UTF8Type';
</code></pre>

<p>The sample data of this column family as below,</p>

<pre><code>RowKey: s2:2222222
=&gt; (column=param1, value=value1
=&gt; (column=param2, value=value2
=&gt; (column=param3, value=value3
-------------------
RowKey: s2:3333333
=&gt; (column=param1, value=value1
=&gt; (column=param2, value=value2
=&gt; (column=param3, value=value3
-------------------
RowKey: s2:1111111
=&gt; (column=param1, value=value1
=&gt; (column=param2, value=value2
=&gt; (column=param3, value=value3
-------------------
RowKey: s1:3333333
=&gt; (column=param1, value=value1
=&gt; (column=param2, value=value2
=&gt; (column=param3, value=value3
-------------------
RowKey: s1:2222222
=&gt; (column=param1, value=value1
=&gt; (column=param2, value=value2
=&gt; (column=param3, value=value3
-------------------
RowKey: s1:1111111
=&gt; (column=param1, value=value1
=&gt; (column=param2, value=value2
=&gt; (column=param3, value=value3
</code></pre>

<p>I want to get all the rows which first component of row key is ""s1"". Is it possible using Hector client? if not then by which cassandra client its possible?</p>

<p>I've tried by using following code, but its not working,</p>

<pre><code>Composite start = new Composite();
        start.addComponent(0, ""s1"", ComponentEquality.EQUAL);

        Composite end = new Composite();
        end.addComponent(0, ""s1"", ComponentEquality.GREATER_THAN_EQUAL);

        RangeSlicesQuery&lt;Composite, String, String&gt; rangeSlicesQuery = HFactory.createRangeSlicesQuery(keyspace, new CompositeSerializer(), StringSerializer.get(),  StringSerializer.get()); 
        rangeSlicesQuery.setKeys(start, end);
        rangeSlicesQuery.setRange(""param1"", ""param3"", false, 100);
        rangeSlicesQuery.setColumnFamily(""CompositeTest"");
        rangeSlicesQuery.setRowCount(11);
        QueryResult&lt;OrderedRows&lt;Composite, String, String&gt;&gt;  queryResult = rangeSlicesQuery.execute();

        Rows&lt;Composite, String, String&gt; rows = queryResult.get();
        Iterator&lt;Row&lt;Composite, String, String&gt;&gt; rowsIterator = rows.iterator();
</code></pre>

<p>Thanks in advance...</p>
",<java><jakarta-ee><cassandra><hector>,"<p>The problem is you are trying to perform a slice on the row keys. 
It is not possible at all if you are using in Cassandra  a random partitioner (e.g RandomPartitioner or Murmur3Partitioner).  It could be possible (but I've never tried) if you are using a order preserving partitioner. In you case should be a CompositeKeyPartitioner that unlucky doesn't exist and thus you should have to write it by yourself. Then you should also configure the cluster by calculating the right tokens in according with your data. 
As you can see, it is not the easiest way. </p>

<p>BUT, you can do the same, if you just put the composite value in the Column name instead of the key. 
You can define you CF in such way:</p>

<pre><code>create column family CompositeTest
   with comparator = 'CompositeType(UTF8Type,UTF8Type)'
   and key_validation_class = 'UTF8Type'
   and default_validation_class = 'UTF8Type';
</code></pre>

<p>And store the data like:</p>

<pre><code>RowKey: s2
=&gt; (column=2222222:param1, value=value1
=&gt; (column=2222222:param2, value=value2
=&gt; (column=2222222:param3, value=value3
=&gt; (column=3333333:param1, value=value1
=&gt; (column=3333333:param2, value=value2
=&gt; (column=3333333:param3, value=value3
=&gt; (column=1111111:param1, value=value1
=&gt; (column=1111111:param2, value=value2
=&gt; (column=1111111:param3, value=value3
-------------------
RowKey: s1:
=&gt; (column=3333333:param1, value=value1
=&gt; (column=3333333:param2, value=value2
=&gt; (column=3333333:param3, value=value3
=&gt; (column=2222222:param1, value=value1
=&gt; (column=2222222:param2, value=value2
=&gt; (column=2222222:param3, value=value3
=&gt; (column=1111111:param1, value=value1
=&gt; (column=1111111:param2, value=value2
=&gt; (column=1111111:param3, value=value3
</code></pre>

<p>With this structure the query you thought it's quite easy, and then you can always slice on the column name to select only those columns inside the interval you want.</p>
",['partitioner']
19394189,19421494,2013-10-16 02:15:42,Cqlsh with Kerberos,"<p>Am trying to <strong>login cqlsh shell</strong> with <strong>kerberos enabled cassandra cluster</strong>. I was able to login to the cqlsh shell with the cassandra user but I was <strong>unable</strong> to login with any other user.</p>

<p>If I try to login with say test user then it shows the below error for me.</p>

<pre><code>Connection error: User test@REALM.COM doesn't exist - create it with CREATE USER query first
</code></pre>

<p>But in my users table I had the test user in it.</p>

<pre><code>cqlsh&gt; SELECT * FROM system_auth.users ;

 name      | super

-----------+-------

  test     | False

 cassandra |  True
</code></pre>

<p>Test user has <strong>ticket</strong> for the <strong>kerberos</strong>. Can anyone help me please.</p>
",<cassandra><kerberos><datastax-enterprise><cqlsh>,"<p>Your username in the system_auth.users table is wrong. 
For kerberos, you should use fully qualified user names, including the realm part. This is 
because you may want to have two users with the same name, but different realms.</p>

<p>The cassandra user is special - we didn't want to hardcode any particular realm name so we decided to strip realm from this one particular user.</p>

<p>So your users table should look like this:</p>

<pre><code> name              | super
-------------------+-------
 test@REALM.COM    | False
 cassandra         |  True
</code></pre>
",['table']
19557188,19561101,2013-10-24 04:50:23,Understanding Cassandra Composite keys,"<p>I just watched <a href=""http://www.youtube.com/watch?v=HdJlsOZVGwM"">this youtube video of Patrick McFadin</a> on cassandra datamodelling.</p>

<p>There was one table, as follows: </p>

<pre><code>create table user_activity_history {
  username varchar,
  interaction_date varchar,
  activity_code varchar,
  detail varchar,
  PRIMARY KEY((username,interaction_date),interaction_time)
);
</code></pre>

<p>Why is the primary key <code>((username,interaction_date),interaction_time)</code>.
How is that different from <code>(username,interaction_date,interaction_time)</code>.</p>
",<cassandra><cql>,"<p>The difference is related to the table's <code>partition_key</code>. Typically the first element in a PRIMARY KEY is also the partition key - this defines the physical location of the data in the cluster, e.g., by using the following:</p>

<pre><code>PRIMARY KEY(username,interaction_date,interaction_time)
</code></pre>

<p>data inserted into the table will be partitioned (located physically) according to <code>username</code>, whereas by using the following:</p>

<pre><code>PRIMARY KEY((username,interaction_date),interaction_time)
</code></pre>

<p>it will be partitioned according to the <code>username,interaction_date</code> combination. The advantage of the latter scheme is that data relating to a single <code>username</code> can be stored across nodes in the cluster.</p>

<p>There is more details on partition_keys in datastax's CQL documentation on <a href=""http://www.datastax.com/documentation/cql/3.1/webhelp/index.html#cql/cql_reference/create_table_r.html"" rel=""noreferrer"">CREATE TABLE</a>:</p>

<blockquote>
  <p>When you use a compound PRIMARY KEY Cassandra treats the first column declared in a definition as the partition key and stores all columns of the row on the same physical node. When you use a composite partition key, Cassandra treats the columns in nested parentheses as partition keys and stores columns of a row on more than one node. You declare a composite 
  partition key using an extra set of parentheses to define which columns partition the data. </p>
</blockquote>
",['table']
19602979,19656246,2013-10-26 04:57:14,How to retrieve only the information that got changed from Cassandra?,"<p>I am working on designing the Cassandra Column Family schema for my below use case.. I am not sure what is the best way to design the cassandra column family for my below use case? I will be using CQL Datastax Java driver for this..</p>

<p>Below is my use case and the sample schema that I have designed for now - </p>

<pre><code>SCHEMA_ID       RECORD_NAME               SCHEMA_VALUE              TIMESTAMP
1                  ABC                     some value                 t1
2                  ABC                     some_other_value           t2
3                  DEF                     some value again           t3
4                  DEF                     some other value           t4
5                  GHI                     some new value             t5
6                  IOP                     some values again          t6
</code></pre>

<p>Now what I will be looking from the above table is something like this - </p>

<ol>
<li>For the first time whenever my application is running, I will ask for everything from the above table.. Meaning give me everything from the above table..</li>
<li>Then every 5 or 10 minutes, my background thread will be checking this table and will ask for give me everything that has changed only (full row if anything got changed for that row).. so that is the reason I am using timestamp as one of the column here..</li>
</ol>

<p>But I am not sure how to design the query pattern in such a way such that both of my use cases gets satisfied easily and what will be the proper way of designing the table for this? Here SCHEMA_ID will be primary key I am thinking to use...</p>

<p>I will be using CQL and Datastax Java driver for this..</p>

<p><strong>Update:-</strong></p>

<p>If I am using something like this, then is there any problem with this approach? </p>

<pre><code>CREATE TABLE TEST (SCHEMA_ID TEXT, RECORD_NAME TEXT, SCHEMA_VALUE TEXT, LAST_MODIFIED_DATE TIMESTAMP, PRIMARY KEY (ID));

INSERT INTO TEST (SCHEMA_ID, RECORD_NAME, SCHEMA_VALUE, LAST_MODIFIED_DATE) VALUES ('1', 't26',  'SOME_VALUE', 1382655211694);
</code></pre>

<p>Because, in my this use case, I don't want anybody to insert same <code>SCHEMA_ID</code> everytime.. <code>SCHEMA_ID</code> should be unique whenever we are inserting any new row into this table.. So with your example (@omnibear), it might be possible, somebody can insert same SCHEMA_ID twice? Am I correct?</p>

<p>And also regarding <code>type</code> you have taken as an extra column, that type column can be <code>record_name</code> in my example..</p>
",<java><cassandra><cql><datastax-java-driver>,"<p>Regarding 1)
Cassandra is used for heavy writing, lots of data on multiple nodes. To retrieve ALL data from this kind of set-up is daring since this might involve huge amounts that have to be handled by one client. A better approach would be to <strong>use pagination</strong>. This is <a href=""http://www.datastax.com/dev/blog/client-side-improvements-in-cassandra-2-0"" rel=""nofollow"">natively supported in 2.0</a>.</p>

<p>Regarding 2)
The point is that partition keys only support EQ or IN queries. For LT or GT (&lt; / >) you use column keys. So if it makes sense to group your entries by some ID like ""type"", you can use this for your partition key, and a timeuuid as a column key. This allows to query for all entries newer than X like so</p>

<pre><code>create table test 
  (type int, SCHEMA_ID int, RECORD_NAME text, 
  SCHEMA_VALUE text, TIMESTAMP timeuuid, 
  primary key (type, timestamp));

select * from test where type IN (0,1,2,3) and timestamp &lt; 58e0a7d7-eebc-11d8-9669-0800200c9a66;
</code></pre>

<p><strong>Update:</strong></p>

<p>You asked: </p>

<blockquote>
  <p>somebody can insert same SCHEMA_ID twice? Am I correct?</p>
</blockquote>

<p>Yes, you can always make an insert with an existing primary key. The values at that primary key will be updated. Therefore, to preserve uniqueness, a UUID is often used in the primary key, for instance, timeuuid. It is a unique value containing a timestamp and the MAC address of the client. There is <a href=""http://www.datastax.com/documentation/cql/3.0/webhelp/index.html#cql/ddl/ddl_anatomy_table_c.html#concept_ds_cz4_lmy_zj"" rel=""nofollow"">excellent documentation on this topic</a>.</p>

<p><strong>General advice:</strong></p>

<ol>
<li>Write down your queries first, then design your model. <strong>(Use case!)</strong></li>
<li>Your queries define your data model which in turn is primarily defined by your <strong>primary keys.</strong></li>
</ol>

<p>So, in your case, I'd just adapt my schema above, like so:</p>

<pre><code>CREATE TABLE TEST (SCHEMA_ID TEXT, RECORD_NAME TEXT, SCHEMA_VALUE TEXT,   
LAST_MODIFIED_DATE TIMEUUID, PRIMARY KEY (RECORD_NAME, LAST_MODIFIED_DATE));
</code></pre>

<p>Which allows this query:</p>

<pre><code>select * from test where RECORD_NAME IN (""componentA"",""componentB"")
  and LAST_MODIFIED_DATE &lt; 1688f180-4141-11e3-aa6e-0800200c9a66;

the uuid corresponds to -&gt; Wednesday, October 30, 2013 8:55:55 AM GMT
so you would fetch everything after that
</code></pre>
",['table']
19621529,21083648,2013-10-27 18:09:56,Real time analytics with cassandra,"<p>I watched this <a href=""http://www.youtube.com/watch?v=wccOk_mRaoU"" rel=""nofollow"">video</a> on doing analystics with cassandra. 
I was pretty interested in the segment from <a href=""http://www.youtube.com/watch?feature=player_detailpage&amp;v=wccOk_mRaoU#t=733"" rel=""nofollow"">12.10</a> to 15.10</p>

<p>I didn't understand the last row key Θ , what did it signify. And given that all row keys are the where clauses and all cf are groub by clauses, how does region (UK,US) came in CF for row key Θ</p>
",<cassandra><olap-cube>,"<p>So this is a super late reply, but I was looking for resources on Cassandra for analytics.</p>

<p>When you build an OLAP table, you aggregate over a particular dimension's values, in their example its the time. So in your table you might have</p>

<pre><code>Hour  Somethings   Other things
0     6            34
1     8            44
2     5            27
3     5            52
...   ...          ...
24    6            42
</code></pre>

<p>The last entry, Θ, represents the sum of all of the above, so you can read it like the total of the columns, so that table might be</p>

<pre><code>Hour  Somethings   Other things
0     6            34
1     8            44
2     5            27
3     5            52
...   ...          ...
24    6            42
Θ     144          960
</code></pre>

<p>(144 is the total number of somethings, and 960 of other things). I made up all these numbers btw, they don't appear in the video.</p>
",['table']
19629187,19631110,2013-10-28 07:27:57,How to alter cassandra table columns,"<p>I need to add additional columns to a table in cassandra. But the existing table is not empty. Is there any way to update it in a simple way? Otherwise what is the best approach to add additional columns to a non empty table? thx in advance.</p>
",<cassandra><cql>,"<p>There's a good example of adding table columns to an existing table in the CQL documentation on <a href=""https://docs.datastax.com/en/cql/3.1/cql/cql_reference/alter_table_r.html#reference_ds_xqq_hpc_xj__adding-a-column"" rel=""nofollow noreferrer"">ALTER</a>. The following statement will add the column <code>gravesite</code> (with type <code>varchar</code>) to to the table <code>addamsFamily</code>:</p>

<blockquote>
  <p>ALTER TABLE addamsFamily ADD gravesite varchar;</p>
</blockquote>
",['table']
19663884,19666901,2013-10-29 16:25:11,Error trying to connect using Astyanax to Cassandra hosted on a EC2 instance,"<p>I am getting the following error ""astyanax.connectionpool.exceptions.PoolTimeoutException:"" when trying to use client Astyanax  to connect to Cassandra on a EC2 instance. Need help</p>

<pre><code>Following is my code snippet.
   import org.mortbay.jetty.servlet.Context;

  import com.netflix.astyanax.AstyanaxContext;
  import com.netflix.astyanax.Keyspace;
  import com.netflix.astyanax.MutationBatch;
  import com.netflix.astyanax.connectionpool.NodeDiscoveryType;
  import com.netflix.astyanax.connectionpool.OperationResult;
  import com.netflix.astyanax.connectionpool.exceptions.ConnectionException;
  import com.netflix.astyanax.connectionpool.impl.ConnectionPoolConfigurationImpl;
  import com.netflix.astyanax.connectionpool.impl.ConnectionPoolType;
  import com.netflix.astyanax.connectionpool.impl.CountingConnectionPoolMonitor;
  import com.netflix.astyanax.impl.AstyanaxConfigurationImpl;
  import com.netflix.astyanax.model.Column;
  import com.netflix.astyanax.model.ColumnFamily;
  import com.netflix.astyanax.model.ColumnList;
  import com.netflix.astyanax.model.CqlResult;
  import com.netflix.astyanax.serializers.StringSerializer;
  import com.netflix.astyanax.thrift.ThriftFamilyFactory;


  public class MetadataRS {


    public static void main(String args[]){
    AstyanaxContext&lt;Keyspace&gt; context = new AstyanaxContext.Builder()
    .forCluster(""ClusterName"")
    .forKeyspace(""KeyspaceName"")
    .withAstyanaxConfiguration(new AstyanaxConfigurationImpl()   
        .setDiscoveryType(NodeDiscoveryType.RING_DESCRIBE)
        .setConnectionPoolType(ConnectionPoolType.ROUND_ROBIN)
    )
    .withConnectionPoolConfiguration(new     ConnectionPoolConfigurationImpl(""MyConnectionPool"")
        .setPort(9042)
        .setMaxConnsPerHost(40)
        .setSeeds(""&lt;EC2-IP&gt;:9042"")
        .setConnectTimeout(5000)
    )
    .withConnectionPoolMonitor(new CountingConnectionPoolMonitor())
    .buildKeyspace(ThriftFamilyFactory.getInstance());

    context.start();
    Keyspace keyspace = context.getEntity();
    System.out.println(keyspace);

    ColumnFamily&lt;String, String&gt; CF_USER_INFO =
              new ColumnFamily&lt;String, String&gt;(
                ""Standard1"",              // Column Family Name
                StringSerializer.get(),   // Key Serializer
                StringSerializer.get());  // Column 

    OperationResult&lt;ColumnList&lt;String&gt;&gt; result = null;
    try {
        result = keyspace.prepareQuery(CF_USER_INFO)
            .getKey(""user_id_hash"")
            .execute();
    } catch (ConnectionException e) {
        // TODO Auto-generated catch block
        e.printStackTrace();
    }
            ColumnList&lt;String&gt; columns = result.getResult();

            // Lookup columns in response by name 

            String uid   = columns.getColumnByName(""user_id_hash"").getStringValue();

            System.out.println(uid);
            // Or, iterate through the columns
            for (Column&lt;String&gt; c : result.getResult()) {
              System.out.println(c.getName());
            }
 }
 }
</code></pre>

<p>Error
    com.netflix.astyanax.thrift.ThriftKeyspaceImpl@1961f4
    com.netflix.astyanax.connectionpool.exceptions.PoolTimeoutException: PoolTimeoutException: [host=():9042, latency=5001(5001), attempts=1] Timed out waiting for connection
        at com.netflix.astyanax.connectionpool.impl.SimpleHostConnectionPool.waitForConnection(SimpleHostConnectionPool.java:201)
        at com.netflix.astyanax.connectionpool.impl.SimpleHostConnectionPool.borrowConnection(SimpleHostConnectionPool.java:158)
        at com.netflix.astyanax.connectionpool.impl.RoundRobinExecuteWithFailover.borrowConnection(RoundRobinExecuteWithFailover.java:60)
        at com.netflix.astyanax.connectionpool.impl.AbstractExecuteWithFailoverImpl.tryOperation(AbstractExecuteWithFailoverImpl.java:50)
        at com.netflix.astyanax.connectionpool.impl.AbstractHostPartitionConnectionPool.executeWithFailover(AbstractHostPartitionConnectionPool.java:229)
        at com.netflix.astyanax.thrift.ThriftColumnFamilyQueryImpl$1.execute(ThriftColumnFamilyQueryImpl.java:180)
        at com.rjil.jiodrive.rs.MetadataRS.main(MetadataRS.java:57)
    Exception in thread ""main"" java.lang.NullPointerException
        at com.rjil.jiodrive.rs.MetadataRS.main(MetadataRS.java:62)</p>
",<amazon-ec2><cassandra><astyanax>,"<p>Since you are running cassandra on EC2 instance, check that the cassandra's port no. (which you have choosen as 9042) is in the allowed list of ec2 security group and that you have access to it. If not add the port no. in the inbound list of the ec2 security group and set the ip ranges as 0.0.0.0.
Alos check the firewall on the ec2 is turned off. By default its false, but its good to check it anyway.</p>

<p>If you have done this, then your client might be behind a firewall that prevents outbound traffic to your choosen port (9042).</p>

<p>Lastly if you have not used any elastic ip, its better to use the ec2 instance dns name, both in your setSeeds section and in the rpc_address of cassandra.yaml</p>
",['rpc_address']
19722075,19722566,2013-11-01 06:47:43,How to generate tokens for my two node Cassandra cluster?,"<p>I am trying to setup two node Cassandra Cluster on windows machine. I have basically two windows machine and I was following this datastax <a href=""http://www.datastax.com/2012/01/how-to-setup-and-monitor-a-multi-node-cassandra-cluster-on-windows"" rel=""nofollow"">tutorial</a></p>

<p>Whenever I use the below command to get the token number from the above tutorial -</p>

<pre><code>python -c ""num=2; print """"\n"""".join([(""""token %d: %d"""" %(i,(i*(2**127)/num))) for i in range(0,num)])""
</code></pre>

<p>I always get this error - </p>

<pre><code>C:\Users\username&gt;python -c ""num=2; print """"\n"""".join([(""""token %d: %d"""" %(i,(i*(2**127)/num))) for i
in range(0,num)])""
  File ""&lt;string&gt;"", line 1
    num=2; print ""\n"".join([(""token %d: %d"" %(i,(i*(2**127)/num))) for i in range(0,num)])
                    ^
SyntaxError: invalid syntax
</code></pre>
",<python><cassandra><datastax-enterprise><opscenter>,"<p>You might have better luck putting that command into an actual Python script.
Here is a similar Python script that I use (saved as newCluster.py):</p>

<pre><code>import sys

if (len(sys.argv) &gt; 1):
        num=int(sys.argv[1])
else:
        num=int(raw_input(""How many nodes are in your cluster? ""))
for i in range(0, num):
        print 'node %d: %d' % (i, (i*(2**127)/num))
</code></pre>

<p>When I run that for two nodes, I get:</p>

<pre><code>How many nodes are in your cluster? 2
node 0: 0
node 1: 85070591730234615865843651857942052864
</code></pre>

<p>Here is exactly how I edit and run it:</p>

<p><img src=""https://i.stack.imgur.com/v421c.jpg"" alt=""enter image description here""></p>

<p>Which version of Python are you using?  I have tested this script in 2.6.7 and 2.7.3.</p>

<p>Also to be balanced, your initial_token values for a two node cluster simply need to have a difference of 85,070,591,730,234,615,865,843,651,857,942,052,864.  They don't necessarily have to be 0 and 85070591730234615865843651857942052864; although those two values should work just fine.</p>
",['initial_token']
19777816,20479890,2013-11-04 21:48:01,"Choosing the right schema for cassandra ""table"" in CQL3","<p>We are trying to store lots of attributes for a particular profile_id inside a table (using CQL3) and cannot wrap our heads around which approach is the best:</p>

<p>a. create table mytable (profile_id, a1 int, a2 int, a3 int, a4 int ... a3000 int) primary key (profile_id);</p>

<p>OR</p>

<p>b. create MANY tables, eg. 
create table mytable_a1(profile_id, value int) primary key (profile_id);
create table mytable_a2(profile_id, value int) primary key (profile_id);
...
create table mytable_a3000(profile_id, value int) primary key (profile_id);</p>

<p>OR</p>

<p>c. create table mytable (profile_id, a_all text) primary key (profile_id);
and just store 3000 ""columns"" inside a_all, like:
insert into mytable (profile_id, a_all) values (1, ""a1:1,a2:5,a3:55, .... a3000:5"");</p>

<p>OR </p>

<p>d. none of the above</p>

<p>The type of query we would be running on this table:
select * from mytable where profile_id in (1,2,3,4,5423,44)</p>

<p>We tried the first approach and the queries keep timing out and sometimes even kill cassandra nodes.</p>
",<cassandra><cql3>,"<p>The answer would be to use a clustering column. A clustering column allows you to create dynamic columns that you could use to hold the attribute name (col name) and it's value (col value).</p>

<p>The table would be </p>

<pre><code>create table mytable ( 
    profile_id text,
    attr_name text,
    attr_value int,
    PRIMARY KEY(profile_id, attr_name)
)
</code></pre>

<p>This allows you to add inserts like</p>

<pre><code>insert into mytable (profile_id, attr_name, attr_value) values ('131', 'a1', 3);
insert into mytable (profile_id, attr_name, attr_value) values ('131', 'a2', 1031);
.....
insert into mytable (profile_id, attr_name, attr_value) values ('131', 'an', 2);
</code></pre>

<p>This would be the optimal solution.</p>

<p>Because you then want to do the following
'The type of query we would be running on this table: select * from mytable where profile_id in (1,2,3,4,5423,44)'</p>

<p>This would require 6 queries under the hood but cassandra should be able to do this in no time especially if you have a multi node cluster.</p>

<p>Also if you use the DataStax Java Driver you can run this requests asynchronously and concurrently on your cluster. </p>

<p>For more on data modelling and the DataStax Java Driver check out DataStax's free online training. Its worth a look
<a href=""http://www.datastax.com/what-we-offer/products-services/training/virtual-training"" rel=""nofollow"">http://www.datastax.com/what-we-offer/products-services/training/virtual-training</a></p>

<p>Hope it helps.</p>
",['table']
19818673,19819025,2013-11-06 17:25:40,Cassandra time sliced data model for unknown data,"<p>I caveat this question by stating: I am somewhat new to NoSQL and very new to Cassandra, but it seems like it might be a good fit for what I'm trying to do.</p>

<p>Say I have a list of sensors giving input at reasonable intervals. My proposed data model is to partition by the name of the sensor, where it is (area) and the date (written as yyyyMMdd), and the cluster the readings for that day by the actual time the reading occurred. The thinking is that the query for ""Get all readings from sensor A on date B"" should be extremely quick. So far so good I think. The table / CF looks like this in CQL:</p>

<pre><code>CREATE TABLE data (
    area_id int,
    sensor varchar,
    date ascii,
    event_time timeuuid,
    PRIMARY KEY ((area_id, sensor, date), event_time)
) WITH CLUSTERING ORDER BY (event_time DESC);
</code></pre>

<p>This doesn't however actually include any data, and I'm not sure how to add this to the model. Each reading (from the same sensor) can have a different set of arbitrary data, and I won't know ahead of time what this. E.g. I could get temperature data, I could get humidity, I could get both, or I could get something I haven't seen before. It's up to the person who actually recorded the data as to what they want to submit (it's not reading from automated sensors).</p>

<p>Given that I want to be doing query operations on this data (which is basically UGC) what are my options? Queries will normally consist of counts on the data (e.g. Count readings from sensor A on date B where some_ugc_valueX = C and some_ugc_valueY = D). It is worth noting that there will be more data points than would normally be queried at once. A reading could have 20 data values, but maybe only 2 or 3 would be queried - just it's unknown which ahead of time.</p>

<p>Currently I have thought of:</p>

<ol>
<li>Store the data for each sensor reading in as a Map type. This would certainly make the model simple, but my understanding is that querying would then be difficult? I think I would need to pull the entire map back for each sensor reading, then check the values and count it outside of Cassandra in Storm/Hadoop/whatever.</li>
<li>Store each of the user values as another column (composite column with event_time uuid). This would mean not using CQL as that doesn't support adding arbitrary new columns at insert time. The Thrift API does however allow this. This means I can get Cassandra to do the counting itself.</li>
</ol>

<p>Maybe I'm going about this the wrong way? Maybe Cassandra isn't even the best choice for this kind of data?</p>
",<cassandra>,"<p>tl;dr. you can't chose both speed and absolute flexibility ;-)</p>

<p>Queries based on data from User Generated Content is going to be complex - you aren't going to be able to produce a one-size-fits-all table definition that will allow quick responses for queries based on UGC-content. Even if you choose to use Maps, Cassandra will have to deserialize the entire data structure on every query so it's not really an option for big Maps - which as you suggest in your question is likely to be the case. </p>

<p>An alternative might be to store the sensor data in a serialised form, e.g., json. This would give maximum flexibility in what is being stored - at the expense of being unable to make complex queries. The serialization/deserialization burden is pushed to the client and all data is sent over the wire. Here's a simple example:</p>

<p>Table creation (slightly simpler than your example - I've dropped <code>date</code>):</p>

<pre><code>create table data(
  area_id int, 
  sensor varchar, 
  event_time timeuuid, 
  data varchar, 
  primary key(area_id,sensor,event_time)
);
</code></pre>

<p>Insertion:</p>

<pre><code>insert into data(area_id,sensor,event_time,data) VALUES (1,'sensor1',now(),'[""datapoint1"":""value1""]');
insert into data(area_id,sensor,event_time,data) VALUES (1,'sensor2',now(),'[""datapoint1"":""value1"",""count"":""7""]');
</code></pre>

<p>Querying by area_id and sensor:</p>

<pre><code>&gt;select area_id,sensor,dateof(event_time),data from data where area_id=1 and sensor='sensor1';

 area_id | sensor  | dateof(event_time)       | data
---------+---------+--------------------------+-------------------------
       1 | sensor1 | 2013-11-06 17:37:02+0000 | [""datapoint1"":""value1""]

(1 rows)
</code></pre>

<p>Querying by area_id:</p>

<pre><code>&gt; select area_id,sensor,dateof(event_time),data from data where area_id=1;

 area_id | sensor  | dateof(event_time)       | data
---------+---------+--------------------------+-------------------------------------
       1 | sensor1 | 2013-11-06 17:37:02+0000 |             [""datapoint1"":""value1""]
       1 | sensor2 | 2013-11-06 17:40:49+0000 | [""datapoint1"":""value1"",""count"":""7""]

(2 rows)
</code></pre>

<p>(Tested using <code>[cqlsh 4.0.1 | Cassandra 2.0.1 | CQL spec 3.1.1 | Thrift protocol 19.37.0]</code>.)</p>
",['table']
19827690,19828117,2013-11-07 03:50:47,how can I add csv to cassandra db?,"<p>I know it can be done in traditional way, but if I were to use Cassandra DB, is there a easy/quick and agaile way to add csv to the DB as a set of key-value pairs ?</p>

<p>Ability to add a time-series data coming via CSV file is my prime requirement. I am ok to switch to any other database such as mongodb, rike, if it is conviniently doable there..</p>
",<cassandra>,"<p><strong>Edit 2</strong> Dec 02, 2017<br>
Please use port 9042. Cassandra access has changed to CQL with default port as 9042, 9160 was default port for Thrift.</p>

<p><strong>Edit 1</strong><br>
There is a better way to do this without any coding. Look at this answer <a href=""https://stackoverflow.com/a/18110080/298455"">https://stackoverflow.com/a/18110080/298455</a> </p>

<p>However, if you want to pre-process or something custom you may want to so it yourself. here is a lengthy method:</p>

<hr>

<ol>
<li><p>Create a column family.</p>

<pre><code>cqlsh&gt; create keyspace mykeyspace 
with strategy_class = 'SimpleStrategy' 
and strategy_options:replication_factor = 1;

cqlsh&gt; use mykeyspace;

cqlsh:mykeyspace&gt; create table stackoverflow_question 
(id text primary key, name text, class text);
</code></pre>

<p>Assuming your CSV is like this:</p>

<pre><code>$ cat data.csv 
id,name,class
1,hello,10
2,world,20
</code></pre></li>
<li><p>Write a simple Python code to read off of the file and dump into your CF. Something like this:</p>

<pre class=""lang-py prettyprint-override""><code>import csv 
from pycassa.pool import ConnectionPool
from pycassa.columnfamily import ColumnFamily

pool = ConnectionPool('mykeyspace', ['localhost:9160'])
cf = ColumnFamily(pool, ""stackoverflow_question"")

with open('data.csv', 'rb') as csvfile:
  reader = csv.DictReader(csvfile)
  for row in reader:
    print str(row)
    key = row['id']
    del row['id']
    cf.insert(key, row)

pool.dispose()
</code></pre></li>
<li><p>Execute this:</p>

<pre><code>$ python loadcsv.py 
{'class': '10', 'id': '1', 'name': 'hello'}
{'class': '20', 'id': '2', 'name': 'world'}
</code></pre></li>
<li><p>Look the data:</p>

<pre><code>cqlsh:mykeyspace&gt; select * from stackoverflow_question;
 id | class | name
----+-------+-------
  2 |    20 | world
  1 |    10 | hello
</code></pre></li>
<li><p>See also:</p>

<p>a. Beware of <a href=""http://docs.python.org/2/library/csv.html#csv.DictReader"" rel=""nofollow noreferrer"">DictReader</a><br>
b. Look at <a href=""http://pycassa.github.io/pycassa/tutorial.html"" rel=""nofollow noreferrer"">Pycassa</a><br>
c. Google for existing CSV loader to Cassandra. I guess there are.<br>
d. There may be a simpler way using CQL driver, I do not know.<br>
e. Use appropriate data type. I just wrapped them all into text. Not good.</p></li>
</ol>

<p>HTH</p>

<hr>

<p>I did not see the time-series requirement. Here is how you do for time series.</p>

<ol>
<li><p>This is your data</p>

<pre><code>$ cat data.csv
id,1383799600,1383799601,1383799605,1383799621,1383799714
1,sensor-on,sensor-ready,flow-out,flow-interrupt,sensor-killAll
</code></pre></li>
<li><p>Create traditional wide row. (CQL suggests not to use <a href=""http://www.datastax.com/dev/blog/does-cql-support-dynamic-columns-wide-rows"" rel=""nofollow noreferrer"">COMPACT STORAGE</a>, but this is just to get you going quickly.)</p>

<pre><code>cqlsh:mykeyspace&gt; create table timeseries 
(id text, timestamp text, data text, primary key (id, timestamp)) 
with compact storage;
</code></pre></li>
<li><p>This the altered code:</p>

<pre class=""lang-py prettyprint-override""><code>import csv
from pycassa.pool import ConnectionPool
from pycassa.columnfamily import ColumnFamily

pool = ConnectionPool('mykeyspace', ['localhost:9160'])
cf = ColumnFamily(pool, ""timeseries"")

with open('data.csv', 'rb') as csvfile:
  reader = csv.DictReader(csvfile)
  for row in reader:
    print str(row)
    key = row['id']
    del row['id']
    for (timestamp, data) in row.iteritems():
      cf.insert(key, {timestamp: data})

pool.dispose()
</code></pre></li>
<li><p>This is your timeseries</p>

<pre><code>cqlsh:mykeyspace&gt; select * from timeseries;
 id | timestamp  | data
----+------------+----------------
  1 | 1383799600 |      sensor-on
  1 | 1383799601 |   sensor-ready
  1 | 1383799605 |       flow-out
  1 | 1383799621 | flow-interrupt
  1 | 1383799714 | sensor-killAll
</code></pre></li>
</ol>
",['table']
19870728,19935462,2013-11-09 00:12:57,"Cassandra DB: is it favorable, or frowned upon, to index multiple criteria per row?","<p>I've been doing a lot of reading lately on Cassandra, and specifically how to structure rows to take advantage of indexing/sorting, but there is one thing I am still unclear on; how many ""index"" items (or filters if you will) should you include in a column family (CF) row?</p>

<p>Specifically: I am building an app and will be using Cassandra to archive log data, which I will use for analytics.</p>

<p>Example types of analytic searches will include (by date range):</p>

<ul>
<li>total visits to specific site section</li>
<li>total visits by Country</li>
<li>traffic source</li>
</ul>

<p>I plan to store the whole log object in JSON format, but to avoid having to go through each item to get basic data, or to create multiple CF just to get basic data, I am curious to know if it's a good idea to include these above ""filters"" as columns (compound column segment)?</p>

<p>Example:</p>

<pre><code>   Row Key       | timeUUID:data | timeUUID:country | timeUUID:source |
                 ======================================================
timeUUID:section |  JSON Object  |       USA        |   example.com   |
</code></pre>

<p>So as you can see from the structure, the row key would be a compound key of timeUUID (say per day) plus the site section I want to get stats for. This lets me query a date range quite easily.</p>

<p>Next, my dilemma, the columns. Compound column name with timeUUID lets me sort &amp; do a time based slice, but does the concept make sense?</p>

<p>Is this type of structure acceptable by the current ""best practice"", or would it be frowned upon? Would it be advisable to create a separate ""index"" CF for each metric I want to query on? (even when it's as simple as this?)</p>

<p>I would rather get this right the first time instead of having to restructure the data and refactor my application code later.</p>
",<data-structures><cassandra>,"<p>I think the idea behind this is OK. It's a pretty common way of doing timeslicing (assuming I've understood your schema anyway - a create table snippet would be great). Some minor tweaks ...</p>

<p>You don't need a timeUUID as your row key. Given that you suggest partitioning by individual days (which are inherently unique) you don't need a UUID aspect. A timestamp is probably fine, or even simpler a varchar in the format YYYYMMDD (or whatever arrangement you prefer).</p>

<p>You will probably also want to swap your row key composition around to section:time. The reason for this is that if you need to specify an IN clause (i.e. to grab multiple days) you can only do it on the last part of the key. This means you can do WHERE section = 'foo' and time IN (....). I imagine that's a more common use case - but the decision is obviously yours.</p>

<p>If your common case is querying the most recent data don't forget to cluster your timeUUID columns in descending order. This keeps the hot columns at the head.</p>

<p>Double storing content is fine (i.e. once for the JSON payload, and denormalised again for data you need to query). Storage is cheap.</p>

<p>I don't think you need indexes, but it depends on the queries you intend to run. If your queries are simple then you may want to store counters by (date:parameter) instead of values and just increment them as data comes in.</p>
",['table']
19940513,20001792,2013-11-12 21:43:45,Hash value from keys on Cassandra,"<p>I'm developing a mechanism for Cassandra using Hector. 
What I need at this moment is to know which are the hash values of the keys to look at which node is stored (looking at the tokens of each one), and ask directly this node for the value. What I understood is that depending on the partitioner Cassandra uses, the values are stored independently from one partitioner to other. So, are the hash values of all keys stored in any table? In case not, how could I implement a generic class that once I read from System Keyspace the partitioner that is using Cassandra this class could be an instance of it without the necessity of modifying the code depending on the partitioner? I would need it to call the getToken method to calculate the hash value for a given key.</p>
",<hash><cassandra><key><hector><partitioner>,"<p>Finally after testing different implementations I found the way to get the partitioner using the next code:</p>

<pre><code>            CqlQuery&lt;String, String, String&gt; cqlQuery = new CqlQuery&lt;String, String, String&gt;(
            ksp, StringSerializer.get(), StringSerializer.get(),   StringSerializer.get());
            cqlQuery.setQuery(""select partitioner from local"");
            QueryResult&lt;CqlRows&lt;String, String, String&gt;&gt; result = cqlQuery.execute();
            CqlRows rows = result.get();
            for (int i = 0; i &lt; rows.getCount(); i++) {
                RowImpl&lt;String, String, String&gt; row = (RowImpl&lt;String, String, String&gt;) rows
                .getList().get(i);
                List&lt;HColumn&lt;String, String&gt;&gt; column = row.getColumnSlice().getColumns();
                for (HColumn&lt;String , String&gt; c: column) {
                    System.out.println(c.getValue());
            }

    } 
</code></pre>
",['partitioner']
19992430,20012429,2013-11-15 02:28:45,Bring back a dead datacenter: repair or rebuild,"<p>I had Cassandra cluster running across two data centers, for some reason, one data center was taken down for a while, and now I'm planning to bring it back. I'm thinking about two approaches: 
One is to start up all Cassandra nodes of this data center and run ""nodetool repair "" on each node one by one. But It looks like 'repair' takes long time. I had an experience to repair 6GB data on a node before, it took me 5 hours on one node (3 nodes cluster). I have much more data on the cluster now, can't image how long it will take. 
So I'm thinking if I can run re-build instead of repair. I can delete all old data on this data center and re-build it as adding a new data center. But not sure if it works and how performance would be. </p>

<p>Any idea on it? Any suggestion would be appreciated. Thanks in advance. </p>
",<cassandra>,"<p>If the data center was down for more than 10 days then rebuild is the only option. This has to do with the <a href=""http://wiki.apache.org/cassandra/DistributedDeletes"" rel=""nofollow"">tombstones</a>. I am not 100% sure how that works across different data centers, but if you have a server that was down for more than 10 days, any data that has been deleted in the live server has been tombstoned and kept there for 10 days, then removed completely. If all suddenly your downed server wakes up from the sleep, with all deleted data not tombstoned, then it will be repopulated back to the ring via read repair or regular repair operation.</p>

<p>Another thing to consider, is how much data has changed/deleted since the datacenter went down. If a lot, then it obviously it is less work to rebuild. If not then maybe repair will be faster.</p>

<p>You can create another datacenter, add nodes to it with <code>auto_bootstrap: false</code> and then run <code>nodetool rebuild &lt;live dc name&gt;</code></p>

<p>Good luck!</p>
",['dc']
20136083,20145217,2013-11-22 02:48:36,Cassandra CLI create column family with primary key,"<p>I am trying to create a column family in Cassandra CLI Version 1.1.6, I am not sure how to specify primary key as movieid.</p>

<pre><code>CREATE COLUMN FAMILY movies
WITH comparator = UTF8Type
AND key_validation_class=UTF8Type
AND column_metadata = [
{column_name: movieid, validation_class: UTF8Type}
{column_name: title, validation_class: UTF8Type}
{column_name: genres, validation_class: UTF8Type}];
</code></pre>
",<cassandra><cassandra-cli>,"<p>Creating a column family via the CLI doesn't create a schema that you have to stick to. It depends on how you insert the data, that is what defines the primary key. When you create the column family via the CLi you only have to define what kind of value the primary key will contain i.e. is it a string (UTF8Type), int (IntegerType) etc. </p>

<p>Also you cant actually have an alias for the KEY column (aka the primary key in your table) via the CLI. You <a href=""https://issues.apache.org/jira/browse/CASSANDRA-4158"" rel=""nofollow"">have to use CQL for that</a>. If you want defined schemas and structured queries rather than using wide rows you should target a newer version of cassandra (why now 1.2.x?) and use CLQ3.</p>

<p>A more visual representation of what I mean, your cassandra-cli statement creates this when viewed from cqlsh via the describe table command:</p>

<pre><code>CREATE TABLE movies (
  KEY text PRIMARY KEY,    &lt;------- pk 
  genres text,
  title text,
  movieid text
) WITH // and a bunch of cf options
</code></pre>

<p>But this doesn't mean you cant insert another column that isn't defined there, because thrift doesn't really care about the CF's schema. </p>
",['table']
20219934,20220872,2013-11-26 14:34:08,I can't start Cassandra Server in Eclipse( Unknown Commitlog version 4),"<p>I'm trying to run Cassandra in eclipse, but I'm getting this exception</p>

<pre><code>java.lang.IllegalStateException: Unknown commitlog version 4Exception encountered during startup: Unknown commitlog version 4

at org.apache.cassandra.db.commitlog.CommitLogDescriptor.getMessagingVersion(CommitLogDescriptor.java:81)
at org.apache.cassandra.db.commitlog.CommitLogReplayer.recover(CommitLogReplayer.java:118)
at org.apache.cassandra.db.commitlog.CommitLogReplayer.recover(CommitLogReplayer.java:93)
at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:146)
at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:126)
at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:305)
at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:461)
at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:504)
</code></pre>

<p>What am I doing wrong?</p>
",<eclipse><cassandra>,"<p>Sounds like a version mismatch - possibly from downgrading Cassandra[?]</p>

<p>What version of Cassandra are you using in eclipse? Also, did you have another version running <em>and sharing the same commitlogs</em>? It is likely you have commitlogs from one version of cassandra being read from another. (That was my experience.)</p>

<p>Adding the solution, as provided by @LyubenTodorov in the comments:</p>

<blockquote>
  <p>To solve this either change your commitlog_directory or empty your current commitlog dir (default is /var/lib/cassandra/commitlog) </p>
</blockquote>
",['commitlog_directory']
20332462,20336125,2013-12-02 16:00:10,insert into column family with case sensitive column name,"<p>I am using the following Cassandra/CQL versions:</p>

<p>[cqlsh 4.0.1 | Cassandra 2.0.1 | CQL spec 3.1.1 | Thrift protocol 19.37.0]</p>

<p>I am trying to insert data into a pre-existing CF with case sensitive column names. I hit ""unknown identifier"" errors when trying to insert data.</p>

<p>Following is how the column family is described:</p>

<pre><code>CREATE TABLE ""Sample_List_CS"" (
  key text,
  column1 text,
  ""fName"" text,
  ""ipSubnet"" text,
  ""ipSubnetMask"" text,
  value text,
  PRIMARY KEY (key, column1)
) WITH COMPACT STORAGE AND
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=0 AND
  index_interval=128 AND
  read_repair_chance=0.000000 AND
  replicate_on_write='false' AND
  populate_io_cache_on_flush='false' AND
  default_time_to_live=0 AND
  speculative_retry='NONE' AND
  memtable_flush_period_in_ms=0 AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'LZ4Compressor'};

CREATE INDEX ipSubnet ON ""Sample_List_CS"" (""ipSubnet"");
</code></pre>

<p>The insert statements result in errors:</p>

<pre><code>cqlsh:Sample_KS&gt; INSERT INTO ""Sample_List_CS"" (key,column1,""fName"") VALUES    ('123','1','myValue');
Bad Request: Unknown identifier fName

cqlsh:Sample_KS&gt; INSERT INTO ""Sample_List_CS"" (key,column1,""ipSubnet"") VALUES    ('123','1','255');
Bad Request: Unknown identifier ipSubnet
</code></pre>

<p>Any idea what I am doing wrong?</p>
",<cassandra><cassandra-2.0>,"<p>As I understand it when using <code>WITH COMPACT STORAGE</code> a table may only have one column other than the primary key. </p>

<p>As quoted in <a href=""http://www.datastax.com/documentation/cql/3.1/webhelp/cql/cql_reference/create_table_r.html"" rel=""nofollow"">the manual</a>:</p>

<blockquote>
  <p>Using the compact storage directive prevents you from adding more than
  one column that is not part of the PRIMARY KEY.</p>
</blockquote>

<p>For you that means you can only have one of these 4 columns in your table:</p>

<ul>
<li>""fName""</li>
<li>""ipSubnet""</li>
<li>""ipSubnetMask""</li>
<li>value</li>
</ul>

<p>(Alternatively, you could add 3 of them to the primary key definition.)</p>

<p>Thus it makes sense that the other three columns lead to an <code>Unknown identifier</code> error.</p>
",['table']
20356403,20363750,2013-12-03 16:24:29,Cassandra - Exactly one wide row per node for a given ColumnFamily?,"<p>Subject says it all. I want to be able to randomly distribute a large set of records but keep them clustered in one wide row per node.</p>

<p>As an example, lets say I’ve got a collection of about 1 million records each with a unique id. If I just go ahead and set the primary key (and therefore the partition key) as the unique id, I’ll get very good random distribution across my server cluster. However, each record will be its own row. I’d like to have each record belong to one large wide row (per server node) so I can have them sorted or clustered on some other column.</p>

<p>If I say have 5 nodes in my cluster, I could randomly assign a value of 1 - 5 at the time of creation and have the partition key set to this value. But this becomes troublesome if I add or remove nodes. What effectively I want is to partition on the unique id of the record modulus N (id % N; where N is the number of nodes).</p>

<p>I have to imagine there’s a mechanism in Cassandra to essentially randomize the partitioning without even using a key (and then clustering on some column).</p>

<p>Thanks for any help.</p>
",<database><nosql><cassandra>,"<p>You really do not want to do what you are saying you want to do. </p>

<p>First of all, there really is no good mechanism to ensure even distribution of one row per node within Cassandra. You could easily do it once by calculating the tokens so they would be distributed among your nodes initially, but if you ever did change the cluster topology (e.g. add or remove nodes or datacenters) then you would need to manually recalculate and move data around. All of this is exactly what Cassandra is designed to do for you.</p>

<p>Instead of going with your strict goal of one row per node, compromise a bit, and go with around 100-1000 totals rows. Use the last 2 or 3 digits (as a convenience, you can use anything else as well) as the shard id, and create a table like so:</p>

<pre><code> create table test (shard_id int, id int, value text, primary key (shard_id,id));
 insert into test (shard_id, id, value) values(72,193727872, 'value1');
 insert into test (shard_id, id, value) values(73,193727873, 'value2');
 insert into test (shard_id, id, value) values(73,7234243873, 'value3');
 insert into test (shard_id, id, value) values(73,193727874, 'value4');

 select * from test where shard_id = 73;

  shard_id | id        | value
 ----------+-----------+--------
        73 | 193727873 | value2
        73 | 193727874 | value4
        73 | 723423873 | value3
</code></pre>

<p>So you achieve even distribution of your data across the cluster because of the shard_id, and by quickly enumerating through the shard_ids, you can retrieve all of the values. Each read is wide enough (with a million+ total cells) that you take advantage of linear disk reads, and there are few enough random seeks.</p>

<p>You can also perform any of the other operations (gt/lt comparisons). You just have to do a little bit of extra work in your code to make the read use the correct shard id, and continue on to the next shard if necessary.</p>

<p>Slight increase in complexity. </p>

<p>Very small decrease in linear read performance.</p>

<p>Very good operational runtime characteristics.</p>
",['table']
20434036,20434244,2013-12-06 21:38:58,Creating a custom index on a collection using CQL 3.0,"<p>I have been looking at the CQL 3.0 <a href=""http://www.datastax.com/documentation/cql/3.0/webhelp/cql/ddl/ddl_anatomy_table_c.html#concept_ds_qqw_1dy_zj"" rel=""nofollow"">data modelling documentation</a> which describes a column family of songs with tags, created like this:</p>

<pre><code>CREATE TABLE songs (
    id uuid PRIMARY KEY,
    title text,
    tags set&lt;text&gt;
);
</code></pre>

<p>I would like to get a list of all songs which have a specific tag, so I need to add an appropriate index.</p>

<p>I can create an index on the <code>title</code> column easily enough, but if I try to index the <code>tags</code> column which is a collection, like this:</p>

<pre><code>CREATE INDEX ON songs ( tags );
</code></pre>

<p>I get the following error from the DataStax Java driver 1.0.4:</p>

<pre><code>Exception in thread ""main"" com.datastax.driver.core.exceptions.InvalidQueryException: Indexes on collections are no yet supported
at com.datastax.driver.core.exceptions.InvalidQueryException.copy(InvalidQueryException.java:35)
at com.datastax.driver.core.ResultSetFuture.extractCauseFromExecutionException(ResultSetFuture.java:269)
</code></pre>

<p>It looks like this may be fixed in a later version of Cassandra (2.1) according to JIRA issue <a href=""https://issues.apache.org/jira/browse/CASSANDRA-4511"" rel=""nofollow"">CASSANDRA-4511</a>. I am currently using Apache Cassandra 1.2.11 however, and do not want to upgrade yet. According to issue <a href=""https://issues.apache.org/jira/browse/CASSANDRA-5615"" rel=""nofollow"">CASSANDRA-5615</a> though, in Cassandra 1.2.6 there <em>is</em> support for <em>custom indexes</em> on collections.</p>

<p>The problem is, the only <a href=""http://www.datastax.com/documentation/cql/3.1/webhelp/cql/cql_reference/create_index_r.html"" rel=""nofollow"">documentation</a> available states:</p>

<blockquote>
  <p>Cassandra supports creating a custom index, which is for internal use and beyond the scope of this document.</p>
</blockquote>

<p>But, it <em>does</em> suggest the following syntax:</p>

<pre><code>CREATE CUSTOM INDEX ON songs ( tags ) USING 'class_name';
</code></pre>

<p>What is the <code>class_name</code> that is specified in this CQL statement?</p>

<p>Is there a better way of indexing the tags so that I can query the <em>songs</em> table for a list of songs that have a specific tag?</p>
",<java><cassandra><cql3><datastax-java-driver>,"<p>The way you are trying to do this isn't the best way to model it within Cassandra in my view. You build models based on your queries, not your data. If you need to find songs based by tag, then you make another table for that and duplicate the data. Something like ...</p>

<pre><code>CREATE TABLE tagged_songs (
  tag varchar,
  song_id uuid,
  song_title varchar,
  ... anything else you might need with your songs here ...
  PRIMARY KEY ((tag), song_id)
);
</code></pre>

<p>The premise in Cassandra is that storage is cheap. Duplicate your data to meet your queries. Writes are fast, and writing the same data 3,4,10 times is normally fine. </p>

<p>You also want to store your song title and any other info you need into this table. You don't want to grab a load of IDs and try join on it when reading. This isn't a relational DB.</p>

<p>When someone tags a song, you might want to insert the tag into the set as you have it as present, AND add it to the tagged_songs table too. Querying for all songs with tag X is then basically O(1).</p>
",['table']
20502001,20552508,2013-12-10 18:05:40,How much load can cassandra handle on m1.xlarge instance?,"<p>I setup 3 nodes of Cassandra (1.2.10) cluster on 3 instances of EC2 m1.xlarge. </p>

<p>Based on default configuration with several guidelines included, like:</p>

<ul>
<li>datastax_clustering_ami_2.4</li>
<li>not using EBS, raided 0 xfs on ephemerals instead,</li>
<li>commit logs on separate disk,</li>
<li>RF=3,</li>
<li>6GB heap, 200MB new size (also tested with greater new size/heap values),</li>
<li>enhanced limits.conf.</li>
</ul>

<p>With <strong>500 writes per second</strong>, the cluster works only for couple of hours. After that time it seems like not being able to respond because of CPU overload (mainly GC + compactions).</p>

<p>Nodes remain Up, but their load is huge and logs are full of GC infos and messages like:</p>

<pre><code>ERROR [Native-Transport-Requests:186] 2013-12-10 18:38:12,412 ErrorMessage.java (line 210) Unexpected exception during request java.io.IOException: Broken pipe
</code></pre>

<p>nodetool shows many dropped mutations on each node:</p>

<pre><code>Message type           Dropped
RANGE_SLICE                  0
READ_REPAIR                  7
BINARY                       0
READ                         2
MUTATION               4072827
_TRACE                       0
REQUEST_RESPONSE          1769
</code></pre>

<p>Is 500 wps too much for 3-node cluster of m1.xlarge and I should add nodes? Or is it possible to further tune GC somehow? <strong>What load are you able to serve with 3 nodes of m1.xlarge? What are your GC configs?</strong></p>
",<amazon-ec2><garbage-collection><cassandra>,"<p>Cassandra is perfectly able to handle <em>tens of thousands</em> small writes per second on a single node. I just checked on my <em>laptop</em> and got about 29000 writes/second from cassandra-stress on Cassandra 1.2. So 500 writes per second is not really an impressive number even for a single node. </p>

<p>However beware that there is also a limit on how fast data can be flushed to disk and you definitely don't want your incoming data rate to be close to the physical capabilities of your HDDs. Therefore 500 writes per second can be too much, if those writes are big enough.</p>

<p>So first - what is the average size of the write? What is your replication factor? Multiply number of writes by replication factor and by average write size - then you'll approximately know what is required write throughput of a cluster. But you should take some safety margin for other I/O related tasks like compaction. There are various benchmarks on the Internet telling a single m1.xlarge instance should be able to write anywhere between 20 MB/s to 100 MB/s...</p>

<p>If your cluster has sufficient I/O throughput (e.g. 3x more than needed), yet you observe OOM problems, you should try to:</p>

<ol>
<li>reduce memtable_total_space_mb (this will cause C* to flush smaller memtables, more often, freeing heap earlier)</li>
<li>lower write_request_timeout to e.g. 2 seconds instead of 10 (if you have big writes, you don't want to keep too many of them in the incoming queues, which reside on the heap)</li>
<li>turn off row_cache (if you ever enabled it)</li>
<li>lower size of the key_cache</li>
<li>consider upgrading to Cassandra 2.0, which moved quite a lot of things off-heap (e.g. bloom filters and index-summaries); this is especially important if you just store lots of data per node</li>
<li>add more HDDs and set multiple data directories, to improve flush performance</li>
<li>set larger new generation size; I usually set it to about 800M for a 6 GB heap, to avoid pressure on the tenured gen. </li>
<li>if you're sure memtable flushing lags behind, make sure sstable compression is enabled - this will reduce amount of data physically saved to disk, at the cost of additional CPU cycles</li>
</ol>
",['write_request_timeout']
20522570,20524792,2013-12-11 15:04:42,Cassandra; best practice regarding Indexes?,"<p>I am modelling a Cassandra schema to get a bit more familiar on the subject and was wondering what is the best practice regarding creating indexes.</p>

<p>For example:</p>

<pre><code>create table emailtogroup(email text, groupid int, primary key(email));
select * from emailtogroup where email='joop';
create index on emailtogroup(groupid);
select * from emailtogroup where groupid=2 ;
</code></pre>

<p>Or i can create a entire new table:</p>

<pre><code>create table grouptoemail(groupid int, email text,  primary key(groupid, email));
select * from grouptoemail where groupid=2;
</code></pre>

<p>They both do the job.</p>

<p>I would expect creating a new table is faster cause now groupid becomes the partition key. But i'm not sure what ""magic"" is happening when creating a index and if this magic has a downside.</p>
",<java><cassandra>,"<p>According to me your first approach is correct.</p>

<pre><code>create table emailtogroup(email text, groupid int, primary key(email));
</code></pre>

<p>because 1) in your case email is sort of unique, good candidate for primary key and 2) multiple emails can belong to same group, good candidate for secondary index. Please refer to this post - <a href=""https://stackoverflow.com/questions/18168379/cassandra-choosing-a-partition-key"">Cassandra: choosing a Partition Key</a></p>

<p>The partitioning key is used to distribute data across different nodes, and if you want your nodes to be balanced (i.e. well distributed data across each node) then you want your partitioning key to be as random as possible. </p>

<p>The second form of table creation is useful for range scans. For example if you have a use case like </p>

<p>i) List all the email groups which the user has joined from 1st Jan 2010 to 1st Jan 2013.</p>

<p>In that case you may have to design a table like </p>

<pre><code>create table grouptoemail(email text, ts timestamp, groupid int, primary key(email, ts));
</code></pre>

<p>In this case all the email gropus which the user joined will be clustered on disk.(stored together on disk)</p>
",['table']
20643515,20645781,2013-12-17 19:59:36,Cassandra not balancing data over existing nodes in cluster,"<p>Greeings,
I have configured 3 node Cassandra 1.2.12 cluster and I am able to connect to master and create keyspaces and tables over all nodes. However, I want to run YCSB over my cluster so when I run YCSB and load data it is all loaded on Master. Since I am loading 1000000 records I calculated initial tokens by dividing that number by number of nodes I have.
When I run nodetool I get something like:</p>

<pre><code>Address    Rack    Status    State    Load    Owns    Token
10.3.2.8   2       Up        Normal   1.08GB  100%    0
10.3.1.231 2       Up        Normal   67.58KB  0%     330000
10.3.1.128 2       Up        Normal   52.79KB  0%     660000
</code></pre>

<p>Did someone had same problem? I have tried using tokengentool to assign tokes and diffrenet partitions (Murmur3 and Random) and it was all same, just loading all data on Master node.</p>

<p>Regards, Veronika.</p>
",<cassandra><data-partitioning><ycsb>,"<p>A ""row"" does not equal a token in Cassandra.  Regardless of the number of rows you intend to store, Cassandra's RandomPartitioner supports 2^127 tokens.  For a 3-node cluster, those initial tokens should be increments of 56,713,727,820,156,410,577,229,101,238,628,035,242 apart from each other.</p>

<p>Using DataStax's Python script for computing initial tokens, these RandomPartitioner values should work for you:</p>

<pre><code>node 0: 0
node 1: 56713727820156410577229101238628035242
node 2: 113427455640312821154458202477256070485
</code></pre>

<p>If you are using the Murmur3 Partitioner (-2^63 to +2^63 tokens), use these values:</p>

<pre><code>node 0: -9223372036854775808
node 1: -3074457345618258603
node 2: 3074457345618258602
</code></pre>

<p>So at this point, you have two choices:</p>

<p>1 - Decommission 10.3.1.231 and 10.3.1.128, stop the nodes, alter their initial_token values to match what I have above, and restart them.  But given the fact that you have mentioned trying both the Murmur3 and RandomPartitioner, I'm thinking that it might be best for your to go with option #2 below.</p>

<p>2 - Stop all nodes, delete your data, <a href=""http://www.datastax.com/documentation/cassandra/1.2/webhelp/index.html#cassandra/initialize/initializeSingleDS.html"" rel=""nofollow"">follow these instructions</a>, and reload your data.</p>

<p>Also, you may want to adjust the replication factor you defined for your keyspace(s).  For a 3-node cluster, you will want a replication factor of at least 2.  This will ensure that if one server goes down, you will still have one copy of the data out there.  And that should still allow your application to resolve (as long as your read/write consistency is set to ONE).</p>
",['initial_token']
20646598,24172932,2013-12-17 23:12:37,Token Aware Astyanax Connection pool connecting on nodes without distributing connections over nodes,"<p>I was using astyanax connection pool defined as this:</p>

<pre><code>ipSeeds = ""LOAD_BALANCER_HOST:9160"";
conPool.setSeeds(ipSeeds)
.setDiscoveryType(NodeDiscoveryType.TOKEN_AWARE)
.setConnectionPoolType(ConnectionPoolType.TOKEN_AWARE);
</code></pre>

<p>However, my cluster have 4 nodes and I have 8 client machines connecting on it. <code>LOAD_BALANCER_HOST</code> forwards requests to one of my four nodes.</p>

<p>On a client node, I have:</p>

<pre><code>$netstat -an | grep 9160 | awk '{print $5}' | sort |uniq -c
    235 node1:9160
    680 node2:9160
      4 node3:9160
      4 node4:9160
</code></pre>

<p>So although the ConnectionPoolType is <code>TOKEN_AWARE</code>, my client seems to be connecting mainly to node2, sometimes to node1, but almost never to nodes 3 and 4.<br>
Question is: 
Why is this happening? Shouldn't a token aware connection pool query the ring for the node list and connect to all the active nodes using round robin algorithm?</p>
",<cassandra><astyanax><amazon-elb><cassandra-2.0>,"<p><code>William Price</code> is totally right: the fact you're using a <code>TokenAwarePolicy</code> and possibly a default <code>Partitioner</code> means that
- first your data will be stored biased across your nodes and
- then on querying the <code>LoadbalancingPolicy</code> makes your driver remember the correct nodes to ask for</p>

<p>You can improve your cluster's performance by using some deviating or may be a custom partitioner to equally distribute your data. To randomly query nodes use either </p>

<ul>
<li><code>RoundRobinPolicy</code> (<a href=""http://www.datastax.com/doc-source/developer/java-apidocs/com/datastax/driver/core/policies/RoundRobinPolicy.html"" rel=""nofollow"">http://www.datastax.com/doc-source/developer/java-apidocs/com/datastax/driver/core/policies/RoundRobinPolicy.html</a>) or </li>
<li><code>DatacenterAwareRoundRobinPolicy</code> (<a href=""http://www.datastax.com/doc-source/developer/java-apidocs/com/datastax/driver/core/policies/DCAwareRoundRobinPolicy.html"" rel=""nofollow"">http://www.datastax.com/doc-source/developer/java-apidocs/com/datastax/driver/core/policies/DCAwareRoundRobinPolicy.html</a>).</li>
</ul>

<p>The latter, of course, needs the definition of data centers in your keyspace.</p>

<p>Without any further information I would suggest to just change the partitioner as a TokenAware load balancing policy is usually a good idea. The main load will end up on these nodes in the end -- the TokenAware policy get's you to the right coordinator just quicker.</p>
",['partitioner']
20661450,20688535,2013-12-18 14:58:36,"Cassandra , Hector :how to retrieve specific set of columns for specific keys from a column family in 1 call?","<p>If there are row keys List rowkeys
and for every row key i want some specific set of columns , for some row keys columns might be different and for some row keys they might be different.</p>

<p>How can i do it in Hector ?</p>

<pre><code>mutliget_slice methods requires keys to be in range plus it does not allow 
different ranges for different keys too.
</code></pre>

<p>I am not able to find any relevant Hector class which fulfils this requirement of multiget </p>

<p>EDIT:-</p>

<p>I could not find any way , currently i am using
mutliget_slice with combined list of columns for all keys and then filtering</p>
",<java><nosql><cassandra><hector>,"<p>You don't.</p>

<p>The native Java driver (<a href=""https://github.com/datastax/java-driver"" rel=""nofollow"">https://github.com/datastax/java-driver</a>) addresses this by making everything asynchronous, so you can easily request appropriate columns from a bunch of rows, then wait for all the results.  (This has no more overhead than a multiget.)</p>

<p>That said, if you can denormalize into a separate table instead of doing a multiget, that's probably best of all.</p>
",['table']
20782755,20827913,2013-12-26 09:57:29,Cassandra simple insert doesn't works,"<p>I'm trying to make simple insert in cassandra with Hector client.
I've created simple table:</p>

<pre><code>create table customs (id uuid PRIMARY KEY, char_col_1 varchar);
</code></pre>

<p>And try to insert something:</p>

<pre><code>UUID columnId = UUID.randomUUID();

        HColumn&lt;String, String&gt; column =
                HFactory.createColumn(
                        ""char_col_1"",
                        ""test"",
                        12345,
                        StringSerializer.get(),
                        StringSerializer.get()
                );


        Mutator&lt;UUID&gt; mutator = HFactory.createMutator(keyspace, UUIDSerializer.get());
        mutator.insert(columnId, ""customs"", column);
</code></pre>

<p>But I always get error:</p>

<pre><code>InvalidRequestException(why:Not enough bytes to read value of component 0)
</code></pre>
",<cassandra><hector>,"<p>I think this is because Hector is unable to insert data (through  thrift) to a table with non-compact storage. I'm not a hector expert though. </p>

<p>If you create your column family as follows, it might work.</p>

<pre><code>create table customs (id uuid PRIMARY KEY, char_col_1 varchar) with compact storage;
</code></pre>

<p>But, why do you need to create a column family in CQL and insert data through hector. That is not a recommended way. You should choose either CQL or thrift. </p>

<p>In your case if you are planning to use hector for all data operations, you can create column family using hector itself or cassandra-cli. </p>

<p>Or if you want CQL only to create column families you can use datastax java driver instead of hector. </p>
",['table']
20895365,20928948,2014-01-03 02:08:39,Does having 1000's of CF's will lead to OOM in Cassandra,"<p>I am having a cluster with multiple CF's (around 1000 maybe more). And I get OOM errors time to time from different nodes. We have three Cassandra nodes? Is it an expected behavior in cassandra?</p>
",<cassandra>,"<p>Each table (columnfamily) requires a minimum of 1MB of heap memory, so it's quite possible this is causing some pressure for you.</p>

<p>The best solution is to redesign your application to use less tables; most of the time I've seen this it's because someone designed it to have ""one table per X"" where X is a customer or a data source or even a time period.  Instead, combine tables with a common schema and add a column to the primary key with the distinguishing element.</p>

<p>In the short term, you probably need to increase your heap size.</p>
",['table']
20946259,20951988,2014-01-06 08:56:39,conditional query is not working in cql 3.0.0,"<p>i am trying to execute conditional query in cassandra CQL but it is giving me error like</p>

<pre><code>cqlsh:events&gt; select * from standardevents where name=ActivityPg_view;
</code></pre>

<p><strong>i am executing above query, it is giving me below error</strong></p>

<p>Bad Request: line 1:55 no viable alternative at input ';'
Perhaps you meant to use CQL 2? Try using the -2 option when starting cqlsh.</p>

<p>cqlsh:events> select * from standardevents where name='ActivityPg_view';</p>

<p><strong>i am executing above query, it is giving me below error</strong></p>

<p>Bad Request: No indexed columns present in by-columns clause with
Equal operator</p>

<p><strong>when i am trying to execute using CQL 2</strong></p>

<pre><code>cqlsh:events&gt; select * from standardevents where name=ActivityPg_view;
</code></pre>

<p><strong>it is giving me below error,</strong></p>

<p>Bad Request: No indexed columns present in by-columns clause with ""equals"" opera
tor</p>

<p>cqlsh:events> select * from standardevents where name='ActivityPg_view';</p>

<p><strong>it is giving me below error,</strong></p>

<p>Bad Request: No indexed columns present in by-columns clause with ""equals"" opera
tor
Perhaps you meant to use CQL 3? Try using the -3 option when starting cqlsh.</p>

<p>so can somebody suggest the problem and how to solve this thing and execute the conditional query?</p>

<pre><code>cqlsh&gt; DESCRIBE COLUMNFAMILY events.standardevents;

CREATE TABLE standardevents (
    uuid timeuuid PRIMARY KEY,
    data text,
    name text,
    time text,
    tracker text,
    type text,
    userid text
  ) WITH bloom_filter_fp_chance=0.010000
    AND caching='KEYS_ONLY'
    AND comment=''
    AND dclocal_read_repair_chance=0.000000
    AND gc_grace_seconds=864000
    AND read_repair_chance=0.100000
    AND replicate_on_write='true'
    AND populate_io_cache_on_flush='false'
    AND compaction={'class': 'SizeTieredCompactionStrategy'}
    AND compression={'sstable_compression': 'SnappyCompressor'};
</code></pre>
",<cassandra><cql><cql3><cqlsh>,"<p>Your table (CF) has no field called courseid (hence the error ""Undefined name courseid in where clause""). You can't query on something that doesn't exist.</p>

<p>Were you expecting that field there? (Your PK for that table is called 'uuid' if that helps) Are you querying the right table? Not much else to suggest.</p>

<p><strong>Edit: After update</strong></p>

<p>Your CQL3 attempt is missing the quotes around the name, but that aside ... In Cassandra you can't randomly query by non key columns. Cassandra is a partitioned row store, and it is not really designed to do query's in the manner you are trying.</p>

<p>You could add a secondary index to fix this, but you should be aware it not like in traditional SQL. Having a 2ndary index will need to hit all nodes in a cluster do to the query. It's also not ideal if your data has high cardinality.</p>

<p>In Cassandra the general premise is that storage is cheap, and you should base your model on your query rather than your data. Denormalise everything. For example, if you need to pull events by name, then you should make a table that is key'd by name, and includes all the event data you need. That way reading them is (essentially) and O(1) operation.</p>
",['table']
20981075,20984398,2014-01-07 20:20:19,How Can I Search for Records That Have A Null/Empty Field Using CQL?,"<p>How can I write a query to find all records in a table that have a null/empty field?  I tried tried the query below, but it doesn't return anything.</p>

<pre><code>SELECT * FROM book WHERE author = 'null';
</code></pre>
",<cassandra><cql>,"<p><code>null</code> fields don't exist in Cassandra unless you add them yourself.</p>

<p>You might be thinking of the CQL data model, which hides certain implementation details in order to have a more understandable data model. Cassandra is sparse, which means that only data that is used is actually stored. You can visualize this by adding in some test data to Cassandra through CQL.</p>

<pre><code>cqlsh&gt; CREATE KEYSPACE test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1 } ;
cqlsh&gt; use test ;
cqlsh:test&gt; CREATE TABLE foo (name text, age int, pet text, primary key (name)) ;
cqlsh:test&gt; insert into foo (name, age, pet) values ('yves', 81, 'german shepherd') ;
cqlsh:test&gt; insert into foo (name, pet) values ('coco', 'ferret') ;

cqlsh:test&gt; SELECT * FROM foo ;

name | age  | pet
-----+-----+------------------
coco | null | ferret
yves |  81  | german shepherd
</code></pre>

<p>So even it appears that there is a null value, the actual value is nonexistent -- CQL is showing you a <code>null</code> because this makes more sense, intuitively.</p>

<p>If you take a look at the table from the Thrift side, you can see that the table contains no such value for <code>coco</code>'s age.</p>

<pre><code>$ bin/cassandra-cli
[default@unknown] use test;
[default@test] list foo;
RowKey: coco
=&gt; (name=, value=, timestamp=1389137986090000)
=&gt; (name=age, value=00000083, timestamp=1389137986090000)
-------------------
RowKey: yves
=&gt; (name=, value=, timestamp=1389137973402000)
=&gt; (name=age, value=00000051, timestamp=1389137973402000)
=&gt; (name=pet, value=6765726d616e207368657068657264, timestamp=1389137973402000)
</code></pre>

<p>Here, you can clearly see that <code>yves</code> has two columns: <code>age</code> and <code>pet</code>, while <code>coco</code> only has one: <code>age</code>.</p>
",['table']
21013303,21031411,2014-01-09 06:42:39,"About Datastax ""Monitoring a Cassandra cluster"" Documentation","<p>In this [1] document, when it describes cfstats output it says, <strong>Read count</Strong> is the <code>Number of pending read requests</code>. Is that correct? I was thinking that is all read requests received since last server restart. </p>

<p>can someone please clarify this?</p>

<p>[1] <a href=""http://www.datastax.com/documentation/cassandra/1.2/webhelp/cassandra/operations/ops_monitoring_c.html"" rel=""nofollow"">http://www.datastax.com/documentation/cassandra/1.2/webhelp/cassandra/operations/ops_monitoring_c.html</a></p>

<p>Thanks,
Bhathiya</p>
",<cassandra><datastax-enterprise><datastax><nodetool>,"<p>Yes, you're right, the docs are wrong. cfstats read count is the number of local read requests for the table since start up.</p>
",['table']
21092524,21103375,2014-01-13 13:30:04,Cassandra 2 - list existing indexes with CQL 3,"<p>Is there a CQL query to list all existing indexes for particular key space, or column family?</p>
",<cassandra><cql>,"<p>You can retrieve primary keys and secondary indexes using the system keyspace:</p>

<pre><code>SELECT column_name, index_name, index_options, index_type, component_index 
FROM system.schema_columns 
WHERE keyspace_name='samplekp'AND columnfamily_name='sampletable';
</code></pre>

<p>Taking, for example, the following table declaration:</p>

<pre><code>CREATE TABLE sampletable (
key text,
date timestamp,
value1 text,
value2 text,
PRIMARY KEY(key, date));

CREATE INDEX ix_sample_value2 ON sampletable (value2);
</code></pre>

<p>The query mentioned above would get something this results:</p>

<pre><code> column_name | index_name       | index_options | index_type | component_index
-------------+------------------+---------------+------------+-----------------
        date |             null |          null |       null |               0
         key |             null |          null |       null |            null
      value1 |             null |          null |       null |               1
      value2 | ix_sample_value2 |            {} | COMPOSITES |               1
</code></pre>
",['table']
21206353,21214372,2014-01-18 15:53:56,Cassandra efficient table walk,"<p>I'm currently working on a benchmark (which is part of my bachelor thesis) that compares SQL and NoSQL Databases based on an abstract data model an abstract queries to achieve fair implementation on all systems.</p>

<p>I'm currently working on the implementation of a query that is specified as follows:
I have a table in Cassandra that is specified as follows:</p>

<pre><code>CREATE TABLE allocated(
    partition_key int, 
    financial_institution varchar, 
    primary_uuid uuid,
    report_name varchar,
    view_name varchar,
    row_name varchar,
    col_name varchar,
    amount float,
PRIMARY KEY (partition_key, report_name, primary_uuid));
</code></pre>

<p>This table contains about 100,000,000 records (~300GB).</p>

<p>We now need to calculate the sum for the field ""<strong>amount</strong>"" for every possible combination of <strong>report_name</strong>, <strong>view_name</strong>, <strong>col_name</strong> and <strong>row_name</strong>.</p>

<p>In SQL this would be quite easy, just select sum (amount) and group it by the fields you want.
However, since Cassandra does not support these operations (which is perfectly fine) I need to achieve this on another way. </p>

<p>Currently I achieve this by doing a full-table walk, processing each record and storing the sum in a HashMap in Java for each combination.
The prepared statement I use is as follows:</p>

<pre><code>SELECT 
   partition_key, 
   financial_institution,
   report_name, 
   view_name, 
   col_name, 
   row_name, 
   amount 
FROM allocated; 
</code></pre>

<p>That works partially on machines with lots on RAM for both, cassandra and the Java app, but crashes on smaller machines.</p>

<p>Now I'm wondering whether it's possible to achieve this on a faster way?
I could imagine using the partition_key, which serves also as the cassandra partition key and do this for every partition (I have 5 of them).</p>

<p>Also I though of doing this multithreaded by assigning every partition and report to a seperate thread and running it parallel. But I guess this would cause a lot of overhead on the application side.</p>

<p>Now to the actual question: Would you recommend another execution strategy to achieve this?
Maybe I still think too much in a SQL-like way.</p>

<p>Thank you for you support.</p>
",<nosql><cassandra><sum><aggregate-functions><full-table-scan>,"<p>Here are two ideas that may help you.</p>

<p>1) You can efficiently scan rows in any table using the following approach. Consider a table with PRIMARY KEY (pk, sk, tk). Let's use a fetch size of 1000, but you can try other values.</p>

<p>First query (Q1): </p>

<pre><code>select whatever_columns from allocated limit 1000;
</code></pre>

<p>Process these and then record the value of the three columns that form the primary key. Let's say these values are pk_val, sk_val, and tk_val. Here is your next query (Q2):</p>

<pre><code>select whatever_columns from allocated where token(pk) = token(pk_val) and sk = sk_val and tk &gt; tk_val limit 1000;
</code></pre>

<p>The above query will look for records for the same pk and sk, but for the next values of tk. Keep repeating as long as you keep getting 1000 records. When get anything less, you ignore the tk, and do greater on sk. Here is the query (Q3):</p>

<pre><code>select whatever_columns from allocated where token(pk) = token(pk_val) and sk &gt; sk_val limit 1000;
</code></pre>

<p>Again, keep doing this as long as you get 1000 rows. Once you are done, you run the following query (Q4):</p>

<pre><code>select whatever_columns from allocated where token(pk) &gt; token(pk_val) limit 1000;
</code></pre>

<p>Now, you again use the pk_val, sk_val, tk_val from the last record, and run Q2 with these values, then Q3, then Q4..... </p>

<p>You are done when Q4 returns less than 1000.</p>

<p>2) I am assuming that 'report_name, view_name, col_name and row_name' are not unique and that's why you maintain a hashmap to keep track of the total amount whenever you see the same combination again. Here is something that may work better. Create a table in cassandra where key is a combination of these four values (maybe delimited). If there were three, you could have simply used a composite key for those three. Now, you also need a column called amounts which is a list. As you are scanning the allocate table (using the approach above), for each row, you do the following:</p>

<pre><code>update amounts_table set amounts = amounts + whatever_amount where my_primary_key = four_col_values_delimited;
</code></pre>

<p>Once you are done, you can scan this table and compute the sum of the list for each row you see and dump it wherever you want. Note that since there is only one key, you can scan using only token(primary_key) > token(last_value_of_primary_key).</p>

<p>Sorry if my description is confusing. Please let me know if this helps.</p>
",['table']
21304381,23239990,2014-01-23 09:39:59,how to get the system-date using cql query in cassandra,"<p>I want to retrieve those data which belongs to today's date, i tried below thing</p>

<pre><code>select * from tablename where fieldname = dateof(now());
</code></pre>

<p>This query gives date and system-time both, so result will not be proper. </p>

<p>How can I get only current(today's) date?</p>
",<cassandra><cql><cql3>,"<p>Based on your question the column family tablename is using the timestamp fieldname as primary key. In that case, each row is identified by the timestamp and not the date so you can not query by the date.</p>

<p>Based on this, you have to change your data model to define the primary key on another way. A possibility is use a table like created like this:</p>

<pre><code>USE test;

create table posts(username varchar, 
    time timeuuid, 
    post_text varchar, 
    primary key(username, time)
);

insert into posts(username, time, post_text) 
values ('me', now(), 'Example');

insert into posts(username, time, post_text) 
values ('me', now(), 'Example');

select username, dateOf(time), post_text from posts;
</code></pre>

<p>In this case you obtain two results like this</p>

<p><img src=""https://i.stack.imgur.com/nweGf.png"" alt=""enter image description here""></p>

<p>So if you want to query on a specific date you can use:</p>

<p>select username, 
dateOf(time), 
post_text 
from posts 
where time >= minTimeuuid('2014-04-23 00:00') 
and time &lt; minTimeuuid('2014-04-24 00:00') 
and username = 'me';</p>

<hr>

<p>Sources:</p>

<p><a href=""http://cassandra.apache.org/doc/cql3/CQL.html#usingdates"" rel=""nofollow noreferrer"">http://cassandra.apache.org/doc/cql3/CQL.html#usingdates</a></p>

<p><a href=""http://christopher-batey.blogspot.nl/2013/05/time-series-based-queries-in-cassandra.html"" rel=""nofollow noreferrer"">http://christopher-batey.blogspot.nl/2013/05/time-series-based-queries-in-cassandra.html</a></p>
",['table']
21363046,21378330,2014-01-26 11:59:17,How to select data from a table and insert into another table?,"<p>I want to select specific fields of a table in cassandra and insert them into another table. I do this in sql server like this:  </p>

<pre><code>INSERT INTO Users(name,family)
SELECT name,family FROM Users
</code></pre>

<p>How to to this in cassandra-cli or cqlsh?</p>
",<cassandra><cassandra-cli><cqlsh>,"<pre><code>COPY keyspace.columnfamily1 (column1, column2,...) TO 'temp.csv';
COPY keyspace.columnfamily2 (column1, column2,...) FROM 'temp.csv';
</code></pre>

<p>here give your keyspace(schema-name) and instead of columnfamilyname1 use the table to which you want to copy and in columnfamily2 give the tablename in which you want to copy..</p>

<p>And yes this is solution for CQL,however I have never tried in with CLI.</p>
",['table']
21384856,21390188,2014-01-27 15:32:21,Cassandra Partial Replication,"<p>This is my configuration for 4 Data Centers of Cassandra: </p>

<pre><code>create KEYSPACE mySpace WITH replication = {'class': 'NetworkTopologyStrategy', 'DC1' : 1, 'DC2' : 1, 'DC3' : 1, 'DC4' : 1};
</code></pre>

<p>In this configuration (Murmur3Partitioner + 256 tokens) , each DC is storing roughly 25% of the key space. And this 25% are replicated 3 times on each other DC. Meaning that every single row has 4 copies over all. </p>

<p>For instance if my data base is to big to keep 4 complete copies of it, how can I configure cassandra so that each DC is replicated only once or twice (instead of total number of DCs (x3)).</p>

<p>For example: 25% of the key space that is stored on DC1 I want to replicate once on DC2 only. I am not looking for selecting any particular DC for replication neither I care if 25% of DC1 will be split over multiple DC1,2,3 I just want to use NetworkTopologyStrategy but reduce storage costs.</p>

<p>Is it possible ?</p>

<p>Thank you
Best Regards</p>
",<cassandra><replication>,"<p>Your keyspace command shows that each of the DCs hold 1 copy of the data. This means that if you have 1 node in each DC, then each node will have 100% of your data. So, I am not sure how you concluded that each of your DCs store only 25% of keys as it is obvious they are storing 100%. Chances are when you run nodetool command you are not specifying the keyspace so the command shows you load which is based on the token range assigned to each node which would be misleading for NetworkTopology setup. Try running it with your keyspace name and see if you notice the difference.</p>

<p>I don't think there is a way to shift data around DCs using any of existing Snitches the way you want it. If you really wanted to have even distribution and you had equal number of nodes in each DC with initial tokens spaced evenly, you could have used SimpleSnitch to achieve what you want. You can change the Snitch to SimpleSnitch and run nodetool cleanup/repair on each node. Bare in mind that during this process you will have some outage because after the SnitchChange, previously written keys may not be available on some nodes until the repair job is done.</p>

<p>The way NetworkTopology works is that if you say you have DC1:1 and you have for example 2 nodes in DC1, it will evenly distribute keys across 2 nodes leading to 50% effective load on each node. With that in mind, I think what you really want to have done is to keep 3 copies of your data, 1 in each DC. So, you can really discard one DC and save money. I am saying this because I think these DCs you have are virtual in the notion of your NetworkTopology and not real physical DC because no one would want to have only 25% of data in one DC as it will not be an available setup. So, I recommend if your nodes are grouped into virtual DCs, you group them into 4 racks instead and maintain 1 DC:</p>

<pre>
DC1:
nd1-ra_1 rack-a
nd1-rb_1 rack-b
nd1-rc_1 rack-c

nd2-ra_2 rack-a
nd2-rb_2 rack-b
nd2-rc_2 rack-c

nd3-ra_3 rack-a
nd3-rb_3 rack-b
nd3-rc_3 rack-c

nd3-ra_4 rack-a
nd3-rb_4 rack-b
nd3-rc_4 rack-c
</pre>

<p>In this case, if you set your replication option to DC1:3, each of the racks a,b,and c will have 100% of your data (each node in each rack 25%).</p>
",['rack']
21573136,21586285,2014-02-05 09:25:15,Change the data in cassandra using update command?,"<p>Is there a way to update Cassandra database to update the table like in MySQL?
  I have data stored in cassandra and I need to update the data. Is there any update command to do so?</p>
",<cassandra><cql>,"<p>Yes, you can perform an <code>UPDATE</code> (from the CQL shell) to a row in the database.  However, unlike MySQL you can only specify the primary key(s) in the <code>WHERE</code> clause.  This example assumes a table called ""products"" with a primary key of ""productNumber.""</p>

<pre><code>UPDATE products
SET description1='PROD DESC1'
WHERE productNumber='ITEM00123';
</code></pre>

<p>If a row does not exist for the primary key you have specified, a new one will be created.</p>

<p>For more information, see the DataStax documentation on the <code>UPDATE</code> CQL command that abhi linked in his answer.</p>

<p>Edit: I'm not sure about pure Javascript, but there is a Cassandra driver (<a href=""https://github.com/racker/node-cassandra-client"" rel=""nofollow"">node-cassandra-client</a>) for <a href=""http://nodejs.org/"" rel=""nofollow"">Node.js</a>.  You can use that driver to connect to Cassandra and peform an update in Javascript.  Here's an example of how to perform the above <code>UPDATE</code>:</p>

<pre><code> var conOptions = { hosts: ['127.0.0.1:19170'],
                    keyspace: 'productKeyspace',
                    use_bigints: false };
 var con = new PooledConnection(conOptions);
 var cql = 'UPDATE products SET description1=? where productNumber=?';
 var params = ['PROD DESC1', 'ITEM00123'];
 con.execute(cql, params, function(err) {
   if (err) {
     console.log(err);
   }
   con.shutdown(callback);
 });
</code></pre>

<p>For more information, check out <a href=""http://www.rackspace.com/blog/rackspace-contributes-cassandra-cql-driver-for-node-js/"" rel=""nofollow"">Rackspace's doc on the node.js driver</a>.</p>
",['table']
21617385,21634082,2014-02-07 01:00:13,Range query - Data modeling for time series in CQL Cassandra,"<p>I have a table like this:</p>

<blockquote>
  <p>CREATE TABLE test (   partitionkey text,   rowkey text, date
  timestamp,  policyid text,  policyname text, primary key
  (partitionkey, rowkey));</p>
</blockquote>

<p>with some data:</p>

<blockquote>
  <p>partitionkey |    rowkey |    policyid |    policyname |  date</p>

<pre><code>     p1 |   r1 |    pl1 |  plicy1 | 2007-01-02 00:00:00+0000
     p1 |   r2 |    pl2 |  plicy2 | 2007-01-03 00:00:00+0000
     p2 |   r3 |    pl3 |  plicy3 | 2008-01-03 00:00:00+0000
</code></pre>
</blockquote>

<p>I want to be able to find:</p>

<pre><code>1/ data from a particular partition key
2/ data from a particular partition key &amp; rowkey
3/ Range query on date given a partitionkey
</code></pre>

<p>1/ and 2/ are trivial:</p>

<blockquote>
  <p>select * from test where partitionkey='p1';</p>
  
  <p>partitionkey | rowkey | policyid | policyname | range</p>

<pre><code>    p1 |     r1 |      pl1 |     plicy1 | 2007-01-02 00:00:00+0000
    p1 |     r2 |      pl2 |     plicy2 | 2007-01-03 00:00:00+0000
</code></pre>
</blockquote>

<p>but what about 3/? 
Even with an index it doesnt work:</p>

<blockquote>
  <p>create index i1 on test (date); </p>
  
  <p>select * from test where partitionkey='p1' and date =
  '2007-01-02';</p>
  
  <p>partitionkey | rowkey | policyid | policyname | date</p>

<pre><code>  p1 |  r1 |   pl1   plicy1 | 2007-01-02 00:00:00+0000
</code></pre>
  
  <p>but</p>
  
  <p>select * from test where partitionkey='p1' and
  date > '2007-01-02'; </p>
  
  <p>Bad Request: No indexed columns present in
  by-columns clause with Equal operator</p>
</blockquote>

<p>Any idea?
thanks,
Matt</p>
",<cassandra><cql><cql3><cqlsh>,"<blockquote>
  <p>CREATE TABLE test ( <strong>partitionkey</strong> text, <strong>rowkey</strong> text, date timestamp,
  policyid text, policyname text, primary key (partitionkey, rowkey));</p>
</blockquote>

<p>First of all, you really should use more descriptive column names instead of partitionkey and rowkey (and even date, for that matter).  By looking at those column names, I really can't tell what kind of data this table is supposed to be indexed by.</p>

<pre><code>select * from test where partitionkey='p1' and date &gt; '2007-01-02';
</code></pre>

<blockquote>
  <p>Bad Request: No indexed columns present in by-columns clause with Equal operator</p>
</blockquote>

<p>As for this issue, try making your ""date"" column a part of your primary key.</p>

<pre><code>primary key (partitionkey, rowkey, date)
</code></pre>

<p>Once you do that, I think your date range queries will function appropriately.</p>

<p>For more information on this, check out <a href=""https://academy.datastax.com/"" rel=""nofollow"">DataStax Academy's</a> (free) course called Java Development With Apache Cassandra.  Session 5, Module 104 discusses how to model time series data and that should help you out.</p>
",['table']
21667425,21714396,2014-02-10 01:03:44,Datastax QueryBuilder insert statement,"<p>What would be the right way to build a insert statement with QueryBuilder from Datastax Java Driver for Cassandra</p>

<p>I am using Cassandra 2.x with Java Driver 2.0.0-rc1</p>

<p>I know i could use a prepared statement to achieve the same but i am looking forward to using the QueryBuilder</p>

<pre><code>@Test
public void testTableInsert() {
    Insert insert = QueryBuilder
            .insertInto(KEYSPACE_NAME, TABLE_NAME)
            .value(""username"", ""jdoe"")
            .value(""first"", ""John"")
            .value(""last"", ""Doe"");
    System.out.println(insert.toString());
    ResultSet result = session.execute(insert.toString());
    System.out.println(result);

}
</code></pre>

<p>I can confirm the query string is valid because it succeeds when attempting manually on cqlsh</p>

<pre><code>INSERT INTO test.user(username,first,last) VALUES ('jdoe','John','Doe');
</code></pre>

<p>The error reported by the driver is </p>

<pre><code>    com.datastax.driver.core.exceptions.InvalidQueryException: unconfigured columnfamily user
    at com.datastax.driver.core.exceptions.InvalidQueryException.copy(InvalidQueryException.java:35)
    at com.datastax.driver.core.ResultSetFuture.extractCauseFromExecutionException(ResultSetFuture.java:271)
    at       com.datastax.driver.core.ResultSetFuture.getUninterruptibly(ResultSetFuture.java:187)
    at com.datastax.driver.core.Session.execute(Session.java:126)
    at com.datastax.driver.core.Session.execute(Session.java:77)
    at SimpleClientTest.testTableInsert(SimpleClientTest.java:61)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
    at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
    at org.junit.runner.JUnitCore.run(JUnitCore.java:157)
    at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:77)
    at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:195)
    at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:63)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:120)
Caused by: com.datastax.driver.core.exceptions.InvalidQueryException: unconfigured columnfamily user
    at com.datastax.driver.core.Responses$Error.asException(Responses.java:96)
    at com.datastax.driver.core.ResultSetFuture$ResponseCallback.onSet(ResultSetFuture.java:122)
    at com.datastax.driver.core.RequestHandler.setFinalResult(RequestHandler.java:224)
    at com.datastax.driver.core.RequestHandler.onSet(RequestHandler.java:359)
    at com.datastax.driver.core.Connection$Dispatcher.messageReceived(Connection.java:510)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.oneone.OneToOneDecoder.handleUpstream(OneToOneDecoder.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:109)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
    at java.lang.Thread.run(Thread.java:695)
</code></pre>

<p>I couldn't find any examples on the official documentation page at <a href=""http://www.datastax.com/documentation/developer/java-driver/1.0/webhelp/index.html#java-driver/reference/queryBuilder_r.html"" rel=""nofollow"">here</a></p>
",<java><cassandra><datastax-java-driver>,"<p>You just forgot to create the keyspace.</p>

<p>Full code used to create a KS, create a CF and use the querybuilder to insert a partition.</p>

<pre><code>public static void main(String[] Args)
{
    Cluster cluster = null;
    cluster = Cluster.builder()
                     .addContactPoint(""127.0.0.1"")
                     .build();
    Session session = cluster.connect();

    try
    {
        String createKS = ""CREATE KEYSPACE test WITH REPLICATION = { 'class': 'SimpleStrategy', 'replication_factor': '2' }"";
        session.execute(createKS);
        String query = ""create table test.user (username text primary key,first text, last text);"";
        session.execute(query);

        System.out.println(""insert executed"");
        Insert insert = QueryBuilder.insertInto(""test"", ""user"")
                                    .value(""username"", ""jdoe"")
                                    .value(""first"", ""John"")
                                    .value(""last"", ""Doe"");
        System.out.println(insert.toString());
        ResultSet result = session.execute(insert.toString());
        System.out.println(result);
    }
    catch (Exception ex)
    {
        ex.printStackTrace();
    }

    System.exit(0);
}
</code></pre>
",['table']
21755049,21765051,2014-02-13 13:03:15,How the select the last record from a time series in Cassandra?,"<p>I want to store some encoded 'data' into cassadra, versioned by timestamp. My tentative schema  is:</p>

<pre><code>CREATE TABLE items (
  item_id varchar,
  timestamp timestamp,
  data blob,
  PRIMARY KEY (item_id, timestamp)
); 
</code></pre>

<p>I would like to be able to return the list of items, returning only the latest ( highest timestamp) for each item_id; Is it possible with this schema?</p>
",<nosql><cassandra>,"<p>It is not possible to express such a query in a single CQL statement for this table, so the answer is no.</p>

<p>You can try creating another table, e.g. <code>latest_items</code>, and only storing the last update there, so the schema would be:</p>

<pre><code>CREATE TABLE latest_items (
  item_id varchar,
  timestamp timestamp,
  data blob,
  PRIMARY KEY (item_id)
); 
</code></pre>

<p>If your rows are inserted in timestamp order, the table would naturally contain only the latest row for each item.  Then you can just run <code>select * from latest_items limit 10000000;</code>.  This will of course be expensive, because you're fetching all rows, but given your requirements where you actually want all of them, there is no way to avoid it.</p>

<p>This second table involves duplicating your data, but this is a common theme with Cassandra.  You can avoid duplicating the blob by storing it indirectly, i.e. as a path or URL or somesuch.</p>
",['table']
21755286,21772581,2014-02-13 13:13:56,What exactly happens when tombstone limit is reached,"<p>According to cassandra's log (see below) queries are getting aborted due to too many <code>tombstones</code> being present. This is happening because once a week I cleanup (delete) rows with a counter that is too low. This 'deletes' <em>hundreds of thousands</em> of rows (marks them as such with a <code>tombstone</code>.)</p>

<p>It is not at all a problem if, in this table, a deleted row re-appears because a node was down during the cleanup process, so I set the <code>gc grace time</code> for the single affected table to 10 hours (down from default 10 days) so the tombstoned rows can get permanently deleted relatively fast.</p>

<p>Regardless, I had to set the <code>tombstone_failure_threshold</code> extremely high to avoid the below exception. (one hundred million, up from one hundred thousand.) My question is, is this necessary? I have absolutely no idea what type of queries get aborted; inserts, selects, deletes?</p>

<p>If it's merely some selects being aborted, it's not that big a deal. But that's assuming abort means 'capped' in that the query stops prematurely and returns whatever live data it managed to gather before too many tombstones were found.</p>

<p>Well, to ask it simpler; what happens when the <code>tombstone_failure_threshold</code> is exceeded?</p>

<pre><code>INFO [HintedHandoff:36] 2014-02-12 17:44:22,355 HintedHandOffManager.java (line 323) Started hinted handoff for host: fb04ad4c-xxxx-4516-8569-xxxxxxxxx with IP: /XX.XX.XXX.XX
ERROR [HintedHandoff:36] 2014-02-12 17:44:22,667 SliceQueryFilter.java (line 200) Scanned over 100000 tombstones; query aborted (see tombstone_fail_threshold)
ERROR [HintedHandoff:36] 2014-02-12 17:44:22,668 CassandraDaemon.java (line 187) Exception in thread Thread[HintedHandoff:36,1,main]
org.apache.cassandra.db.filter.TombstoneOverwhelmingException
    at org.apache.cassandra.db.filter.SliceQueryFilter.collectReducedColumns(SliceQueryFilter.java:201)
    at org.apache.cassandra.db.filter.QueryFilter.collateColumns(QueryFilter.java:122)
    at org.apache.cassandra.db.filter.QueryFilter.collateOnDiskAtom(QueryFilter.java:80)
    at org.apache.cassandra.db.filter.QueryFilter.collateOnDiskAtom(QueryFilter.java:72)
    at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:297)
    at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:53)
    at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1516)
    at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1335)
    at org.apache.cassandra.db.HintedHandOffManager.doDeliverHintsToEndpoint(HintedHandOffManager.java:351)
    at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:309)
    at org.apache.cassandra.db.HintedHandOffManager.access$300(HintedHandOffManager.java:92)
    at org.apache.cassandra.db.HintedHandOffManager$4.run(HintedHandOffManager.java:530)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
</code></pre>

<p>Forgot to mention; running Cassandra version <code>2.0.4</code></p>
",<cassandra><tombstone>,"<p>When a query that returns a range of rows (or columns) is issued to Cassandra, it has to scan the table to collect the result set (this is called a slice).  Now, deleted data is stored in the same manner as regular data, except that it's marked as tombstoned until compacted away.  But the table reader has to scan through it nevertheless.  So if you have tons of tombstones lying around, you will have an arbitrarily large amount of work to do to satisfy your ostensibly limited slice.  </p>

<p>A concrete example:  let's say you have two rows with clustering keys 1 and 3, and a hundred thousand dead rows with clustering key 2 that are located in between rows 1 and 3 in the table.  Now when you issue a <code>SELECT</code> query where the key is to be >= 1 and &lt; 3, you'll have to scan 100002 rows, instead of the expected two.  </p>

<p>To make it worse, Cassandra doesn't just scan through these rows, but also has to accumulate them in memory while it prepares the response.  This can cause an out-of-memory error on the node if things go too far out, and if multiple nodes are servicing the request, it may even cause a multiple failure bringing down the whole cluster.  To prevent this from happening, the service aborts the query if it detects a dangerous number of tombstones.  You're free to crank this up, but it's risky, if your Cassandra heap is close to running out during these spikes.</p>

<p>This exception was introduced in a recent fix, first available in 2.0.2.  <a href=""https://issues.apache.org/jira/browse/CASSANDRA-6117"" rel=""noreferrer"">Here</a> is the bug entry describing the problem the change was trying to address.  Previously everything would have been just fine, until one of your nodes, or potentially several, suddenly crashed.</p>

<blockquote>
  <p>If it's merely some selects being aborted, it's not that big a deal.
  But that's assuming abort means 'capped' in that the query stops
  prematurely and returns whatever live data it managed to gather before
  too many tombstones were found.</p>
</blockquote>

<p>The query doesn't return a limited set, it actually drops the request completely.  If you'd like to mitigate, maybe it's worth doing your bulk row deletion at the same cadence as the grace period, so you don't have this huge influx of tombstones every week.</p>
",['table']
21783996,21789078,2014-02-14 16:06:06,Cassandra write performance regarding consistency level,"<p>Here is quote from cassandra documentation about writes (<a href=""http://www.datastax.com/docs/0.8/dml/about_writes#about-transactions"" rel=""nofollow"">LINK</a>)</p>

<blockquote>
  <p>If all replicas for the affected row key are down, it is still
  possible for a write to succeed if using a write consistency level of
  ANY. Under this scenario, the hint and written data are stored on the
  coordinator node, but will not be available to reads until the hint
  gets written to the actual replicas that own the row. The ANY
  consistency level provides absolute write availability at the cost of
  consistency, as there is no guarantee as to when written data will be
  available to reads (depending how long the replicas are down). <strong>Using
  the ANY consistency level can also potentially increase load on the
  cluster, as coordinator nodes must temporarily store extra rows
  whenever a replica is not available to accept a write.</strong></p>
</blockquote>

<p>My question is: is writing to cassandra slower if we use consistency level of <strong>ANY</strong> than writes when we use consistency level of <strong>ONE</strong> ?</p>
",<nosql><cassandra>,"<p>Hints are generated when appropriate replica nodes are inaccessible at write time.  Write requests are then serialized locally on the request coordinator node.  Once a valid replica node becomes available and the coordinator node learns of it, the request is passed along to the newly available replica.</p>

<p>With that background, there are two write-time scenarios to consider:</p>

<p>1) <strong>At least one replica is up for the affected row</strong>.  In this case, there is no difference between consistency levels of <code>ANY</code> and <code>ONE</code>.  The write just goes to the replica(s), and hinted handoff is not triggered.  <strong>No performance difference</strong>.</p>

<p>2) <strong>All replicas are down for the affected row</strong>.  This is where hints enter the picture.  With consistency <code>ANY</code> there is extra work to be done on the coordinator node at request time, as the hint is written to a local system table for later replay.  With consistency <code>ONE</code>, you would simply get a refused write in the same circumstances.  <strong><code>ONE</code> will expose write failures to the client, and will be faster than <code>ANY</code></strong>.</p>

<p>Essentially, the tradeoff is refusing requests vs. pushing work onto remaining nodes, but <em>only when nodes responsible for storing that row are down</em>.</p>
",['table']
21814850,21816607,2014-02-16 17:36:51,Alter table with COMPACT STORAGE,"<p>I had issues with reading a table from pycassa created with CQL3.</p>

<p>So followed this post 
<a href=""https://stackoverflow.com/questions/14185291/reading-cassandra-1-2-table-with-pycassa"">Reading Cassandra 1.2 table with pycassa</a></p>

<p>Now trying to alter my table </p>

<p><code>ALTER TABLE tweets with COMPACT STORAGE ;</code></p>

<p>But getting an error.</p>

<blockquote>
  <p>Bad Request: line 1:32 missing '=' at 'STORAGE'</p>
</blockquote>
",<cassandra><cql3><pycassa>,"<p>You cant alter the underlying storage model. You'll have to re-create the table and add the <code>WITH COMPACT STORAGE</code> parameter. </p>

<p>On a side note, why not use the newer <a href=""https://github.com/datastax/python-driver"" rel=""nofollow"">cassandra python</a> driver that supports cql3.</p>
",['table']
21873959,21881585,2014-02-19 07:35:39,Is it possible to use cql to query collections in a row?,"<p>Assuming you have movies as a row, and a list of categories, i.e.</p>

<pre><code>create table movie(
    uuid timeuuid,
    name text,
    category list&lt;text&gt;
    primary key (uuid)
);

insert into movie (uuid,name,category) values(now(), 'my movie', ['romance','comedy']);
insert into movie (uuid,name,category) values(now(), 'other movie', ['action','comedy']);
</code></pre>

<p>Is it possible to efficiently do something like:</p>

<pre><code>select * from movie where category='comedy'
select * from movie where category='comedy' and category='action'
</code></pre>

<p>This use case is the most common query against our MySQL database.</p>
",<cassandra><cql>,"<p><a href=""https://issues.apache.org/jira/browse/CASSANDRA-4511"">As of cassandra 2.1</a>, yes it is:</p>

<pre><code>create table cinema.movie(
    id timeuuid,
    name text,
    category list&lt;text&gt;, 
    primary key (id)
);

CREATE index ON cinema.movie (category);
INSERT INTO cinema.movie (id, name, category) VALUES (now(), 'my movie', ['romance','comedy']);
SELECT * FROM cinema.movie WHERE category CONTAINS 'romance';


 uuid                                 | category               | name
--------------------------------------+------------------------+----------
 b9e0d7f0-995f-11e3-b11c-c5d4ddc8930a | ['romance', 'comed`y'] | my movie

(1 rows)
</code></pre>

<p>P.S. you have errors in your queries, you are missing a <code>,</code> in your create table statement after category's declaration and you cant use the <code>now()</code> function on UUID, you are looking for the TIMEUUID type instead. </p>

<p>Bare in mind that performance wont be magnificent (how's that for quantitative) when it come's to using these collections, it might be better to create a separate table for this sort of thing.</p>
",['table']
21921994,21940033,2014-02-20 23:27:59,cassandra: save data in second same table or field filter,"<p>I have table documents with columns:</p>

<pre><code>user_id uuid,
folder_id uuid,
id uuid,
name varchar,
data blob,

and
primary key ((user_id), folder_id, id)
</code></pre>

<p>Some documents could be processed in a long time. I need to store not processed data.</p>

<p>I have two approaches:</p>

<p>1)
Add boolean column 'processed', and will filter the documents in DAO.</p>

<p>2)
Create second table with same structure and save data in it.</p>

<p>Which way is better?
Thanks.</p>

<p>UPDATED:</p>

<p>I use cql.</p>

<p>my queries:</p>

<pre><code>select * from documents;
select * from documents where user_id = ...;
select * from documents where user_id = ... and folder_id = ...;
select * from documents where user_id = ... and folder_id = ... and id = ...;
</code></pre>
",<cassandra><filtering>,"<p>There is no major difference.
If you use one table with boolean variable your key will change to</p>

<pre><code>primary key ((is_processed, user_id), folder_id, id)
</code></pre>

<p>This has the overhead of one variable for each row.</p>

<p>So I would use two tables.</p>
",['table']
21925525,21945231,2014-02-21 04:49:44,Cassandra NOT EQUAL Operator,"<p>Question to all Cassandra experts out there.</p>

<p>I have a column family with about a million records.</p>

<p>I would like to query these records in such a way that I should be able to perform a <code>Not-Equal-To</code> kind of operation.</p>

<p>I Googled on this and it seems I have to use some sort of <code>Map-Reduce</code>.</p>

<p>Can somebody tell me what are the options available in this regard.</p>
",<mapreduce><cassandra><cql3>,"<p>I can suggest a few approaches.  </p>

<p>1) If you have a limited number of values that you would like to test for not-equality, consider modeling those as a <code>boolean</code> columns (i.e.: column <code>isEqualToUnitedStates</code> with true or false).</p>

<p>2) Otherwise, consider emulating the unsupported query <code>!= X</code> by combining results of two separate queries, <code>&lt; X</code> and <code>&gt; X</code> on the client-side.</p>

<p>3) If your schema cannot support either type of query above, you may have to resort to writing custom routines that will do client-side filtering and construct the not-equal set dynamically.  This will work if you can first narrow down your search space to manageable proportions, such that it's relatively cheap to run the query without the not-equal.</p>

<p>So let's say you're interested in all purchases of a particular customer of every product type except Widget.  An ideal query could look something like <code>SELECT * FROM purchases WHERE customer = 'Bob' AND item != 'Widget';</code>  Now of course, you cannot run this, but in this case you should be able to run <code>SELECT * FROM purchases WHERE customer = 'Bob'</code> without wasting too many resources and filter <code>item != 'Widget'</code> in the client application.</p>

<p>4) Finally, if there is no way to restrict the data in a meaningful way before doing the scan (querying without the equality check would returning too many rows to handle comfortably), you may have to resort to MapReduce.  This means running a distributed job that would scan all rows in the table across the cluster.  Such jobs will obviously run a lot slower than native queries, and are quite complex to set up.  If you want to go this way, please look into <em>Cassandra Hadoop integration</em>.</p>
",['table']
21943747,21943919,2014-02-21 19:42:29,Cassandra - Import from CSV and create new table?,"<p>I am attempting to copy a csv file in a Cassandra table using </p>

<p>so in cqlsh, something like:</p>

<pre><code>COPY mytable FROM 'test.csv' WITH header=TRUE;
</code></pre>

<p>problem is that in practice, my CSV file has more than a hundred columns. Is it necessary to define each column using </p>

<pre><code>CREATE TABLE mytable (&lt;all my columns here&gt;);
</code></pre>

<p>or hopefully, is there some way to create the table f
rom the column headers? I mean, I could write a short python script that reads the column headers and then outputs them into a format i can just copy and paste into , but I'm wondering if there's a more elegant way? </p>

<p>Thanks
Alex</p>
",<cassandra><cql3>,"<p>No, there is no way to create a table from CSV in CQLSH. Unfortunately CSV doesn't contain information about columns' types, so CQLSH can't figure that out to create a table.</p>
",['table']
21954283,21955220,2014-02-22 12:25:22,"How does ""DROP TABLE IF EXISTS"" work in Cassandra?","<p>I'm trying to understand the syntax of <code>DROP TABLE IF EXISTS</code> in Cassandra.</p>

<p><code>DROP TABLE IF EXISTS nonexistanttable;</code> does not seem to work:</p>

<pre><code>$ ./cqlsh
Connected to Test Cluster at localhost:9160.
[cqlsh 4.1.1 | Cassandra 2.0.5 | CQL spec 3.1.1 | Thrift protocol 19.39.0]
Use HELP for help.
cqlsh&gt; USE Foo;
cqlsh:foo&gt; DROP TABLE IF EXISTS bar;
Bad Request: unconfigured columnfamily bar
</code></pre>

<p>What am I doing wrong?</p>
",<cassandra>,"<p>The idea of <code>DROP TABLE IF EXISTS</code> is that you avoid getting the InvalidRequestException ""Bad Request: unconfigured columnfamily"" by droping the table only if it is actually created so your query statement is valid.</p>

<p>You are getting the exception because this was a <a href=""https://issues.apache.org/jira/browse/CASSANDRA-6687"" rel=""nofollow"">bug in 2.0.5.</a> It has been fixed for C* 2.0.6 but if you want to see <code>DROP TABLE IF EXISTS</code> immediately, try downloading and building cassandra from source:</p>

<pre><code>git clone -b cassandra-2.0 git://git.apache.org/cassandra.git cassandra
cd cassandra
ant build
</code></pre>
",['table']
22005978,22013188,2014-02-25 06:08:01,Shuffling items into a column family depending on date?,"<p>I'll use a blog as an analogy. Our main document type has active and inactive entries in the table. Think of it like blog posts that are set to be published on a pre defined date. They also disappear on a pre defined date.</p>

<p>Does Cassandra offer a way to cleanly query objects basted on if they are currently active, aka active on today's date? The only option I can see is having a second table that contains the active items, along with a regular scheduled job that scans the entire main table for items that are becoming active or inactive.</p>

<p>Is there a neater way?</p>
",<cassandra><cql>,"<p>To save having to scan the entire document table, you could have a second table which contains ""buckets"" for which items need to be added or removed from the active list. That way you only have to scan ""todays bucket"" for info on what items are becoming or ceasing to be active.</p>
",['table']
22006887,22007669,2014-02-25 06:59:28,cassandra - Saved cluster name Test Cluster != configured name,"<p>How am I supposed to bot a new Cassandra node when I get this error?</p>

<pre><code>INFO [SSTableBatchOpen:1] 2014-02-25 01:51:17,132 SSTableReader.java (line 223) Opening /var/lib/cassandra/data/system/local/system-local-jb-5 (5725 bytes)
ERROR [main] 2014-02-25 01:51:17,377 CassandraDaemon.java (line 237) Fatal exception during initialization
org.apache.cassandra.exceptions.ConfigurationException: Saved cluster name Test Cluster != configured name thisisstupid
        at org.apache.cassandra.db.SystemKeyspace.checkHealth(SystemKeyspace.java:542)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:233)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:462)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:552)
</code></pre>

<p>Name of cluster in the cassandra.yaml file is:</p>

<pre><code>cluster_name: 'thisisstupid'
</code></pre>

<p>How do I resolve?</p>
",<cassandra>,"<p>You can rename the cluster without deleting data by updating it's name in the system.local table (but you have to do this <strong>for each node</strong>...)</p>

<pre><code>cqlsh&gt; UPDATE system.local SET cluster_name = 'test' where key='local';
# flush the sstables to persist the update.
bash $ ./nodetool flush
</code></pre>

<p>Finally you need to rename the cluster to the new name in cassandra.yaml (again <strong>on each node</strong>)</p>
","['cluster_name', 'table']"
22016057,22029847,2014-02-25 13:46:18,import ascii data into cassandra database,"<p>I have got C-ISAM data exported to ASCII file and now I would like to import that into Cassandra database. Is there any way to do so? 
For information the ASCII file is based on offsets!</p>

<p>Thanks for the help!</p>
",<import><cassandra><ascii><cql><isam>,"<p>You'll need to use <code>COPY FROM</code>, but you'll need some preprocessing.  COPY is not a CQL statement, but rather a CQLSH command, so you can only run this from the CQL Shell.  </p>

<p>First, you'll need to create the table you want to import into.  This is where you define all the columns.</p>

<p>Secondly, here is the command syntax for import (delimiter is optional and more options are supported: see <a href=""http://www.datastax.com/docs/1.1/references/cql/COPY"" rel=""nofollow"">here</a>):</p>

<pre><code>COPY mytable([col1, col2, etc...]) FROM ('file.txt') WITH DELIMITER=',';
</code></pre>

<p>Lastly, and here comes a little bit of extra work, Cassandra can only import CSV data (where columns are delimited by separators) as opposed to offset-based.  To solve this issue, I would write a small script in your favorite language that would read your existing ASCII file and convert it to CSV format line by line.  This should take not more than 2 dozen of lines of code, so it should be a no-brainer.</p>

<p><strong>EDIT:</strong> as <em>zubs</em> observed in the comments below, some third-party applications such as Excel can be leveraged to convert to CSV format removing the need to write custom code.</p>
",['table']
22052321,25061932,2014-02-26 20:17:35,Cassandra based Mahout user friend recommendations,"<p>I want to recommend a user , a list of users which the current user can add as friends.</p>

<p>I am using Cassandra and mahout. there is already a implementation of <a href=""https://svn.apache.org/repos/asf/mahout/trunk/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/cassandra/CassandraDataModel.java"" rel=""nofollow"">CassandraDataModel</a> in mahout integration package. I want to use this class.</p>

<p>So my recommend-er class looks like follows </p>

<pre><code>public class UserFriendsRecommender {

@Inject
private CassandraDataModel dataModel;

public List&lt;RecommendedItem&gt; recommend(Long userId, int number) throws TasteException{
    UserSimilarity userSimilarity = new PearsonCorrelationSimilarity(dataModel);
    // Optional:
    userSimilarity.setPreferenceInferrer(new AveragingPreferenceInferrer(dataModel));

    UserNeighborhood neighborhood =
              new NearestNUserNeighborhood(3, userSimilarity, dataModel);
    Recommender recommender = new GenericUserBasedRecommender(dataModel, neighborhood, userSimilarity); 
    Recommender cachingRecommender = new CachingRecommender(recommender);
    List&lt;RecommendedItem&gt; recommendations = cachingRecommender.recommend(userId, number);
    return recommendations;
}
</code></pre>

<p>}</p>

<p>CassandraDataModel has 4 column familys</p>

<pre><code>static final String USERS_CF = ""users"";
  static final String ITEMS_CF = ""items"";
  static final String USER_IDS_CF = ""userIDs"";
  static final String ITEM_IDS_CF = ""itemIDs"";
</code></pre>

<p>i have a hard time understanding this class especially the column family's. is there any example where i can look for or if someone can explain will be great with a small example.?</p>

<p>javadoc says this </p>

<pre><code>* &lt;p&gt;
 * First, it uses a column family called ""users"". This is keyed by the user ID
 * as an 8-byte long. It contains a column for every preference the user
 * expresses. The column name is item ID, again as an 8-byte long, and value is
 * a floating point value represnted as an IEEE 32-bit floating poitn value.
 * &lt;/p&gt;
 * 
 * &lt;p&gt;
 * It uses an analogous column family called ""items"" for the same data, but
 * keyed by item ID rather than user ID. In this column family, column names are
 * user IDs instead.
 * &lt;/p&gt;
 * 
 * &lt;p&gt;
 * It uses a column family called ""userIDs"" as well, with an identical schema.
 * It has one row under key 0. It contains a column for every user ID in the
 * model. It has no values.
 * &lt;/p&gt;
 * 
 * &lt;p&gt;
 * Finally it also uses an analogous column family ""itemIDs"" containing item
 * IDs.
 * &lt;/p&gt;
</code></pre>
",<cassandra><mahout><mahout-recommender>,"<p>All the following instructions about required column families by CassandraDataMdoel should be performed in cassandra-cli under the keyspace you created (recommender or other name).</p>

<p>1: Table users</p>

<p>userID is the row key, each itemID has a separate column name, and value is the preference:</p>

<pre><code>CREATE COLUMN FAMILY users
WITH comparator = LongType
AND key_validation_class=LongType
AND default_validation_class=FloatType;
</code></pre>

<p>Insert values: </p>

<pre><code>set users[0][0]='1.0';
set users[1][0]='3.0';
set users[2][2]='1.0';
</code></pre>

<p>2: Table items</p>

<p>itemID is the row key, each userID has a separate column name, and value is the preference:</p>

<pre><code>CREATE COLUMN FAMILY items
WITH comparator = LongType
AND key_validation_class=LongType
AND default_validation_class=FloatType;
</code></pre>

<p>Insert Values:</p>

<pre><code>set items[0][0]='1.0';
set items[0][1]='3.0';
set items[2][2]='1.0';
</code></pre>

<p>3: Table userIDs</p>

<p>This table just has one row, but many columns, i.e. each userID has a separate column:</p>

<pre><code>CREATE COLUMN FAMILY userIDs
WITH comparator = LongType
AND key_validation_class=LongType;
</code></pre>

<p>Insert Values:</p>

<pre><code>set userIDs[0][0]='';
set userIDs[0][1]='';
set userIDs[0][2]='';
</code></pre>

<p>4: Table itemIDs:</p>

<p>This table just has one row, but many columns, i.e. each itemID has a separate column:</p>

<pre><code>CREATE COLUMN FAMILY itemIDs
WITH comparator = LongType
AND key_validation_class=LongType;
</code></pre>

<p>Insert Values:</p>

<pre><code>set itemIDs[0][0]='';
set itemIDs[0][1]='';
set itemIDs[0][2]='';
</code></pre>
",['table']
22077552,22078483,2014-02-27 18:43:50,Cassandra CQL - clustering order with multiple clustering columns,"<p>I have a column family with primary key definition like this:</p>

<pre><code>...
PRIMARY KEY ((website_id, item_id), user_id, date)
</code></pre>

<p>which will be queried using queries such as:</p>

<pre><code>SELECT * FROM myCF
WHERE website_id = 30 AND item_id = 10
AND user_id = 0 AND date &gt; 'some_date' ;
</code></pre>

<p>However, I'd like to keep my column family ordered by date only, such as <code>SELECT date FROM myCF ;</code> would return the most recent inserted date.</p>

<p>Due to the order of clustering columns, what I get is an order per user_id then per date.
If I change the primary key definition to:</p>

<pre><code>PRIMARY KEY ((website_id, item_id), date, user_id)
</code></pre>

<p>I can no longer run the same query, as date must be restricted is user_id is.</p>

<p>I thought there might be some way to say:</p>

<pre><code>...
  PRIMARY KEY ((website_id, shop_id), store_id, date)
) WITH CLUSTERING ORDER BY (store_id RANDOMPLEASE, date DESC) ;
</code></pre>

<p>But it doesn't seem to exist. Worst, maybe this is completely stupid and I don't get why.</p>

<p>Is there any ways of achieving this? Am I missing something?</p>

<p>Many thanks!</p>
",<cassandra><cql><cql3><cqlsh>,"<p>Your query example restricts user_id so that should work with the second table format. But if you are actually trying to run queries like</p>

<pre><code>SELECT * FROM myCF
WHERE website_id = 30 AND item_id = 10
AND date &gt; 'some_date'
</code></pre>

<p>Then you need an additional table which is created to handle those queries, it would only order on Date and not on user id</p>

<pre><code>Create Table LookupByDate ... PRIMARY KEY ((website_id, item_id), date)
</code></pre>
",['table']
22139747,22140030,2014-03-03 06:04:26,alter composite primary key in cassandra CQL 3.0,"<p>I'm in a situation where I need to change the the composite primary key as follows:</p>

<p><strong>Old Primary Key: (id, source, attribute_name, updated_at);</strong></p>

<p><strong>New Primary Key I want: (source, id, attribute_name, updated_at);</strong></p>

<p>I issued the following (mysql like) command:</p>

<pre><code>ALTER TABLE general_trend_table 
DROP PRIMARY KEY, 
ADD PRIMARY KEY(source, id, attribute_name, updated_at);
</code></pre>

<p>I got the following error:</p>

<p><strong>Bad Request: line 1:38 no viable alternative at input 'PRIMARY'</strong></p>

<p>any idea how to get around this problem? more specifically I want to know is there any way to change the primary key in cassandra?</p>
",<cassandra><composite-primary-key><alter><cql3><cqlsh>,"<p>There is no way to change a primary key, as it defines how your data is physically stored.</p>

<p>You can create a new table with the new primary key, copy data from the old one, and then drop the old table.</p>
",['table']
22224082,22240984,2014-03-06 12:09:33,Caching types in Cassandra 2.0,"<p>I have a small confusion with caching types in Cassandra 2.0. Is 'all' meaning entire table data caching or recently accessed (keys + rows) caching ? For example, if the table has 'n' rows, will it cache only recently accessed 'x' rows (or) cache all 'n' rows irrespective of accessibility ? Can you please explain more details about all/keys_only/rows_only types ? </p>

<p><a href=""http://www.datastax.com/documentation/cassandra/2.0/cassandra/reference/referenceTableAttributes.html"" rel=""nofollow"">http://www.datastax.com/documentation/cassandra/2.0/cassandra/reference/referenceTableAttributes.html</a></p>

<blockquote>
  <p>caching
      (Default: keys_only) Optimizes the use of cache memory without manual tuning. Set caching to one of the following values:</p>

<pre><code>    all
    keys_only
    rows_only
    none

Cassandra weights the cached data by size and access frequency. Use this parameter to specify a key or row cache instead of a table
</code></pre>
  
  <p>cache, as in earlier versions.</p>
</blockquote>

<p>Thanks,</p>

<p>Ramesh</p>
",<cassandra>,"<p><code>ALL</code> means a union of <code>ROWS_ONLY</code> and <code>KEYS_ONLY</code> caching.  It does not imply that the whole table is going to be cached.</p>

<p>Source:  from the source code (I looked at trunk == 2.1 currently); search for uses of <code>Caching.ALL</code>, <code>Caching.KEYS_ONLY</code> and <code>Caching.ROWS_ONLY</code>.</p>
",['table']
22294555,22302497,2014-03-10 07:40:33,Cassandra rack concept and database structure,"<p>I am new to Cassandra and I would like to learn more about Cassandra's racks and structure.</p>

<p>Suppose I have around 70 column families in Cassandra and two AWS2 instances.</p>

<ol>
<li>How many Data Centres will be used?</li>
<li>How many nodes will each rack have?</li>
<li>Is it possible to divide a column family in multiple keyspaces?</li>
</ol>
",<cassandra><nosql><cassandra-cli>,"<p>The intent of making Cassandra aware of logical racks and data centers is to provide additional levels of fault tolerance.  The idea (<a href=""http://www.datastax.com/documentation/cassandra/2.0/cassandra/architecture/architectureDataDistributeReplication_c.html"" rel=""noreferrer"">as described in this document</a>, under the ""Network Topology Strategy"") is that the application should still be able to function if one rack or data center goes dark.  Essentially, Cassandra...</p>

<blockquote>
  <p>places replicas in the same data center by walking the ring clockwise
  until reaching the first node in another rack. NetworkTopologyStrategy
  attempts to place replicas on distinct racks because nodes in the same
  rack (or similar physical grouping) often fail at the same time due to
  power, cooling, or network issues.</p>
</blockquote>

<p>In this way, you can also query your data by LOCAL_QUORUM, in which QUORUM ((replication_factor / 2) + 1) is only computed from the nodes present in the same data center as the coordinator node.  This reduces the effects of inter-data center latency.</p>

<p>As for your questions:</p>

<ol>
<li><p>How many data centers are used are entirely up to you.  If you only have two AWS instances, putting them in different logical data centers is possible, but only makes sense if you are planning to use consistency level ONE.  As-in, if one instance goes down, your application only needs to worry about finding one other replica.  But even then, the <a href=""http://www.datastax.com/documentation/cassandra/2.0/cassandra/architecture/architectureSnitchesAbout_c.html"" rel=""noreferrer"">snitch</a> can only find data on one instance, or the other.</p></li>
<li><p>Again, you can define the number of nodes that you wish to have for each rack.  But as I indicated with #1, if you only have two instances, there isn't much to be gained by splitting them into different data centers or racks.</p></li>
<li><p>I do not believe it is possible to divide a column family over multiple keyspaces.  But I think I know what you're getting at.  Each keyspace will be created on each instance.  As you have 2 instances, you will be able to specify a replication factor of 1 or 2.  If you had 3 instances, you could set a replication factor of 2, and then if you lost 1 instance you would still have access to all the data.  As you only have 2 instances, you need to be able to handle one going dark, so you will want to make sure both instances have a copy of every row (replication factor of 2).</p></li>
</ol>

<p>Really, the logical datacenter/rack structure becomes more-useful as the number of nodes in your cluster increases.  With only two, there is little to be gained by splitting them with additional logical barriers.  For more information, read through the two docs I linked above:</p>

<p><a href=""http://www.datastax.com/documentation/cassandra/2.0/cassandra/architecture/architectureDataDistributeReplication_c.html"" rel=""noreferrer"">Apache Cassandra 2.0: Data Replication</a></p>

<p><a href=""http://www.datastax.com/documentation/cassandra/2.0/cassandra/architecture/architectureSnitchesAbout_c.html"" rel=""noreferrer"">Apache Cassandra 2.0: Snitches</a></p>
",['rack']
22361583,22367305,2014-03-12 19:13:37,Secondary indexes on composite keys in cassandra,"<p>I have this table in cassandra</p>

<pre><code>CREATE TABLE global_product_highlights (
  deal_id text,
  product_id text,
  highlight_strength double,
  category_id text,
  creation_date timestamp,
  rank int,
  PRIMARY KEY (deal_id, product_id, highlight_strength)
)
</code></pre>

<p>When i fire below query in Golang</p>

<pre><code>err = session.Query(""select product_id from global_product_highlights where category_id=? order by highlight_strength DESC"",default_category).Scan(&amp;prodId_array)
</code></pre>

<p>I get ERROR : ORDER BY with 2ndary indexes is not supported.</p>

<p>I have an index on category_id.</p>

<p>I don't completely understand how is secondary index applied on composite keys in cassandra.</p>

<p>Appreciate if anyone would explain and rectify this one.</p>
",<go><cassandra>,"<p>The <code>ORDER BY</code> clause in Cassandra only works on your first clustering column (2nd column in the primary key), which in this case is your product_id.  <a href=""http://www.datastax.com/documentation/cql/3.1/cql/cql_reference/select_r.html"" rel=""nofollow"">This DataStax doc</a> states that:</p>

<blockquote>
  <p>Querying compound primary keys and sorting results ORDER BY clauses
  can select a single column only. That column has to be the second
  column in a compound PRIMARY KEY.</p>
</blockquote>

<p>So, if you want to have your table sorted by highlight_strength, then you'll need to make that field the first clustering column.</p>
",['table']
22393533,22442091,2014-03-13 23:58:41,DataStax Java Driver loop repeating rows,"<p>I'm developing a new product using cassandra as DB. Right now installed on a single ubuntu 13.10 development laptop core i7. I have a column family and a query. This query, executed in cqlsh give 33267 rows. Executed on my java program, using the datastax java driver 2.0, some executions give the correct rows, others got into an infinite loop repeating again and again the same rows:</p>

<pre><code>while (!rs.isExhausted()) {
  Row row = rs.one();
  long hora = row.getDate(1).getTime();
  String clave = row.getString(0);
  List&lt;Long&gt; data = row.getList(2, Long.class);
  ordenados.put(hora, new Object[]{clave, data.get(0) / 100000000.0, data.get(1)});
  contador2 +=1;
  if (Math.floor(contador2/1000.0) == contador2/1000.0) {
    System.out.println(""sitio ""+ contador2+ "" ""+clave+ "" ""+hora);
  }
}
</code></pre>

<p>When profiling the app, I see lock contention betweeen new I/O workers threads, 98% time is spend on sun.nio.ch.EPollArrayWrapper.poll method. 
Someone has experienced this issue and know a solution?
Someone can me direct to a link to download the cassandra-driver-core-2.0.0.src.jar so I can debug the error with the sources and report to datastax?
This is an exciting technology, but is the first time in my career a production DB give me so unreliable behaviour.
By the way: The original query had an order by that I removed. With the order by, I got this exception:
    Exception in thread ""main"" com.datastax.driver.core.exceptions.InvalidQueryException:    Cannot page queries with both ORDER BY and a IN restriction on the partition key; you must either remove the ORDER BY or the IN and sort client side, or disable paging for this query
When yesterday worked on similar queries and on cqlsh it works without problem with the order by added. I just talk about this problem because maybe both are related.
Regards</p>
",<cassandra><cql><datastax-java-driver>,"<p>You can get the source from <a href=""https://github.com/datastax/java-driver"" rel=""nofollow"">githib datastax/java-driver</a>. It doesn't look like the source is included in either the maven or tarball downloads.</p>

<p>I think you are encountering <a href=""https://issues.apache.org/jira/browse/CASSANDRA-6722"" rel=""nofollow"">CASSANDRA-6722</a> when you used IN and ORDER BY in your query. The java-driver automatically does paging with a default fetch size of 5000. You can disable automatic paging with <code>Statement.setFetchSize(Integer.MAX_VALUE)</code>. There is more info about automatic paging in this <a href=""http://www.datastax.com/dev/blog/client-side-improvements-in-cassandra-2-0"" rel=""nofollow"">blog post</a>.</p>

<p>What version of Cassandra is you application connecting to? If you could share more about your table definition and query maybe it will be possible to reproduce the repeating rows issue.</p>
",['table']
22446939,22461365,2014-03-17 04:23:25,How to mimic GROUP BY in Cassandra,"<p>Is it possible to mimic GROUP BY functionality from SQL using Cassandra? If yes, please provide a short example that does that. </p>
",<sql><cassandra>,"<p>I was thinking, if the groups where known a head of time, then a loop of multiple async queries on each different group would have a similar effect.</p>

<p>For example group by on months.</p>

<pre><code>for month in range(1,12):
    query = ""select * from table where col_month = "" + month
    session.execute_async(query)
</code></pre>

<p>If this isn't an option you would have to first select what you are grouping on and take the set of all data. </p>

<pre><code>query = ""select col_month from table""
rows = session.execute(query) 
values = Set()
for row in rows:
    values.add(row)

query = ""select * from table where col_month = ""
for value in values:
    session.execute_async(query+value)
</code></pre>
",['table']
22650126,22652974,2014-03-26 02:04:08,Why do we need secondary indexes in cassandra and how do they really work?,"<p>I was trying to understand why secondary indexes were even necessary on Cassandra.</p>

<p>I know that secondary indexes are used because:</p>

<p>""Secondary indexes allow for efficient querying by specific values using equality predicates (where column x = value y). Also, queries on indexed values can apply additional filters to perform operations such as range queries.""</p>

<p>from: <a href=""http://www.datastax.com/docs/0.7/data_model/secondary_indexes"" rel=""nofollow"">http://www.datastax.com/docs/0.7/data_model/secondary_indexes</a></p>

<p>But what I did not understand is why a query like:</p>

<pre><code>get users where birth_date = 1973;
</code></pre>

<p>required that the birth_date had a secondary index. Why is it necessary for secondary indexes to even exist? Can't cassandra just go through the table and then return the values when the constrained is matched? Why do we need to treat things that we might want to query in that way in any special way?</p>

<p>I am assuming that the fact that cassandra is distributed and going through the whole table might not be easy due to each row key being allocated to a different node making it a little complicated. But I didn't really understand how making it distributed complicated the problem and how secondary indices resolved it (i.e. how does cassandra resolve this issue?).</p>

<p>Related to this question, is it true that secondary indexes and primary keys are the only things that can be queried in the for of <code>SELECT * FROM column_family_table WHERE col_x = constraint</code>? Why is the primary key special?</p>
",<cassandra><cql>,"<p>With amount of data these nosql databases meant to deal with, going for table scan or region scan is not an option. That's what Cassandra has restricted and allowed queries over non row key columns only if secondary indxes are enabled. That way such indices and data would be co located on same data node. </p>

<p>Hope it helps.</p>

<p>-Vivek</p>
",['table']
22706658,22708461,2014-03-28 07:26:16,Row ordering in Cassandra,"<p>I have the following columnfamily in Cassandra 2.0.5, using a <code>Murmur3Partitioner</code>. Inside this columnfamily I store the number of apparitions of unique hashes in a timeframe (hashes extracted from events occuring over time - not really relevant).</p>

<p>My use case is to select all the hashes and their counts for a given timeframe (the <code>hour</code> field).</p>

<p>Since the amount of data can be very large, I tried to do pagination like using <code>LIMIT</code> and continuing from the last returned hash, like in the example below. It <em>seems</em> to work, as the hashes <em>seem</em> to be returned in a sorted ascending order. </p>

<p>Can someone explain if this really works and why? Especially since I found <a href=""http://www.datastax.com/documentation/cql/3.0/cql/cql_using/paging_c.html"" rel=""nofollow"">this link</a> which states that the rows are...not ordered, so now that I think about it, the hashes should be returned randomly. </p>

<p>I did validate the procedure by counting the number of rows using the pagination approach and by using <code>COUNT</code> in cqlsh, but I can't really
check if all the right hashes are returned due to the large amount of data.</p>

<pre><code>cqlsh:db&gt; DESCRIBE COLUMNFAMILY hashes ;
CREATE TABLE hashes (
  hour text,
  hash text,
  count counter,
  PRIMARY KEY (hour, hash)
) WITH COMPACT STORAGE AND
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND 
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  index_interval=128 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  default_time_to_live=0 AND
  speculative_retry='99.0PERCENTILE' AND
  memtable_flush_period_in_ms=0 AND 
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'LZ4Compressor'};


cqlsh:db&gt; SELECT * FROM hashes WHERE hour = '2014032710' LIMIT 10;

 hour       | hash                                                             | count
------------+------------------------------------------------------------------+----------
 2014032710 | 000034d4b821c9af90bbf39cd803d45b25d7c14777697b8d9fc71c3a102c360f |        1
 2014032710 | 000063b39f526788dc026a07abe1bc1365652772e9c66be9a7408b16c61962fa |        2
 2014032710 | 00009c38834cedfb37bfd95355bba1a225aea6ee74f5ddc4ace820bfc33eb7a6 |        1
 2014032710 | 0000a68de59092e0326b3ceff8d9a1167c7f5ea0aac804389c259f336956e520 |        1
 2014032710 | 0000b0fed9e2f8f70e5e46f084be1872f0d1944c0e89a8850e6b7c3be17b8935 |        9
 2014032710 | 0001204a0fb29d3a8ac7164e451662069d19307ea56e014215a64cc606cf4df9 |        1
 2014032710 | 00015c165622a3c8b88d33e471d740088d9b6203dd81235d50ec129c40282229 |        1
 2014032710 | 00019ed1b3287ed808c24146d1f2e145238478b49ad3740fb58cb46bc509965a |       10
 2014032710 | 00019fa833cee60e7a1b8ed5d5c6fbef8c401a144e1537e15c9a5f65672d44fb |        1
 2014032710 | 0001df8d8319524a93ed523382a6cce8de9234211d5f3dc46bb4c530d9385150 |        1

(10 rows)

cqlsh:db&gt; SELECT * FROM hashes WHERE hour = '2014032710' AND hash &gt; '0001df8d8319524a93ed523382a6cce8de9234211d5f3dc46bb4c530d9385150' LIMIT 10;

 hour       | hash                                                             | count
------------+------------------------------------------------------------------+----------
 2014032710 | 000200428d93eb478c6a9ae0d9daa21fac88ca8dd4e536f60ae992dbea6155d4 |        2
 2014032710 | 00024447d8983fc0f022df4301eb69eca4ccc7cf0fc2e9361046dbaedbe830bc |        1
 2014032710 | 00025c6b3ef861fa3ef047d618f078927c9f8cf875e9b935c8e556189969bc17 |        1
 2014032710 | 00026f67e525bd11b67062e3122eb625799c6878f7812da8f23f0c8e9bd9f9d5 |        2
 2014032710 | 00028ded6dfe5d8616cc0eef559cfdf15fd51d5a36c17f2b9852785e8ca55c27 |        4
 2014032710 | 00028f8fab859c702fe0cc51db390ce7ae85ca97807a751ddf12fed57639239f |        1
 2014032710 | 0002f4046ef35e169fa79e2abf0b92212c1438487819dd8318301991ff99acac |       32
 2014032710 | 000381054a59d46c87164fcfb69952afa1e77acd71f88b25e09eab3eacc1b21a |        1
 2014032710 | 0003aca7fd2cab16a03d79fa7ac1505f144f9ba04fea87a050bef919aa628e74 |        1
 2014032710 | 0003e6a549b01cf1634c1b2844618d4e96ac00d74be30b9401b3fbbbc5bdb7e2 |        1

(10 rows)
</code></pre>
",<python><cassandra><cql><cassandra-2.0>,"<p>Please read about Sorted wide rows and Clustering ORDER KEY. Some excerpts from CQL specification page ""Partition key and clustering columns</p>

<p>In CQL, the order in which columns are defined for the PRIMARY KEY matters. The first column of the key is called the partition key. It has the property that all the rows sharing the same partition key (even across table in fact) are stored on the same physical node. Also, insertion/update/deletion on rows sharing the same partition key for a given table are performed atomically and in isolation. Note that it is possible to have a composite partition key, i.e. a partition key formed of multiple columns, using an extra set of parentheses to define which columns forms the partition key.</p>

<p>The remaining columns of the PRIMARY KEY definition, if any, are called __clustering columns. On a given physical node, rows for a given partition key are stored in the order induced by the clustering columns, making the retrieval of rows in that clustering order particularly efficient (see SELECT).</p>

<p>"" </p>
",['table']
22766331,27721284,2014-03-31 15:55:31,Correct Way to Scale Cassandra Cluster Using Authentication,"<p>Upon starting and adding a new node to my Cassandra cluster that is configured to use authentication, I'm getting the following stack trace:</p>

<pre><code>java.lang.RuntimeException: org.apache.cassandra.exceptions.AlreadyExistsException: Cannot add already existing column family ""credentials"" to keyspace ""system_auth""
    at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:242)
    at org.apache.cassandra.auth.PasswordAuthenticator.process(PasswordAuthenticator.java:266)
    at org.apache.cassandra.auth.PasswordAuthenticator.setupCredentialsTable(PasswordAuthenticator.java:214)
    at org.apache.cassandra.auth.PasswordAuthenticator.setup(PasswordAuthenticator.java:171)
    at org.apache.cassandra.auth.Auth.setup(Auth.java:132)
    at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:830)
    at org.apache.cassandra.service.StorageService.initServer(StorageService.java:583)
    at org.apache.cassandra.service.StorageService.initServer(StorageService.java:482)
    at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:345)
    at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:462)
    at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:552)
Caused by: org.apache.cassandra.exceptions.AlreadyExistsException: Cannot add already existing column family ""credentials"" to keyspace ""system_auth""
        at org.apache.cassandra.service.MigrationManager.announceNewColumnFamily(MigrationManager.java:209)
        at org.apache.cassandra.cql3.statements.CreateTableStatement.announceMigration(CreateTableStatement.java:114)
        at org.apache.cassandra.cql3.statements.SchemaAlteringStatement.execute(SchemaAlteringStatement.java:71)
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:188)
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:222)
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:234)
        ... 10 more
</code></pre>

<p>This makes sense, as one of the prior nodes will already have created it.</p>

<p>What is the correct approach to bringing new nodes online, given that the <code>system_auth</code> column family already exists? Additionally, how does one cater for increasing the replication factor of the <code>system_auth</code> table as the size of the cluster increases?</p>
",<java><cassandra><datastax><cassandra-2.0>,"<p>Although I haven't been able to find anything specific on that error it might well be a warning as you correctly state, it should already exist. I'm assuming that the node started and joined the cluster ok.</p>
<p>To answer your questions:</p>
<blockquote>
<p>What is the correct approach to bringing new nodes online, given that the system_auth column family already exists?</p>
<p>Additionally, how does one cater for increasing the replication factor of the system_auth table as the size of the cluster increases?</p>
</blockquote>
<p>I wasnt able to post more than one link on this answer, however searching the Datastax documentation for adding nodes will give you the general proceedure for adding nodes which will be the same wether using authentication or not,  the only difference being the authentication settings in the cassandra.yaml</p>
<p>Configuring the system_auth keyspace with replication is covered in the following documentation:</p>
<p><a href=""http://www.datastax.com/documentation/datastax_enterprise/4.5/datastax_enterprise/sec/secConfSysAuthKeyspRepl.html"" rel=""nofollow noreferrer"">http://www.datastax.com/documentation/datastax_enterprise/4.5/datastax_enterprise/sec/secConfSysAuthKeyspRepl.html</a></p>
",['table']
22871775,22873075,2014-04-04 19:44:46,How to check vnode disabled on a hadoop node,"<p>Linking back to this question:
<a href=""https://stackoverflow.com/questions/19969329/why-not-enable-virtual-node-in-an-hadoop-node/19974621#19974621"">Why not enable virtual node in an Hadoop node?</a></p>

<p>I'm running a mixed 3 node cluster with 2 cassandra and 1 analytics nodes and disabled the virtual nodes by generating 3 tokens with the utility given by DataStax enterprise.
But when I run 'nodetool status' command, I still see 256 tokens with each node and when a mapreduce job is created, it creates 257 mappers and takes a very long time to execute a query with small data.
So my specific questions are:</p>

<ol>
<li><p>Is virtual node setting still not disabled? How can I verify if its disabled?</p></li>
<li><p>If its disabled then why 257 mappers are still created for each job? Is there a different configuration for that?</p></li>
</ol>

<p>Thanks much for any help!!</p>
",<hadoop><cassandra><datastax-enterprise>,"<p>1) It's not disabled. You can tell because it still says 256 tokens in nodetool status. </p>

<p>To disable vnodes you need to make sure that you change the num_tokens variable in the cassandra.yamnl</p>

<pre><code># If you already have a cluster with 1 token per node, and wish to migrate to 
# multiple tokens per node, see http://wiki.apache.org/cassandra/Operations
# num_tokens: 256  &lt;&lt; Make sure this line is commented out

# initial_token allows you to specify tokens manually.  While you can use it with
# vnodes (num_tokens &gt; 1, above) -- in which case you should provide a 
# comma-separated list -- it's primarily used when adding nodes to legacy clusters 
# that do not have vnodes enabled.
initial_token:  &lt;&lt; Your generated token goes here
</code></pre>
","['initial_token', 'num_tokens']"
22886259,22888017,2014-04-05 20:17:32,How to DROP tables or column families in cassandra 2.0.5?,"<p>I am trying to drop a table/column family from my database but I am unable to do it.</p>

<p>I have tried the following commands and their responses are:</p>

<pre><code>cqlsh:testreducedb&gt; DROP_COLUMNFAMILY largest_time_total;
Bad Request: line 1:0 no viable alternative at input 'DROP_COLUMNFAMILY'
cqlsh:testreducedb&gt; DROP COLUMNFAMILY largest_time_total;
Bad Request: unconfigured columnfamily largest_time_total
cqlsh:testreducedb&gt; DROP COLUMN FAMILY largest_time_total;
Bad Request: line 1:5 no viable alternative at input 'COLUMN'
cqlsh:testreducedb&gt; DROP COLUMN FAMILY 'largest_time_total';
Bad Request: line 1:5 no viable alternative at input 'COLUMN'
cqlsh:testreducedb&gt; DROP COLUMN FAMILY ""largest_time_total"";
Bad Request: line 1:5 no viable alternative at input 'COLUMN'
</code></pre>

<p>and also:</p>

<pre><code>cqlsh:testreducedb&gt; DROP_TABLE largest_time_total;
Bad Request: line 1:0 no viable alternative at input 'DROP_TABLE'
cqlsh:testreducedb&gt; DROP TABLE largest_time_total;
Bad Request: unconfigured columnfamily largest_time_total
cqlsh:testreducedb&gt; DROP TABLE 'largest_time_total';
Bad Request: line 1:11 no viable alternative at input 'largest_time_total'
cqlsh:testreducedb&gt; DROP TABLE ""largest_time_total"";
Bad Request: unconfigured columnfamily largest_time_total
</code></pre>

<p>does someone know how to drop tables/column families in Cassandra 2.0.5?</p>

<p>I am using:</p>

<pre><code>[cqlsh 4.1.1 | Cassandra 2.0.5 | CQL spec 3.1.1 | Thrift protocol 19.39.0]
</code></pre>
",<cassandra><cql><cassandra-2.0>,"<p>I'm going to go through the errors you got so you can get a clear picture of what's going on, but firstly a note that columnfamily == table. </p>

<blockquote>
  <p>cqlsh:testreducedb> DROP_COLUMNFAMILY largest_time_total;
  Bad Request: line 1:0 no viable alternative at input 'DROP_COLUMNFAMILY'</p>
</blockquote>

<p><code>DROP_COLUMNFAMILY</code> Isn't a valid command. It should be <strong>DROP COLUMNFAMILY </strong> or <strong>DROP TABLE </strong> assuming you're already using the keyspace (database) that stores the aforementioned table (aka columnfamily). If you haven't specified the keyspace to your client then you can specify it in the drop statement:</p>

<pre><code>DROP TABLE &lt;keyspace&gt;.&lt;columnfamily&gt;;
DROP TABLE &lt;keyspace&gt;.&lt;table&gt;;
# the below is an actual statement assuming grocerystore is the keyspace and
# shoppers is the columnfamily 
DROP TABLE ""grocerystore"".""shoppers"";
</code></pre>

<blockquote>
  <p>cqlsh:testreducedb> DROP COLUMNFAMILY largest_time_total;
  Bad Request: unconfigured columnfamily largest_time_total</p>
</blockquote>

<p>The column family doesn't actually exist. Based on just seeing <strong>cqlsh</strong> my bet is that you haven't specified using the keyspace that stores *largest_time_total*. Try using <code>USE &lt;keyspace&gt;</code> in cqlsh, e.g. <code>USE grocerystore;</code></p>

<p>The rest of the errors are just repetitions of the above. </p>

<p><strong>P.S</strong> You were really close with this one, but there's one space too many between <code>COLUMN</code> and <code>FAMILY</code> :)</p>

<blockquote>
  <p>cqlsh:testreducedb> DROP COLUMN FAMILY largest_time_total;
  Bad Request: line 1:5 no viable alternative at input 'COLUMN'</p>
</blockquote>

<p>Try:</p>

<pre><code>USE testreducedb;
DROP COLUMNFAMILY largest_time_total;
</code></pre>
",['table']
22915237,22918324,2014-04-07 14:24:55,How to generate Cassandra Token for composite partition key?,"<p>My Cassandra ColumnFamily uses the Murmur3Partitioner, and has a composite partition key.
With this partitioner I was trying to create a Token, however it seems this token factory only allows Long values.
Is it possible to generate these hashes for something like ""token(partition_column1, partition_column2)""?</p>
",<cassandra><murmurhash>,"<p>It's supposed to work.  In fact, if your partition key is composite, you should be unable to create a token for only a single column.  Are you sure that you've defined the composite key properly?</p>

<pre><code>cqlsh:testks&gt; create table t1(k1 int, k2 text, v text, primary key((k1, k2)));
cqlsh:testks&gt; insert into t1(k1, k2, v) values (1, 'key', 'value');
cqlsh:testks&gt; select * from t1;

 k1 | k2  | v
----+-----+-------
  1 | key | value

(1 rows)

cqlsh:testks&gt; select token(k1) from t1;
Bad Request: Invalid number of arguments in call to function token: 2 required but 1 provided
cqlsh:testks&gt; select token(k2) from t1;
Bad Request: Invalid number of arguments in call to function token: 2 required but 1 provided
cqlsh:testks&gt; select token(k1, k2) from t1;

 token(k1, k2)
---------------------
 8064425790465501062

    (1 rows)
</code></pre>
",['table']
22927435,22942290,2014-04-08 04:17:27,Cassandra storage engine representation of data in Cassandra 2.0.X,"<p>Cassandra CLI is being deprecated in Cassandra 3.0. Was wondering if its possible to get the storage engine representation of wide rows(dynamic columns), and not the relational view.</p>

<p>For example, executing the following in cassandra-cli would yield for key 'COEXEIGLE', with dynamic columns and counter values: </p>

<pre><code>list table;

RowKey: COEXEIGLE
=&gt; (counter=2014-04-07 18\:45\:00-0700:count, value=5)
=&gt; (counter=2014-04-07 19\:30\:00-0700:count, value=1)
=&gt; (counter=2014-04-07 19\:31\:00-0700:count, value=1)
=&gt; (counter=2014-04-08 19\:31\:00-0700:count, value=2)

1 Row Returned.
</code></pre>

<p>Although, using cqlsh, the following sql statement, we you would get the traditional relational view of the data:</p>

<pre><code>select * from table;

 serialId    | time                     | count
-------------+--------------------------+-------
   COEXEIGLE | 2014-04-07 18:45:00-0700 |     5
   COEXEIGLE | 2014-04-07 19:30:00-0700 |     1
   COEXEIGLE | 2014-04-07 19:31:00-0700 |     1
   COEXEIGLE | 2014-04-08 19:31:00-0700 |     2

(4 rows)
</code></pre>

<p>Again, curious if we could still see the storage engine representation and not the relational view.</p>

<p>Cassandra version:</p>

<pre><code>[cqlsh 4.1.1 | Cassandra 2.0.6 | CQL spec 3.1.1 | Thrift protocol 19.39.0]
</code></pre>
",<cassandra><cassandra-2.0><cqlsh><cassandra-cli><cassandra-3.0>,"<p>The Cassandra-Cli Actually doesn't show you an engine representation of the data (a large amount of information is not displayed). The only way to actually see how the data is laid out on disk is to use the sstable2json program. This will show the actually on disk representation of the data as the storage engine sees it.</p>

<p>My suggestion if you really need to see this view is to flush a table and the use sstable2json to transform it into a human readable format.</p>
",['table']
22992666,23010156,2014-04-10 15:40:38,How to store different Cassandra tables on different storage media?,"<p>Reading <a href=""http://readwrite.com/2012/04/27/cassandra-11-brings-cache-tuning-mixed-storage-support#awesm=~oB2NtcpgjL8sFE"" rel=""nofollow"">this somewhat old page about Cassandra 1.1 features</a>, it seems that it's possible to configure tables to go to a certain storage medium.  After trying to search through the documentation, I could not find any information on it.</p>

<p>How should Cassandra's storage configuration be to support this sort of setup?</p>

<p><br/>
Thanks so much!</p>
",<database><cassandra><storage><media>,"<pre><code>Cassandra 1.1 and later releases provide fine-grained control of table storage on disk, writing tables to disk
using separate table directories within each keyspace directory. Data files are stored using this directory
and file naming format:
/var/lib/cassandra/data/ks1/cf1/ks1-cf1-hc-1-Data.db
The new file name format includes the keyspace name to distinguish which keyspace and table the file
contains when streaming or bulk loading data. Cassandra creates a subdirectory for each table, which
allows you to symlink a table to a chosen physical drive or data volume. This provides the capability to
move very active tables to faster media, such as SSD’s for better performance, and also divvy up tables
across all attached storage devices for better I/O balance at the storage layer.
</code></pre>

<p>With this , hope you can understand what they mean by that. You shall create symlinks to the tables that will contain media or any other data. Since cassandra stores all the column family information in a particular manner , that is predictable. This facilitates this usage. </p>
",['table']
22995792,23123382,2014-04-10 18:11:26,Unmarshal timestamp using gocql query,"<p>How to accept Cassandra's timestamp and convert into string using gocql?
What will be the query?</p>
",<go><cassandra>,"<p>gocql can read it into a <a href=""http://golang.org/pkg/time/#Time"">time.Time</a>, which can then be converted to string by calling <a href=""http://golang.org/pkg/time/#Time.String"">String()</a>. For example:</p>

<pre><code>if err := session.Query(`CREATE TABLE events (
        event text,
        event_time timestamp,
        PRIMARY KEY (event, event_time),
    ) WITH CLUSTERING ORDER BY (event_time DESC);
    `).Exec(); err != nil {
    log.Fatal(""create table events:"", err)
}

tm := time.Now()
if err := session.Query(`INSERT INTO events
        (event, event_time)
        VALUES (?, ?)`,
    ""click"", tm).Exec(); err != nil {
    log.Fatal(""insert into events: "", err)
}

var event_time time.Time

if err := session.Query(`SELECT event_time FROM events LIMIT 1`).
    Consistency(gocql.One).Scan(&amp;event_time); err != nil {
    log.Fatal(""select from events:"", err)
}

// prints 2014-04-17 01:55:26.434 +0000 UTC
fmt.Printf(event_time.String())
</code></pre>
",['table']
23048306,23053369,2014-04-13 20:51:24,Cassandra DB (with PHP and Android application access),"<p>I am new with NoSQL databases (I have always working with Oracle and MySql databases), but now I am just starting to develop a mobil/web application (a mobile oriented social network), but now I have a silly, but very big doubt:</p>

<p>Should I set up a Cassandra database (like you implement a MySQL tables, relations, etc) </p>

<p>and...</p>

<p>develop a PHP functionality web (social network will also have a web access), and from that Php web develop the android/iphone client application (it will only replicate web functionalities).</p>

<p>I am very confused in the approach I should follow. Could anyone give me an advice???</p>

<p>(I have said Php because is a very optimal web language with server support (Java and related hostings are more expensive, etc but that is not the debate, even I also accept advice in this point if relevant).</p>

<p>Thank you everybody so much!!!</p>
",<php><mobile><cassandra><social-networking>,"<p>Yes you can create 1 keyspace which is similar to schema in DBMS and then you can create columnfamily which is similar to table in DBMS.</p>

<p>their is nothing like foreign key in cassandra, so you have to create you columnfamily structure in a way that only one table will going to satisfied all your requirement(individual columnfamily).</p>

<p>and facebook is using cassandra and php so you have to search how can you do similar for your project.</p>
",['table']
23058015,23062697,2014-04-14 10:41:31,Cassandra table structure suggestion and way of query,"<p>I am trying to create a following hierarchy:
<strong>UserId</strong> as rowKey, <strong>Hourly time series</strong> as columns and inside each hourly column I want to have a <strong>user specific information</strong> such as hourly activity.</p>

<pre><code>{
   UserId:long
   {
      Timestamp:datetime{
         pageview: integer,
         clicks:integer
      }
   }
</code></pre>

<p>I've read that it is possible to achieve it using <code>supercolumns</code> but at the same time it was mentioned that <code>supercolumns</code> are outdated right now. If it is true, any alternatives I can use? </p>

<p>Could you please provide me CQL / Java thrift example how should I create and insert such type of structure in Cassandra? </p>

<p>Thanks!</p>
",<cassandra><cassandra-cli>,"<p>You can user composite primary key for this, I add a table creation CQL query for the table. And you can use counter column for clicks. </p>

<pre><code>CREATE TABLE user_click_by_hour(
userid long,
time_stamp timestamp,
clicks int,
pageview int,
PRIMARY KEY(userid,time_stamp)
</code></pre>

<p>)</p>
",['table']
23076647,23082205,2014-04-15 07:00:12,Cassandra range slicing on composite key,"<p>I have columnfamily with composite key like this</p>

<pre><code>CREATE TABLE sometable(
    keya varchar,
    keyb varchar,
    keyc varchar,
    keyd varchar,
    value int,
    date timestamp,
    PRIMARY KEY (keya,keyb,keyc,keyd,date)
);
</code></pre>

<p>What I need to do is to</p>

<pre><code>SELECT * FROM sometable
WHERE
    keya = 'abc' AND
    keyb = 'def' AND
    date &lt; '2014-01-01'
</code></pre>

<p>And that is giving me this error</p>

<pre><code>Bad Request: PRIMARY KEY part date cannot be restricted (preceding part keyd is either not restricted or by a non-EQ relation)
</code></pre>

<p>What's the best way to solve this? Do I need to alter my columnfamily? 
I also need to query those table with all keya, keyb, <strong>keyc</strong>, and date.</p>
",<cassandra><cql><cassandra-2.0>,"<p>You cannot do it in cassandra. Moreover, such a range slicing is costlier too. You are trying to slice through a set of equalities that have the lower priority according to your schema.</p>

<blockquote>
  <p>I also need to query those table with all keya, keyb, keyc, and date.</p>
</blockquote>

<p>If you are considering to solve this problem, considering having this schema. What i would suggest is to have the keys in a separate schema </p>

<p>create table (
timeuuid id,
keyType text,
primary key (timeuuid,keyType))</p>

<p>Use the timeuuid to store the values and do a range scan based on that.</p>

<p>create table(
timeuuid prevTableId,
value int,
date timestamp,
primary key(prevTableId,date))</p>

<p>Guess , in this way, your table is normalized for better scalability in your use case and may save a lot of disk space if keys are repetitive too. </p>
",['table']
23096572,23121812,2014-04-15 23:54:35,Cassandra long row with different data types,"<p>I have read the following article about Cassandra CQL3 and Thrift API
<a href=""http://www.datastax.com/dev/blog/does-cql-support-dynamic-columns-wide-rows"" rel=""nofollow"">http://www.datastax.com/dev/blog/does-cql-support-dynamic-columns-wide-rows</a></p>

<p>In the article, they give an example on creating a scheme for gathering data from sensors.
They show a “wide row” solution by making the timestamp as a column. Cassandra's strength, as I see it is by supporting 2 billion columns and a fast way to extract data according to column.</p>

<p>In the article, with CQL3 they build a table</p>

<pre><code>CREATE TABLE data (
  sensor_id int,
  collected_at timestamp,
  volts float,
  PRIMARY KEY (sensor_id, collected_at)
) WITH COMPACT STORAGE;
</code></pre>

<p>which translates to:</p>

<pre><code> sensor_id | collected_at             | volts
         1 | 2013-06-05 15:11:00-0500 |   3.1
         1 | 2013-06-05 15:11:10-0500 |   4.3
         1 | 2013-06-05 15:11:20-0500 |   5.7
         2 | 2013-06-05 15:11:00-0500 |   3.2
         3 | 2013-06-05 15:11:00-0500 |   3.3
         3 | 2013-06-05 15:11:10-0500 |   4.3
</code></pre>

<p>In Thrift it translates to:</p>

<pre><code>list data;
RowKey: 1

=&gt; (cell=2013-06-05 15:11:00-0500, value=3.1, timestamp=1370463146717000)
=&gt; (cell=2013-06-05 15:11:10-0500, value=4.3, timestamp=1370463282090000)
=&gt; (cell=2013-06-05 15:11:20-0500, value=5.7, timestamp=1370463282093000)

RowKey: 2

=&gt; (cell=2013-06-05 15:11:00-0500, value=3.2, timestamp=1370463332361000)

RowKey: 3

=&gt; (cell=2013-06-05 15:11:00-0500, value=3.3, timestamp=1370463332365000)
=&gt; (cell=2013-06-05 15:11:10-0500, value=4.3, timestamp=1370463332368000)
</code></pre>

<p>I'm trying to think of a Cassandra Schema example for the following sensor data gathering problem.
Let's say I add a new set of sensors which have a bigint (long) value (instead of float).</p>

<p>Any ideas how to design such a table schema to include both sensor types with different data types yet keep the columns based on timestamp?</p>

<p>Thanks,
Guy</p>
",<cassandra><cql><cql3><cqlsh>,"<p>If you don't need to use COMPACT STORAGE (and backwards compatibility with Thrift), just create your table as </p>

<pre><code>CREATE TABLE data (
  sensor_id int,
  collected_at timestamp,
  other_field bigint,
  volts float,
  PRIMARY KEY (sensor_id, collected_at)
)
</code></pre>

<p>Cassandra supports sparse columns with basically no overhead, so if you programmatically decide to only populate one of the two fields for any cql row, you will achieve your goal.</p>

<p>Alternatively, you can continue to use COMPACT STORAGE and just switch to blob type. The blob type will do absolutely no interpretation or transformation of the bytes that you insert into it, so accuracy can be guaranteed. I would <strong>not</strong> recommend using a text type for this.</p>
",['table']
23098546,23102308,2014-04-16 03:38:38,Cassandra Email as primary key? And what exactly is a dynamic column family?,"<p>Would it be bad practice to use an email as a primary key in Cassandra? Would this cause problems with the replication (since first primary key is used for replication - partition key)?</p>

<p>According to the documentation it is not a good idea to use high-cardinality 'keys' as indices. It says that one should create a dynamic column family (table) for queries against high cardinality columns. </p>

<p>It doesn't seem to make sense to me if the main thing I'm keeping track of in the database is the USER, who logs in with their EMAIL (to the app), to use anything else, but the EMAIL as the primary key..</p>

<p>Is it efficient to use EMAIL as the row key? Would there be a reason to use UUID over this?</p>

<p>The problem I am (perhaps ignorantly) foreseeing is that using UUID as the row key and then adding the email as another primary key is loss of uniqueness (that being the uniqueness of the email address). Multiple accounts could then be created with the same email (without extra checks to ensure that that email has not already been used -- which either necessitates an index or a dynamic table?)</p>

<p>This leads to the second question. What exactly is a dynamic table? I don't see where this high-cardinality key is used in the dynamic table.. Is it now the row key (why not make it the row key to begin with..)?</p>

<p>Does the search for the row key have higher performance than created indexes?</p>

<p>Does anyone have any insight into this? I would really appreciate it!</p>

<p>If dynamic column family just means that the columns are 'dynamically' added then I don't see how this helps for high-cardinality columns in terms of indexing.</p>
",<database-design><cassandra><database-performance><cassandra-2.0><nosql>,"<p>You are mixing up primary keys with secondary indexes. The cardinality vs. efficiency trade-off applies to secondary indexes but not the primary key. The primary key values are unique by definition and are also the most efficient means of finding and accessing a single row. Have a look at this summary about <a href=""http://www.datastax.com/docs/1.1/ddl/indexes"" rel=""nofollow"">indexes in Cassandra</a>.</p>

<p>There is absolutely no problem with using the user's email address as the primary key of a user table if that is what uniquely identifies your users and associates them with their detail information.</p>

<p>A dynamic column family is a ""table"" for which the number if columns is not fixed. You add information not only by adding rows but also by adding columns on the fly. E.g. to build a time series of events. A column family is always dynamic, though I think the CQL layer obscures the fact. Whether you treat it as such or as a fixed set of columns is up to you. To find some theoretical background look for the <a href=""http://en.wikipedia.org/wiki/BigTable"" rel=""nofollow"">BigTable</a> concept and how Cassandra implements it.</p>
",['table']
23145435,23358506,2014-04-18 00:03:32,Cassandra multiple disk per node setup,"<p><strong>Intro</strong></p>

<p>I have a cassandra 1.2 cluster, all the nodes have SSDs. Now I want to add more disks to the existing nodes, but I want to be able to choose which tables are stored on different disks.</p>

<p><strong>Problem</strong></p>

<p>For example, node 1 will have 3 SSDs and 1 regular disk drive and I want all the column families except 1 (let's call it ""discord"" table) to be stored on the SSDs only, the final table ""discord"" needs to be stored on the regular disk.</p>

<p>According to the documentation this should be possible; however, the only way of doing it that I can see is:</p>

<ol>
<li>Setting up Cassandra to use multiple <code>data_files_directories</code> in <code>cassandra.yaml</code>.</li>
<li>Creating the tables.</li>
<li>Creating a link from the data directory on each SSD to the directory on the hard disk where I want to store the column family.</li>
</ol>

<p><strong>Question</strong></p>

<p>Is this the only way of doing it? Or there is a simpler way of configuring a node to work in this way?</p>
",<cassandra><storage><database><nosql>,"<p>You can set multiples files using the data_file_directories property, but the data is distributed over the folders internally by Cassandra. You can not take decisions on which keyspace or column family goes to each directory.</p>

<p>So the symbolic links is the way to go in my opinion.</p>
",['data_file_directories']
23185331,23187032,2014-04-20 17:15:09,CQL: Bad Request: Missing CLUSTERING ORDER for column,"<p>What is the problem with this CQL query</p>

<pre><code>cqlsh&gt; create table citybizz.notifications(
   ...      userId varchar,
   ...      notifId UUID,
   ...      notification varchar,
   ...      time bigint,read boolean,
   ...      primary key (userId, notifId,time)
   ... ) with clustering order by (time desc);
</code></pre>

<p>It throws <code>Bad Request: Missing CLUSTERING ORDER for column notifid</code>. I am using cassandra 1.2.2</p>
",<cassandra><cql3>,"<p>You need to specify the order for notifId too:</p>

<pre><code>create table citybizz.notifications(
    userId varchar,
    notifId UUID,
    notification varchar,
    time bigint,read boolean,
    primary key (userId, notifId,time)
) with clustering order by (notifId asc, time desc);
</code></pre>

<p>Cassandra doesn't assume default ordering (asc) for the other clustering keys so you need to specify it.</p>
",['table']
23294298,23296128,2014-04-25 13:16:39,Replication in cassandra,"<p>How does replication work in Cassandra? If I have 3 racks and 3 RF with <code>NetworkTopologyStratagy</code> then will the data be replicated to all the 3 racks? </p>

<p>How exactly will data be replicated across the cluster? I ask because we are designing our cluster to cater for the worst case scenario that 2 of 3 racks go down, and we don't want to lose data.</p>

<p>We have only one datacenter with 3 racks.</p>

<p>If I use:</p>

<pre><code>CREATE KEYSPACE ""myKeyspaceName""
WITH REPLICATION = {'class' : 'NetworkTopologyStrategy', 'DC1' : 3  }
</code></pre>

<p>Will this replicate to all three racks?</p>
",<cassandra><cassandra-2.0>,"<p>In the Cassandra 1.0 docs, they have an article that explains this pretty well: <a href=""http://www.datastax.com/docs/1.0/cluster_architecture/replication"" rel=""noreferrer"">About Replication in Cassandra</a>.</p>

<p>I'll assume that you have two (logical?) datacenters.  Let's say that you have two racks in one DC and the last rack in another, and on each RACK you have 2 nodes.  You'll have these defined in your topology file to look something like this:</p>

<pre><code>server1IP=DC1:RACK1
server2IP=DC1:RACK1
server3IP=DC2:RACK1
server4IP=DC2:RACK1
server5IP=DC2:RACK2
server6IP=DC2:RACK2
</code></pre>

<p>If you want to have 3 copies of the data out there (one for each logical rack) then you'll define your keyspace to use the NetworkTopologyStrategy, with replications settings for each DC, like this:</p>

<pre><code>CREATE KEYSPACE ""myKeyspaceName""
WITH REPLICATION = {'class' : 'NetworkTopologyStrategy', 'DC1' : 1, 'DC2' : 2};
</code></pre>

<p>The PropertyFile snitch is also ""rack aware,"" so when a write occurs it will make sure that one copy of the data is on a node in <code>RACK1</code> in <code>DC1</code>, and one copy is on each <code>RACK</code> in <code>DC2</code>.  Based on what you are saying, it might make sense to have three logical datacenters, each with one rack.</p>

<p>You should also have a look at <a href=""http://www.datastax.com/docs/1.0/cluster_architecture/cluster_planning"" rel=""noreferrer"">this doc in the ""Choosing Keyspace Replication Options"" section</a> that further explains how to configure replication.</p>

<p>EDIT:</p>

<blockquote>
  <p>If I use the CREATE KEYSPACE ""myKeyspaceName"" WITH REPLICATION =
  {'class' : 'NetworkTopologyStrategy', replicationfactor: 3 }. Will
  this replicate to all the three racks?</p>
</blockquote>

<p>I'm not sure.  But every example I'm finding about defining a keyspace's replication strategy using NetworkTopologyStrategy with only one DC, specifically names the DC instead of stating ""replicationfactor.""  Even the doc I linked states:</p>

<blockquote>
  <p>NetworkTopologyStrategy takes as options the number of replicas you
  want per data center. Even for single data center (or single node)
  clusters, you can use this replica placement strategy and just define
  the number of replicas for one data center.</p>
</blockquote>

<p>So assuming that you name your DC <code>DC1</code>, it would look like this:</p>

<pre><code>CREATE KEYSPACE ""myKeyspaceName""
WITH REPLICATION = {'class' : 'NetworkTopologyStrategy', 'DC1' : 3}
</code></pre>

<p>If you did that, and had your three racks defined underneath that DC, this will replicate one copy to all three racks.</p>
",['rack']
23306654,23306841,2014-04-26 04:30:08,"NoSQL ( Cassandara ) coming from MySQL, Duplicate data db design","<p>From my understanding, in noSQL, data should be duplicated.  So, for instance, if you have a users table and a posts table, you'd store the user's info in the users table—as usual—but then you'd store the relevant user data in the posts table.</p>

<p><strong>Question 1:</strong> is my understanding correct?</p>

<p><strong>Question 2:</strong> if so, that means if I change a user detail I'll have make an update to all affected posts entries?</p>
",<database-design><cassandra><nosql>,"<p>From the Cassandra perspective, it mostly depends on the queries that you need to support efficiently. When you query posts, do you also need user data? If so, it will generally be more efficient to include the required data where the post is stored.</p>

<p>So for question 1, yes in many circumstances, what you describe is the common practice, but it depends on the application's needs.</p>

<p>For question 2, this is also an application concern. If you foresee user data changing regularly, then your application should perhaps performa a lookup to the users table when displaying a post. However, if that introduces too many reads to display the required posts in a timely fashion, then including the user data in the posts data means that changes to the user data will need to be changed in two places. But it is important to ask if the historical data needs to be changed. For example, if you change your username on Twitter, it doesn't go back and update all prior references to you to your new username. This is an application choice. What is the user data that you anticipate might change? In the case of a username change where you do want the new value to be reflected in all previous posts, how timely does that change need to be? Should it be reflected immediately, or can you wait for a batch process to handle it?</p>

<p>The important thing to understand, is how to perform efficient queries and to understand the referential integrity tradeoff that is made when we denormalize to achieve high performance applications. Always consider the application query patterns when designing the data model.</p>
",['table']
23395171,23908002,2014-04-30 18:10:39,Inserting to cassandra table with a composite primary key from hadoop reduce,"<p>I'm using Apache Hadoop, MapReduce and Cassandra to run a MapReduce job that reads in from a Cassandra table, and outputs to another Cassandra table.</p>

<p>I have a few jobs that output to a table with a single primary key. For example, this table for counting the number of each type of word has a single key.</p>

<pre><code>    CREATE TABLE word_count(
        word text,
        count int,
        PRIMARY KEY(text)
    ) WITH COMPACT STORAGE;
</code></pre>

<p>The associated reduce class looks a bit like this:</p>

<pre><code>public static class ReducerToCassandra 
    extends Reducer&lt;Text, IntWritable, ByteBuffer, List&lt;Mutation&gt;&gt;
{
    public void reduce(Text word, Iterable&lt;IntWritable&gt; values, Context context) 
        throws IOException, InterruptedException
    {
        int sum = 0;
        for (IntWritable val : values){
            sum += val.get();
        }

        org.apache.cassandra.thrift.Column c 
                = new org.apache.cassandra.thrift.Column();
        c.setName(ByteBufferUtil.bytes(""count"");
        c.setValue(ByteBufferUtil.bytes(sum));
        c.setTimestamp(System.currentTimeMillis());

        Mutation mutation = new Mutation();
        mutation.setColumn_or_supercolumn(new ColumnOrSuperColumn());
        mutation.column_or_supercolumn.setColumn(c);

        ByteBuffer keyByteBuffer = ByteBufferUtil.bytes(word.toString());
        context.write(keyByteBuffer, Collections.singletonList(mutation));
    }
}
</code></pre>

<p>If I want to add an extra column, then I just need to add another mutation to the <code>List&lt;Mutation&gt;</code> already being output by <code>reduce</code> but I can't work out how to output to a table that has the new column in a composite primary key. For example, this table does the same as the one above, but also indexes words along with the hour of their publication.</p>

<pre><code>    CREATE TABLE word_count(
        word text,
        publication_hour bigint,
        count int,
        PRIMARY KEY(word, publication_hour)
    ) WITH COMPACT STORAGE;
</code></pre>

<p>I've tried a few different approaches, like trying to output a custom <code>WritableComparable</code> (that holds both a word and a hour) and updating the <code>class</code> and <code>method</code> signatures and <code>job</code> configuration accordingly, but that makes <code>reduce</code> throw a <code>ClassCastException</code> when it tries to cast the custom <code>WritableComparable</code> to <code>ByteBuffer</code>.</p>

<p>I've tried building the appropriate column name with the <code>Builder</code>.</p>

<pre><code>public static class ReducerToCassandra 
    //              MappedKey     MappedValue  ReducedKey  ReducedValues
    extends Reducer&lt;WordHourPair, IntWritable, ByteBuffer, List&lt;Mutation&gt;&gt;
{
    //                 MappedKey                  Values with the key wordHourPair
    public void reduce(WordHourPair wordHourPair, Iterable&lt;IntWritable&gt; values, 
    Context context) 
        throws IOException, InterruptedException
    {
        int sum = 0;
        for (IntWritable val : values){
        sum += val.get();
        }
        long hour = wordHourPair.getHourLong();

        org.apache.cassandra.thrift.Column c 
            = new org.apache.cassandra.thrift.Column();
        c.setName(ByteBufferUtil.bytes(""count"");
        c.setValue(ByteBufferUtil.bytes(sum));
        c.setTimestamp(System.currentTimeMillis());

        Mutation mutation = new Mutation();
        mutation.setColumn_or_supercolumn(new ColumnOrSuperColumn());
        mutation.column_or_supercolumn.setColumn(c);

        //New Code
        List&lt;AbstractType&lt;?&gt;&gt; keyTypes = new ArrayList&lt;AbstractType&lt;?&gt;&gt;(); 
        keyTypes.add(UTF8Type.instance);
        keyTypes.add(LongType.instance);
        CompositeType compositeKey = CompositeType.getInstance(keyTypes);

        Builder builder = new Builder(compositeKey);
        builder.add(ByteBufferUtil.bytes(word.toString());
        builder.add(ByteBufferUtil.bytes(hour));

        ByteBuffer keyByteBuffer = builder.build();
        context.write(keyByteBuffer, Collections.singletonList(mutation));
    }
}
</code></pre>

<p>But that throws an <code>IOException</code></p>

<pre><code>java.io.IOException: InvalidRequestException(why:String didn't validate.)
    at org.apache.cassandra.hadoop.ColumnFamilyRecordWriter$RangeClient.run(ColumnFamilyRecordWriter.java:204)
Caused by: InvalidRequestException(why:String didn't validate.)
    at org.apache.cassandra.thrift.Cassandra$batch_mutate_result$batch_mutate_resultStandardScheme.read(Cassandra.java:28232)
    at org.apache.cassandra.thrift.Cassandra$batch_mutate_result$batch_mutate_resultStandardScheme.read(Cassandra.java:28218)
    at org.apache.cassandra.thrift.Cassandra$batch_mutate_result.read(Cassandra.java:28152)
    at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
    at org.apache.cassandra.thrift.Cassandra$Client.recv_batch_mutate(Cassandra.java:1069)
    at org.apache.cassandra.thrift.Cassandra$Client.batch_mutate(Cassandra.java:1055)
    at org.apache.cassandra.hadoop.ColumnFamilyRecordWriter$RangeClient.run(ColumnFamilyRecordWriter.java:196)
</code></pre>

<p>This question: <a href=""https://stackoverflow.com/questions/18566882/cassandra-cql3-composite-key-not-written-by-hadoop-reducer"">Cassandra CQL3 composite key not written by Hadoop reducer</a> seems to exhibit the kind of code I'm looking for, but it calls <code>context.write</code> with parameters of type <code>HashMap, ByteBuffer</code> and I'm not sure how I'd make <code>context.write</code> accept those params.</p>

<p>How can I get the data I want (word-hour keys, int values) into my table?</p>
",<java><hadoop><mapreduce><cassandra><composite-key>,"<p>The answer to this was to use Cassandra's CQL interface, rather than the Thrift API.</p>

<p>Now I can write to a table with a composite key by declaring the output key/value classes of my reduce class as ""Map, List"", then create a Map for the composite key, where the Key (of type string) is a column name, and the Value (of type ByteBuffer) is the columns value converted to a ByteBuffer with ByteBufferUtil.</p>

<p>For example, to write to a table defined as such:</p>

<pre><code>CREATE TABLE foo (
    customer_id uuid,
    time timestamp,
    my_value int,
    PRIMARY KEY (customer_id, time)
)
</code></pre>

<p>I can write:</p>

<pre><code>String customerID = ""the customer's id"";
long time = DateTime.now().getMillis();
int myValue = 1;

Map&lt;String, ByteBuffer&gt; key = new Map&lt;String, ByteBuffer&gt;();
key.put(""customer_id"",ByteBufferUtil.bytes(customerID));
key.put(""time"",ByteBufferUtil.bytes(time));

List&lt;ByteBuffer&gt; values = Collections.singletonList(ByteBufferUtil.bytes(myValue));

context.write(key, values);
</code></pre>
",['table']
23456079,23486339,2014-05-04 12:11:52,Cassandra/Redis: Way to create feed without Cassandra 'IN' secondary index?,"<p>I'm having a bit of an issue with my application functionality integrating with Cassandra. I'm trying to create a content feed for my <code>users</code>. Users can create posts which, in turn, have the field <code>user_id</code>. I'm using Redis for the entire social graph and using Cassandra columns solely for objects. In Redis, user 1 has a set named <code>user:1:followers</code> with all of his/her follower ids. These follower ids correspond with the Cassandra ids in the users table and user_ids in the posts table.</p>

<p>My goal was originally to simply plug all of the <code>user_id</code>s from this Redis set into a query that would use <code>FROM posts WHERE user_id IN (user_ids here)</code> and grab all of the posts from the secondary index <code>user_id</code>. The issue is that Cassandra purposely does not <a href=""https://issues.apache.org/jira/browse/CASSANDRA-6318"" rel=""nofollow"">support</a> the <code>IN</code> operator in secondary indexes because that index would force Cassandra to search ALL of its nodes for that value. I'm left with only two options I can see: Either create a Redis list of <code>user:1:follow_feed</code> for the post IDs then search Cassandra's primary index for those posts in a single query, or keep it the way I have it now and run an individual query for every <code>user_id</code> in the <code>user:1:follower</code> set.</p>

<p>I'm really leaning against the first option because I already have tons and tons of graph data in Redis, and this option would add a new list for every user. The second way is far worse. I would put a massive read load on Cassandra and it would take a long time to run individual queries for a set of ids. I'm kind of stuck between a rock and a hard place, as far as I see it. Is there any way to query the secondary indexes with multiple values? If not, is there a more efficient way to load these content feeds (RAM and speed wise) compared to the options of more Redis lists or multiple Cassandra queries? Thanks in advance.</p>
",<cassandra><redis><nosql>,"<p>Without knowing the schema of the posts table (and preferably the others, as well), it's really hard to make any useful suggestions. </p>

<p>It's unclear to me why you need to have user_id be a secondary index, as opposed to your primary key. </p>

<p>In general it's quite useful to key content like posts off of the user that created it, since it allows you to do things like retrieve all posts (optionally over a given range, assuming they are chronologically sorted) very efficiently. </p>

<p>With Cassandra, if you find that a table can effectively answer some of the queries that you want to perform but not others, you are usually best of denormalizing that table and creating another table with a different structure in order to keep your queries to a single CQL partition and node.</p>

<pre><code>CREATE TABLE posts (
  user_id int,
  post_id int,
  post_text text,
  PRIMARY KEY (user_id, post_id)
  ) WITH CLUSTERING ORDER BY (post_id DESC)
</code></pre>

<p>This table can answer queries such as:</p>

<pre><code> select * from posts where user_id = 1234;

 select * from posts where user_id = 1 and post_id = 53;

 select * from posts where user_id = 1 and post_id &gt; 5321 and post_id &lt; 5400;
</code></pre>

<p>The reverse clustering on post_id is to make retrieving the most recent posts the most efficient by placing them at the beginning of the partition physically within the sstable.</p>

<p>In that example, user_id being a partition column, means ""all cql rows with this user_id will be hashed to the same partition, and hence the same physical nodes, and eventually, the same sstables. That's why it's possible to </p>

<ol>
<li>retrieve all posts with that user_id, as they are store contiguously</li>
<li>retrieve a slice of them by doing a ranged query on post_id</li>
<li>retrieve a single post by supplying both the partition column(user_id) and the clustering column (post_id)</li>
</ol>

<p>In effect, this become a hashmap of a hashmap lookup. The one major caveat, though, is that when using partition and clustering columns, you <strong>always</strong> need to supply all columns from left to right in your query, without skipping any. So in this case, that means you can't retrieve an individual post without knowing the user_id that the post_id belongs to. That is addressable in user-code(by storing a reverse mapping and doing the lookup when necessary, or by encoding the user_id into the post_id that is passed around your application), but is definitely something to take into consideration.</p>
",['table']
23482841,23486061,2014-05-05 22:17:02,Replace a table in Apache Cassandra,"<p>I have a table that stores several connections' metadata.
This table have to be rebuilt after some specific events.</p>

<p>Is there a way to build a temporary table and replace the original one with the newer?
After a Google search I found out that it is not possible to rename a table.</p>

<p>Does anyone have a suggestion?</p>
",<cassandra><cql><database><nosql>,"<p>If by ""rebuilt"", you mean you want to start from scratch with the same schema, but no data, then you can call TRUNCATE on the table and then repopulate it with new data. If you mean that you need to swap in new contents all at once, then your best bet is probably to version your table, so that you keep creating new tables (""foo_v1, foo_v2"" etc). You then need to parameterize your client code to know what version they need to talk to at some point. You can track the version in another Cassandra table.</p>
",['table']
23513556,23623289,2014-05-07 09:12:44,Cassandra: rpc_timeout when inserting in a new table in a cluster,"<p>I am using Cassandra 2.0.3 and I drop and recreate a simple table via cqlsh by loading a file (source command). In the same file, I insert some rows in the newly created table.</p>

<p>About once every 3-4 tries, I get rpc_timeout on some of the inserts.
When this is the case, I always have this exception on one node of the cluster:</p>

<pre><code> WARN [Thread-63] 2014-05-07 10:52:39,658 IncomingTcpConnection.java (line 83) UnknownColumnFamilyException reading from socket; closing
org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=15a8520e-bb08-3a79-82a0-f735287315bf
    at org.apache.cassandra.db.ColumnFamilySerializer.deserializeCfId(ColumnFamilySerializer.java:178)
    at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:103)
    at org.apache.cassandra.db.RowMutation$RowMutationSerializer.deserializeOneCf(RowMutation.java:304)
    at org.apache.cassandra.db.RowMutation$RowMutationSerializer.deserialize(RowMutation.java:284)
    at org.apache.cassandra.db.RowMutation$RowMutationSerializer.deserialize(RowMutation.java:312)
    at org.apache.cassandra.db.RowMutation$RowMutationSerializer.deserialize(RowMutation.java:254)
    at org.apache.cassandra.net.MessageIn.read(MessageIn.java:99)
    at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:153)
    at org.apache.cassandra.net.IncomingTcpConnection.handleModernVersion(IncomingTcpConnection.java:130)
    at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:74)
</code></pre>

<p>Even if I do the INSERT direcly in cqlsh, it fails also with rpc_timeout. Usually after about one minute, the insert is successful.</p>

<p>My nodes are time synchronized (I use 3 VMs on my PC) and the LAN is of course super fast on all VMs are running locally.</p>

<p>I created the cluster by adding 2 nodes to an existing Cassandra running on a single node.
My keyspace is not using replication:</p>

<pre><code>CREATE KEYSPACE eras
  WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };
</code></pre>

<p>Here is the content of the file I use to reproduce the problem:</p>

<pre><code>DROP TABLE IF EXISTS erasconfig;

CREATE TABLE erasconfig (
  name text,
  category text,
  description text,
  ismodifiablebyuser int,
  value text,
  format text,
  PRIMARY KEY (name, category)
);

INSERT INTO ErasConfig (isModifiableByUser, format, name, value, category, description) VALUES (1, '', 'RECORD_IN_BASE', 'garbage', 'Path', 'Absolute path used for RECORD INPUT files');
</code></pre>

<p>This INSERT goes into the 3rd node of the cluster which is the one failing sometimes during table creation with the exception above.</p>
",<cassandra><cql3><cqlsh>,"<p>The issue is that schema replication is asynchronous to the create finishing.  So in a multinode cluster you need to verify that the schema changes have propagated to all the nodes before you try to use them.  The <code>nodetool describecluster</code> can be used to check if the schemas agree.  From a client you can check the system.peers table to verify that all the schema versions have updated.</p>
",['table']
23589266,23596529,2014-05-11 05:28:00,Cassandra terrible performance on select,"<p>I have two nodes Cassandra cluster. In order to test Cassandra i built a File table (Fid Integer,Sid Integer), Which Fid is key. I built index on Sid, Insert rate is about 10,000 in 1 second. But when i select from table the performance is terrible, and for low limit like 1000 it generate error, bellow is my sample code,</p>

<pre><code>from cassandra.cluster import Cluster

cluster = Cluster(['127.0.0.1'])
session = cluster.connect('myk')
rows = session.execute('SELECT * FROM File WHERE sid = 1 limit 1000')
for user_row in rows:
    print user_row
</code></pre>

<p>Error message is:</p>

<pre><code>Traceback (most recent call last):
  File ""Test.py"", line 5, in &lt;module&gt;
    rows = session.execute('SELECT * FROM File WHERE sid = 1 limit 1000')
  File ""build\bdist.win32\egg\cassandra\cluster.py"", line 1065, in execute
  File ""build\bdist.win32\egg\cassandra\cluster.py"", line 2427, in result
cassandra.OperationTimedOut: errors={}, last_host=172.16.47.130
</code></pre>

<p>by changing </p>

<pre><code>rows = session.execute('SELECT * FROM File WHERE sid = 1 limit 1000')
</code></pre>

<p>to </p>

<pre><code>rows = session.execute('SELECT * FROM File WHERE sid = 1 limit 1000',timeout=20.0)
</code></pre>

<p>Error has gone, but why performance (for fetching 1000 rows from a 800,000 records table) is very slow? Any hints?</p>
",<cassandra>,"<blockquote>
  <p>I built index on Sid</p>
</blockquote>

<p>The key to the lack of performance here is your use of secondary indexes in place of what should be either a clustering key or part of a composite key. Secondary indexes in Cassandra are for assisting in full table scans (an expensive operation) for batch analytics or for early development testing. They are <em>not</em> analogous to relational indexes. </p>

<p>So if you want to execute queries like</p>

<pre><code>rows = session.execute('SELECT * FROM File WHERE sid = 1 limit 1000')
</code></pre>

<p>then you need a table whose primary key is sid. If you would like to query based on FID as well then you need two complimentary tables, one keyed on FID and one on SID. At insert time you would place the information in both tables.</p>
",['table']
23594570,23596717,2014-05-11 15:37:38,Is CQL limiting Cassandra from being 'schema free'?,"<p>As I've understood it, one feature of Cassandra is that it's supposedly 'schema free', that is (coming from a RDBS perspective) a table could contain several rows with varying columns.</p>

<p>In fact I never found the word 'table' used in the official documentation, instead Cassandra was supposed to use SuperColumns and ColumnFamilies:  <a href=""http://en.wikipedia.org/wiki/Column_family"" rel=""nofollow"">http://en.wikipedia.org/wiki/Column_family</a></p>

<p>But when reading up on <strong>CQL</strong>, the replacement language for SQL, creating these kinds of structures seems imposible, instead it closely mirrors a relation database structure that requires the creation and updating of tables and prohibiting on the fly attaching a new column to an row if it's not already defined in the table.</p>

<p>This is not at all what I need for my current project, as I need to store a lot of different information for each entity in the database (it's an experiment combining logic programming with probabilistic information and machine learning) </p>

<p><strong>I only just begun reading up CQL, Am I missing something or is Cassandra perhaps not the right choice for my project?</strong> </p>
",<cassandra><database-schema><nosql>,"<p>For documentation you will most likely find more up to date information at the Datastax Website as there is a paid documentation team constantly working  on Cassandra Docs.
<a href=""http://www.datastax.com/documentation/cassandra/2.0/cassandra/gettingStartedCassandraIntro.html"" rel=""nofollow"">Datastax C* Docs</a></p>

<p>The simplest solution for having arbitrary information in a row would be to use a MAP, LIST, or SET type in your logical row. </p>

<p><a href=""http://www.datastax.com/documentation/cql/3.1/cql/cql_reference/cql_data_types_c.html"" rel=""nofollow"">CQL Datatypes Reference</a></p>

<p>But if you want an arbitrary layout you can create a table in CQL like</p>

<pre><code>CREATE TABLE entityvalue ( key blob, column_name blob, value blob , PRIMARY KEY (key, column_name)) 
</code></pre>

<p>and as a final option you could just use any of the old thrift drivers and have the old style if you like but there has been a vote to stop thrift development in Cassandra.</p>
",['table']
23610771,23622634,2014-05-12 13:45:28,Paging large resultsets in Cassandra with CQL3 with varchar keys,"<p>I’m working on updating an old thrift-based code to CQL3. </p>

<p>One part of the code is walking through the entire dataset of a table consisting of  20M+ rows. This part was initially crashing the program due to memory usage, so I created a RowIterator class which iterated through the column family using TokenRanges (and Hector). </p>

<p>When trying to rewrite this using CQL3, I’m having trouble paging through the data. I found some info over at <a href=""http://www.datastax.com/documentation/cql/3.0/cql/cql_using/paging_c.html"" rel=""nofollow"">http://www.datastax.com/documentation/cql/3.0/cql/cql_using/paging_c.html</a>, but when trying this code for the first ""page""</p>

<pre><code>resultSet = session.execute(""select * from "" + TABLE + "" where token(key) &lt;= token("" + offset + "")"");
</code></pre>

<p>I get the error</p>

<blockquote>
  <p>com.datastax.driver.core.exceptions.InvalidTypeException: Invalid type
  for value 0 of CQL type varchar, expecting class java.lang.String but
  class java.lang.Integer provided</p>
</blockquote>

<p>Granted, the example at the link uses numerical keys. Is there a way to do this with varchar (UTF8Type) keys?</p>

<p>It seems that there is now a built-in functionality for this (<a href=""https://issues.apache.org/jira/browse/CASSANDRA-4415"" rel=""nofollow"">https://issues.apache.org/jira/browse/CASSANDRA-4415</a>), but I can’t find examples that get me going. Besides, I have to solve it for Cassandra 1.2.9 for now.</p>
",<java><cassandra><cql3>,"<p>So the easy answer is to upgrade to Cassandra 2.0.X and use the new built in paging functionality.  But to get it done on Cassandra 1.2 you are on the right path.  Your syntax should be working, if you run the query you are trying in cqlsh do you get the same error?  When paging like this it is best to use "">"" like in the example, that might be the issue.  You want to start with <code>select * from table limit 100</code> then go to <code>select * from table where token(key)&gt;token('last key') limit 100</code></p>

<p>Also I would try it with a prepared statement.  The string manipulations may be doing something funny to the offset.</p>
",['table']
23685240,23704345,2014-05-15 17:45:01,Hadoop timing out trying to write to Cassandra in AWS multi-region configuration,"<p>I am running a multi-DC Cassandra (open-source, not DSE) cluster in AWS, where one DC (us-west-2) is set up for analytics and the other (us-east) is the transactional store.  I'm using NetworkTopologyStrategy with the EC2 snitch, and a consistency level of LOCAL_ONE in my Hadoop config.  Hadoop <strong>can read from Cassandra without issue</strong>, but <strong>attempting to write produces a timeout exception</strong>.</p>

<p>Running <code>nodetool status</code> shows the DCs are properly configured:</p>

<pre><code>Datacenter: us-west-2
=====================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address         Load       Owns   Host ID                               Token                                    Rack
UN  x.x.x.x       1.01 GB     9.9%   9e7f4393-7ac9-4559-b3ff-de48be50016f  -9127921345534057723                     2a
UN  x.x.x.x       1001.16 MB  11.4%  d0760383-c3dd-474c-9261-239b71dba3f1  -9221279003374097975                     2b
UN  x.x.x.x       1.05 GB     11.7%  3f09fbf5-0d85-4283-9009-0ec0e29223c0  -9140104347498952504                     2c
Datacenter: us-east
===================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address         Load       Owns   Host ID                               Token                                    Rack
UN  x.x.x.x       1.1 GB     11.3%  5bbd2de4-e1d2-4a17-9f40-034f60b35954  -9061054426204373981                     1b
UN  x.x.x.x       1.15 GB    11.5%  e34c590e-6176-45b2-a8f9-18b4a9a80032  -9216519687724118609                     1c
UN  x.x.x.x       1.18 GB    10.9%  fa0b0a1a-f156-40fc-a267-970d1eb9cddb  -9207673937991303291                     1a
UN  x.x.x.x       1.46 GB    10.7%  b18ae406-c9ec-42b7-a365-b0c6e2fe582f  -9206671929961171506                     1a
UN  x.x.x.x       1.13 GB    11.4%  1ac9c1c5-55ad-4048-b1ba-3b9768933ecc  -9146100851344467112                     1c
UN  x.x.x.x       1.53 GB    11.2%  dad665bb-68d9-4811-b421-f33333261867  -9178920986366339267                     1b
</code></pre>

<p>Stack trace using ColumnFamilyOutputFormat:</p>

<pre><code>java.io.IOException: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection timed out
    at org.apache.cassandra.hadoop.ColumnFamilyRecordWriter$RangeClient.run(ColumnFamilyRecordWriter.java:224)
Caused by: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection timed out
    at org.apache.thrift.transport.TSocket.open(TSocket.java:185)
    at org.apache.thrift.transport.TFramedTransport.open(TFramedTransport.java:81)
    at org.apache.cassandra.thrift.TFramedTransportFactory.openTransport(TFramedTransportFactory.java:41)
    at org.apache.cassandra.hadoop.AbstractColumnFamilyOutputFormat.createAuthenticatedClient(AbstractColumnFamilyOutputFormat.java:123)
    at org.apache.cassandra.hadoop.ColumnFamilyRecordWriter$RangeClient.run(ColumnFamilyRecordWriter.java:215)
Caused by: java.net.ConnectException: Connection timed out
    at java.net.PlainSocketImpl.socketConnect(Native Method)
    at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
    at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
    at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
    at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
    at java.net.Socket.connect(Socket.java:579)
    at org.apache.thrift.transport.TSocket.open(TSocket.java:180)
    ... 4 more
</code></pre>

<p>... and using CqlOutputFormat:</p>

<pre><code>java.io.IOException: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection timed out
    at org.apache.cassandra.hadoop.cql3.CqlRecordWriter$RangeClient.run(CqlRecordWriter.java:271)
Caused by: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection timed out
    at org.apache.thrift.transport.TSocket.open(TSocket.java:185)
    at org.apache.thrift.transport.TFramedTransport.open(TFramedTransport.java:81)
    at org.apache.cassandra.thrift.TFramedTransportFactory.openTransport(TFramedTransportFactory.java:41)
    at org.apache.cassandra.hadoop.AbstractColumnFamilyOutputFormat.createAuthenticatedClient(AbstractColumnFamilyOutputFormat.java:123)
    at org.apache.cassandra.hadoop.cql3.CqlRecordWriter$RangeClient.run(CqlRecordWriter.java:262)
Caused by: java.net.ConnectException: Connection timed out
    at java.net.PlainSocketImpl.socketConnect(Native Method)
    at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
    at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
    at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
    at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
    at java.net.Socket.connect(Socket.java:579)
    at org.apache.thrift.transport.TSocket.open(TSocket.java:180)
    ... 4 more
</code></pre>

<p>Both traces ultimately point to <code>AbstractColumnFamilyOutputFormat.createAuthenticatedClient(host, port, conf)</code>.</p>

<p>I then opened that source and added some detail to the exception so it would output the host name it's connecting to, which resulted in this trace:</p>

<pre><code>java.io.IOException: java.lang.Exception: Unable to connect to host [hostname]
    at org.apache.cassandra.hadoop.cql3.CqlRecordWriter$RangeClient.run(CqlRecordWriter.java:271)
Caused by: java.lang.Exception: Unable to connect to host [hostname]
    at org.apache.cassandra.hadoop.AbstractColumnFamilyOutputFormat.createAuthenticatedClient(AbstractColumnFamilyOutputFormat.java:139)
    at org.apache.cassandra.hadoop.cql3.CqlRecordWriter$RangeClient.run(CqlRecordWriter.java:262)
Caused by: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection timed out
    at org.apache.thrift.transport.TSocket.open(TSocket.java:185)
    at org.apache.thrift.transport.TFramedTransport.open(TFramedTransport.java:81)
    at org.apache.cassandra.thrift.TFramedTransportFactory.openTransport(TFramedTransportFactory.java:41)
    at org.apache.cassandra.hadoop.AbstractColumnFamilyOutputFormat.createAuthenticatedClient(AbstractColumnFamilyOutputFormat.java:124)
    ... 1 more
Caused by: java.net.ConnectException: Connection timed out
    at java.net.PlainSocketImpl.socketConnect(Native Method)
    at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
    at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
    at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
    at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
    at java.net.Socket.connect(Socket.java:579)
    at org.apache.thrift.transport.TSocket.open(TSocket.java:180)
    ... 4 more
</code></pre>

<p><strong>The problem is [hostname] is a machine that's not in the analytics cluster (it's in us-east)</strong>.  Why doesn't it know this automagically, especially when reads work properly?  It seems like it's trying all the nodes in the ring regardless of DC.</p>

<p>For the record, writes fail using <code>CqlOutputFormat</code>, <code>ColumnFamilyOutputFormat</code>, and through Pig using <code>CqlStorage</code> and <code>CassandraStorage</code>.</p>
",<hadoop><amazon-web-services><amazon-ec2><cassandra>,"<p>This issue came down to two things:</p>

<ol>
<li><p>For multi-region EC2 setups, Cassandra requires setting broadcast_address to the public IP and the listen_address to the internal IP.  In most cases you'll want rpc_address to be the internal IP, but this potentially breaks Cassandra's Hadoop client, which is determining endpoints to talk to based on broadcast_address.</p></li>
<li><p>Cassandra's Hadoop client (RingCache specifically) doesn't respect data center on node discovery, and tries to discover all nodes in the ring--including non-local ones.  It respects the consistency level on the actual write, but in our case it never got there due to #1.</p></li>
</ol>

<p>I filed a ticket and submitted a patch to address these issues:</p>

<p><a href=""https://issues.apache.org/jira/browse/CASSANDRA-7252"" rel=""nofollow"">https://issues.apache.org/jira/browse/CASSANDRA-7252</a></p>
","['broadcast_address', 'rpc_address', 'listen_address']"
23783070,23783474,2014-05-21 12:24:01,Need to process all records in cassandra,"<p>I've designed a software to store news from hundreds of news agencies in the world. I also have created a URL to access them from my application like this:<br>
<a href=""http://www.myweb.com/news/health/1234567"" rel=""nofollow"">http://www.myweb.com/news/health/1234567</a><br>
Now I want to change this format to something else but I need to create a redirect module which handles old URL requests from users that are coming from Google, so I need to read millions of records and convert them.<br>
As far as I know it's not possible to read all records of a table in Cassandra. what should I do for this case?</p>
",<cassandra>,"<p>I'm guessing ALTER TABLE will not work for you in this case?</p>

<p>The DataStax Java driver 2.0 provides automatic result set pagination.
I'd probably read all rows a batch at a time and write out the changes to a new table and then drop the old table. Probably a job for the weekend or evening.</p>

<p>""Allows you to iterate indefinitely over the ResultSet, having the rows fetched block by block until the rows available on the client side are exhausted. This makes it easier to manipulate large result sets while also shielding the client application from instatianting an accidentally large ResultSet object.""</p>

<p><a href=""http://www.datastax.com/documentation/developer/java-driver/2.0/java-driver/whatsNew2.html"" rel=""nofollow"">http://www.datastax.com/documentation/developer/java-driver/2.0/java-driver/whatsNew2.html</a></p>

<p>Example CQL here:</p>

<p><a href=""http://www.datastax.com/dev/blog/client-side-improvements-in-cassandra-2-0"" rel=""nofollow"">http://www.datastax.com/dev/blog/client-side-improvements-in-cassandra-2-0</a></p>

<p>For the Astyanax driver:</p>

<p><a href=""https://github.com/Netflix/astyanax/wiki/AllRowsReader-All-rows-query"" rel=""nofollow"">https://github.com/Netflix/astyanax/wiki/AllRowsReader-All-rows-query</a></p>

<p>Cheers,</p>
",['table']
23785781,23788435,2014-05-21 14:12:57,Data Versioning in Cassandra with CQL3,"<p>I am quite a n00b in Cassandra (I'm mainly from an RDBMS background with some NoSQL here and there, like Google's BigTable and MongoDB) and I'm struggling with the data modelling for the use cases I'm trying to satisfy.  I looked at <a href=""https://stackoverflow.com/questions/18575560/versioning-in-cassandra"">this</a> and <a href=""https://stackoverflow.com/questions/4183945/ways-to-implement-data-versioning-in-cassandra"">this</a> and even <a href=""https://groups.google.com/forum/#!topic/nosql-databases/535LyWOJKsc"" rel=""nofollow noreferrer"">this</a> but they're not exactly what I needed. </p>

<p>I have this basic table:</p>

<pre><code>CREATE TABLE documents (
    itemid_version text,       
    xml_payload text,
    insert_time timestamp,
    PRIMARY KEY (itemid_version)
); 
</code></pre>

<p><code>itemid</code> is actually a UUID (and unique for all documents), and <code>version</code> is an int (version 0 is the ""first"" version).  <code>xml_payload</code> is the full XML doc, and can get quite big.  Yes, I'm essentially creating a versioned document store.</p>

<p>As you can see, I concatenated the two to create a primary key and I'll get to why I did this later as I explain the requirements and/or use cases:</p>

<ol>
<li>user needs to get the single (1) doc he wants, he knows the item id and version (not necessarily the latest)</li>
<li>user needs to get the single (1) doc he wants, he knows the item id but does not know the latest version</li>
<li>user needs the version history of a single (1) doc.</li>
<li>user needs to get the list (1 or more) of docs he wants, he knows the item id AND version (not necessarily the latest)</li>
</ol>

<p>I will be writing the client code that will perform the use cases, please excuse the syntax as I'm trying to be language-agnostic</p>

<p>first one's straightforward:</p>

<pre><code>$itemid_version = concat($itemid, $version)
$doc = csql(""select * from documents where itemid_version = {0};"" 
    -f $itemid_version)
</code></pre>

<p>now to satisfy the 2nd and 3rd use cases, I am adding the following table:</p>

<pre><code>CREATE TABLE document_versions (
    itemid uuid,
    version int,
    PRIMARY KEY (itemid, version)
) WITH clustering order by (version DESC);
</code></pre>

<p>new records will be added as new docs and new versions of existing docs are created</p>

<p>now we have this (use case #2):</p>

<pre><code>$latest_itemid, $latest_version = csql(""select itemid, 
    version from document_versions where item_id = {0} 
    order by version DESC limit 1;"" -f $itemid)
$itemid_version = concat($latest_itemid, $latest_version)
$doc = csql(""select * from documents where itemid_version = {0};"" 
    -f $itemid_version)
</code></pre>

<p>and this (use case #3):</p>

<pre><code>$versions = csql(""select version from document_versions where item_id = {0}"" 
    -f $itemid)
</code></pre>

<p>for the 3rd requirement, I am adding yet another table:</p>

<pre><code>CREATE TABLE latest_documents (
    itemid uuid,
    version int,
    PRIMARY KEY (itemid, version)
)
</code></pre>

<p>records are inserted for new docs, records are updated for existing docs</p>

<p>and now we have this:</p>

<pre><code>$latest_itemids, $latest_versions = csql(""select itemid, version 
    from latest_documents where item_id in ({0})"" -f $itemid_list.toCSV())

foreach ($one_itemid in $latest_itemids, $one_version in $latest_versions)
    $itemid_version = concat($latest_itemid, $latest_version)
    $latest_docs.append(
        cql(""select * from documents where itemid_version = {0};"" 
        -f $itemid_version))        
</code></pre>

<p>Now I hope it's clear why I concatenated <code>itemid</code> and <code>version</code> to create an index for <code>documents</code> as opposed to creating a compound key: I cannot have <code>OR</code> in the <code>WHERE</code> clause in <code>SELECT</code></p>

<p>You can assume that only one process will do the inserts/updates so you don't need to worry about consistency or isolation issues.</p>

<p>Am I on the right track here?  there are quite a number of things that doesn't sit well with me...but mainly because I don't understand Cassandra yet:</p>

<ul>
<li>I feel that the primary key for <code>documents</code> should be a composite of (itemid, version) but I can't satisfy use case #4 (return a list from a query)...I can't possibly use a separate SELECT statement for each document due to the performance hit (network overhead)...or can (should) I?</li>
<li>2 trips to get a document if the version is not known beforehand.  probably a compromise I have to live with, or maybe there's a better way.</li>
</ul>
",<cassandra><cassandra-2.0>,"<p>How would this work Dexter?</p>

<p>It is actually very similar to your solution actually except you can store all versions and be able to fetch the 'latest' version just from one table (document_versions).</p>

<p>In most cases I think you can get what you want in a single SELECT except use case #2 where fetching the most recent version of a document where a pre SELECT is needed on document_versions first.</p>

<p><em>SECOND ATTEMPT</em></p>

<p>(I removed the code from the first attempt, apologies to anyone who was following in the comments).</p>

<pre><code>CREATE TABLE documents (
        itemid_version text,
        xml_payload text,
        insert_time timestamp,
        PRIMARY KEY (itemid_version)
);

CREATE TABLE document_versions (
        itemid text,
        version int,
        PRIMARY KEY (itemid, version)
) WITH CLUSTERING ORDER BY (version DESC);


INSERT INTO documents (itemid_version, xml_payload, insert_time) VALUES ('doc1-1', '&lt;?xml&gt;1st&lt;/xml&gt;', '2014-05-21 18:00:00');
INSERT INTO documents (itemid_version, xml_payload, insert_time) VALUES ('doc1-2', '&lt;?xml&gt;2nd&lt;/xml&gt;', '2014-05-21 18:00:00');
INSERT INTO documents (itemid_version, xml_payload, insert_time) VALUES ('doc2-1', '&lt;?xml&gt;1st&lt;/xml&gt;', '2014-05-21 18:00:00');
INSERT INTO documents (itemid_version, xml_payload, insert_time) VALUES ('doc2-2', '&lt;?xml&gt;2nd&lt;/xml&gt;', '2014-05-21 18:00:00');

INSERT INTO document_versions (itemid, version) VALUES ('doc1', 1);
INSERT INTO document_versions (itemid, version) VALUES ('doc1', 2);
INSERT INTO document_versions (itemid, version) VALUES ('doc2', 1);
INSERT INTO document_versions (itemid, version) VALUES ('doc2', 2);
</code></pre>

<ol>
<li><p>user needs to get the single (1) doc he wants, he knows the item id and version (not necessarily the latest)</p>

<p>SELECT * FROM documents WHERE itemid_version = 'doc1-2';</p></li>
<li><p>user needs to get the single (1) doc he wants, he knows the item id but does not know the latest version
(You would feed concatenated itemid + version in result of first query into second query)</p>

<p>SELECT * FROM document_versions WHERE itemid = 'doc2' LIMIT 1;</p>

<p>SELECT * FROM documents WHERE itemid_version = 'doc2-2'; </p></li>
<li><p>user needs the version history of a single (1) doc.</p>

<p>SELECT * FROM document_versions WHERE itemid = 'doc2';</p></li>
<li><p>user needs to get the list (1 or more) of docs he wants, he knows the item id AND version (not necessarily the latest)</p>

<p>SELECT * FROM documents WHERE itemid_version IN ('doc1-2', 'doc2-1');</p></li>
</ol>

<p>Cheers,</p>
",['table']
23798563,24134844,2014-05-22 05:35:53,Unable to connect to remote cassandra from titan,"<p>I am using cassandra 2.0.7 sitting on a remote server listening on non-default port </p>

<pre><code>&lt;code&gt;
---cassandra.yaml
rpc_address: 0.0.0.0
rpc_port: 6543
&lt;/code&gt;
</code></pre>

<p>I am trying to connect to the server using titan-0.4.4 (java API, also tried with rexster) using the following config:</p>

<pre><code>&lt;code&gt;
storage.hostname=172.182.183.215
storage.backend=cassandra
storage.port=6543
storage.keyspace=abccorp
&lt;/code&gt;
</code></pre>

<p>It does not connect and I see the the following exceptions below. However, if I use cqlsh on the same host from where I am trying to execute my code/rexster, I am able to connect without any issues. Anybody seen this?</p>

<pre><code>&lt;code&gt;
0    [main] INFO  com.netflix.astyanax.connectionpool.impl.ConnectionPoolMBeanManager  - Registering mbean: com.netflix.MonitoredResources:type=ASTYANAX,name=ClusterTitanConnectionPool,ServiceType=connectionpool
49   [main] INFO  com.netflix.astyanax.connectionpool.impl.CountingConnectionPoolMonitor  - AddHost: 172.182.183.215
554  [main] INFO  com.netflix.astyanax.connectionpool.impl.ConnectionPoolMBeanManager  - Registering mbean: com.netflix.MonitoredResources:type=ASTYANAX,name=KeyspaceTitanConnectionPool,ServiceType=connectionpool
555  [main] INFO  com.netflix.astyanax.connectionpool.impl.CountingConnectionPoolMonitor  - AddHost: 172.182.183.215
999  [main] INFO  com.netflix.astyanax.connectionpool.impl.CountingConnectionPoolMonitor  - AddHost: 127.0.0.1
1000 [main] INFO  com.netflix.astyanax.connectionpool.impl.CountingConnectionPoolMonitor  - RemoveHost: 172.182.183.215
2366 [main] INFO  com.thinkaurelius.titan.diskstorage.Backend  - Initiated backend operations thread pool of size 16
41523 [RingDescribeAutoDiscovery] WARN  com.netflix.astyanax.impl.RingDescribeHostSupplier  - Failed to get hosts from abccorp via ring describe.  Will use previously known ring instead
61522 [RingDescribeAutoDiscovery] WARN  com.netflix.astyanax.impl.RingDescribeHostSupplier  - Failed to get hosts from abccorp via ring describe.  Will use previously known ring instead
63080 [main] INFO  com.thinkaurelius.titan.diskstorage.util.BackendOperation  - Temporary storage exception during backend operation. Attempting backoff retry
com.thinkaurelius.titan.diskstorage.TemporaryStorageException: Temporary failure in storage backend
    at com.thinkaurelius.titan.diskstorage.cassandra.astyanax.AstyanaxOrderedKeyColumnValueStore.getNamesSlice(AstyanaxOrderedKeyColumnValueStore.java:138)
    at com.thinkaurelius.titan.diskstorage.cassandra.astyanax.AstyanaxOrderedKeyColumnValueStore.getSlice(AstyanaxOrderedKeyColumnValueStore.java:88)
    at com.thinkaurelius.titan.graphdb.configuration.KCVSConfiguration$1.call(KCVSConfiguration.java:70)
    at com.thinkaurelius.titan.graphdb.configuration.KCVSConfiguration$1.call(KCVSConfiguration.java:64)
    at com.thinkaurelius.titan.diskstorage.util.BackendOperation.execute(BackendOperation.java:30)
    at com.thinkaurelius.titan.graphdb.configuration.KCVSConfiguration.getConfigurationProperty(KCVSConfiguration.java:64)
    at com.thinkaurelius.titan.diskstorage.Backend.initialize(Backend.java:277)
    at com.thinkaurelius.titan.graphdb.configuration.GraphDatabaseConfiguration.getBackend(GraphDatabaseConfiguration.java:1174)
    at com.thinkaurelius.titan.graphdb.database.StandardTitanGraph.&lt;init&gt;(StandardTitanGraph.java:75)
    at com.thinkaurelius.titan.core.TitanFactory.open(TitanFactory.java:40)
    at com.thinkaurelius.titan.core.TitanFactory.open(TitanFactory.java:29)
    at com.abccorp.grp.graphorm.GraphORM.&lt;init&gt;(GraphORM.java:23)
    at com.abccorp.grp.graphorm.GraphORM.getInstance(GraphORM.java:47)
    at com.abccorp.grp.utils.dataloader.MainLoader.main(MainLoader.java:150)
Caused by: com.netflix.astyanax.connectionpool.exceptions.NoAvailableHostsException: NoAvailableHostsException: [host=None(0.0.0.0):0, latency=0(0), attempts=0]No hosts to borrow from
    at com.netflix.astyanax.connectionpool.impl.RoundRobinExecuteWithFailover.&lt;init&gt;(RoundRobinExecuteWithFailover.java:30)
    at com.netflix.astyanax.connectionpool.impl.TokenAwareConnectionPoolImpl.newExecuteWithFailover(TokenAwareConnectionPoolImpl.java:83)
    at com.netflix.astyanax.connectionpool.impl.AbstractHostPartitionConnectionPool.executeWithFailover(AbstractHostPartitionConnectionPool.java:256)
    at com.netflix.astyanax.thrift.ThriftColumnFamilyQueryImpl$4.execute(ThriftColumnFamilyQueryImpl.java:519)
    at com.thinkaurelius.titan.diskstorage.cassandra.astyanax.AstyanaxOrderedKeyColumnValueStore.getNamesSlice(AstyanaxOrderedKeyColumnValueStore.java:136)
    ... 13 more
91522 [RingDescribeAutoDiscovery] WARN  com.netflix.astyanax.impl.RingDescribeHostSupplier  - Failed to get hosts from abccorp via ring describe.  Will use previously known ring instead
121522 [RingDescribeAutoDiscovery] WARN  com.netflix.astyanax.impl.RingDescribeHostSupplier  - Failed to get hosts from abccorp via ring describe.  Will use previously known ring instead
&lt;/code&gt;
</code></pre>

<p>Any help greatly appreciated. I am evaluating titan on cassandra and am a bit stuck on this as previously I was using cassandra (same version) on localhost and everything was fine.</p>

<p>thanks</p>
",<java><cassandra><titan>,"<p>Changing the listen_address to 172.182.183.215 in the configuration had done the trick. Initially it was not clear if just setting the rpc_address was enough.</p>
","['rpc_address', 'listen_address']"
24012641,24046196,2014-06-03 10:20:18,Cassandra CLUSTERING ORDER BY not doing what I'd expect,"<p>I'm pretty new to Cassandra, I have created a table like so:</p>

<pre><code>CREATE TABLE IF NOT EXISTS notifications (
  id uuid,
  raised timeuuid,
  recorded timeuuid,
  customer varchar,
  region varchar,
  location varchar,
  operator varchar,
  till varchar,
  receipt_id varchar,
  value decimal,
  details text,
  is_refund boolean,
  has_promos boolean,
  utc_offset int,
  frame_count int,
  expecting_video boolean,
  PRIMARY KEY (id, raised)
) WITH CLUSTERING ORDER BY (raised desc);
</code></pre>

<p>And then inserted 1,000 rows from the DataStax .NET Cassandra adapter like so:</p>

<pre><code>for (var i = 0; i &lt; 1000; i++)
{
                    var id = Guid.NewGuid();
                    var now = DateTime.Now;

                    var insertNotif = session.Prepare(@""
                    INSERT INTO notifications 
                    (id,customer,region,location,operator,till,receipt_id,value,details,is_refund,has_promos,raised,recorded,utc_offset,frame_count,expecting_video)
                    VALUES (?,?,?,?,?,?,?,?,?,?,?,now(),now(),?,?,?)"");
                    var insertNotifStatement = insertNotif.Bind(id, ""cust1"", ""reg1"", ""loc1"", ""tomm"", ""london"", i.ToString(), i % 10.99D, ""DATA_HERE"", false, true, (int)TimeZone.CurrentTimeZone.GetUtcOffset(now).TotalMinutes, 0, false);

                    session.Execute(insertNotifStatement);
                    Thread.Sleep(10);
}
</code></pre>

<p>What I'd expect to happen is that all the records will be stored in time descending order based on the <code>raised</code> column. When I use CQLSH to inspect the data, it appears to be in a random order:</p>

<pre><code>cqlsh:my_keyspace&gt; select dateOf(raised) from notifications limit 5;

 dateOf(raised)
--------------------------------------
 2014-06-03 11:12:45GMT Daylight Time
 2014-06-03 11:12:48GMT Daylight Time
 2014-06-03 11:12:56GMT Daylight Time
 2014-06-03 11:12:32GMT Daylight Time
 2014-06-03 11:12:34GMT Daylight Time

(5 rows)
</code></pre>

<p>Have I done something wrong?
BTW, I have also tried to set <code>raised</code> to a timestamp column and expected the same behaviour (but I passed in DateTimes from .NET instead of now()), but it behaves in the exact same way.</p>

<p>How can I get my records <strong>store and retrieved</strong> in time descending order?</p>
",<database><cassandra><cql><cql3><cqlsh>,"<p>The clustering order refers to the ordering within the partition i.e. for the same ID. So if you inserted multiple rows with different <code>raised</code> values and queried, you would see them returned in descending order.</p>

<p>The order you are seeing is the order of IDs. As you observed, these are essentially random. This order is the partitioner order i.e. ordered by the hash of the ID.</p>

<p>You can't change this order, so you can't read all your partitions back in time order. You can only order within each partition. </p>
",['partitioner']
24076932,24084668,2014-06-06 07:58:26,Counting instances in a column,"<p>I have the following table</p>

<pre><code>mykey.mytable

id | user   | item
------------------
 0 | 'matt' | 'ball'
 1 | 'bob'  | 'bat'
 2 | 'bill' | 'fridge'
 3 | 'matt' | 'beer'
</code></pre>

<p>What I would like is a count of users, for example</p>

<pre><code>user   | count
--------------
'matt' | 2
'bob'  | 1
'bill' | 1
</code></pre>

<p>How do I do this in Cassandra?</p>

<p>FWIW, I have experimented with count(*), batch, counter and using the result of one select in another. Documentation is not much help and my google foo is failing to find any examples. I'm getting a little frustrated!</p>
",<cassandra><cql>,"<p>Unfortunately, Cassandra CQL does not have an aggregation framework.  So while you could easily do this in a relational database with a <code>GROUP BY</code> and a <code>COUNT</code>, Cassandra has no such mechanism.  </p>

<p>One of the main principles of non-relational (NoSQL) data modeling, is to build your tables to match your query patterns.  If querying the number of user items is something you forsee needing to do, then you could create another table to hold that information:</p>

<pre><code>CREATE TABLE userItemCount (
user text,
itemCount counter,
PRIMARY KEY (user)
);
</code></pre>

<p>Via CQL you could increment that counter like this (assuming that Matt gets another item):</p>

<pre><code>UPDATE userItemCount SET itemCount=itemCount+1 WHERE user='matt';
</code></pre>

<p>Of course, the down-side of this, is that you'll need to build a mechanism to add/increment the appropriate data when an entry is stored in your original table.</p>

<p>For a video walk through on this and other Cassandra development concepts, check out the free Java/Cassandra class on Datastax Academy: <a href=""https://datastaxacademy.elogiclearning.com"" rel=""nofollow"">https://datastaxacademy.elogiclearning.com</a>.  Session #2, Module 43 covers the use of ""counter tables.""</p>
",['table']
24207937,24208973,2014-06-13 14:38:27,noSQL rollback feature,"<p>I'm new to noSQL technologies, and I was surprised there is no transaction support whatsoever. My main problem is when i make some of our insert task, that insert consist of ~5 seperate insert. We have to find a document by 4 different IDs. The problem is that the document is fairly large, and it's really expensive to store it like this:</p>

<ul>
<li><p>key   | value</p>

<hr></li>
<li><p>user1   HugeDoc1</p></li>
<li>user2   HugeDoc1</li>
<li>user3   HugeDoc1</li>
</ul>

<p>So we come up with an internal Id, that points to the document. Yes, I know this design somewhat violates the whole noSQL concept, but it saves a lot of memory. If document insert fails, the Ids have no meaning, and should be removed. 
Is it a good idea to write my own rollback handling, keep track of successful inserts/updates? Or the whole concept is wrong?</p>
",<cassandra><couchbase><infinispan><nosql>,"<blockquote>
  <p>I know this design somewhat violates the whole noSQL concept, but it saves a lot of memory.</p>
</blockquote>

<p>That is a very 1970's way of thinking.  Relational database theory originated at a time when disk space was expensive.  In 1975 IBM was selling hard drives at $11k per <em>megabyte</em>.  By 1980 prices dropped so that you could buy a gigabyte's worth of storage space for under $1 million.  Today, you can go on NewEgg and buy a terabyte drive for $60.  Now disk space is cheap, and <em>processing time</em> is the expensive part.</p>

<p>In non-relational (NoSQL) data modeling, you should build your table structures according to how it makes sense to query your data.  This is a departure from relational data modeling, where you build your tables according to how it makes sense to <em>store</em> your data.  Often times, query-based modeling results in storage of redundant data...<strong>and that's ok</strong>.  Duplicate data for speed, reference data for integrity.</p>

<blockquote>
  <p>Is it a good idea to write my own rollback handling, keep track of
  successful inserts/updates? Or the whole concept is wrong?</p>
</blockquote>

<p>I was on a Cassandra project where we did implement something similar to an application-side transaction/rollback.  It really didn't work very well, and ended-up creating several tombstones.  Ultimately, I would ask yourself exactly why your application needs a non-relational database, because it sounds like you still need some of the benefits of a relational database.  If you're sure that you absolutely need a non-relational database, then you may want to re-think your approach to data modeling.</p>
",['table']
24219953,24223191,2014-06-14 12:33:58,cassandra - only superuser is allowed to perform CREATE USER queries,"<p>I'm logging in with on Ubuntu 14.10 on Cassandra 2.0.8 with Java 1.7.0_60-b19</p>

<pre><code>cqlsh -u cassandra -p cassandra
</code></pre>

<p>I'm running:</p>

<pre><code>CREATE USER a WITH PASSWORD 'a' NOSUPERUSER;
</code></pre>

<p>I'm getting the error:</p>

<pre><code>Bad Request: Only superusers are allowed to perform CREATE USER queries
</code></pre>

<p>The problem with reasoning - I am logged in as the superuser. </p>

<p>My question is: <strong>If I'm logged into cqlsh as the Cassandra user, why am I told that I'm not the superuser?</strong></p>
",<cassandra><cql><cqlsh>,"<p>You need to enable PasswordAuthenticator in cassandra.yaml file.
To enable PasswordAuthenticator you need to change authenticator property in cassandra.yaml</p>

<p>Change</p>

<pre><code>authenticator: AllowAllAuthenticator
</code></pre>

<p>to </p>

<pre><code>authenticator: PasswordAuthenticator
</code></pre>

<p>After that login with following command and then you will be able to add new user</p>

<pre><code>cqlsh -u cassandra -p cassandra
</code></pre>
",['authenticator']
24587312,24587833,2014-07-05 14:17:01,Difference between start_key and start_token in Cassandra KeyRange,"<p>I'm wondering what the difference is between the two listed here: <a href=""http://wiki.apache.org/cassandra/API10"" rel=""nofollow"">http://wiki.apache.org/cassandra/API10</a></p>

<p>I tend to use start_key more but am wondering what the word 'token' relates to in this case.</p>
",<cassandra>,"<p>From your link: </p>

<blockquote>
  <p>The semantics of start keys and tokens are slightly different. Keys
  are start-inclusive; tokens are start-exclusive. Token ranges may also
  wrap -- that is, the end token may be less than the start one. Thus, a
  range from keyX to keyX is a one-element range, but a range from
  tokenY to tokenY is the full ring (one exception is if keyX is mapped
  to the minimum token, then the range from keyX to keyX is the full
  ring).</p>
</blockquote>

<p>I've used tokens to scan all the rows within a CF without knowing any of these -- setting start token empty and end token empty (this is equivalent to what they say from tokenY to tokenY)</p>

<p>As far as tokens are concerned I couldn't find a valid usage for rows scanning if not the one to iterate over all the table but I guess this is due to the RandomPartitioner -- I guess that could be more helpful with OPP</p>

<p>HTH, Carlo</p>
",['table']
24622511,24622819,2014-07-08 01:47:22,What is the difference between a clustering column and secondary index in cassandra,"<p>I'm trying to understand the difference between these two and the scenarios in which you would prefer to use one over the other.</p>

<p>My specific use case is using cassandra as an event ingestion system backed by an analytics engine that interprets the event.</p>

<p>My model includes</p>

<ul>
<li>event id (the partition key)</li>
<li>event time (a clustering column)</li>
<li>event type (i'm not sure whether to use clustering column or secondary index)</li>
</ul>

<p>I figure the most common read scenario will be to get the events over a time range hence event time is the clustering column. A less frequent read scenario might involve further filtering the event query by event type.</p>
",<cassandra>,"<p>A secondary index is pretty similar to what we know from regular relational databases. If you have a query with a where clause that uses column values that are not part of the primary key, lookup would be slow because a full row search has to be performed. Secondary indexes make it possible to service such queries efficiently. Secondary indexes are stored as extra tables, and just store extra data to make it easy to find your way in the main table. </p>

<p>So that's a good ol' index, which we already know about. So far, there's nothing new to cassandra and its distributed nature.</p>

<p>Partitioning and clustering is all about deciding how rows from the main table are spread among the nodes. This is unique to cassandara since it determines the distribution of data. So, the primary key consists of at least one column. The first column in the primary key is used as the partition key. The partition key is used to decide which node to store a row. If the primary key has additional columns, the columns are used to cluster the data on a given node - the data is stored in lexicographic order on a node by clustering columns.</p>

<p>This question has more specifics on clustering columns: <a href=""https://stackoverflow.com/questions/18635381"">Clustering Keys in Cassandra</a></p>

<p>So an index on a given column X makes the lookup <code>X --&gt; primary key</code> efficient. The partition key (first column in the primary key) determines which node a row is stored on. Clustering columns (additional columns in the primary key) determine which order rows are stored in on their assigned node.</p>

<p>So your intuition sounds about right - the event ID is presumably guaranteed unique, so is great for building a primary key. Event time is a great way to order rows on disk on a given node.</p>

<p>If you never needed to lookup data by event type, eg, never had a query like <code>SELECT * FROM Events WHERE Type = Warning</code>, then you have no need for your additional indexes, but your demands for partitioning don't change. Indexes make it easy to serve queries with different predicates. Since you mentioned that you indeed were planning on performing queries like that, you do in fact likely want an index on your EventType column.</p>

<p>Check out the cassandra documentation: <a href=""http://www.datastax.com/documentation/cql/3.0/cql/ddl/ddl_compound_keys_c.html"" rel=""noreferrer"">http://www.datastax.com/documentation/cql/3.0/cql/ddl/ddl_compound_keys_c.html</a></p>

<blockquote>
  <p>Cassandra uses the first column name in the primary key definition as the partition key.
  <br> ... <br>
   In the case of the playlists table, the song_order is the clustering column. The data for each partition is clustered by the remaining column or columns of the primary key definition. On a physical node, when rows for a partition key are stored in order based on the clustering columns</p>
</blockquote>
",['table']
24645318,24647272,2014-07-09 04:16:31,C# Cassandra - Find And Modify one Row,"<p>Using the C# interface, how do I lock 1 row in Cassandra, update it, and then unlock it? Until the lock is released, no other process on any machine or threads on the same machine should be able to read the row.</p>
",<c#><cassandra>,"<p>You can't -- the Lock concept does not exists in Cassandra -- you have to perform this operation in your application using a distributed lock. For the purpose you can give a look at ZooKeeper, Hazelcast or Memcached -- you can achieve the goal with any of these -- I'm not into c# world but I see that each of these products provides a c# client. You can also take a look at Cages, a library based on ZooKeeper created just for the goal (<a href=""http://ria101.wordpress.com/2010/05/12/locking-and-transactions-over-cassandra-using-cages/"" rel=""nofollow"">http://ria101.wordpress.com/2010/05/12/locking-and-transactions-over-cassandra-using-cages/</a>)</p>

<p>Alternatively you could implement your distributed-lock using lightweight transactions (<a href=""http://www.datastax.com/documentation/cassandra/2.0/cassandra/dml/dml_ltwt_transaction_c.html"" rel=""nofollow"">http://www.datastax.com/documentation/cassandra/2.0/cassandra/dml/dml_ltwt_transaction_c.html</a>). It's just an idea but you could have a locking CF. For each ROW you need to lock you put an entry into this locking-table (with Serial CL) using IF NOT EXISTS clause -- if you ""acquired the lock"" (cassandra will tell you as the result of the operation) you can  start working. One advantage I see in this model is that you can write your lock entry with a TTL -- if you imagine your operation would last maximum X you can put a TTL on the row of 2X, so if your machine dies before removing the entry from the lock table (""releasing the lock"") Cassandra will do this for you.</p>

<p>HTH,
Carlo</p>
",['table']
24684457,24774099,2014-07-10 19:09:24,data auditing in Cassandra,"<p>How to implement auditing for cassandra data?
I am looking for a open source option.</p>

<p>Are there any features of cassandra that help with auditing?
Can I use triggers to log the records into a table? I followed <a href=""https://github.com/apache/cassandra/tree/trunk/examples/triggers"" rel=""nofollow noreferrer"">Triggers</a> example and was able to get a record inserted into <code>triggers_log</code> table when the updates occur on another table.
But not sure how do I capture the <code>user/session</code> details that triggered the update. I have From <code>CQLSH</code> terminal, create <code>users</code> and <code>trigger_log table</code></p>

<pre>
create table AUDIT_LOG ( 
       transaction_id int,
       entries map&lt;text, text&gt;,  --> to capture the modifications done to the tables
       user varchar,  //authenticated user
       time timestamp, 
       primary key(transaction_id));
</pre>

<pre>
CREATE TABLE users (
  user_id int PRIMARY KEY,
  fname text,
  lname text
);
</pre>

<p>Define the trigger on users table using <code>CREATE TRIGGER</code> syntax from <code>cqlsh</code></p>

<p>Below code so far. </p>

<pre class=""lang-java prettyprint-override""><code>public class AuditTrigger implements ITrigger {

    @Override
    public Collection&lt;RowMutation&gt; augment(ByteBuffer key, ColumnFamily update) {

        List&lt;RowMutation&gt; mutations = new ArrayList&lt;RowMutation&gt;();
        for (Column column : update) {
            if (column.value().remaining() &gt; 0) {
                RowMutation mutation = new RowMutation(""mykeyspace"", key);
           //What do I need here to capture the updates to users 
           //table and log the updates into various columns of audit_log
                mutations.add(mutation);
            }
        }
        return mutations;
    }
}
</code></pre>

<p>If triggers is not the correct approach (any spring AOP approach?), please suggest alternatives. I also tried <a href=""https://stackoverflow.com/questions/9604554/cassandra-vs-logging-activity"">Cassandra vs logging activity</a> solution but it does not print the sql executed, authenticated user information.</p>
",<cassandra><cassandra-2.0>,"<p>Unfortunately at this time, Triggers cannot be used as what you need is the ClientState which contains the user information and is not passed to Triggers.</p>

<p>There are 2 approaches I can think of.(You will need to look at the Cassandra code base for better understanding these approaches)</p>

<p>One approach is AOP i.e to add an agent which would AOP and start Cassandra with the Agent. The class that will need to be pointcut is the QueryProcessor#processStatement method. The call to this method will have the prepared statement and the QueryState as parameters. From the PreparedStatement you can identify the intention of the user. QueryState.getClientState will return the ClientState which is where the user information resides.</p>

<p>The other approach involves custom authenticators and authorizers. Configuring this in Cassandra is described here.</p>

<p><a href=""http://www.datastax.com/documentation/cassandra/2.0/cassandra/security/secure_about_native_authenticate_c.html"" rel=""noreferrer"">http://www.datastax.com/documentation/cassandra/2.0/cassandra/security/secure_about_native_authenticate_c.html</a></p>

<p>You can have a custom authorizer extending the AllowAllAuthorizer(this will disable permission caching). Whenever you get an authorize request on the Authorizer you can log it. The downside of this approach is that you do not know what the user intends to do with the table, only that he is request some authorization on it. Permission is the one which contains what he wants to do with the table, but it is not passed on to the authorizer.</p>

<p>If you decide on either of these approaches, you are free to post followups if you need more detail.</p>
",['authorizer']
24719276,24720759,2014-07-13 03:27:38,Cassandra In-Memory option,"<p>There is an In-Memory option introduced in the Cassandra by DataStax Enterprise 4.0:
<a href=""http://www.datastax.com/documentation/datastax_enterprise/4.0/datastax_enterprise/inMemory.html"" rel=""nofollow"">http://www.datastax.com/documentation/datastax_enterprise/4.0/datastax_enterprise/inMemory.html</a>
But with 1GB size limited for an in-memory table.</p>

<p>Anyone know the consideration why limited it as 1GB? And possible extend to a large size of in-memory table, such as 64GB?</p>
",<cassandra><datastax-enterprise>,"<p>To answer your question: today it's not possible to bypass this limitation. 
In-Memory tables are stored within the JVM Heap, regardless the amount of memory available on single node allocating more than 8GB to JVM Heap is not recommended. 
The main reason of this limitation is that Java Garbage Collector slow down when dealing with huge memory amount.</p>

<p>However if you consider Cassandra as a distributed system 1GB is not the real limitation.</p>

<blockquote>
  <p>(nodes*allocated_memory)/ReplicationFactor</p>
</blockquote>

<p>allocated_memory is max 1GB -- So your table may contains many GB in memory allocated in different nodes.</p>

<p>I think that in future something will improve but dealing with 64GB in memory it could be a real problem when you need to flush data on disk. One more consideration that creates limitation: avoid TTL when working with In-Memory tables. TTL creates tombstones, a tombstone is not deallocated until the GCGraceSeconds period passes -- so considering a default value of 10 days each tombstone will keep the portion of memory busy and unavailable, possibly for long time. </p>

<p>HTH,
Carlo</p>
",['table']
24755488,24833313,2014-07-15 10:11:30,Error while executing query on shark shell with DSE 4.5,"<p>I am using datastax 4.5 and trying to use shark .i am able to open shark shell but queries are not working ,Error is :</p>

<p>shark> use company2;
OK<code></code>
Time taken: 0.126 seconds
shark> select count(*) from nhanes;</p>

<pre><code>java.lang.RuntimeException: Could not get input splits
    at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:158)
    at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:65)
    at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1414)
    at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1192)
    at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1020)
    at org.apache.hadoop.hive.ql.Driver.run(Driver.java:888)
    at shark.SharkCliDriver.processCmd(SharkCliDriver.scala:347)
    at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:413)
    at shark.SharkCliDriver$.main(SharkCliDriver.scala:240)
    at shark.SharkCliDriver.main SharkCliDriver.scala
    FAILED: Execution Error, return code -101 from shark.execution.SparkTask
</code></pre>

<p>Any idea about this error?</p>

<p>My second question is related to backup.
As i am using opscenter for taking backup but in production is it reliable or do i go for nodetool backup and schedule it on individual node.</p>

<p>Thanks   </p>
",<cassandra><datastax-enterprise><datastax>,"<p>Check <a href=""https://stackoverflow.com/questions/21853078/could-not-get-input-splits-error-with-hive-cassandra-cqlstoragehandler"">&quot;Could not get input splits&quot; Error, with Hive-Cassandra-CqlStorageHandler</a>. You can first test it using hive. If it fails in hive, you need check you keyspace partitioner. I would suggest to create a clean new keyspace and table to test it. Most likely it's something wrong with your KS settings. You can also check the replication of the keyspace, make sure it's replicated to the datacenter the cassandra node starts.</p>

<p>For the second question, it's recommend to use opscenter to backup which is fully tested and easy to use. You can also manually backup by using node tool for each node which causes some human error.</p>
",['table']
24802819,24812651,2014-07-17 11:53:44,How to determine node's workload type in datastax thru java driver,"<p>How to determine node's workload type in datastax thru java driver?
By workload i mean either node is Solr node in cluster or hadoop node or cassandra node.</p>
",<cassandra><datastax-enterprise><datastax-java-driver>,"<p>The java driver is focused on pure Apache Cassandra, so it is unaware of DataStax Enterprise-specific features such as ""workload"" type.</p>

<p>The ""dsetool ring"" command can be used to manually check workload.</p>

<p>The workload is stored in a DSE system table called ""system.peers"", so you can fetch the workload for all nodes EXCEPT the coordinator node using the query ""SELECT workload FROM system.peers"". Unfortunately, there is no workload column in the system.local table, so, you would have to execute the same query on another node to determine the workload for a specific node.</p>

<p>I'll file an internal DSE issue on this since it is undocumented, and should be easier and obvious.</p>
",['table']
24881957,24897886,2014-07-22 08:06:36,what is the way for counting number of columns in row in Cassandra?,"<p>what is  best practice for counting number of columns? I tried find any way to do this simply but not found any information about this. I think that i can do some query for this, but it maybe too slow. Then i know about way with use counter column, but i have not any idea how it would work with columns with ttl. summary, i need some method or way in Hector for check that row has not more than 5 columns in one minute. 
Thank for you advices and sorry for my bad language and noob question.</p>
",<java><cassandra><hector>,"<p>Don't really think counters would be useful for this. </p>

<p>Are you trying to count the number of columns, or the number of ""rows"" inside of a single ""multi partition row""? Say you're table looks like this:</p>

<p>create table foo (
   id text,
   colid text,
   somethingcol text,
   othercol text,
   primary key (id, colid));</p>

<p>Are you trying to count the number of entries for id='something specific'? </p>

<p>Assuming that you are, you can add a timestamp column as the first clustering key like this:</p>

<p>create table foo (
   id text,
   ts timestamp,
   colid text,
   somethingcol text,
   othercol text,
   primary key (id, ts, colid));</p>

<p>And run a query like: select count(*) from foo where id='theId' and [ts condition here];</p>

<p>If you're looking to aggregate to see if there is ANY case where there are more than five entries in any one minute, then a different structure might be appropriate (e.g. you have the minute as a clustering key and entries within it inside a SET column). Whether you can use that for your original purpose is something you'll need to check.</p>

<p>If you don't have too many entries in a partition, it might be possible to simple query the timestamps for a partition, and do the grouping + check client side. Spark is another possibility (maybe with Shark) if you want more sophisticated aggregation. Of course, that means having access to a bit more infra.</p>

<p>Does that help?</p>

<p>UPDATE:
Cassandra maintains a timestamp for each cell automatically. You can access that in a query:</p>

<p>SELECT id, bar, baz, writetime(bar) from foo where ...;</p>

<p>Will have the last updated timestamp for the bar column for each entry. The timestamp is assigned by the coordinator unless the client specifies it during write [during insert, you can do a with timestamp=n if you wish to specify it]. It's worth noting this is the behaviour using CQL, not thrift.</p>
",['table']
24894393,24916916,2014-07-22 17:52:42,Cassandra CQL range query rejected despite equality operator and secondary index,"<p>From the table schema below, I am trying to select all pH readings that are below 5.</p>

<p>I have followed these three pieces of advice:</p>

<ol>
<li>Use ALLOW FILTERING</li>
<li>Include an equality comparison</li>
<li>Create a secondary index on the reading_value column.</li>
</ol>

<p>Here is my query:</p>

<pre><code>select * from todmorden_numeric where sensor_name = 'pHradio' and reading_value &lt; 5  allow filtering;
</code></pre>

<p>Which is rejected with this message:</p>

<pre><code>Bad Request: No indexed columns present in by-columns clause with Equal operator
</code></pre>

<p>I tried adding a secondary index to the sensor_name column and was told that it was already part of the key and therefore already indexed.</p>

<p>I created the index after the table had been in use for a while - could that be the problem? I ran ""nodetool refresh"" in the hope it would make the index available but this did not work. Here is the output of <code>describe table todmorden_numeric</code> :</p>

<pre><code>CREATE TABLE todmorden_numeric (
  sensor_name text,
  reading_time timestamp,
  reading_value float,
  PRIMARY KEY ((sensor_name), reading_time)
) WITH
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='Data that suits being stored as floats' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  index_interval=128 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  default_time_to_live=0 AND
  speculative_retry='99.0PERCENTILE' AND
  memtable_flush_period_in_ms=0 AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'LZ4Compressor'};

CREATE INDEX todmorden_numeric_reading_value_idx ON todmorden_numeric (reading_value);
</code></pre>
",<cassandra><cql><secondary-indexes><range-query>,"<p>Cassandra allows range search only on:</p>

<p>a) Partition Key only if ByteOrderPartitioner is used (default now is murmur3).</p>

<p>b) any single clustering key ONLY IF any clustering keys defined BEFORE the target column in the primary key definition are already specified by an = operator in the predicate. </p>

<p>They don't work on secondary indices. </p>

<p>Consider the following table definition:</p>

<pre><code>CREATE TABLE tod1 (name text, time timestamp, 
    val float, PRIMARY KEY (name, time));
</code></pre>

<p>You CAN'T do a range on the val in this case.</p>

<p>Consider this one:</p>

<pre><code>CREATE TABLE tod2 (name text, time timestamp, 
    val float, PRIMARY KEY (name, time, val));
</code></pre>

<p>Then the following is valid:</p>

<pre><code>SELECT * FROM tod2 WHERE name='X' AND time='timehere' AND val &lt; 5; 
</code></pre>

<p>Kinda pointless, but this is not valid:</p>

<pre><code>SELECT * from tod2 WHERE name='X' AND val &lt; 5; 
</code></pre>

<p>It's not valid as you haven't filtered by a previous clustering key in the primary key def (in this case, time).</p>

<p>For your query, you may want to do this:</p>

<pre><code>CREATE TABLE tod3 (name text, time timestamp, 
    val float, PRIMARY KEY (name, val, time));
</code></pre>

<p>Note the order of columns in the primary key: val's before time.</p>

<p>This will allow you to do:</p>

<pre><code>SELECT * from tod3 WHERE name='asd' AND val &lt; 5;
</code></pre>

<p>On a different note, how long do you intend to hold data? How frequently do you get readings? This can cause your partition to grow quite large quite quickly. You may want to bucket it readings into multiple partitions (manual sharding). Perhaps one partition per day? Of course, such things would greatly depend on your access patterns.</p>

<p>Hope that helps.</p>
",['table']
24919732,24920922,2014-07-23 19:54:12,Performing range queries for cassandra table,"<p>I am trying to store data with following schema:</p>

<pre><code> CREATE TABLE temp_humidity_data (
                asset_id text, 
                date text, 
                event_time timestamp, 
                temprature int, 
                humidity int,
                PRIMARY KEY((asset_id, date),event_time)
            )
</code></pre>

<p>I have followed datastax article 'Getting Started with Time Series Modeling' - <a href=""http://planetcassandra.org/blog/post/getting-started-with-time-series-data-modeling/"" rel=""nofollow"">http://planetcassandra.org/blog/post/getting-started-with-time-series-data-modeling/</a></p>

<p>however with this data model one thing that is not working is query that returns me data between two dates. How do I do that?</p>

<p>If I do this:</p>

<pre><code>select * from temp_humidity_data 
where asset_id='1234' AND date &gt;= '2010-04-02' AND date &lt;= '2011-04-03';
</code></pre>

<p>It gives me following error:</p>

<blockquote>
  <p>code=2200 [Invalid query] message=""Only EQ and IN relation are
  supported on the partition key (unless you use the token() function)""</p>
</blockquote>

<p>In understand there is a way to do IN operator but I don't want to put all those dates in a 'IN' operator. Is there a way to query when using the above table definition data between two dates?</p>
",<cassandra><cassandra-2.0><datastax>,"<p>The first key in the primary key (a composite in your case) is responsible for scattering the data across different partitions. Each partition is held in its entirety on a single node (and its replicas). Even if the query you request were possible (it would be if you had only the date as a primary and used a byteorderedpartitioner - the default is murmur3), it would effectively do a full scan across your cluster. Think of this being similar to a full table scan in an rdbms on a column without an index, only now the full table scan spans multiple machines.</p>

<p>The goal with the composite partition key here is to ensure no partition gets unmanageably big. It also takes away your ability to do the range query across dates. If you think your data for an asset can fit in a single partition, you can make the date the first clustering key. That would enable the query. However, all rows for an asset would be on a single partition. This may be an issue (it's typically good to target around 100MB as a max partition size - though there are exceptions), and hotspots may arise in your cluster (nodes holding partitions for very busy stuff will be busy, while other nodes less so). Another way around this is to maintain manual buckets - add a bucketid int as part of the partition key [i.e. (asset_id, bucket_id)], have date as the first clustering key, and maintain the bucketing from application code. This would distribute the data for an asset across multiple partitions that you control. This will need a bit of calculation, and will need you to query each bucket yourself - but will prevent hotspots, and allow your date range queries. You'd obviously only do this if the data for a particular asset is beyond single partition size, but manageable via buckets.</p>

<p>If you absolutely must partition based on date, consider things like Spark and Shark to do efficient post aggregation.</p>

<p>Hope that helps.</p>
",['table']
24945675,24947420,2014-07-24 23:30:48,How many tables/Column Families are created in cassandra for this example,"<p>I am reading this <a href=""http://www.datastax.com/dev/blog/schema-in-cassandra-1-1"" rel=""nofollow"">post</a> on schema's in cassandra.</p>

<p>The author creates two tables:</p>

<pre><code>CREATE TABLE tweets (
    tweet_id uuid PRIMARY KEY,
    author varchar,
    body varchar
);

CREATE TABLE timeline (
    user_id varchar,
    tweet_id uuid,
    author varchar,
    body varchar,
    PRIMARY KEY (user_id, tweet_id)
);
</code></pre>

<p>Note: As for the tables are concerned, they don't know that both table can be ""JOINED"" on tweet_id. Each table sees tweet_id as a unique column name of type uuid.</p>

<p>If my understanding of the post is here, the author says that there are no two column families aka table created physically. It is just ONE HUGE table that contains information for both the logical column families.</p>

<p>But how does the look up happen when I say <code>select * from tweets where tweet_id=""xxx""</code>
(is there an internal marker to determine the columns belongs to tweets)</p>

<p>Please look at the post, as the author illustrates with the good examples.</p>

<p>My question is how does tweet_id in table timeline knows it should ""join"" with tweet_id in table tweets. </p>
",<cassandra><cql>,"<p>No, it is not created as one column family.  Both column families are created separately, and operate independently of each other.  What the author is referring to, is the aspect of non-relational data modeling that involves denormalizing your data and creating tables that match your query patterns.</p>

<p>When a ""tweet"" is made, the application has to be designed to store data about the tweet into two different column families.  It stores once in the <code>tweets</code> column family, and then an entry is made into the <code>timeline</code> column family for each follower.  Essentially, data about a particular tweet is being duplicated once for the <code>tweets</code> column family, and once for every follower that the author has.</p>

<blockquote>
  <p>how does tweet_id in table timeline knows it should ""join"" with
  tweet_id in table tweets.</p>
</blockquote>

<p>Simple, it doesn't know that.  Cassandra does not allow joins, and a properly-designed application backed by Cassandra will not employ client-side joins, either.  Again, each column family is designed in anticipation of each query that might be run.  Sometimes, the application may want to query a specific tweet by <code>tweet_id</code>, and it would use the <code>tweets</code> column family for that.  On the other hand, the post mentions that the application has a use case to query the 20 most-recent tweets from a particular user,"" in which case the <code>timeline</code> column family is designed to handle that.</p>

<p>Summary:</p>

<ul>
<li>There are two column families being defined.</li>
<li>Each column family is designed to handle a specific query.</li>
<li>There are no joins; database or client-side.  The data is denormalized (duplicated) so that the application can quickly query the data in the way that it is required.</li>
</ul>
",['table']
24949676,24953331,2014-07-25 06:49:46,"Difference between partition key, composite key and clustering key in Cassandra?","<p>I have been reading articles around the net to understand the differences between the following <code>key</code> types. But it just seems hard for me to grasp. Examples will definitely help  make understanding better.</p>

<pre><code>primary key,
partition key, 
composite key 
clustering key
</code></pre>
",<database><cassandra><cql>,"<p>There is a lot of confusion around this, I will try to make it as simple as possible.</p>
<p>The primary key is a general concept to indicate one or more columns used to retrieve data from a Table.</p>
<p>The primary key may be <em><strong>SIMPLE</strong></em> and even declared inline:</p>
<pre><code> create table stackoverflow_simple (
      key text PRIMARY KEY,
      data text      
  );
</code></pre>
<p>That means that it is made by a single column.</p>
<p>But the primary key can also be <em><strong>COMPOSITE</strong></em> (aka <em><strong>COMPOUND</strong></em>), generated from more columns.</p>
<pre><code> create table stackoverflow_composite (
      key_part_one text,
      key_part_two int,
      data text,
      PRIMARY KEY(key_part_one, key_part_two)      
  );
</code></pre>
<p>In a situation of <em><strong>COMPOSITE</strong></em> primary key, the &quot;first part&quot; of the key is called <em><strong>PARTITION KEY</strong></em> (in this example <strong>key_part_one</strong> is the partition key) and the second part of the key is the <em><strong>CLUSTERING KEY</strong></em> (in this example <strong>key_part_two</strong>)</p>
<p><strong>Please note that both partition and clustering key can be made by more columns</strong>, here's how:</p>
<pre><code> create table stackoverflow_multiple (
      k_part_one text,
      k_part_two int,
      k_clust_one text,
      k_clust_two int,
      k_clust_three uuid,
      data text,
      PRIMARY KEY((k_part_one, k_part_two), k_clust_one, k_clust_two, k_clust_three)      
  );
</code></pre>
<p>Behind these names ...</p>
<ul>
<li>The <strong>Partition Key</strong> is responsible for data distribution across your nodes.</li>
<li>The <strong>Clustering Key</strong> is responsible for data sorting within the partition.</li>
<li>The <strong>Primary Key</strong> is equivalent to the <strong>Partition Key</strong> in a single-field-key table (i.e. <strong>Simple</strong>).</li>
<li>The <strong>Composite/Compound Key</strong> is just any multiple-column key</li>
</ul>
<p>Further usage information: <a href=""http://www.datastax.com/documentation/cql/3.0/cql/cql_reference/create_table_r.html"" rel=""noreferrer"">DATASTAX DOCUMENTATION</a></p>
<hr>
Small usage and content examples<br/>
***SIMPLE*** KEY:
<pre><code>insert into stackoverflow_simple (key, data) VALUES ('han', 'solo');
select * from stackoverflow_simple where key='han';
</code></pre>
<p><strong>table content</strong></p>
<pre><code>key | data
----+------
han | solo
</code></pre>
<p><em><strong>COMPOSITE/COMPOUND KEY</strong></em> can retrieve &quot;wide rows&quot; (i.e. you can query by just the partition key, even if you have clustering keys defined)</p>
<pre><code>insert into stackoverflow_composite (key_part_one, key_part_two, data) VALUES ('ronaldo', 9, 'football player');
insert into stackoverflow_composite (key_part_one, key_part_two, data) VALUES ('ronaldo', 10, 'ex-football player');
select * from stackoverflow_composite where key_part_one = 'ronaldo';
</code></pre>
<p><strong>table content</strong></p>
<pre><code> key_part_one | key_part_two | data
--------------+--------------+--------------------
      ronaldo |            9 |    football player
      ronaldo |           10 | ex-football player
</code></pre>
<p>But you can query with all keys (both partition and clustering) ...</p>
<pre><code>select * from stackoverflow_composite 
   where key_part_one = 'ronaldo' and key_part_two  = 10;
</code></pre>
<p><strong>query output</strong></p>
<pre><code> key_part_one | key_part_two | data
--------------+--------------+--------------------
      ronaldo |           10 | ex-football player
</code></pre>
<p>Important note: the partition key is the minimum-specifier needed to perform a query using a <code>where clause</code>.
If you have a composite partition key, like the following</p>
<p>eg: <code>PRIMARY KEY((col1, col2), col10, col4))</code></p>
<p>You can perform query only by passing at least both col1 and col2, these are the 2 columns that define the partition key. The &quot;general&quot; rule to make query is you must pass at least all partition key columns, then you can add optionally each clustering key in the order they're set.</p>
<p>so, the valid queries are (<strong>excluding secondary indexes</strong>)</p>
<ul>
<li>col1 and col2</li>
<li>col1 and col2 and col10</li>
<li>col1 and col2 and col10 and col 4</li>
</ul>
<p>Invalid:</p>
<ul>
<li>col1 and col2 and col4</li>
<li>anything that does not contain both col1 and col2</li>
</ul>
",['table']
24981687,24984756,2014-07-27 13:56:18,Row Iteration not working,"<p>My goal is to iterate over all rows in a specific ColumnFamily in a node.<br>
Here is the php code (using my wrapper over phpcassa):    </p>

<pre><code>$ring = $cass_db-&gt;describe_ring();

foreach ($ring as $ring_details)
{
    $start_token = $ring_details-&gt;start_token;
    $end_token   = $ring_details-&gt;end_token;

    if ($start_token != null &amp;&amp; $end_token != null)
    {
        $i = 0;
        $batch_size = 10;

        $params = array(
            'token_start' =&gt; $start_token,
            'token_finish' =&gt; $end_token,
            'row_count'     =&gt; $batch_size,
            'buffer_size'   =&gt; 1000
        );

        while ($batch = $cass_db-&gt;get_range_by_token('myColumnFamily', $params))
        {
            var_dump('Batch# '.$i);

            foreach ($batch as $row)
            {
                $row_key     = $row[0];
                $row_values  = $row[1];
                var_dump($row_key);                 
            }

            $i++;

            //Just to stop infinite loop
            if ($i &gt; 14)
            {
                die(); 
            }

        }
    }
}
</code></pre>

<ul>
<li>get_range_by_token() uses default parameters overwritten by $params.   </li>
</ul>

<p>In each batch I get the same 10 row keys.<br>
How to iterate over all existing rows in a large Cassandra DB?</p>
",<php><cassandra><phpcassa>,"<p>I am not a PHP developer so I may misunderstand something in your code. More, you did not specify which cassandra version you are using.</p>

<p>Iteration on all rows is generally done starting and ending with an empty token, and redefining the start token in each iteration. In your code I can't see where you redefine token_start in each iteration. If you don't redefine it you're querying cassandra everytime for the same range of tokens and you will get always the same resultset.</p>

<p>Your code should do something like this ... </p>

<pre><code>start_token = '';
end_token = '';
page_size = 100;
while ( get_range_by_token('cf', start_token, end_token, page_size) {
   // here I should get page_size rows (unless I'm in last iteration or table rows is smaller than page_size elements)
   start_token = rows[rows.size()].getKey();
}
</code></pre>

<p>HTH,
Carlo</p>
",['table']
24994387,24998141,2014-07-28 11:34:30,Spark Cassandra connector - where clause,"<p>I am trying to do some analytics on time series data stored in cassandra by using spark and the new connector published by Datastax.</p>

<p>In my schema the Partition key is the meter ID and I want to run spark operations only on specifics series, therefore I need to filter by meter ID.</p>

<p>I would like then to run a query like: Select * from timeseries where series_id = X</p>

<p>I have tried to achieve this by doing:</p>

<pre><code>JavaRDD&lt;CassandraRow&gt; rdd = sc.cassandraTable(""test"", ""timeseries"").select(columns).where(""series_id = ?"",ids).toJavaRDD();
</code></pre>

<p>When executing this code the resulting query is:</p>

<pre><code>SELECT ""series_id"", ""timestamp"", ""value"" FROM ""timeseries"" WHERE token(""series_id"") &gt; 1059678427073559546 AND token(""series_id"") &lt;= 1337476147328479245 AND series_id = ? ALLOW FILTERING
</code></pre>

<p>A clause is automatically added on my partition key (token(""series_id"") > X AND token(""series_id"") &lt;=Y) and then mine is appended after that. This obviously does not work and I get an error saying: ""series_id cannot be restricted by more than one relation if it includes an Equal"".</p>

<p>Is there a way to get rid of the clause added automatically? Am I missing something?</p>

<p>Thanks in advance</p>
",<cassandra><apache-spark>,"<p>The driver automatically determines the partition key using table metadata it fetches from the cluster itself.  It then uses this to append the token ranges to your CQL so that it can read a chunk of data from the specific node it's trying to query.  In other words, Cassandra thinks series_id is your partition key and not meter_id.  If you run a describe command on your table, I bet you'll be surprised.</p>
",['table']
25061591,25063131,2014-07-31 14:32:13,how to get the list of all table of a key space in cassandra,"<p>I am new to cassandra, I am currently using CassandraCSharpDriver in a dummy application. I want to get the list of all the tables a user has described in a given key space.</p>

<pre><code>        Cluster cluster = Cluster.Builder().AddContactPoints(""IPAddress"").Build();
        Session session = cluster.Connect(""MyKeySpace"");
</code></pre>

<p>In this code I want to get the list of all the tables of MyKeySpace</p>
",<c#><database><cassandra><datastax><cqlsh>,"<p>I'll run through how I would prepare that list of tables with the DataStax Cassandra C# driver:</p>

<p>Connect to your cluster:</p>

<pre><code>Cluster cluster = Cluster.Builder().AddContactPoints(""IPAddress"").Build();
Session session = cluster.Connect();
</code></pre>

<p>I'll create a <code>List&lt;String&gt;</code> and use a prepared statement (because that's just a good idea) to bind the name of your keyspace.  My CQL statement is only selecting <code>columnfamily_name</code>, which should be what you need.</p>

<pre><code>List&lt;String&gt; tableList = new List&lt;String&gt;();

String strCQL = ""SELECT columnfamily_name ""
    + ""FROM system.schema_columnfamilies WHERE keyspace_name=? "";
PreparedStatement pStatement = _session.Prepare(strCQL);
BoundStatement boundStatement = new BoundStatement(pStatement);
boundStatement.Bind(""MyKeySpace"");
</code></pre>

<p>Now I'll execute the statement, iterate through the results, and add each table name to the List I created above.</p>

<pre><code>RowSet results = session.Execute(boundStatement);

foreach (Row result in results.GetRows())
{
    tableName = result.GetValue&lt;String&gt;(""columnfamily_name"");
    tableList.Add(tableName);
}
</code></pre>

<p>Now you should have a list of tables that you can add to your UI.</p>
",['table']
25084210,25100178,2014-08-01 16:00:13,Why Cassandra throwing com.datastax.driver.core.exceptions.InvalidQueryException: Multiple definitions found for column,"<p><strong>Context</strong>:</p>

<p>I am running a jUnit test in eclipse by using embedded Cassandra to test my DAO class which is using an Astyanax client configured for JavaDriver. When DAO object instance insert into Cassandra I am getting this exception com.datastax.driver.core.exceptions.InvalidQueryException: Multiple definitions found for column ..columnname</p>

<p><strong>TestClass</strong></p>

<pre><code>public class LeaderBoardDaoTest {

    private static LeaderBoardDao   dao;
    public static CassandraCQLUnit  cassandraCQLUnit;

    private String                  hostIp  = ""127.0.0.1"";
    private int                     port    = 9142;
    public Session                  session;
    public Cluster                  cluster;

    @BeforeClass
    public static void startCassandra() throws IOException, TTransportException, ConfigurationException, InterruptedException {
        System.setProperty(""archaius.deployment.applicationId"", ""leaderboardapi"");
        System.setProperty(""archaius.deployment.environment"", ""test"");

        EmbeddedCassandraServerHelper.startEmbeddedCassandra(""cassandra.yaml"");
        // cassandraCQLUnit = new CassandraCQLUnit(new
        // ClassPathCQLDataSet(""simple.cql"", ""lbapi""), ""cassandra.yaml"");
        Injector injector = Guice.createInjector(new TestModule());
        dao = injector.getInstance(LeaderBoardDao.class);

    }

    @Before
    public void load() {
        cluster = new Cluster.Builder().withClusterName(""leaderboardcassandra"").addContactPoints(hostIp).withPort(port).build();
        session = cluster.connect();
        CQLDataLoader dataLoader = new CQLDataLoader(session);
        dataLoader.load(new ClassPathCQLDataSet(""simple.cql"", ""lbapi""));
        session = dataLoader.getSession();
    }


    @Test
    public void test() {
        ResultSet result = session.execute(""select * from mytable WHERE id='myKey01'"");
        Assert.assertEquals(result.iterator().next().getString(""value""), ""myValue01"");
    }

    @Test
    public void testInsert() {
        LeaderBoard lb = new LeaderBoard();
        lb.setName(""name-1"");
        lb.setDescription(""description-1"");
        lb.setActivityType(ActivityType.FUEL);
        lb.setImage(""http:/"");
        lb.setLbId(UUID.fromString(""3F2504E0-4F89-41D3-9A0C-0305E82C3301""));
        lb.setStartTime(new Date());
        lb.setEndTime(new Date());
        dao.insert(lb);
        ResultSet resultSet = session.execute(""select * from leaderboards WHERE leaderboardid='3F2504E0-4F89-41D3-9A0C-0305E82C3301'"");
    }

    @After
    public void clearCassandra() {
        EmbeddedCassandraServerHelper.cleanEmbeddedCassandra();
    }

    @AfterClass
    public static void stopCassandra() {
        EmbeddedCassandraServerHelper.stopEmbeddedCassandra();
    }
}
</code></pre>

<p><strong>Class under test</strong></p>

<pre><code>@Singleton
public class LeaderBoardDao {

    private static final Logger                 log                 = LoggerFactory.getLogger(LeaderBoardDao.class);

    @Inject
    private AstyanaxMutationsJavaDriverClient   client;

    private static final String                 END_TIME            = ""end_time"";
    private static final String                 START_TIME          = ""start_time"";
    private static final String                 IMAGE               = ""image"";
    private static final String                 ACTIVITY_TYPE       = ""activity_type"";
    private static final String                 DESCRIPTION         = ""description"";
    private static final String                 NAME                = ""name"";
    private static final String                 LEADERBOARD_ID      = ""leaderboardID"";
    private static final String                 COLUMN_FAMILY_NAME  = ""leaderboards"";

    private ColumnFamily&lt;UUID, String&gt;          cf;

    public LeaderBoardDao() throws ConnectionException {
        cf = ColumnFamily.newColumnFamily(COLUMN_FAMILY_NAME, UUIDSerializer.get(), StringSerializer.get());
    }

    /**
     * Writes the Leaderboard to the database.
     * 
     * @param lb
     */
    public void insert(LeaderBoard lb) {
        try {
            MutationBatch m = client.getKeyspace().prepareMutationBatch();
            cf.describe(client.getKeyspace());
            m.withRow(cf, lb.getLbId()).putColumn(LEADERBOARD_ID, UUIDUtil.asByteArray(lb.getLbId()), null).putColumn(NAME, lb.getName(), null).putColumn(DESCRIPTION, lb.getDescription(), null)
                    .putColumn(ACTIVITY_TYPE, lb.getActivityType().name(), null).putColumn(IMAGE, lb.getImage()).putColumn(START_TIME, lb.getStartTime()).putColumn(END_TIME, lb.getEndTime());

            m.execute();
        } catch (ConnectionException e) {
            Throwables.propagate(e);
        }
    }

    /**
     * Reads leaderboard from database
     * 
     * @param id
     * @return {@link LeaderBoard}
     */
    public LeaderBoard read(UUID id) {
        OperationResult&lt;ColumnList&lt;String&gt;&gt; result;
        LeaderBoard lb = null;
        try {
            result = client.getKeyspace().prepareQuery(cf).getKey(id).execute();

            ColumnList&lt;String&gt; cols = result.getResult();
            if (!cols.isEmpty()) {
                lb = new LeaderBoard();
                lb.setLbId(cols.getUUIDValue(LEADERBOARD_ID, null));
                lb.setName(cols.getStringValue(NAME, null));
                lb.setActivityType(ActivityType.valueOf(cols.getStringValue(ACTIVITY_TYPE, null)));
                lb.setDescription(cols.getStringValue(DESCRIPTION, null));
                lb.setEndTime(cols.getDateValue(END_TIME, null));
                lb.setStartTime(cols.getDateValue(START_TIME, null));
                lb.setImage(cols.getStringValue(IMAGE, null));
            } else {
                log.warn(""read: is empty: no record found for "" + id);
            }

            return lb;
        } catch (ConnectionException e) {
            log.error(""failed to read from C*"", e);
            throw new RuntimeException(""failed to read from C*"", e);
        }
    }

}
</code></pre>
",<cassandra><datastax-java-driver><astyanax>,"<p>When the Java driver throws an <code>InvalidQueryException</code>, it's rethrowing an error from Cassandra. The error ""Multiple definitions found for column..."" indicates that a column is mentioned more than once in an update statement. You can simulate it in cqlsh:</p>

<pre><code>cqlsh&gt; create table test(i int primary key);
cqlsh&gt; insert into test (i, i) values (1, 2);
code=2200 [Invalid query] message=""Multiple definitions found for column i""
</code></pre>

<p>I'm not familiar with Astyanax, but my guess is that it already adds the id to the query when you call <code>withRow</code>, so you don't need to add it again with <code>putColumn</code>. Try removing that call (second line in reformatted sample below):</p>

<pre><code>m.withRow(cf, lb.getLbId())
 .putColumn(LEADERBOARD_ID, UUIDUtil.asByteArray(lb.getLbId()), null)
 ... // other putColumn calls
</code></pre>
",['table']
25098083,25112097,2014-08-02 18:21:02,nodetool repair across replicas of data center,"<p>Just want to understand the performance of 'nodetool repair' in a multi data center setup with Cassandra 2.</p>

<p>We are planning to have keyspaces with 2-4 replicas in each data center. We may have several tens of data centers. Writes are done with LOCAL_QUORUM/EACH_QUORUM consistency depending on the situation and reads are usually done with LOCAL_QUORUM consistency. Questions:</p>

<ol>
<li><p>Does nodetool repair complexity grow linearly with number of replicas across all data centers?</p></li>
<li><p>Or does nodetool repair complexity grow linearly with a combination of number of replicas in the current data center, and number of data centers? Vaguely, this model could possibly sync data with each of the individual nodes in current data center, but at EACH_QUORUM-like operation against replicas in other data centers.</p></li>
<li><p>To scale the cluster, is it better to add more nodes in an existing data center or add a new data center assuming constant number of replicas as a whole? I ask this question in the context of nodetool repair performance.</p></li>
</ol>
",<cassandra><cassandra-2.0><repair><nodetool>,"<p>To understand how nodetool repair affects the cluster or how the cluster size affects repair, we need to understand what happens during repair. There are two phases to repair, the first of which is building a Merkle tree of the data. The second is having the replicas actually compare the differences between their trees and then streaming them to each other as needed. </p>

<p>This first phase can be intensive on disk io since it will touch almost all data on the disk on the node on which you run the repair. One simple way to avoid repair touching the full disk is to use the -pr flag. When using -pr, it will disksize/RF instead of disksize data that repair has to touch. Running repair on a node also sends a message to all nodes that store replicas of any of these ranges to build merkle trees as well. This can be a problem, since all the replicas will be doing it at the same time, possibly making them all slow to respond for that portion of your data. </p>

<p>The factor which determines how the repair operation affects other data centers is the use of the replica placement strategy. Since you are going to need consistency across data centers (EACH_QOURUM cases) it is imperative that you use a cross-dc replication strategy like the Network Topology strategy in your case. For repair this will mean that you cannot limit yourself to local dc while running the repair since you have some EACH_QUORUM consistency cases. To avoid a repair affecting all replicas in all data centers, you should a) Wrap your replication strategy using Dynamic snitch and configure the badness threshold properly b) Use -snapshot option while running the repair.
What this will do is take a snapshot of your data (snapshots are just hardlinks to existing sstables, exploiting the fact that sstables are immutable, thus making snapshots extremely cheap) and sequentially repair from the snapshot. This means that for any given replica set, only one replica at a time will be performing the validation compaction, allowing the dynamic snitch to maintain performance for your application via the other replicas.</p>

<p>Now we can answer the questions you have.</p>

<ol>
<li><p>Does nodetool repair complexity grow linearly with number of replicas across all data centers?
You can limit this by wrapping your replication strategy with Dynamic snitch  and pass -snapshot option during repair.</p></li>
<li><p>Or does nodetool repair complexity grow linearly with a combination of number of replicas in the current data center, and number of data centers? Vaguely, this model could possibly sync data with each of the individual nodes in current data center, but at EACH_QUORUM-like operation against replicas in other data centers.
The complexity will grow in terms of running time with the number of replicas if you use the approach above. This is because the above approach will do a sequential repair on one replica at a time.</p></li>
<li><p>To scale the cluster, is it better to add more nodes in an existing data center or add a new data center assuming constant number of replicas as a whole? I ask this question in the context of nodetool repair performance.
From nodetool repair perspective IMO, this does not make any difference if you take the above approach. Since it depends on the overall number of replicas.</p></li>
</ol>

<p>Also, the goal of repair using nodetool is so that deletes do not come back. The hard requirement for routine repair frequency is the value of gc_grace_seconds. In systems that seldom delete or overwrite data, you can raise the value of gc_grace with minimal impact to disk space. This allows wider intervals for scheduling repair operations with the nodetool utility. One of the recommended ways to avoid frequent repairs is to have immutability of records by design. This may be important to you since you need to run on a tens of data centers and ops will otherwise already be painful.</p>
",['dc']
25102522,25103483,2014-08-03 07:20:02,"Which is write efficient ""create table with option With compact storage"" or ""create table with option With clustering order storage""?","<p>I am designing schema for a read as as well Write critical Problem statement.
Which will be more write and read efficient Create table with compact storage or create table with the Clustering order.</p>

<p>As per my requirement Clustering order helps me to safe some time during reading. but at the same time i fear that it could effect the insertion.</p>

<p>can any one tell ?</p>
",<cassandra><cql3>,"<p>Compact storage is for backwards compatibility with thrift apps..I'd recommend avoiding it. From the official docs:</p>

<blockquote>
  <p>Using compact storage¶</p>
  
  <p>The compact storage directive is used for backward compatibility of
  old applications with CQL. Use the directive to store data in the
  legacy (Thrift) storage engine format. To take advantage of CQL
  capabilities, do not use this directive in new applications.</p>
  
  <p>CREATE TABLE sblocks (   block_id uuid,   subblock_id uuid,   data
  blob,   PRIMARY KEY (block_id, subblock_id) ) WITH COMPACT STORAGE;
  Using the compact storage directive prevents you from defining more
  than one column that is not part of a compound primary key. A compact
  table using a primary key that is not compound can have multiple
  columns that are not part of the primary key.</p>
  
  <p>A compact table that uses a compound primary key must define at least
  one clustering column. Columns cannot be added nor removed after
  creation of a compact table. Unless you specify WITH COMPACT STORAGE,
  CQL creates a table with non-compact storage.¶</p>
</blockquote>
",['table']
25132887,25133059,2014-08-05 06:58:46,order by caluse not working in cassandra CQLSH,"<p>I want query similar to this</p>

<pre><code>SELECT uuid,data,name,time,tracker,type,userid FROM standardevents080406 ORDER BY userid DESC;
</code></pre>

<p>but it is not working where as simple where clause queries are working.</p>

<pre><code>SELECT uuid,data,name,time,tracker,type,userid FROM standardevents080406 where userid='64419';
</code></pre>

<p>am i doing something wrong..</p>

<p>Description of column-family is as below</p>

<pre><code>CREATE TABLE standardevents080406 (   uuid uuid PRIMARY KEY,   data text,   name text,   time text,   tracker text,   type text,   userid text ) WITH 
  bloom_filter_fp_chance=0.010000 AND   caching='KEYS_ONLY' AND   comment=''
 AND   dclocal_read_repair_chance=0.000000 AND   gc_grace_seconds=864000 AND  read_repair_chance=0.100000 AND   replicate_on_write='true'
 AND   populate_io_cache_on_flush='false' AND 
  compaction={'class': 'SizeTieredCompactionStrategy'} AND  
 compression={'sstable_compression': 'SnappyCompressor'};

CREATE INDEX time_ind ON standardevents080406 (time);

CREATE INDEX userid_ind ON standardevents080406 (userid);
</code></pre>
",<cassandra><cqlsh>,"<p>You can't perform an <code>ORDER BY</code> on a column which is not part of the clustering key.
Your table definition does not include any clustering key, but a simple primary key. Sorting, in your situation, can be performed only client-side. </p>

<p>HTH,
Carlo</p>
",['table']
25137054,25138430,2014-08-05 10:53:37,Cassandra data model for forum-type application,"<p>I am designing a database for a forum type application. I am thinking of using Cassandra (I am going to have a huge amount of data, so RDBMS might not be a viable option) but stuck at how to design the db. The main functionality would be search i.e. searching for keyword and all posts with that keyword will be returned. Search could also be by some characteristics of the user e.g reputation, geography etc.</p>

<p>What I have thought so far:</p>

<ol>
<li>Primary key: Composite of date and all other searchable features e.g. geography, reputation etc. except the words in the post so that all other features can be easily searched </li>
<li>Secondary indices: Put all words in separate columns and make all of them secondary index therefore to search for ""keyword"" I could do <code>where col1 = keyword or col2 = keyword ...</code></li>
<li>All posts with the column name = the second the message was posted (or column names = post IDs)</li>
</ol>

<p>But I think this is really a round-about way to go about it. Any better ideas?</p>

<p>Thanks!</p>
",<database-design><cassandra>,"<p>To define a schema in Cassandra is very important to know what queries you're gonna do. </p>

<p>1 - A Primary Key made of X fields doesn't mean you'll be able to query for any field of the primary key. If you define </p>

<pre><code>PRIMARY KEY(reputation, geography, category)
</code></pre>

<p>Using such a key you can not query for category unless you know both reputation and geography. You can not query for geography unless you know reputation. So, reading <strong>left-to-rigth</strong>, you can put fields in <code>WHERE</code> conditions.</p>

<pre><code>select * from abc where reputation = 'good';
select * from abc where reputation = 'good' and geography = 'usa';
select * from abc where reputation = 'good'  and geography = 'usa' and category='pizza';
</code></pre>

<p>Any other combination is not allowed. To solve this problem in cassandra you should denormalize, putting your data into different tables. For instance 3 tables with 3 different primary keys</p>

<pre><code>table info_by_category ... PRIMARY KEY(category, geography, reputation)
table info_by_reputation ... PRIMARY KEY(reputation, category, geography)
table info_by_geography PRIMARY KEY(geography, reputation, category)
</code></pre>

<p>Now you should choose which table to query based on the where conditions. If you have reputation and geography you should query table <code>info_by_geography</code>, if you have geography and category you should go to <code>info_by_category</code></p>

<p>2 - Cassandra is not suitable to perform full-text searches: to do the job you should use a full-text search engine (like solr/elasticsearch/whateveryouprefer) beside your db. More, even if you put many lookup indexes (formerly secondary idexes) you can not combine them with <code>AND</code> operator. And many L.I. is a deprecated behaviour -- if you can't use a full-text search I'd rather use a Map, putting a secondary index on Map value. Said that, you still won't be able to search for different words using <code>AND</code> operator.</p>

<p>3 - Can be easily done using a L.I.</p>

<p>HTH, Carlo</p>
",['table']
25166833,25167102,2014-08-06 17:46:54,Cassandra composite key queries,"<p>This is more of a design question. </p>

<p>Let's imagine I have this table (most columns omitted for readability):</p>

<pre><code>CREATE TABLE IF NOT EXISTS users (
userid uuid,
emailaddress text,
passwordhash text,
passwordsalt text,
datecreated timestamp,
PRIMARY KEY (userid, emailaddress)
);
</code></pre>

<p>On this table, I need to sometimes get userid by emailaddress and other times, simply use the userid to get the rest of the values, so the queries are:</p>

<ol>
<li>Search by <code>emailaddress</code>, get <code>userid</code> and use in other places</li>
<li>Search by <code>userid</code>, get all the values for specific user and use that elsewhere.</li>
</ol>

<p>Querying by only <code>userid</code> works since it's the first key in the composite key table but for querying by <code>emailaddress</code> (not knowing the userid in advance), I will have to turn on the <code>ALLOW FILTERING</code> which is strongly discouraged.</p>

<p>Question is: knowing this design is not good in terms of how Cassandra operates, how else would I achieve my goal (to get user details by EITHER <code>userid</code> OR <code>emailaddress</code>)? Should I create 2 tables such as these?</p>

<pre><code>CREATE TABLE IF NOT EXISTS users (
userid uuid,
passwordhash text,
passwordsalt text,
datecreated timestamp,
PRIMARY KEY (userid)
);

CREATE TABLE IF NOT EXISTS useremails (
emailaddress text,
userid uuid,
PRIMARY KEY (emailaddress)
);
</code></pre>

<p>Or is there a cleaner way to achieve what I want to do? I apologize in advance for my lack of understanding the ins and outs of Cassandra. I'm still learning it and trying to approach the problems from the correct angle instead of hacking (which <code>ALLOW FILTERING</code> would allow me to do).</p>

<p>Many thanks,</p>
",<cassandra>,"<p>Cassandra stores data in partitions, and the first key in the primary key definition is the partition key. You want your queries to hit a single (or at least n number of) partition(s), instead of doing a cluster wide search (which is what allow filtering is allowing). </p>

<p>Your idea of having a second table is a good one. I'd go with that. Remember, two 1ms queries is better than one 2s query :)</p>
",['table']
25183620,25213403,2014-08-07 13:17:42,What does different fields of nodetool cfstats mean?,"<p>When I am using below command.<br/>
<strong>nodetool -h localhost -p 7199 cfstats demodb</strong> I came up with the following results. I cant get upto any conclusion from  the following results.  I could not decide whether my two node clustered Cassandra is performing good or need  to be tuned. </p>

<pre><code> Keyspace: demodb
            Read Count: 81361
            Read Latency: 0.04145315323066334 ms.
            Write Count: 23114
            Write Latency: 0.06758518646707623 ms.
            Pending Tasks: 0
                    Table: schema1
                    SSTable count: 0
                    Space used (live), bytes: 0
                    Space used (total), bytes: 3560
                    SSTable Compression Ratio: 0.0
                    Number of keys (estimate): 0
                    Memtable cell count: 5686
                    Memtable data size, bytes: 3707713
                    Memtable switch count: 5
                    Local read count: 81361
                    Local read latency: 0.000 ms
                    Local write count: 23114
                    Local write latency: 0.000 ms
                    Pending tasks: 0
                    Bloom filter false positives: 0
                    Bloom filter false ratio: 0.00000
                    Bloom filter space used, bytes: 0
                    Compacted partition minimum bytes: 0
                    Compacted partition maximum bytes: 0
                    Compacted partition mean bytes: 0
                    Average live cells per slice (last five minutes): 1.0
                    Average tombstones per slice (last five minutes): 0.0 
</code></pre>
",<cassandra><nodetool>,"<p>As far as i see, I can tell you that your data in the table schema1 is still fully in memory ""SSTable count: 0"". At this point there is nothing to optimize. The statistics will be more helpful when you have more data and your in-memory state is flushed to disk. It is to early to optimize something.</p>
",['table']
25306785,25309968,2014-08-14 11:19:06,"How to reset Cassandra superuser, when Cassandra does not know 'cassandra' default user?","<p>How to reset default Cassandra credentials without changing source code?</p>

<p>I have check similar problems like <a href=""https://stackoverflow.com/questions/18398987/how-to-reset-a-lost-cassandra-admin-users-password"">How to reset a lost Cassandra admin user&#39;s password?</a>.
 I have three node cluster of Datastax Cassandra 2.0.8 and I am trying to implement authentication. I have set cassandra.yaml in all nodes and restarted them. Problem is that I still cannot login in to cqlsh.</p>

<p>I have also tried to reset password for cassandra user in cqlsh(I have disabled authentication for that):</p>

<pre><code>update system_auth.credentials set salted_hash='$2a$10$vbfmLdkQdUz3Rmw.fF7Ygu6GuphqHndpJKTvElqAciUJ4SZ3pwquu' where username='cassandra';
</code></pre>

<p>In logs there is Info about creating cassandra superuser. I have checked keyspace system_auth and it includes credentials,permissions and users. And credentials column family does contain user cassandra:</p>

<pre><code>cqlsh&gt; use system_auth;
cqlsh:system_auth&gt; select * from credentials;

 username  | options | salted_hash
-----------+---------+----------------------------------------------------------                                ----
 cassandra |    null | $2a$10$vbfmLdkQdUz3Rmw.fF7Ygu6GuphqHndpJKTvElqAciUJ4SZ3pw                                quu

(1 rows)
</code></pre>

<p>But still, when I try:</p>

<pre><code>./cqlsh -u cassandra -p cassandra
</code></pre>

<p>I get exception, that user does not exists, but I dont have permissions to create one.</p>

<pre><code>cql.cassandra.ttypes.AuthenticationException: AuthenticationException(why=""User cassandra doesn't exist - create it with CREATE USER query first"")
</code></pre>
",<authentication><cassandra>,"<p>I don't know for sure, but there's a good chance that the hash you used above changes with each version, and may be particular to a specific version of Cassandra.  With that in-mind, you could (in-theory) install the same version in a VM, and then query that machine's <code>system_auth.credentials</code> for the cassandra user's <code>salted_hash</code>.  Had it not been for the question you linked above, I never would have thought to try that.</p>

<p>Otherwise, this next option <strong>WILL</strong> work.</p>

<ol>
<li>Stop your Cassandra cluster.</li>
<li><p>On each node, <code>cd</code> down to your <code>data</code> directory, and execute: </p>

<p><code>$ mv system_auth system_auth_20140814</code></p></li>
<li><p>Restart each node.</p></li>
</ol>

<p>As long as the authenticator is still set (in your cassandra.yaml) to use the <code>PasswordAuthenticator</code>, Cassandra will rebuild the <code>system_auth</code> keyspace, with the default Cassandra super user, which you can use with <code>cqlsh</code> to get back in.</p>

<pre><code>$ ./cqlsh -u cassandra -p cassandra
Connected to MyCluster at 127.0.0.1:9042.
[cqlsh 5.0.1 | Cassandra 2.1.0-rc5-SNAPSHOT | CQL spec 3.2.0 | Native protocol v3]
Use HELP for help.
cqlsh&gt;
</code></pre>

<p>Notes:</p>

<ul>
<li>You will have to re-add all of your users, and re-apply all off their permissions.</li>
<li>Instead of renaming (<code>mv</code>) the <code>system_auth</code> directory, you could also just delete it (<code>rm</code>).</li>
<li>You will have to re-apply the appropriate replication settings to your <code>system_auth</code> keyspace.  By default, <code>system_auth</code> only has a replication factor of 1.</li>
</ul>
",['authenticator']
25346597,25353520,2014-08-17 05:42:01,Which Cassandra partitioner is better: Random or Murmur3 (in terms of throughput) and what is the difference between them?,"<p>What difference the choice of partitioners could bring in my Cassandra throughput and latency? I have gone through all three partitioners and one thing I noticed is that <code>ByteOrdered</code> partitioner has overhead so I do not use it. Now I am a bit split between <code>Random</code> and <code>Murmur3</code> partitioners.</p>
",<cassandra>,"<p>The main difference between the two, is in how each generates the token hash values.  The Random partitioner used the JDK native MD5 hash (because it was both convenient for the developers and standard across all JDKs).  But since Cassandra really doesn't need a cryptographic hash, that function took much longer than it needed to.</p>

<p>With the Murmur3 partitioner, the token hashing does only what Cassandra needs it to do.  Which, is to generate a token ensuring even distribution across the nodes.  This results in an improvement of 3 to 5 times in token hashing performance, which untimately translates into the overall 10% gain that Carlo mentioned above.</p>

<p>It should also be noted that DataStax warns that the partitioners are not compatible.  Which means, that once you start with one partitioner, you cannot (easily) convert to the other.  Therefore, I would pick the newer, slightly faster Murmur3 partitioner.</p>
",['partitioner']
25449640,25458653,2014-08-22 14:40:03,Efficient modeling of versioned hierarchies in Cassandra,"<p><strong>Disclaimer:</strong><br>
This is quite a long post. I first explain the data I am dealing with, and what I want to do with it.<br>
Then I detail three possible solutions I have considered, because I've tried to do my homework (I swear :]). I end up with a ""best guess"" which is a variation of the first solution.</p>

<p><strong>My ultimate question is:</strong> what's the most sensible way to solve my problem using Cassandra? Is it one of my attempts, or is it something else?<br>
I am looking for advice/feedback from experienced Cassandra users...</p>

<p><strong>My data:</strong><br>
I have many SuperDocuments that own Documents in a tree structure (headings, subheadings, sections, …).</p>

<p>Each SuperDocument structure can change (renaming of headings mostly) over time, thus giving me multiple versions of the structure as shown below.</p>

<p><img src=""https://i.stack.imgur.com/TS5QX.png"" alt=""superdocument versions""></p>

<p><strong>What I'm looking for:</strong><br>
For each SuperDocument I need to timestamp those structures by date as above and I'd like, for a given date, to find the closest earlier version of the SuperDocument structure. (ie. the most recent version for which <code>version_date &lt; given_date</code>)</p>

<p>These considerations might help solving the problem more easily:</p>

<ul>
<li>Versions are immutable: changes are rare enough, I can create a new representation of the whole structure each time it changes.</li>
<li>I do not need to access a subtree of the structure.</li>
<li>I'd say it is OK to say that I do not need to find all the ancestors of a given leaf, nor do I need to access a specific node/leaf inside the tree. I can work all of this out in my client code once I have the whole tree.</li>
</ul>

<p><strong>OK let's do it</strong><br>
<em>Please keep in mind I am really just starting using Cassandra. I've read/watched a lot of resources about data modeling, but haven't got much (any!) experience in the field!<br>
Which also means everything will be written in CQL3... sorry Thrift lovers!</em></p>

<p>My first attempt at solving this was to create the following table:</p>

<pre><code>CREATE TABLE IF NOT EXISTS superdoc_structures (
    doc_id varchar,
    version_date timestamp,
    pre_pos int,
    post_pos int,
    title text,

    PRIMARY KEY ((doc_id, version_date), pre_pos, post_pos)

) WITH CLUSTERING ORDER BY (pre_pos ASC);
</code></pre>

<p>That would give me the following structure:</p>

<p><img src=""https://i.stack.imgur.com/IUnoc.png"" alt=""enter image description here""></p>

<p>I'm using a <a href=""http://en.wikipedia.org/wiki/Nested_set_model"" rel=""noreferrer"">Nested Sets model</a> for my trees here; I figured it would work well to keep the structure ordered, but I am open to other suggestions.</p>

<p>I like this solution: each version has its own row, in which each column represents a level of the hierarchy.<br>
The problem though is that I (candidly) intended to query my data as follows:</p>

<pre><code>SELECT * FROM superdoc_structures 
    WHERE doc_id=""3399c35...14e1"" AND version_date &lt; '2014-03-11' LIMIT 1
</code></pre>

<p>Cassandra quickly reminded me I was not allowed to do that! (because the partitioner does not preserve row order on the cluster nodes, so it is not possible to scan through partition keys)</p>

<p><strong>What then...?</strong><br>
Well, because Cassandra won't let me use inequalities on partition keys, so be it!<br>
I'll make <code>version_date</code> a clustering key and all my problems will be gone. Yeah, not really...</p>

<p>First try:</p>

<pre><code>CREATE TABLE IF NOT EXISTS superdoc_structures (
    doc_id varchar,
    version_date timestamp,
    pre_pos int,
    post_pos int,
    title text,

    PRIMARY KEY (doc_id, version_date, pre_pos, post_pos)

) WITH CLUSTERING ORDER BY (version_date DESC, pre_pos ASC);
</code></pre>

<p>I find this one less elegant: all versions <strong>and</strong> structure levels are made into columns of a now very wide row (compared to my previous solution):</p>

<p><img src=""https://i.stack.imgur.com/yYDSA.png"" alt=""second modeling attempt""></p>

<p>Problem: with the same request, using <code>LIMIT 1</code> will only return the first heading. And using no <code>LIMIT</code> would return all versions structure levels, which I would have to filter to only keep the most recent ones.</p>

<p>Second try:</p>

<p>there's no second try yet... I have an idea though, but I feel it's not using Cassandra wisely.</p>

<p>The idea would be to cluster by <code>version_date</code> only, and <em>somehow</em> store whole hierarchies in each column values. Sounds bad doesn't it?</p>

<p>I would do something like this:</p>

<pre><code>CREATE TABLE IF NOT EXISTS superdoc_structures (
    doc_id varchar,
    version_date timestamp,
    nested_sets map&lt;int, int&gt;,
    titles list&lt;text&gt;,

    PRIMARY KEY (doc_id, version_date)

) WITH CLUSTERING ORDER BY (version_date DESC);
</code></pre>

<p>The resulting row structure would then be:</p>

<p><img src=""https://i.stack.imgur.com/TgKqG.png"" alt=""third modeling attempt""></p>

<p>It looks kind of all right to me in fact, but I will probably have more data than the level title to de-normalize into my columns. If it's only two attributes, I could go with another map (associating titles with ids for instance), but more data would lead to more lists, and I have the feeling it would quickly become an anti-pattern.<br>
Plus, I'd have to merge all lists together in my client app when the data comes in!</p>

<p><strong>ALTERNATIVE &amp; BEST GUESS</strong><br>
After giving it some more thought, there's an ""hybrid"" solution that might work and may be efficient and elegant:</p>

<p>I could use another table that would list only the version dates of a SuperDocument &amp; cache these dates into a Memcache instance (or Redis or whatever) for real quick access.<br>
That would allow me to quickly find the version I need to fetch, and then request it using the composite key of my first solution.</p>

<p>That's two queries, plus a memory cache store to manage. But I may end up with one anyway, so maybe that'd be the best compromise?<br>
Maybe I don't even need a cache store?</p>

<p>All in all, I really feel the first solution is the most elegant one to model my data. What about you?!</p>
",<cassandra><data-modeling><hierarchical-data><cassandra-2.0>,"<p>First, you don't need to use memcache or redis. Cassandra will give you very fast access to that information. You could certainly have a table that was something like:</p>

<pre><code>create table superdoc_structures {
    doc_id varchar;
    version_date timestamp;
    /* stuff */
    primary key (doc_id, version_date)
} with clustering order by (version_date desc);
</code></pre>

<p>which would give you a quick way to access a given version (this query may look familiar ;-):</p>

<pre><code>select * from superdoc_structures 
    where doc_id=""3399c35...14e1"" and
        version_date &lt; '2014-03-11'
    order by version_date desc
    limit 1;
</code></pre>

<p>Since nothing about the document tree structure seems to be relevant from the schema's point of view, and you are happy as a clam to create the document in its entirety every time there is a new version, I don't see why you'd even bother breaking out the tree in to separate rows. Why not just have the entire document in the table as a text or blob field?</p>

<pre><code>create table superdoc_structures {
    doc_id varchar;
    version_date timestamp;
    contents text;
    primary key (doc_id, version_date)
} with clustering order by (version_date desc);
</code></pre>

<p>So to get the contents of the document as existed at the new year, you'd do:</p>

<pre><code>select contents from superdoc_structures
where doc_id=""...."" and 
    version_date &lt; '2014-01-1'
order by version_date &gt; 1
</code></pre>

<p>Now, if you <em>did</em> want to maintain some kind of hierarchy of the document components, I'd recommend doing something like a <a href=""http://technobytz.com/closure_table_store_hierarchical_data.html"" rel=""noreferrer"">closure table</a> table to represent it. Alternatively, since you are willing to copy the entire document on each write anyway, why not copy the entire section info on each write, why not do so and have a schema like:</p>

<pre><code>create table superdoc_structures {
    doc_id varchar;
    version_date timestamp;
    section_path varchar;
    contents text;
    primary key (doc_id, version_date, section_path)
) with clustering order by (version_date desc, section_path asc);
</code></pre>

<p>Then have section path have a syntax like, ""first_level next_level sub_level leaf_name"". As a side benefit, when you have the version_date of the document (or if you create a secondary index on section_path), because a space is lexically ""lower"" than any other valid character, you can actually grab a subsection very cleanly:</p>

<pre><code>select section_path, contents from superdoc_structures
where doc_id = '....' and
    version_date = '2013-12-22' and
    section_path &gt;= 'chapter4 subsection2' and
    section_path &lt; 'chapter4 subsection2!';
</code></pre>

<p>Alternatively, you can store the sections using Cassandra's support for collections, but again... I'm not sure why you'd even bother breaking them out as doing them as one big chunk works just great.</p>
",['table']
25465904,25466945,2014-08-23 20:07:08,How can I restore Cassandra snapshots?,"<p>I'm building a backup and restore process for a Cassandra database so that it's ready when I need it, and so that I understand the details in order to build something that will work for production. I'm following Datastax's instructions here:</p>

<p><a href=""http://www.datastax.com/documentation/cassandra/2.0/cassandra/operations/ops_backup_restore_c.html"" rel=""noreferrer"">http://www.datastax.com/documentation/cassandra/2.0/cassandra/operations/ops_backup_restore_c.html</a>.</p>

<p>As a start, I'm seeding the database on a dev box then attempting to make the backup/restore work. Here's the backup script:</p>

<pre><code>#!/bin/bash

cd /opt/apache-cassandra-2.0.9
./bin/nodetool clearsnapshot -t after_seeding makeyourcase
./bin/nodetool snapshot -t after_seeding makeyourcase

cd /var/lib/
tar czf after_seeding.tgz cassandra/data/makeyourcase/*/snapshots/after_seeding
</code></pre>

<p>Yes, tar is not the most efficient way, perhaps, but I'm just trying to get something working right now. I've checked the tar, and all the files are there.</p>

<p>Once the database is backed up, I shut down Cassandra and my app, then <code>rm -rf /var/lib/cassandra/</code> to simulate a complete loss.</p>

<p>Now to restore the database. Restoration ""Method 2"" from <a href=""http://www.datastax.com/documentation/cassandra/2.0/cassandra/operations/ops_backup_snapshot_restore_t.html"" rel=""noreferrer"">http://www.datastax.com/documentation/cassandra/2.0/cassandra/operations/ops_backup_snapshot_restore_t.html</a> is more compatible with my schema-creation component than Method 1.</p>

<p>So, Method 2/Step 1, ""Recreate the schema"": Restart Cassandra, then my app. The app is built to re-recreate the schema on startup when necessary. Once it's up, there's a working Cassandra node with a schema for the app, but no data.</p>

<p>Method 2/Step 2 ""Restore the snapshot"": They give three alternatives, the first of which is to use sstableloader, documented at <a href=""http://www.datastax.com/documentation/cassandra/2.0/cassandra/tools/toolsBulkloader_t.html"" rel=""noreferrer"">http://www.datastax.com/documentation/cassandra/2.0/cassandra/tools/toolsBulkloader_t.html</a>. The folder structure that the loader requires is nothing like the folder structure created by the snapshot tool, so everything has to be moved into place. Before going to all that trouble, I'll just try it out on one table:</p>

<pre><code>&gt;./bin/sstableloader makeyourcase/users
Error: Could not find or load main class org.apache.cassandra.tools.BulkLoader
</code></pre>

<p>Hmmm, well, that's not going to work. BulkLoader is in ./lib/apache-cassandra-2.0.9.jar, but the loader doesn't seem to be set up to work out of the box. Rather than debug the tool, let's move on to the second alternative, copying the snapshot directory into the makeyourcase/users/snapshots/ directory. This should be easy, since we're throwing the snapshot directory right back where it came from, so <code>tar xzf after_seeding.tgz</code> should do the trick:</p>

<pre><code>cd /var/lib/
tar xzf after_seeding.tgz
chmod -R u+rwx cassandra/data/makeyourcase
</code></pre>

<p>and that puts the snapshot directories back under their respective 'snapshots' directories, and a refresh should restore the data:</p>

<pre><code>cd /opt/apache-cassandra-2.0.9
./bin/nodetool refresh -- makeyourcase users
</code></pre>

<p>This runs without complaint. Note that you have to run this for each and every table, so you have to generate the list of tables first. But, before we do that, note that there's something interesting in the Cassandra logs:</p>

<pre><code>INFO 14:32:26,319 Loading new SSTables for makeyourcase/users...
INFO 14:32:26,326 No new SSTables were found for makeyourcase/users
</code></pre>

<p>So, we put the snapshot back, but Cassandra didn't find it. I also tried moving the snapshot directory under the existing SSTables directory, and copying the old SSTable files into the existing directory, with the same error in the log. Cassandra doesn't log where it expects to find them, just that it can't find them. The docs say to put them into a directory named data/keyspace/table_name-UUID, but there is no such directory. There is one named data/makeyourcase/users/snapshots/1408820504987-users/, but putting the snapshot dir there, or the individual files, didn't work.</p>

<p>The third alternative, the ""Node restart method"" doesn't look suitable for a multi-node production environment, so I didn't try that.</p>

<p>Edit:</p>

<p>Just to make this perfectly explicit for the next person, here are the preliminary, working backup and restore scripts that apply the accepted answer.</p>

<p>myc_backup.sh:</p>

<pre><code>#!/bin/bash

cd ~/bootstrap/apache-cassandra-2.0.9
./bin/nodetool clearsnapshot -t after_seeding makeyourcase
./bin/nodetool snapshot -t after_seeding makeyourcase

cd /var/lib/
tar czf after_seeding.tgz cassandra/data/makeyourcase/*/snapshots/after_seeding
</code></pre>

<p>myc_restore.sh:</p>

<pre><code>#!/bin/bash

cd /var/lib/
tar xzf after_seeding.tgz
chmod -R u+rwx cassandra/data/makeyourcase

cd ~/bootstrap/apache-cassandra-2.0.9
TABLE_LIST=`./bin/nodetool cfstats makeyourcase | grep ""Table: "" | sed -e 's+^.*: ++'`
for TABLE in $TABLE_LIST; do
    echo ""Restore table ${TABLE}""
    cd /var/lib/cassandra/data/makeyourcase/${TABLE}
    if [ -d ""snapshots/after_seeding"" ]; then
        cp snapshots/after_seeding/* .
        cd ~/bootstrap/apache-cassandra-2.0.9
        ./bin/nodetool refresh -- makeyourcase ${TABLE}
        cd /var/lib/cassandra/data/makeyourcase/${TABLE}
        rm -rf snapshots/after_seeding
        echo ""    Table ${TABLE} restored.""
    else
        echo ""    &gt;&gt;&gt; Nothing to restore.""
    fi
done
</code></pre>
",<cassandra><datastax>,"<p>Added more details:</p>

<p>You can run the snapshot for your particular keyspace using:</p>

<pre><code>$ nodetool snapshot &lt;mykeyspace&gt; -t &lt;SnapshotDirectoryName&gt;
</code></pre>

<p>This will create the snapshot files inside the snapshots directory in data.</p>

<p>When you delete your data, <strong>make sure you don't delete the snapshots folder or you will not be able to restore it (unless you are moving it to another location / machine.)</strong></p>

<pre><code>$ pwd
/var/lib/cassandra/data/mykeyspace/mytable
$ ls
mykeyspace-mytable-jb-2-CompressionInfo.db mykeyspace-mytable-jb-2-Statistics.db
mykeyspace-mytable-jb-2-Data.db mykeyspace-mytable-jb-2-Filter.db mykeyspace-mytable-jb-2-Index.db
mykeyspace-mytable-jb-2-Summary.db mykeyspace-mytable-jb-2-TOC.txt snapshots


$ rm *
rm: cannot remove `snapshots': Is a directory
</code></pre>

<p>Once you are ready to restore, copy back the snapshot data into the keyspace/table directory (one for each table):</p>

<pre><code>$ pwd
/var/lib/cassandra/data/mykeyspace/mytable
$ sudo cp snapshots/&lt;SnapshotDirectoryName&gt;/* .
</code></pre>

<p>You mentioned:</p>

<blockquote>
  <p>and that puts the snapshot directories back under their respective 'snapshots' directories, and a refresh >should restore the data:</p>
</blockquote>

<p>I think the issue is that you are restoring the Snapshot data into the snapshot directory. It should go right in the table directory. Everything else seems right, let me know.</p>
",['table']
25483308,25487398,2014-08-25 09:55:10,Deleting all rows from Cassandra cql table,"<p>Is there a command to all the rows present in a cql table in cassandra like the one in sql?</p>

<pre><code>delete from TABLE
</code></pre>

<p>Going by the documentation, I don't find any way to perform delete operation without a where condition.</p>

<pre><code>DELETE col1 FROM SomeTable WHERE userID = 'some_key_value'; 
</code></pre>
",<cassandra><cql>,"<p>To remove all rows from a CQL Table, you can use the <a href=""http://www.datastax.com/documentation/cql/3.1/cql/cql_reference/truncate_r.html"">TRUNCATE</a> command:</p>

<pre><code>TRUNCATE keyspace_name.table_name;
</code></pre>

<p>Or if you are already using the keyspace that contains your target table:</p>

<pre><code>TRUNCATE table_name;
</code></pre>

<p>Important to note, but by default Cassandra creates a snapshot of the table just prior to TRUNCATE.  Be sure to clean up old snapshots, or set <code>auto_snapshot: false</code> in your cassandra.yaml.</p>
",['table']
25515602,25516704,2014-08-26 21:41:45,What do you do change in the such a data model Cassandra?,"<p>I have task to create a social feed(news feed). I think no need to explain the standard functionality - all are how as FB.
I chose solution <em>apache cassandra</em> and designed a data column <strong>Posts</strong> for storing information about posts users:</p>

<pre><code>CREATE TABLE Posts (
  post_id  uuid,
  post_at  timestamp,
  user_id  text,
  name     varchar,
  category set&lt;text&gt;,
  link     varchar,
  image    set&lt;varchar&gt;,
  video    set&lt;varchar&gt;,
  content  map&lt;text, text&gt;,
  private  boolean,

  PRIMARY KEY ((post_id, user_id), post_at)
)
WITH CLUSTERING ORDER BY (post_at DESC) COMPACT STORAGE;
</code></pre>

<p>The next table contains id user posts:</p>

<pre><code>CREATE TABLE posts_user (
  post_id  bigint,
  post_at  timestamp,
  user_id  bigint,
  PRIMARY KEY ((post_id), post_at, user_id)
)

WITH CLUSTERING ORDER BY (post_at DESC) AND COMPACT STORAGE;
</code></pre>

<p>How do you think, is it good? What do you do change in the such a data model?</p>
",<cassandra><cassandra-2.0><cassandra-cli>,"<p>There are a couple of questions and a couple of improvements that jump out.</p>

<ol>
<li><p>COMPACT STORAGE is deprecated now (if you want to take advantage of CQL 3 features).  I do not think that you can create your table <code>Posts</code> as you have defined above since it uses CQL 3 features (collections) with COMPACT STORAGE as well as declaring more than one column that is not part of the primary key.</p></li>
<li><p><code>posts_user</code> has completely different key types than <code>Posts</code> does.  I am not clear on what the relationship between the two tables is, but I imagine that <code>post_id</code> is supposed to be consistent between them, whereas you have it as a <code>uuid</code> in one table and a <code>bigint</code> in the other.  There are also discrepancies with the other fields.</p></li>
<li><p>Assuming <code>post_id</code> is unique and represents the id of an individual post, it is strange to have it as the first part of a compound primary key in the <code>Posts</code> table since if you know the <code>post_id</code> then you can already uniquely access the record. Furthermore, as it is part of the partition key it also prevents you from doing wider selects of multiple posts and taking advantage of your <code>post_at</code> ordering.</p></li>
</ol>

<p>The common method to fix this is to create a dedicated index table to sort the data the way you want.</p>

<p>E.g.</p>

<pre><code>CREATE TABLE posts (
  id       uuid,
  created  timestamp,
  user_id  uuid,
  name     text,
  ...
  PRIMARY KEY (id)
);

CREATE TABLE posts_by_user_index (
  user_id    uuid,
  post_id    uuid,
  post_at    timestamp,
  PRIMARY KEY (user_id,post_at,post_id)
  WITH CLUSTERING ORDER BY (post_at DESC)
);
</code></pre>

<p>Or more comprehensively:</p>

<pre><code>CREATE TABLE posts_by_user_sort_index (
  user_id    uuid,
  post_id    uuid,
  sort_field text,
  sort_value text,
  PRIMARY KEY ((user_id,sort_field),sort_value,post_id)
);
</code></pre>

<p>However, in your case if you only wish to select the data one way, then you can get away with using your <code>posts</code> table to do the sorting:</p>

<pre><code>CREATE TABLE posts (
  id       uuid,
  post_at  timestamp,
  user_id  uuid,
  name     text,
  ...
  PRIMARY KEY (user_id,post_at,id)
  WITH CLUSTERING ORDER BY (post_at DESC)
);
</code></pre>

<p>It will just make it more complicated if you wish to add additional indexes later since you will need to index each post not just by its post id, but by its user and post_at fields as well.</p>
",['table']
25554843,25558890,2014-08-28 17:39:23,How to perform intersection operation on two datasets in Key-Value store?,"<p>Let's say I have 2 datasets, one for rules, and the other for values.</p>

<p>I need to filter the values based on rules.</p>

<p>I am using a Key-Value store (couchbase, cassandra etc.). I can use multi-get to retrieve all the values from one table, and all rules for the other one, and perform validation in a loop.</p>

<p>However I find this is very inefficient. I move massive volume of data (values) over the network, and the client busy working on filtering.</p>

<p>What is the common pattern for finding the intersection between two tables with Key-Value store?</p>
",<cassandra><redis><couchbase><key-value-store><nosql>,"<p>The idea behind the nosql data model is to write data in a denormalized way so that a table can answer to a precise query. To make an example imagine you have reviews made by customers on shops. You need to know the reviews made by a user on shops and also reviews received by a shop. This would be modeled using two tables</p>

<p>ShopReviews<br>
UserReviews</p>

<p>In the first table you query by shop id in the second by user id but data are written twice and accessed directly using just a key access.</p>

<p>In the same way you should organize values by rules (can't be more precise without knowing what's the relation between them) and so on. One more consideration: newer versions of nosql db supports collections which might help to model 1 to many relations.</p>

<p>HTH, Carlo</p>
",['table']
25580848,25582158,2014-08-30 09:16:30,Non-EQ relation error Cassandra - how fix primary key?,"<p>I created a one table <strong>posts</strong>. When I make request SELECT:</p>

<pre><code>return $this-&gt;db-&gt;query('SELECT * FROM ""posts"" WHERE ""id"" IN(:id) LIMIT '.$this-&gt;limit_per_page, ['id' =&gt; $id]);
</code></pre>

<p>I get error:</p>

<blockquote>
  <p>PRIMARY KEY column ""id"" cannot be restricted (preceding column
  ""post_at"" is either not restricted or by a non-EQ relation)</p>
</blockquote>

<p>My table dump is:</p>

<pre><code>CREATE TABLE posts (
  id       uuid,
  post_at  timestamp,
  user_id  bigint,
  name     text,
  category set&lt;text&gt;,
  link     varchar,
  image    set&lt;varchar&gt;,
  video    set&lt;varchar&gt;,
  content  map&lt;text, text&gt;,
  private  boolean,
  PRIMARY KEY (user_id,post_at,id)
  )
  WITH CLUSTERING ORDER BY (post_at DESC);
</code></pre>

<p>I read some article about PRIMARY AND CLUSTER KEYS, and understood, when there are some primary keys - I need use operator <strong>=</strong> with <strong>IN</strong>. In my case, i can not use a one PRIMARY KEY. What you advise me to change in table structure, that error will disappear?</p>
",<cassandra><cassandra-2.0><cassandra-cli>,"<p>My dummy table structure</p>

<pre><code>CREATE TABLE posts (
  id       timeuuid,
  post_at  timestamp,
  user_id  bigint,
  PRIMARY KEY (id,post_at,user_id)
  )
  WITH CLUSTERING ORDER BY (post_at DESC);
</code></pre>

<p>And after inserting some dummy data</p>

<p>I ran query <code>select * from posts where id in (timeuuid1,timeuuid2,timeuuid3);</code></p>

<p>I was using cassandra 2.0 with cql 3.0</p>
",['table']
25594527,25607714,2014-08-31 17:33:46,How to build a sorted list of ranks from Cassandra table?,"<p>I store my data in a single Cassandra 2.0.10 table. There is one column (named <code>score</code>), type of integer, can take any values. I need to write a background job that would assign a value to another column, <code>rank</code>, giving value 1 for the row with the highest value in the score field, value 2 for the next to highest and so on. The row with the smallest <code>score</code> value must get the the total row count assigned to the <code>rank</code>. It is currently defined in CQL as</p>

<pre><code>CREATE TABLE players
    (user int, rank int, score int, details blob, PRIMARY KEY(user))
</code></pre>

<p>Bet it something like PostgreSQL, I would do something like</p>

<pre><code>select id, rank from players order by score desc offset A limit 100;
</code></pre>

<p>using increasing values for A and this way iterating the database in pages of size 100. It would give me top 100 players in one query, top 100 to 200 in the second, etc. Then I can fire update statements by id, one by one or in batches.</p>

<p>When I try to do the same in Cassandra CQL, turns out that many needed features are not supported (no order, not offset, no clear way how to visit all rows). I tried to build the index for the score column but this was not helpful.</p>

<p>This rank assignment is a helper job. It is no problem for it to take days or even weeks to iterate. It is ok to have it slightly inconsistent as scores may change while the job is running. It is not the main feature of the application. The main features do not use ranges queries and Cassandra works well there. </p>

<p>Is it possible to implement this rank assignment combining Java and CQL somehow or the limitations are severe enough I need to use a different database engine?</p>
",<sorting><indexing><cassandra><columnsorting>,"<p>As per my experience, Cassandra does not fit well for such types of tasks. You can definitely make this working, but the solution will not be simple and effective. There is no problem to iterate over all rows in one table to update ranks, however there is a problem to iterate all the rows in the order of your ranks. You could potentially keep two tables:</p>

<p>players(id, rank) and rank_to_id(rank, id_list). Then you should query the second page using:</p>

<p>select * from rank_to_id where rank > 100 limit 100</p>

<p>The responsibility of your rank assigner will be to update both tables correctly when rank is changing. Basically by this you will implement a simple database index which PostgreSQL has out of the box.</p>

<p>Also I'd recommended you to take a look at Redis DB instead. It has such a great data type as Sorted Set which implements almost exactly what you need: <a href=""http://redis.io/commands#sorted_set"" rel=""nofollow"">http://redis.io/commands#sorted_set</a>. However it depends on the data volume you have. Redis is in-memory database.</p>

<p>PostgreSQL also might be a good solution. Why don't you want to use it?</p>
",['table']
25597243,25600441,2014-08-31 23:38:04,Cassandra database schema and select IN issue,"<p>I have following database schema </p>

<pre><code>CREATE TABLE sensor_info_table 
(
  asset_id text,
  event_time timestamp,
  ""timestamp"" timeuuid,
  sensor_reading map&lt;text, text&gt;,
  sensor_serial_number text,
  sensor_type int,
  PRIMARY KEY ((asset_id), event_time, ""timestamp"")
);

CREATE INDEX event_time_index ON sensor_info_table (event_time);

CREATE INDEX timestamp_index ON sensor_info_table (""timestamp"");
</code></pre>

<p>Now I am able to insert the data into this table however I am unable to do following query where I want to select items with specific timeuuid values. It gives me following error. </p>

<pre><code>SELECT * from mydb.sensor_info_table where timestamp IN ( bfdfa614-3166-11e4-a61d-b888e30f5d17 , bf4521ac-3166-11e4-87a3-b888e30f5d17) ;
</code></pre>

<blockquote>
  <p>Bad Request: PRIMARY KEY column ""timestamp"" cannot be restricted (preceding column ""event_time"" is either not restricted or by a non-EQ relation)</p>
</blockquote>

<p>What do I have to do to make this work? Below is software version info.</p>

<p>show VERSION ;</p>

<pre><code>[cqlsh 4.1.1 | Cassandra 2.0.9 | CQL spec 3.1.1 | Thrift protocol 19.39.0]
</code></pre>

<p>I really don't understand what the error message preceding column ""event_time"" is either not restricted or by no-EQ relation?</p>

<p>-Subodh</p>
",<cassandra><cassandra-2.0>,"<p>You can't make it work like this. <code>IN</code> predicate on lookup indexed columns is not supported.
So you won't be able to use <code>IN</code> on ""timestamp"" column. You could use <code>IN</code> if you also provide both asset_id and event_time -- that's what cqlsh is telling you with the message </p>

<blockquote>
  <p>Bad Request: PRIMARY KEY column ""timestamp"" cannot be restricted
  (preceding column ""event_time"" is either not restricted or by a non-EQ
  relation)</p>
</blockquote>

<p>The solution for your problem is denormalization. What happens under the hood when creating a  index is that Cassandra will create and handle for you a new table having the indexed as partition key -- so Cassandra has already denormalized and duplicated your data. Remove your index and create a new table </p>

<pre><code>CREATE TABLE sensor_info_table_by_timestamp
(
  asset_id text,
  event_time timestamp,
  ""timestamp"" timeuuid,
  sensor_reading map&lt;text, text&gt;,
  sensor_serial_number text,
  sensor_type int,
  PRIMARY KEY (""timestamp"", asset_id, event_time )
);
</code></pre>

<p>Now anytime you write data you put in both tables (you can use a batch).
And your query will be</p>

<pre><code>SELECT * from mydb.sensor_info_table_by_timestamp where timestamp IN ( bfdfa614-3166-11e4-a61d-b888e30f5d17 , bf4521ac-3166-11e4-87a3-b888e30f5d17) ;
</code></pre>

<p>I see you also created a lookup index on event_time -- if you have to perform query using <code>IN</code> predicate you will have to create a third table ...</p>

<p>HTH,
Carlo</p>
",['table']
25615978,25617670,2014-09-02 04:41:58,Not quite clear about a Cassandra's anti-pattern,"<p>Suppose,there is a table with the following structure:</p>

<pre><code>create table cities (
  root text,
  name text,
  primary key(root,name)
) with clustering order by (name asc); -- for getting them sorted

insert into cities(root,name) values('.','Moscow');
insert into cities(root,name) values('.','Tokio');
insert into cities(root,name) values('.','London');

select * from cities where root='.'; -- get'em sorted asc
</code></pre>

<p>When specifying the replication factor of 3 for the keyspace and using RandomPartitioner,there will be 3 replicas of each row on 3 nodes: the main node determined for storing by the row's hash and 2 next ones. Why should there be a hotspot? Reading from all replicas is not load balanced?</p>
",<cassandra>,"<p>Definining such a table the partition key is <code>root</code> while <code>name</code> is a clustering key.
As the name suggest, partition is responsible for partitioning -- how partitioning work?
Let's say you have 4 nodes cluster -- and we have an hash function that generates only 8 keys, (A,B,C,D,E,F,G,H) -- here is how hashes are distributed in the cluster</p>

<p>node 1 - (A,B)<br>
node 2 - (C,D)<br>
node 3 - (E,F)<br>
node 4 - (G,H)<br></p>

<p>each node will use as replica's the following 2, so replica for node 1 are (2,3), replica for node 2 are (3,4), replica for node 3 are (4,1) and finally replica for node 4 are (1,2).</p>

<p>Let's say our function <code>hash(root)</code>, when root value is <code>.</code> returns <code>B</code> that belongs to node 1 -- node 1 will store the information and nodes (2,3) will store the replica. Node 4 is <strong>NEVER</strong> involved into <code>cities</code> table, it will not contain any data concerning this table (exception made for hints situations which are not part of the concept) because of the fix partition key. In this example you use about 75% of your cluster which may look like an acceptable situation ... let's say in one moment your application suffers because the 3 nodes involved are not capable of handling read/write requests. Now you can add as many nodes as you want to the cluster but using this data model you won't be able to scale horizontally, because <strong>NO OTHER NODE WILL EVER BE INVOLVED INTO cities TABLE</strong>. The only way I see to solve your problem in such a situation is to increment power of these 3 nodes (vertical scaling) by adding more memory, more powerful cpu and I/O. Creating a schema that does not allow horizontal scaling is an anti pattern</p>
",['table']
25693090,25704036,2014-09-05 19:54:39,Altering cassandra table by adding new column while inserting data,"<p>I have high frequency of data for insertion in cassandra table.And column are dynamic .As per my scenario i want to alter my static table and add new column while insertion process.
Does it impact on insertion or create any lock Or performance issue?</p>

<p>Note :I dont want to use dynamic column family.
Please suggest ..</p>

<p>Thanks</p>
",<cassandra><datastax-enterprise><datastax>,"<p>It should not impact your performance very dramatically. Cassandra is good for such types of operations. (E.g. in relational databases like MySQL it might be quite heavy operation). In case you use Cassandra 2.0, you can use batch to execute both operations: 1. alter table and 2. update column. In this case you will have only one network round-trip.</p>
",['table']
25709656,25710538,2014-09-07 11:23:12,Is a read with one secondary index faster than a read with multiple in cassandra?,"<p>I have this structure that I want a user to see the other user's feeds.
One way of doing it is to fan out an action to all interested parties's feed.</p>

<p>That would result in a query like select from feeds where userid=</p>

<p>otherwise i could avoid writing so much data and since i am already doing a read I could do:</p>

<p>select from feeds where userid IN (list of friends).</p>

<p>is the second one slower? I don't have the application yet to test this with a lot of data/clustering. As the application is big writing code to test a single node is not worth it so I ask for your knowledge.</p>
",<cassandra><cql><database><nosql>,"<p>If your title is correct, and <code>userid</code> is a secondary index, then running a <code>SELECT/WHERE/IN</code> is not even possible.  The <code>WHERE/IN</code> clause only works with primary key values.  When you use it on a column with a secondary index, you will see something like this:</p>

<pre><code>Bad Request: IN predicates on non-primary-key columns (columnName) is not yet supported
</code></pre>

<p>Also, the DataStax CQL3 <a href=""http://www.datastax.com/documentation/cql/3.0/cql/cql_reference/select_r.html"" rel=""nofollow"">documentation for SELECT</a> has a section worth reading about using <code>IN</code>:</p>

<blockquote>
  <p><strong>When not to use IN</strong></p>
  
  <p>The recommendations about when not to use an index apply to using IN
  in the WHERE clause. Under most conditions, using IN in the WHERE
  clause is not recommended. Using IN can degrade performance because
  usually many nodes must be queried. For example, in a single, local
  data center cluster with 30 nodes, a replication factor of 3, and a
  consistency level of LOCAL_QUORUM, a single key query goes out to two
  nodes, but if the query uses the IN condition, the number of nodes
  being queried are most likely even higher, up to 20 nodes depending on
  where the keys fall in the token range.</p>
</blockquote>

<p>As for your first query, it's hard to speculate about performance without knowing about the cardinality of <code>userid</code> in the feeds table.  If <code>userid</code> is unique or has a very high number of possible values, then that query will not perform well.  On the other hand, if each <code>userid</code> can have several ""feeds,"" then it might do ok.</p>

<p>Remember, Cassandra data modeling is about building your data structures for the expected queries.  Sometimes, if you have 3 different queries for the same data, the best plan may be to store that same, redundant data in 3 different tables.  And that's ok to do.</p>

<p>I would tackle this problem by writing a table geared toward that specific query.  Based on what you have mentioned, I would build it like this:</p>

<pre><code>CREATE TABLE feedsByUserId
    userid UUID,
    feedid UUID,
    action text,
    PRIMARY KEY (userid, feedid));
</code></pre>

<p>With a composite primary key made up of <code>userid</code> as the <a href=""http://www.datastax.com/documentation/cql/3.1/share/glossary/gloss_partition_key.html"" rel=""nofollow"">partitioning key</a> you will then be able to run your <code>SELECT/WHERE/IN</code> query mentioned above, and achieve the expected results.  Of course, I am assuming that the addition of <code>feedid</code> will make the entire key unique.  if that is not the case, then you may need to add an additional field to the <code>PRIMARY KEY</code>.  My example is also assuming that <code>userid</code> and <code>feedid</code> are version-4 UUIDs.  If that is not the case, adjust their types accordingly.</p>
",['table']
25728237,25729760,2014-09-08 15:54:35,Cassandra CQL SELECT/DELETE issue due to primary key constraints,"<p>I need to store latest updates that needs to be pushed to users' newsfeed page in Cassandra table for later retrieval and my table's schema is as follow:</p>

<pre><code>CREATE TABLE newsfeed (user_name text, 
                       post_id bigint,
                       post_type text, 
                       favorited boolean, 
                       shared boolean, 
                       own boolean, 
                       date timestamp, 
       PRIMARY KEY (user_name,date,post_id,post_type) );
</code></pre>

<p>The first three column (username, postid, and posttype) in combination will build the actual primary-key of the table, however since I wanted to ORDER the SELECT queries on this table based on ""date""s of rows I placed the date-column into the primary key fields as the ""second"" entry (did I have to do this?).</p>

<p>When I want to delete a row by giving only ""user_name, post_id, and post_type"" as follow:</p>

<pre><code> DELETE FROM newsfeed WHERE user_name='pooria' and post_id=36 and post_type='p';
</code></pre>

<p>I will get the following error:</p>

<pre><code>Bad Request: Missing PRIMARY KEY part date since post_id is set
</code></pre>

<p>I need the date-column to be part of the primary key since I want to use it in my ORDER BY clauses and on the other hand I have to delete some rows without knowing their ""date"" values!</p>

<p>So how such problems are tackled in Cassandra? should I be fixing my Data Model and have different schema for job?</p>
",<cassandra><cql>,"<p>DataStax's Chief Evangelist Patrick McFadden posted an article demonstrating a few time series modeling patterns.  Definitely makes for a good read, and should be of some help to you: <a href=""http://planetcassandra.org/getting-started-with-time-series-data-modeling/"" rel=""nofollow"">Getting Started with Time Series Data Modeling</a>.</p>

<p>I think your table is just fine.  Although, with the way that composite primary keys work in Cassandra, if you cannot skip primary key components in a query.  So if you do end up needing to query data by <code>user_name</code>, <code>post_id</code>, and/or <code>post_type</code> differently (without date), you should create a table specifically for that query (which does not include date in the primary key).</p>

<p>I will however say that in-general, creating a table which will process regular delete operations is not a good idea.  In fact, I'm pretty sure that has been classified as a Cassandra ""anti-pattern.""  Data really isn't deleted from Cassandra; it is tombstoned.  Tombstones are reconciled at compaction time (assuming that the tombstone threshold time has been met), and having too many of them has been known to cause performance issues.</p>

<p>If you read the article I linked above, go down to the section named ""Time Series Pattern 3.""  You will notice that the <code>INSERT</code> statements are run with the <code>USING TTL</code> clause.  This gives the data a time-to-live in seconds, after which it will ""quietly disappear.""  For instance, if you wanted to keep your data around for 24 hours (86400 seconds) you could do something like this:</p>

<pre><code>INSERT INTO newsfeed (...) VALUES (...) USING TTL 86400
</code></pre>

<p>Using the TTL feature is a preferable alternative to regular cleansing by <code>DELETE</code>.</p>
",['table']
25768180,25774613,2014-09-10 14:33:56,composite column keys & composite row keys,"<p>I'm trying to understand the concept of Composite Column key &amp; Composite Row Key but unable to get it. I think it's used for getting a range of values but can't we use timestamp column as cluster key for this purpose ? What is the use case, advantages, disadvantages &amp; implementation. Please explain using CQL 3.</p>

<p>Composite Column key: <a href=""http://i.stack.imgur.com/qLboK.png"" rel=""nofollow"">http://i.stack.imgur.com/qLboK.png</a></p>

<p>Composite Row Key: <a href=""http://i.stack.imgur.com/xA6Hz.png"" rel=""nofollow"">http://i.stack.imgur.com/xA6Hz.png</a></p>
",<cassandra><cql3><cassandra-2.0><datastax>,"<p>Maybe you confuse both terms which refers how cassandra stores compounds primary keys. I'll explain how composite keys works and how is stored physically easily with Cassandra 2.0 and CQL3.</p>

<p>Cassandra stores all the logical rows with the same partition key as a single physical wide row. Partition key is divided in  (partition_key, clustering_key). </p>

<ul>
<li>partition_key: Identifies the row in Cassandra. All registers with the same partition_key will go to the same machine and will be store together. You can compound a partition_key. </li>
<li>clustering_key: keep the data with the same partition_key ordered. You can set multiple clustering keys separated by commas.</li>
</ul>

<p>Imagine you have a table purchase with this definition:</p>

<pre><code>CREATE TABLE purchase(
  user text,
  item text,
  time timestamp,
  PRIMARY KEY ((user, item), time)
);
</code></pre>

<p>And this data</p>

<pre><code>john  car  09/01/14...
john  car 09/05/13...
john  house 09/07/11...
penny laptop  09/08/08...
penny laptop 09/03/11...
rachel tv 09/01/09...
</code></pre>

<p>Cassandar will store that data </p>

<pre><code>john || car  || 09/05/13 - 09/01/14
john || house || 09/07/11
penny || laptop || 09/08/08 - 09/03/11
rachel || tv || 09/01/09
</code></pre>

<p>If you want to retrieve cars that john bought you can assure that two registers are stored together and ordered by time. </p>

<p>For querys you always have to set partition key fields ( = ) and you can if you want compare order of clustering keys with ( &lt;  or  > ). </p>

<p>Examples:</p>

<ul>
<li>select * from purchase where user='penny' and item='laptop'.  Return 2 registers.</li>
<li>select * from purchase where user='john' and item='car' where date 01/01/14.  Return 1 register.</li>
</ul>

<p>Hope it helps.</p>
",['table']
25776438,25783218,2014-09-10 23:01:17,Primary Key related CQL3 Queries cases & errors when sorting,"<p>I have two issues while querying Cassandra:</p>

<h2>Query 1</h2>

<pre><code>&gt; select * from a where author='Amresh' order by tweet_id DESC;
Order by with 2ndary indexes is not supported
</code></pre>

<p>What I learned: secondary indexes are made to be used only with a <code>WHERE</code> clause and not <code>ORDER BY</code>? If so, then how can I sort?</p>

<h2>Query 2</h2>

<pre><code>&gt; select * from a where user_id='xamry' ORDER BY tweet_device DESC;
Order by currently only supports the ordering of columns following their
declared order in the PRIMARY KEY.
</code></pre>

<p>What I learned: The <code>ORDER BY</code> column should be in the 2nd place in the primary key, maybe? If so, then what if I need to sort by multiple columns? </p>

<p>Table:   </p>

<pre><code>CREATE TABLE a(
  user_id      varchar,
  tweet_id     varchar,
  tweet_device varchar,
  author       varchar,
  body         varchar,
  PRIMARY KEY(user_id,tweet_id,tweet_device)
);

INSERT INTO a (user_id, tweet_id, tweet_device, author, body)
    VALUES ('xamry', 't1', 'web', 'Amresh', 'Here is my first tweet');
INSERT INTO a (user_id, tweet_id, tweet_device, author, body)
    VALUES ('xamry', 't2', 'sms', 'Saurabh', 'Howz life Xamry');
INSERT INTO a (user_id, tweet_id, tweet_device, author, body)
    VALUES ('mevivs', 't1', 'iPad', 'Kuldeep', 'You der?');
INSERT INTO a (user_id, tweet_id, tweet_device, author, body)
    VALUES ('mevivs', 't2', 'mobile', 'Vivek', 'Yep, I suppose');

Create index user_index on a(author);
</code></pre>
",<cassandra><cql><cql3><cassandra-2.0><datastax>,"<p>To answer your questions, let's focus on your choice of primary key for this table:</p>

<pre><code>PRIMARY KEY(user_id,tweet_id,tweet_device)
</code></pre>

<p>As written, the <code>user_id</code> will be used as the partition key, which distributes your data around the cluster but also keeps all of the data for the same user ID on the same node.  Within a single partition, unique rows are identified by the pair <code>(tweet_id, tweet_device)</code> and those rows will be <em>automatically ordered</em> by <code>tweet_id</code> because it is the second column listed in the primary key.  (Or put another way, the first column in the PK that is <em>not</em> a part of the partition key determines the sort order of the partition.)</p>

<h2>Query 1</h2>

<p>The <code>WHERE</code> clause is <code>author='Amresh'</code>.  Note that this clause does not involve <em>any</em> of the columns listed in the primary key; instead, it is filtering using a secondary index on <code>author</code>.  Since the WHERE clause does not specify an exact value for the partition key column (<code>user_id</code>) using the index involves <em>scanning <strong>all</strong> cluster nodes</em> for possible matches.  Results cannot be sorted when they come from more than one replica (node) because that would require holding the entire result set on the coordinator node before it could return any results to the client.  The coordinator can't know what is the <em>real</em> ""first"" result row until it has confirmed that it has received and sorted every possible matching row.</p>

<p>If you need the information for a specific author name, separate from user ID, and sorted by tweet ID, then consider storing the data again in a different table.  The data design philosophy with Cassandra is to <em>store the data in the format you need when reading it</em> and to actually denormalize (store redundant information) as necessary.  This is because in Cassandra, <em>writes are cheap</em> (though it places the burden of managing multiple copies of the same logical data on the application developer).</p>

<h2>Query 2</h2>

<p>Here, the <code>WHERE</code> clause is <code>user_id = 'xamry'</code> which happens to be the partition key for this table.  The good news is that this will go directly to the replica(s) holding this partition and not bother asking the other nodes.  However, you cannot <code>ORDER BY tweet_device</code> because of what I explained at the top of this answer.  Cassandra stores rows (within a single partition) sorted by a single column, generally the second column in the primary key.  In your case, you can access data for <code>user_id = 'xamry' ORDER BY tweet_id</code> but not ordered by <code>tweet_device</code>.  The answer, if you really need the data sorted by device, is the same as for Query 1: store it in a table where that is the second column in the primary key.</p>

<p>If, when looking up the tweets by <code>user_id</code> you only ever need them sorted by device, simply flip the order of the last two columns in your primary key.  If you need to be able to sort either way, store the data twice in two different tables.</p>

<p>The Cassandra storage engine does not offer multi-column sorting other than the order of columns listed in your primary key.</p>
",['table']
25811312,25812021,2014-09-12 15:04:08,Unable to copy data from CSV file,"<p>Unable to import data into Cassandra Table / Column Family using  the 'COPY from' command.</p>

<p><strong>We tried</strong></p>

<pre><code>cqlsh&gt; copy eqdata from '/home/swiftguy/cassandra/earthquakedata/weather-data-with-uuid.csv';
Bad Request: line 1:298 no viable alternative at input ')'
Aborting import at record #0 (line 1). Previously-inserted values still present.
</code></pre>

<p><strong>Table details</strong></p>

<blockquote>
  <p>create table earthquakedata (  eqtime timestamp,  Longitude float,
  Latitude float, Depth float, Magnitude float, MagType text, NbStations
  float, Gapv float, distance float, RMS float, Source text, EventID
  text, Version text, id uuid, primary key (eqtime, id));</p>
</blockquote>

<p>This is the  <a href=""https://docs.google.com/spreadsheets/d/12zlaJFd64mkHTBp5_2FHUr9cQS4WLorgyJHPYtmgDck/pubhtml"" rel=""nofollow"">csv file</a> we  are trying to import </p>

<p>Please help me out. Thanks in advance.</p>
",<python><csv><cassandra><nosql>,"<p>Download the gis.zip from <a href=""http://www.datastax.com/documentation/tutorials/gis.zip"" rel=""nofollow"">http://www.datastax.com/documentation/tutorials/gis.zip</a> with the earthquake data and compare the CSV with the CSV you're using. There's a header and commas are used as the delimiter, but essentially unaltered from the available public data. In cqlsh, do this remembering to adjust the 'path/earthquakes.csv' in the COPY command and using the default COPY options:</p>

<pre>
CREATE TABLE earthquakes ( 
             datetime text PRIMARY KEY, 
             latitude double, 
             longitude double, 
             depth double, 
             magnitude double, 
             magtype text, 
             nbstations int, 
             gap double, 
             distance double, 
             rms double, 
             source text, 
             eventid int 
           );
COPY earthquakes (datetime, latitude, longitude, depth, magnitude, magtype, nbstations, gap, distance, rms, source, eventid) FROM 'path/earthquakes.csv' WITH HEADER = 'true';
</pre>

<p>After running Brian's python script to add UUIDs, and adjusting the table for the UUID, running this command imported all the rows:</p>

<pre>
cqlsh:mykeyspace> COPY earthquakes (datetime, latitude, longitude, depth, magnitude, magtype, nbstations, gap, distance, rms, source, eventid,newid) FROM '/Users/krishahn/Downloads/gis/out.csv' WITH HEADER = 'true';
77037 rows imported in 2 minutes and 35.913 seconds.
</pre>
",['table']
25817451,25818415,2014-09-12 21:55:58,"Row Inserts having same primary key, are replacing previous writes in Cassandra","<p>Created a table in Cassandra where the primary key is based on two columns(groupname,type). When I'm trying to insert more than 1 row where the groupname and type is same, then in such situation its not storing more than one row, subsequent writes where in the groupname and type are same.. then the latest write is replacing the previous similar writes. Why Cassandra is replacing in this manner instead of writing every row im inserting?</p>

<p><strong>Write 1</strong></p>

<pre><code>cqlsh:resto&gt; insert into restmaster (rest_id,type,rname,groupname,address,city,country)values(blobAsUuid(timeuuidAsBlob(now())),'SportsBar','SportsDen','VK Group','Majestic','Bangalore','India');
</code></pre>

<p><strong>Write 2</strong></p>

<pre><code>insert into restmaster (rest_id,type,rname,groupname,address,city,country)values(blobAsUuid(timeuuidAsBlob(now())),'SportsBar','Sports Spot','VK Group','Bandra','Mumbai','India');
</code></pre>

<p><strong>Write 3</strong></p>

<pre><code>cqlsh:resto&gt; insert into restmaster (rest_id,type,rname,groupname,address,city,country)values(blobAsUuid(timeuuidAsBlob(now())),'SportsBar','Cricket Heaven ','VK Group','Connaught Place','New Delhi','India');
</code></pre>

<p><strong>The result Im expecting(check rows 4,5,6)</strong></p>

<pre><code> groupname      | type       | rname
----------------+------------+-----------------
           none |      Udipi |  Gayatri Bhavan
           none |     dinein |    Blue Diamond
       VK Group |  FoodCourt |        FoodLion
       VK Group |  SportsBar |      Sports Den
       VK Group |  SportsBar |     Sports Spot
       VK Group |  SportsBar |  Cricket Heaven
  Viceroy Group | Vegetarian |  Palace Heights
 Mainland Group |    Chinese |  MainLand China
      JSP Group |  FoodCourt |        Nautanki
          Ohris |  FoodCourt |           Ohris
</code></pre>

<p><strong>But this is the actual result (write 3 has replaced previous 2 inserts [rows 4,5])</strong></p>

<pre><code> cqlsh:resto&gt; select groupname,type,rname From restmaster;

 groupname      | type       | rname
----------------+------------+-----------------
           none |      Udipi |  Gayatri Bhavan
           none |     dinein |    Blue Diamond
       VK Group |  FoodCourt |        FoodLion
       VK Group |  SportsBar | Cricket Heaven 
  Viceroy Group | Vegetarian |  Palace Heights
 Mainland Group |    Chinese |  MainLand China
      JSP Group |  FoodCourt |        Nautanki
          Ohris |  FoodCourt |           Ohris


cqlsh:resto&gt; describe table restmaster;

CREATE TABLE restmaster (
  groupname text,
  type text,
  address text,
  city text,
  country text,
  rest_id uuid,
  rname text,
  PRIMARY KEY ((groupname), type)
) WITH
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  index_interval=128 AND
  read_repair_chance=0.000000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  default_time_to_live=0 AND
  speculative_retry='99.0PERCENTILE' AND
  memtable_flush_period_in_ms=0 AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'LZ4Compressor'};
</code></pre>
",<cassandra><cqlsh><nosql>,"<p>All inserts to the Cassandra database are actually insert/update operations and there can only be on set of non-key values per uniquely defined primary key. This means that you can not ever have more than one set of values for one primary key and that you will only see the last write.</p>

<p>More info:
<a href=""http://www.datastax.com/documentation/cql/3.1/cql/cql_intro_c.html"" rel=""noreferrer"">http://www.datastax.com/documentation/cql/3.1/cql/cql_intro_c.html</a></p>

<p>Update: A datamodel </p>

<p>If you used a key like</p>

<pre><code>Primary Key ((groupname),type,rname)
</code></pre>

<p>As long as you have unique restaurant names you will be able to get the results you are expecting. But what you really should be asking is ""What queries would I like to perform on this data?"" All Cassandra Tables should be based around satisfying a class of queries. The key I wrote above basically says ""This table is constructed to quickly look up all the restaurants in a particular group and the only conditionals I will use will be on type and on restaurant name""</p>

<p>Examples queries you could perform with that schema</p>

<pre><code> SELECT * FROM restmaster WHERE groupname = 'Lettuce Entertain You' ;
 SELECT * FROM restmaster WHERE groupname = 'Lettuce Entertain You' and type = 'Formal'  ;
 SELECT * FROM restmaster WHERE groupname = 'Lettuce Entertain You' and type = 'Formal' 
    and rname &gt; 'C' and rname &lt; 'Y' ;
</code></pre>

<p>If that isn't the kind of queries you want to be performing in your application or you want other queries in addition to those, you most likely will need additional tables. </p>
",['table']
25838195,25840559,2014-09-14 21:29:03,Overview for SQL-familiar person moving to Cassandra/NoSQL,"<p>After years of dealing with relational DBs, I am quite <del>comfortable</del> <del>brainwashed into</del> normalized into thinking about tables, columns and rows.  Cassandra is still eluding my mind fully wrapping around it.</p>
<p>I understand its more of persisting maps, and you can only query on the unique keys for those maps, but my understanding is incomplete.</p>
<p>Yes, I am RTFM.  Still, can someone give me a nice, concise description of how Cassandra structures data vs. a SQL db?  I mean, 1000 foot view, how it works differently?</p>
<p>For instance, at the Ebay tech blog, it says:</p>
<blockquote>
<p>Don’t think of a relational table.</p>
<p>Instead, think of a nested, sorted map data structure.
Source: <a href=""http://www.ebaytechblog.com/2012/07/16/cassandra-data-modeling-best-practices-part-1/"" rel=""nofollow noreferrer"">http://www.ebaytechblog.com/2012/07/16/cassandra-data-modeling-best-practices-part-1/</a></p>
</blockquote>
<p>And I almost get it fully... but not quite.</p>
<hr />
<p>These are some great answers.  Added a bounty to see if it inspires anyone to make an even more authoritative response.</p>
",<cassandra><nosql>,"<p>Note: As the original question was about Cassandra, that will be the focus of this answer.  While Cassandra and other non-relational (NoSQL) datastores often do share similar concepts, it cannot be assumed that the ideas detailed here will work with other non-relational datastores.</p>

<p>The best way to go about this, is to remember that it is standard practice (for Cassandra) to build your data model to suit your queries.  The main difference, is that RDBMS tables are built with efficient <em>storage</em> of the data as the main focus.  In the Cassandra (non-relational) world, the main focus changes to how you want your queries to look.  Often, that may translate to storing the same, redundant data keyed multiple ways...and that's ok.  This is explained in the DataStax doc <a href=""http://www.datastax.com/docs/1.0/ddl/about-data-model#comparing-the-cassandra-data-model-to-a-relational-database"" rel=""nofollow"">Comparing the Cassandra Data Model to a Relational Database</a>.</p>

<p>Let's say that I have the following user table:</p>

<pre><code>CREATE TABLE users (
    username TEXT,
    firstname TEXT,
    lastname TEXT,
    phone TEXT,
    PRIMARY KEY (username));
</code></pre>

<p>After inserting some sample data, that table looks like this:</p>

<pre><code>username  | firstname | lastname | phone
------------------------------------------------
mreynolds | Malcolm   | Reynolds | 111-555-1234
jcobb     | Jayne     | Cobb     | 111-555-3464
sbook     | Derial    | Book     | 111-555-2349
stam      | Simon     | Tam      | 111-555-8899
</code></pre>

<p>The <code>users</code> table will allow me to query my users by <code>username</code>, as that is our PRIMARY KEY.  But what if we wanted to query that data by phone number?  You may be tempted to add a secondary index on <code>phone</code>, but the cardinality of <code>phone</code> would probably be just as high as <code>username</code>.  The proper way to solve this, is to create a new table to allow you to query by phone.  Note: This model assumes that <code>username</code> and <code>phone</code> are both unique.</p>

<pre><code>CREATE TABLE usersbyphone (
    phone TEXT,
    username TEXT,
    firstname TEXT,
    lastname TEXT,
    PRIMARY KEY (phone));
</code></pre>

<p>Let's assume that our next entries are for the users ""Hoban Washburne"" and ""Zoe Washburne.""  As they are married, they will have the same (home) phone number.  Cassandra will not allow entries to share a primary key, and will overwrite (with the last entry winning).  So we'll need to change the primary key on our <code>usersbyphone</code> table, like this:</p>

<pre><code>PRIMARY KEY (phone,username));
</code></pre>

<p>Here <code>phone</code> is our partitioning key (key that determines the partition where this row is stored) and <code>username</code> is our clustering key (key that determines our on-disk sort order).  Using these two together in a composite primary key will ensure uniqueness.  This will allow us to select our <code>usersbyphone</code> table like this:</p>

<pre><code>SELECT username, firstname, lastnamea, phone FROM usersbyphone;

username  | firstname | lastname | phone
------------------------------------------------
hwashburne| Hoban     | Washburne| 111-555-1212
jcobb     | Jayne     | Cobb     | 111-555-3464
mreynolds | Malcolm   | Reynolds | 111-555-1234
sbook     | Derial    | Book     | 111-555-2349
stam      | Simon     | Tam      | 111-555-8899
zwashburne| Zoe       | Washburne| 111-555-1212
</code></pre>

<p>It is important to note, that the <code>usersbyphone</code> table does not <em>replace</em> the <code>users</code> table...it works in conjunction <em>with</em> it.  You will have some queries on that data set which need to be served by the <code>users</code> table, and some that will need to be served by the <code>usersbyphone</code> table.  The downside is that keeping the two tables in-sync is done outside the database, typically by your application code.  This is one example showing how your thinking must differ in the non-relational paradigm.</p>

<p>As DataStax MVP John Berryman explains (<a href=""http://planetcassandra.org/blog/understanding-how-cql3-maps-to-cassandras-internal-data-structure/"" rel=""nofollow"">Understanding How CQL3 Maps To Cassandra’s Internal Data Structure</a>), under the hood, Casssandra will store our <code>users</code> data in a structure that looks something like this:</p>

<pre><code>RowKey:mreynolds
=&gt; (column=, value=, timestamp=1374546754299000)
=&gt; (column=firstname, value=Malcolm, timestamp=1374546754299000)
=&gt; (column=lastname, value=Reynolds, timestamp=1374546754299000)
=&gt; (column=phone, value=111-555-1234, timestamp=1374546754299000)
------------------------------------------------------
RowKey:hwashburne
=&gt; (column=, value=, timestamp=1374546757815000)
=&gt; (column=firstname, value=Hoban, timestamp=1374546757815000)
=&gt; (column=lastname, value=Washburne, timestamp=1374546757815000)
=&gt; (column=phone, value=111-555-1212, timestamp=1374546757815000)
------------------------------------------------------
RowKey:zwashburne
=&gt; (column=, value=, timestamp=1374546761055000)
=&gt; (column=firstname, value=Zoe, timestamp=1374546761055000)
=&gt; (column=lastname, value=Washburne, timestamp=1374546761055000)
=&gt; (column=phone, value=111-555-1212, timestamp=1374546761055000)
</code></pre>

<p>The Map-of-a-Map concept (mentioned in the eBay article) definitely comes into play here.  Our <code>usersbyphone</code> table  will look a little different from our <code>users</code> table:</p>

<pre><code>RowKey:111-555-1234
=&gt; (column=mreynolds, value=, timestamp=1374546754299000)
=&gt; (column=mreynolds:firstname, value=Malcolm, timestamp=1374546754299000)
=&gt; (column=mreynolds:lastname, value=Reynolds, timestamp=1374546754299000)
------------------------------------------------------
RowKey:111-555-1212
=&gt; (column=hwashburne, value=, timestamp=1374546757815000)
=&gt; (column=hwashburne:firstname, value=Hoban, timestamp=1374546757815000)
=&gt; (column=hwashburne:lastname, value=Washburne, timestamp=1374546757815000)
=&gt; (column=zwashburne, value=, timestamp=1374546761055000)
=&gt; (column=zwashburne:firstname, value=Zoe, timestamp=1374546761055000)
=&gt; (column=zwashburne:lastname, value=Washburne, timestamp=1374546761055000)
</code></pre>

<p>Due to the primary key structure, notice how Wash (hwashburne) and Zoe (zwashburne) are technically stored in the same row.  This structure allows us to quickly query all users who share the same phone number.</p>

<p>In summary:</p>

<ul>
<li>Tables (column families) are to be used in the most efficient way to <em>query</em> the data.  RDBMSs encourage the use of normalization to most efficiently <em>store</em> the data.</li>
<li>With composite keys, Cassandra groups similar data together in nearby rows.</li>
<li>Cassandra takes advantage of on-disk (clustering) sort order to optimize operations.</li>
<li>Cassandra primary keys are always unique.  Different writes to data with the same primary key will overwrite each other (last write wins).</li>
<li>CQL (Cassandra Query Language) is a limited subset of SQL.  While CQL provides those coming from a RDBMS background with familiar syntax, it is important to remember that many SQL keywords and concepts are not present in CQL.</li>
</ul>
",['table']
25872900,25877577,2014-09-16 15:33:33,Efficient access of ordered results in Cassandra,"<p>I am trying to translate a relatively common requirement in SQL to an efficient data model in Cassandra. I'm trying to decide how best to model my data so that I can order my rows in Cassandra in the same order that I wish to report them in the application. Normally this would be a good case for a clustering column, except that the data by which I want to order my result is a metric that will be updated several times daily. </p>

<p>I am going to explain the problem in SQL and then share what data modeling approaches have occurred to me. What I would like to know is, has anyone faced a similar requirement to mine and, if so, how did you model the data in Cassandra.</p>

<p>Here's the problem I'm trying to solve. </p>

<p>Suppose I have a raw_data table defined like so:</p>

<pre><code>CREATE TABLE raw_data (
  A varchar,
  B varchar,
  C varchar,
  D varchar,
  ts timestamp,
  val varint
  PRIMARY KEY (ts,A,B,C,D)
);
</code></pre>

<p>And I also have a summary table</p>

<pre><code>CREATE TABLE summary_table (
  A varchar,
  B varchar,
  C varchar,
  total_val varint
  PRIMARY KEY (A,B,C)
);
</code></pre>

<p>Where the data in my summary table is aggregated by my application in a way that corresponds to </p>

<pre><code>SELECT A, B, C, SUM(val) FROM raw_data GROUP BY A, B, C
</code></pre>

<p>What I want to be able to do is execute a query like the following:</p>

<pre><code>SELECT B, C, total_val FROM summary_table WHERE A = ""Something"" ORDER BY total_val DESC LIMIT 1000;
</code></pre>

<p>That is to say, I want to subset my summary table for a particular value of A and then return the top 1000 rows, ordered by total_val</p>

<p>Total_val is updated every few minutes by my application, as additional data are streamed into my raw_data table. So I can't use total_val as a clustering column for my data</p>

<p>What I'm trying to decide is how best to model this type of problem in Cassandra -- one in which I need to subset a summary table with a WHERE CLAUSE and order the result set (which are being updated constantly) in DESC order.</p>

<p>Some of the result sets can be expected to be fairly large -- several hundreds of thousands of rows (that is to say, there are some values for A in my summary table for which <code>SELECT COUNT(*) FROM summary_table WHERE A = ""some value""</code> would be very, very large, in the hundreds of thousands). It's obviously inefficient to sort these data and discard prior to sending to my application. </p>

<p>Also, this doesn't seem to be a good use case for secondary indices. On smaller result sets, they are very performant. For larger ones, they are lagging and I suspect there may be a better way to handle this problem. </p>

<p>Another way I've considered modeling this involves caching the larger result sets into memory, so that at least where I would need to sort many thousands of rows, I'd at least be doing so in memory. I've also considered having a secondary summary table that's already prepopulated with the top 1000 rows that I want to expose to my application... although I can't think of a good way to keep those data up to date and avoid the exact same problem I'm having with my original summary table.   </p>

<p>Has anyone encountered a problem like this, one in which you need to filter your summary data with a WHERE clause and order your (frequently changing) results in Desc order? If so, have you found a way to make this performant when certain WHERE clauses would return many thousands of rows? If so, how did you go about doing it? </p>
",<cassandra><data-modeling>,"<p>The best way I can think of to do this would be the following:</p>

<pre><code>CREATE TABLE summary_table (
  time_bucket long,
  A varchar,
  total_val int,
  timestamp long,
  B varchar,
  C varchar,
  PRIMARY KEY ((time_bucket, A), total_val, timestamp, B, C)
) WITH CLUSTERING ORDER BY (total_val DESC);
</code></pre>

<p>With this structure, you don't actually overwrite <code>total_val</code>. Instead, you insert a new row for each new value, then discard all but the latest timestamp at query time.  The value of <code>time_bucket</code> should be your timestamp rounded to some interval you can calculate at query time (you may have to query multiple buckets at a time, but try to limit this to only two if possible). In case you're wondering, <code>time_bucket</code> and <code>A</code> become your partition key, which prevents unbounded row growth over time.</p>

<p>In other words, you've turned your summary table into time-series data.  If need be you can add a TTL to the old columns so they die off naturally.  As long as your time buckets are sane, you won't run into the issue of querying large numbers of tombstones.</p>
",['table']
25884598,25892212,2014-09-17 07:19:45,When to use Cassandra vs. Solr in DSE?,"<p>I'm using DSE for Cassandra/Solr integration so that data are stored in Cassandra and indexed in Solr. It's very natural to use Cassandra to handle CRUD operation and use Solr for full text search respectively, and DSE can really simplify data synchronization between Cassandra and Solr. </p>

<p>When it comes to query, however, there are actually two ways to go: Cassandra secondary/manual configured index vs. Solr. I want to know when to use which method and what's the performance difference in general, especially under DSE setup. </p>

<p>Here is one example use case in my project. I have a Cassandra table storing some item entity data. Besides the basic CRUD operation, I also need to retrieve items by equality on some field (say category) and then sort by some order (in my case here, a like_count field).</p>

<p>I can think of three different ways to handle it:</p>

<ol>
<li>Declare 'indexed=true' in Solr schema for both category and like_count field and query in Solr</li>
<li>Create a denormalized table in Cassandra with primary key (category, like_count, id)</li>
<li>Create a denormalized table in Cassandra with primary key (category, order, id) and use an external component, such as Spark/Storm，to sort the items by like_count</li>
</ol>

<p>The first method seems to be the simplest to implement and maintain. I just write some trivial Solr accessing code and the rest heavy lifting are handled by Solr/DSE search.</p>

<p>The second method requires manual denormalization on create and update. I also need to maintain a separate table. There is also tombstone issue as the like_count can possibly be updated frequently. The good part is that the read may be faster (if there are no excessive tombstones). </p>

<p>The third method can alleviate the tombstone issue at the cost of one extra component for sorting. </p>

<p>Which method do you think is the best option? What is the difference in performance? </p>
",<solr><cassandra><datastax-enterprise>,"<p>Cassandra secondary indexes have limited use cases:</p>

<ol>
<li>No more than a couple of columns indexed.</li>
<li>Only a single indexed column in a query.</li>
<li>Too much inter-node traffic for high cardinality data (relatively unique column values)</li>
<li>Too much inter-node traffic for low cardinality data (high percentage of rows will match)</li>
<li>Queries need to be known in advance so data model can be optimized around them.</li>
</ol>

<p>Because of these limitations, it is common for apps to create ""index tables"" which are indexed by whatever column is desired. This requires either that data be duplicated from the main table to each index table, or an extra query will be needed to read the index table and then read the actual row from the main table after reading the main key from the index table. Queries on multiple columns will have to be manually indexed in advance, making ad hoc queries problematic. And any duplicated will have to be manually updated by the app into each index table.</p>

<p>Other than that... they will work fine in cases where a ""modest"" number of rows will be selected from a modest number of nodes, and queries are well specified in advance and not ad hoc.</p>

<p>DSE/Solr is better for:</p>

<ol>
<li>A moderate number of columns are indexed.</li>
<li>Complex queries with a number of columns/fields referenced - Lucene matches all specified fields in a query in parallel. Lucene indexes the data on each node, so nodes query in parallel.</li>
<li>Ad hoc queries in general, where the precise queries are not known in advance.</li>
<li>Rich text queries such as keyword search, wildcard, fuzzy/like, range, inequality.</li>
</ol>

<p>There is a performance and capacity cost to using Solr indexing, so a proof of concept implementation is recommended to evaluate how much additional RAM, storage, and nodes are needed, which depends on how many columns you index, the amount of text indexed, and any text filtering complexity (e.g., n-grams need more.) It could range from 25% increase for a relatively small number of indexed columns to 100% if all columns are indexed. Also, you need to have enough nodes so that the per-node Solr index fits in RAM or mostly in RAM if using SSD. And vnodes are not currently recommended for Solr data centers.</p>
",['table']
25887641,25906479,2014-09-17 10:04:17,CQL query on 'validFrom/validTo timestamps',"<p>I'm currently trying to model a column family that has two timestamps specifying whether an entry is valid (or 'active') at a given date (typically execution time).</p>

<p>No big issue with traditional SQL, 64 gigs of RAM and some indices, we're doing that quite often with our SQL server.</p>

<p>However, in CQL I haven't managed to model this scenario and write valid queries for it. </p>

<p>My basic model is (<strong>I skipped the PK definition!</strong>)</p>

<pre><code>create table myTable(
    id uuid,
    validFrom timeuuid,
    validTo timeuuid,
    someInformationalData varChar
);
</code></pre>

<p>Some explanations:</p>

<ul>
<li>due to the fact, that a validity date is not unique, I need a combined key in my final application this is going to be a usergroup reference (would be an ideal partition key)</li>
<li>validFrom/To are designed to be optional, but I could deal with by using boundary values (1970, 2038) for 'null' values passed through the persistence layer</li>
</ul>

<p>I tried various combinations of partitioning/clustering keys, however neither of them resulted in valid CQL</p>

<pre><code>-- only active results
select * 
from
     myTable
where
     validFrom &lt; now()
and
     validTo &gt; now()
</code></pre>

<p>I'm quite new to the NoSQL/CQL world and am struggling a bit with converting some of our applications. I could do it in memory, but I'm afraid, this could get a bottleneck at some point...</p>

<p>No sure if this kind of 'I have no idea what I'm doing' yell is appropriate, but any kind of help would be appreciated. :)</p>

<p><strong><em>edit</em> Here's one of the approaches I've been messing around with</strong></p>

<pre><code>drop table if exists myTable;

create table myTable(
    id int,
    datefrom timeuuid,
    dateto timeuuid,
    someColumns varChar,
    primary key((id,datefrom),dateto)
 );

create index if not exists my_idx on myTable(datefrom); 

insert into myTable(id, datefrom,dateto,somecolumns)
values(0,minTimeuuid('1970-01-01 00:00:00'),minTimeuuid('2020-01-01 00:00:00'),'test');
insert into myTable(id,datefrom,dateto,somecolumns)
values(1,minTimeuuid('1970-01-01 00:00:00'),minTimeuuid('2012-01-01 00:00:00'),'test2');


select * from myTable where dateto &gt; now() allow filtering;
-- invalid (""A column of a partition key can be restricted only if the preceding one is restricted by an Equal relation."")
select * from myTable where datefrom &lt; now() and dateto &gt; now()  allow filtering;
</code></pre>

<p>The first query is limiting my result, the row with 'validTo=2012-01-01' is filtered, but I wasn't able to work out a scheme that worked on both limitations in the where clause.</p>
",<cassandra><cql>,"<p>If I understand your problem, what you are looking for is a way to run a range query based on the timestamp. Basically to be able to do this, your model will have to have the timestamp component as part of the clustering key:</p>

<pre><code>create table myTable(
  eventType uuid,
  ts timestamp,
  val text,
  PRIMARY KEY (eventType, ts)
);
</code></pre>

<p>The above will allow you to run a query like: <code>SELECT eventType, val from myTable where eventType = 'your_event' and ts &gt;= 'start_ts' and ts &lt; 'end_ts'</code>.</p>

<p>What you need to remember is that the clustering keys are dictating the order on disk, thus making it possible to run efficiently queries like above. You can read more details about this in the <a href=""https://cassandra.apache.org/doc/cql3/CQL.html#selectStmt"" rel=""nofollow"">CQL spec SELECT section</a>.</p>
",['table']
25893945,25916565,2014-09-17 14:55:54,Querying Data in Cassandra via Spark in a Java Maven Project,"<p>I'm trying to do a simple code where I create a schema, insert some tables, and then pull some information and print it out. However, I'm getting an error. I'm using the Datastax cassandra spark connector. I have been using these two examples to help me try to accomplish this:</p>

<p><a href=""https://gist.github.com/jacek-lewandowski/278bfc936ca990bee35a"" rel=""nofollow"">https://gist.github.com/jacek-lewandowski/278bfc936ca990bee35a</a></p>

<p><a href=""http://www.datastax.com/documentation/developer/java-driver/1.0/java-driver/quick_start/qsSimpleClientAddSession_t.html"" rel=""nofollow"">http://www.datastax.com/documentation/developer/java-driver/1.0/java-driver/quick_start/qsSimpleClientAddSession_t.html</a></p>

<p>However, the second example doesn't use a cassandra spark connector, or spark in general.</p>

<p>Here is my code: </p>

<pre><code>package com.angel.testspark.test;


import com.datastax.driver.core.ResultSet;
import com.datastax.driver.core.Row;
import com.datastax.driver.core.Session;
import com.datastax.spark.connector.cql.CassandraConnector;
import com.google.common.base.Optional;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.api.java.function.Function2;
import org.apache.spark.api.java.function.PairFlatMapFunction;

import scala.Tuple2;

import java.io.Serializable;
import java.math.BigDecimal;
import java.text.MessageFormat;
import java.util.*;

import static com.datastax.spark.connector.CassandraJavaUtil.*;


public class App 
{
    private transient SparkConf conf;

    private App(SparkConf conf) {
        this.conf = conf;
    }
    private void run() {
        JavaSparkContext sc = new JavaSparkContext(conf);
        createSchema(sc);


        sc.stop();
    }

    private void createSchema(JavaSparkContext sc) {
        CassandraConnector connector = CassandraConnector.apply(sc.getConf());

        // Prepare the schema
        try (Session session = connector.openSession()) {
            session.execute(""DROP KEYSPACE IF EXISTS tester"");
            session.execute(""CREATE KEYSPACE tester WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 3}"");
            session.execute(""CREATE TABLE tester.emp (id INT PRIMARY KEY, fname TEXT, lname TEXT, role TEXT)"");
            session.execute(""CREATE TABLE tester.dept (id INT PRIMARY KEY, dname TEXT)"");       

            session.execute(
                      ""INSERT INTO tester.emp (id, fname, lname, role) "" +
                      ""VALUES ("" +
                          ""0001,"" +
                          ""'Angel',"" +
                          ""'Pay',"" +
                          ""'IT Engineer'"" +
                          "");"");
            session.execute(
                      ""INSERT INTO tester.emp (id, fname, lname, role) "" +
                      ""VALUES ("" +
                          ""0002,"" +
                          ""'John',"" +
                          ""'Doe',"" +
                          ""'IT Engineer'"" +
                          "");"");
            session.execute(
                      ""INSERT INTO tester.emp (id, fname, lname, role) "" +
                      ""VALUES ("" +
                          ""0003,"" +
                          ""'Jane',"" +
                          ""'Doe',"" +
                          ""'IT Analyst'"" +
                          "");"");
                session.execute(
                      ""INSERT INTO tester.dept (id, dname) "" +
                      ""VALUES ("" +
                          ""1553,"" +
                          ""'Commerce'"" +
                          "");"");

                ResultSet results = session.execute(""SELECT * FROM tester.emp "" +
                        ""WHERE role = 'IT Engineer';"");
            for (Row row : results) {
                System.out.print(row.getString(""fname""));
                System.out.print("" "");
                System.out.print(row.getString(""lname""));
                System.out.println(); 
            }
                System.out.println();
            }

        }

    public static void main( String[] args )
    {
        if (args.length != 2) {
            System.err.println(""Syntax: com.datastax.spark.demo.JavaDemo &lt;Spark Master URL&gt; &lt;Cassandra contact point&gt;"");
            System.exit(1);
        }

        SparkConf conf = new SparkConf();
        conf.setAppName(""Java API demo"");
        conf.setMaster(args[0]);
        conf.set(""spark.cassandra.connection.host"", args[1]);

        App app = new App(conf);
        app.run();
    }
}
</code></pre>

<p>here is my error:</p>

<pre><code>14/09/18 11:22:18 WARN util.Utils: Your hostname, APAY-M-R03K resolves to a loopback address: 127.0.0.1; using 10.150.79.164 instead (on interface en0)
14/09/18 11:22:18 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address
14/09/18 11:22:18 INFO slf4j.Slf4jLogger: Slf4jLogger started
14/09/18 11:22:18 INFO Remoting: Starting remoting
14/09/18 11:22:18 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://spark@10.150.79.164:50506]
14/09/18 11:22:18 INFO Remoting: Remoting now listens on addresses: [akka.tcp://spark@10.150.79.164:50506]
14/09/18 11:22:18 INFO spark.SparkEnv: Registering BlockManagerMaster
14/09/18 11:22:18 INFO storage.DiskBlockManager: Created local directory at /var/folders/57/8s5fx3ks06bd2rzkq7yg1xs40000gn/T/spark-local-20140918112218-2c8d
14/09/18 11:22:18 INFO storage.MemoryStore: MemoryStore started with capacity 2.1 GB.
14/09/18 11:22:18 INFO network.ConnectionManager: Bound socket to port 50507 with id = ConnectionManagerId(10.150.79.164,50507)
14/09/18 11:22:18 INFO storage.BlockManagerMaster: Trying to register BlockManager
14/09/18 11:22:18 INFO storage.BlockManagerMasterActor$BlockManagerInfo: Registering block manager 10.150.79.164:50507 with 2.1 GB RAM
14/09/18 11:22:18 INFO storage.BlockManagerMaster: Registered BlockManager
14/09/18 11:22:18 INFO spark.HttpServer: Starting HTTP Server
14/09/18 11:22:18 INFO server.Server: jetty-7.6.8.v20121106
14/09/18 11:22:18 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:50508
14/09/18 11:22:18 INFO broadcast.HttpBroadcast: Broadcast server started at http://10.150.79.164:50508
14/09/18 11:22:19 INFO spark.SparkEnv: Registering MapOutputTracker
14/09/18 11:22:19 INFO spark.HttpFileServer: HTTP File server directory is /var/folders/57/8s5fx3ks06bd2rzkq7yg1xs40000gn/T/spark-a0dc4491-1901-4a7a-86f4-4adc181fe45c
14/09/18 11:22:19 INFO spark.HttpServer: Starting HTTP Server
14/09/18 11:22:19 INFO server.Server: jetty-7.6.8.v20121106
14/09/18 11:22:19 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:50509
14/09/18 11:22:19 INFO server.Server: jetty-7.6.8.v20121106
14/09/18 11:22:19 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/storage/rdd,null}
14/09/18 11:22:19 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/storage,null}
14/09/18 11:22:19 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/stages/stage,null}
14/09/18 11:22:19 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/stages/pool,null}
14/09/18 11:22:19 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/stages,null}
14/09/18 11:22:19 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/environment,null}
14/09/18 11:22:19 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/executors,null}
14/09/18 11:22:19 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/metrics/json,null}
14/09/18 11:22:19 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/static,null}
14/09/18 11:22:19 INFO handler.ContextHandler: started o.e.j.s.h.ContextHandler{/,null}
14/09/18 11:22:19 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040
14/09/18 11:22:19 INFO ui.SparkUI: Started Spark Web UI at http://10.150.79.164:4040
14/09/18 11:22:19 WARN core.FrameCompressor: Cannot find LZ4 class, you should make sure the LZ4 library is in the classpath if you intend to use it. LZ4 compression will not be available for the protocol.
14/09/18 11:22:19 INFO core.Cluster: New Cassandra host /127.0.0.1:9042 added
14/09/18 11:22:19 INFO cql.CassandraConnector: Connected to Cassandra cluster: Test Cluster
Exception in thread ""main"" com.datastax.driver.core.exceptions.InvalidQueryException: No indexed columns present in by-columns clause with Equal operator
    at com.datastax.driver.core.exceptions.InvalidQueryException.copy(InvalidQueryException.java:35)
    at com.datastax.driver.core.DefaultResultSetFuture.extractCauseFromExecutionException(DefaultResultSetFuture.java:256)
    at com.datastax.driver.core.DefaultResultSetFuture.getUninterruptibly(DefaultResultSetFuture.java:172)
    at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:52)
    at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:36)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at com.datastax.spark.connector.cql.SessionProxy.invoke(SessionProxy.scala:33)
    at com.sun.proxy.$Proxy6.execute(Unknown Source)
    at com.angel.testspark.test.App.createSchema(App.java:85)
    at com.angel.testspark.test.App.run(App.java:38)
    at com.angel.testspark.test.App.main(App.java:109)
Caused by: com.datastax.driver.core.exceptions.InvalidQueryException: No indexed columns present in by-columns clause with Equal operator
    at com.datastax.driver.core.Responses$Error.asException(Responses.java:97)
    at com.datastax.driver.core.DefaultResultSetFuture.onSet(DefaultResultSetFuture.java:108)
    at com.datastax.driver.core.RequestHandler.setFinalResult(RequestHandler.java:235)
    at com.datastax.driver.core.RequestHandler.onSet(RequestHandler.java:367)
    at com.datastax.driver.core.Connection$Dispatcher.messageReceived(Connection.java:584)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.oneone.OneToOneDecoder.handleUpstream(OneToOneDecoder.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
14/09/18 11:22:20 INFO cql.CassandraConnector: Disconnected from Cassandra cluster: Test Cluster
</code></pre>

<p>I believe it may just be a syntax error, i'm just unsure of where and what it is.</p>

<p>Any help would be great, thanks. I've scoured the internet and haven't found a simple example of just inserting data and pulling data in java with cassandra and spark.</p>

<p>******edit: @BryceAtNetwork23 and @mikea were correct about my syntax error, so i've edited the question and fixed it. I am getting a new error though so I've pasted in the new error and updated the code</p>
",<java><maven><cassandra><apache-spark><connector>,"<p>Try running your CQL via cqlsh and you should get the same/similar error:</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; CREATE TABLE dept (id INT PRIMARY KEY, dname TEXT);
aploetz@cqlsh:stackoverflow&gt; INSERT INTO dept (id, dname) VALUES (1553,Commerce);
&lt;ErrorMessage code=2000 [Syntax error in CQL query] message=""line 1:50 no viable alternative at
input ')' (... dname) VALUES (1553,Commerce[)]...)""&gt;
</code></pre>

<p>Put single quotes around ""Commerce"" and it should work:</p>

<pre><code>session.execute(
                  ""INSERT INTO tester.dept (id, dname) "" +
                  ""VALUES ("" +
                      ""1553,"" +
                      ""'Commerce'"" +
                      "");"");
</code></pre>

<blockquote>
  <p>I'm getting a new error though now... </p>
</blockquote>

<p>Also try running that from cqlsh.</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; SELECT * FROM emp WHERE role = 'IT Engineer';
code=2200 [Invalid query] message=""No indexed columns present in by-columns clause with Equal operator""
</code></pre>

<p>This is happening because <code>role</code> is not defined as your primary key.  Cassandra doesn't allow you to query by arbitrary column values.  The best way to solve this one, is to create an additional query table called empByRole, with <code>role</code> as the partition key.  Like this:</p>

<pre><code>CREATE TABLE empByRole 
    (id INT, fname TEXT, lname TEXT, role TEXT,
    PRIMARY KEY (role,id)
);

aploetz@cqlsh:stackoverflow&gt; INSERT INTO empByRole (id, fname, lname, role) VALUES (0001,'Angel','Pay','IT Engineer');
aploetz@cqlsh:stackoverflow&gt; SELECT * FROM empByRole WHERE role = 'IT Engineer';

 role        | id | fname | lname
-------------+----+-------+-------
 IT Engineer |  1 | Angel |   Pay

(1 rows)
</code></pre>
",['table']
25935985,25983553,2014-09-19 14:11:58,Cassandra schema design sorted by time,"<p>I'm new on cassandra data modeling, I realy need same advice, here is my problem:</p>

<p>I need to create a new column family that will allow me to store and retrieve last inserted scores : </p>

<pre><code>CREATE TABLE average_score(
    audit_time timestamp PRIMARY KEY,
    pages_count int,
    score float,
)
</code></pre>

<p>The inserted data is not sorted according to primary key (i'm using a random partinioner(default)), do you have any solution please ? Can I specify a different partitionner just for this family column ? </p>

<p>thanks</p>
",<cassandra><schema-design>,"<p>Here is an example of an hour-partitioned series table that might clarify some things for you:</p>

<pre><code>CREATE TABLE average_score(
    hour timestamp,
    audit_time timeuuid,
    pages_count int,
    score float,
    PRIMARY KEY (hour, audit_time)
)
WITH CLUSTERING ORDER BY (audit_time DESC)
</code></pre>

<ul>
<li><p>Because it comes first, <code>hour</code> is our ""partition"" key, i.e. it will be used to physically distribute our data across the cluster. (When you write, you will have to supply this value, rounded down to the start of the current hour.)</p></li>
<li><p><code>audit_time</code> is our first ""clustering"" key, i.e. it is used to order and identify rows in a given <code>hour</code> partition on a particular node. We've chosen <code>timeuuid</code> to prevent overwrites. (You can pull out the actual time with the <code>dateOf</code> function. See <a href=""http://www.datastax.com/documentation/cql/3.1/cql/cql_reference/timeuuid_functions_r.html"" rel=""nofollow"">http://www.datastax.com/documentation/cql/3.1/cql/cql_reference/timeuuid_functions_r.html</a>)</p></li>
<li><p><code>WITH CLUSTERING ORDER BY (audit_time DESC)</code> directs C* to store rows within a partition in descending order on disk, which is probably the right decision if you intend on using <code>ORDER BY audit_time DESC</code> in most of your queries. (See <a href=""http://www.datastax.com/documentation/cql/3.1/cql/cql_reference/refClstrOrdr.html"" rel=""nofollow"">http://www.datastax.com/documentation/cql/3.1/cql/cql_reference/refClstrOrdr.html</a>)</p></li>
</ul>

<p>Caveat: Although we've partitioned the data fairly granularly, you will still have a bit of a write hotspot if you're just appending new scores as they are registered.</p>

<p>Cheers!</p>

<p>P.S. If you're still stuck, the DataStax CQL documentation is a great resource for data modeling help.</p>
",['table']
25975675,25988263,2014-09-22 13:44:53,Defining table compound primary keys in rails for Cassandra,"<p>Given the following pseudo-cql table structure:</p>

<pre><code>CREATE TABLE searches (
  category text, 
  timestamp timestamp, 
  no_of_searches int, 
  avg_searches double, 
  PRIMARY KEY((category, timestamp), no_of_searches)
);
</code></pre>

<p>and the following Rails Cequel model: </p>

<pre><code>class Search
  include Cequel::Record

  # Table columns
  key :category,        :text
  key :timestamp,       :timestamp 
  key :no_of_searches,  :int
  column :avg_searches, :double
end
</code></pre>

<p>when I try to synchronise the model using:</p>

<pre><code>rake cequel:migrate
</code></pre>

<p>the following rake error is thrown:</p>

<pre><code>rake aborted!
Cequel::InvalidSchemaMigration: Existing partition keys category,timestamp differ from specified partition keys timestamp
</code></pre>

<p>I'm trying to get the above rails model to synchronise with the above table using the partition keys, although it's stating that the two sets of keys are different. I've tried defining the keys in the same order, but has not worked.</p>

<p>My objective is to get the pre-defined database table with partition keys working with the rails model. Any help would be grateful!</p>
",<ruby-on-rails><cassandra><cequel>,"<p>The <code>key</code> method supports an options hash as a third parameter. The options hash adds further support to the defined key such as <code>order</code> and <code>partitioning</code>.</p>

<p>Based upon the given table definition it would mean your table columns would look something like this:</p>

<pre><code># Table columns
key :category,        :text,      { partition: true }
key :timestamp,       :timestamp, { partition: true }
key :no_of_searches,  :int
column :avg_searches, :double
</code></pre>

<p>Unfortunately this is not documented within the Cequel README, however it is documented within the code, for which can be found <a href=""https://github.com/cequel/cequel/blob/1ec29b13f603c00d2dffdf8502be16a6b8d1cb65/lib/cequel/record/properties.rb#L81"" rel=""noreferrer"">here</a>.</p>
",['table']
26122100,26123844,2014-09-30 13:28:51,String sorting in Cassandra CQL,"<p>When querying the text primary key in Cassandra CQL, the string comparison works the opposite way of what one expect, i.e.</p>

<pre>
cqlsh:test> select * from sl;

 name                     | data
--------------------------+------
 000000020000000000000003 | null
 000000010000000000000005 | null
 000000010000000000000003 | null
 000000010000000000000002 | null
 000000010000000000000001 | null

cqlsh:test> select name from sl where token(name) &lt token('000000010000000000000005');
name
--------------------------
 000000020000000000000003

(1 rows)

cqlsh:test> select name from sl where token(name) &gt token('000000010000000000000005');
 name
--------------------------
 000000010000000000000003
 000000010000000000000002
 000000010000000000000001

(3 rows)
</pre>

<p>In constrast, this is what I get from the string comparison in Python (and I think in most other languages):</p>

<pre><code>&gt;&gt;&gt;'000000020000000000000003' &lt; '000000010000000000000005'
False
</code></pre>

<p>If I query without the token function, I get the following error:</p>

<pre>
cqlsh:test> select name from sl where name &lt '000000010000000000000005';
Bad Request: Only EQ and IN relation are supported on the partition key (unless you use the token() function)
</pre>

<p>The table description is:</p>

<pre><code>CREATE TABLE sl (
  name text,
  data blob,
  PRIMARY KEY (name)
) WITH
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  index_interval=128 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  default_time_to_live=0 AND
  speculative_retry='99.0PERCENTILE' AND
  memtable_flush_period_in_ms=0 AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'LZ4Compressor'};
</code></pre>

<p>Is there an explanation in the docs I've missed or somewhere else on why such a strange string comparison order is chosen, or does the string comparison operator do not what I expect of it (i.e. returning me some unrelated order, i.e. the order of the rows as they are written to the database). I'm using the Murmur3Partitioner partitioner in case it matters.</p>
",<cassandra><cql><nosql>,"<p>In Cassandra rows are ordered by the hash of their key value.  With the Random and Murmur3 partitioners there is a random element to the hash value, and thus the order is A) not meaningful, and B) designed to be evenly spread across the ring.</p>

<p>Therefore querying for tokens less than <code>token('000000010000000000000005')</code> won't do a comparison based-on the string value of ""000000010000000000000005"".  It will do a comparison on the hashed token value.  By virtue of the results that you are seeing, the token value of the string ""000000020000000000000003"" is less than the token value of ""000000010000000000000005"".</p>

<p>For more information, check through this doc from DataStax: <a href=""http://www.datastax.com/documentation/cql/3.1/cql/cql_using/paging_c.html"" rel=""nofollow"">Paging Through Unordered Partitioner Results</a>.</p>

<p>Assuming that you want to be able to query your data by the value of ""name"", you could build a table kind of like this:</p>

<pre><code>CREATE TABLE sl (
  type text,
  name text,
  data blob,
  PRIMARY KEY (type, name)
)
</code></pre>

<p>I've created <code>type</code> as a partitioning key.  I'm not sure if your data makes sense to be divided by ""type"" (or anything else for that matter), so it's more for the sake of example than anything else.  Anyway, with <code>name</code> as a clustering key (determining the on-disk sort order) this query would work:</p>

<pre><code>select * from sl where type='sometype' AND name &lt; '000000010000000000000005';
</code></pre>

<p>Again its just an example, but I hope that helps to point you in the right direction.</p>
",['table']
26126268,26127947,2014-09-30 17:03:28,Cassandra C# insert seems to be deleting prior data?,"<p>I created a cassandra db like this:</p>

<pre><code>cqlsh:timeseries&gt; describe keyspace timeseries;

CREATE KEYSPACE timeseries WITH replication = {
  'class': 'SimpleStrategy',
  'replication_factor': '1'
};

USE timeseries;

CREATE TABLE option_data (
  ts timestamp,
  ask decimal,
  bid decimal,
  expiry timestamp,
  id text,
  strike decimal,
  symbol text,
  PRIMARY KEY ((ts))
) WITH
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  index_interval=128 AND
  read_repair_chance=0.000000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  default_time_to_live=0 AND
  speculative_retry='99.0PERCENTILE' AND
  memtable_flush_period_in_ms=0 AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'LZ4Compressor'};

CREATE TABLE underlying_data (
  symbol text,
  ask decimal,
  bid decimal,
  ts bigint,
  PRIMARY KEY ((symbol))
) WITH
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  index_interval=128 AND
  read_repair_chance=0.000000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  default_time_to_live=0 AND
  speculative_retry='99.0PERCENTILE' AND
  memtable_flush_period_in_ms=0 AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'LZ4Compressor'};

CREATE INDEX underlying_data_ts_idx ON underlying_data (ts);

cqlsh:timeseries&gt;
</code></pre>

<p>I have a C# function:</p>

<pre><code>public void InsertUnderlying(long timestamp, string symbol, decimal bid, decimal ask)
        {
            var batchStmt = new BatchStatement();
            var v2Insert = new SimpleStatement(""insert into underlying_data "" +
                ""(ts, symbol, bid, ask) values(?, ?, ?, ?);"");
            batchStmt.Add(v2Insert.Bind(timestamp, symbol, bid, ask));

            session.Execute(batchStmt);
        }
</code></pre>

<p>I call this function in realtime to add data. However, when I do a query from the CQL,</p>

<pre><code>cqlsh:timeseries&gt; select * from underlying_data;
</code></pre>

<p>I only see one row even though I have called this function many many times. Not sure how I append data instead of overwriting it?</p>
",<c#><cassandra><cql>,"<p>In Cassandra, primary keys are unique.  Your table <code>underlying_data</code> is only keyed on the <code>symbol</code> column:</p>

<pre><code>PRIMARY KEY ((symbol))
</code></pre>

<p>That means all inserts for a particular symbol will overwrite each other:</p>

<pre><code>INSERT INTO underlying_data (symbol, ts, ask, bid) VALUES ('SPX',1412102636,3.1,4.0);
INSERT INTO underlying_data (symbol, ts, ask, bid) VALUES ('SPX',1412102708,3.0,4.4);
INSERT INTO underlying_data (symbol, ts, ask, bid) VALUES ('SPX',1412102731,2.1,5.0);

SELECT * FROM underlying_data;

 symbol | ts         | ask | bid
--------+------------+-----+-----
    SPX | 1412102731 | 2.1 | 5.0
</code></pre>

<p>To store each <code>INSERT</code>, add <code>ts</code> to your primary key definition:</p>

<pre><code>PRIMARY KEY (symbol, ts)
</code></pre>

<p>Additionally, Cassandra does not differentiate between an <code>INSERT</code> and an <code>UPDATE</code> (essentially an ""UPSERT"").  While syntactically different, they both accomplish the same thing: storing column values for a specific key.  This means that you can insert new records with an <code>UPDATE</code>, as well as update existing records with an <code>INSERT</code>.  Ike Walker has a good blog posting that describes this: <a href=""http://planetcassandra.org/blog/how-to-do-an-upsert-in-cassandra/"" rel=""nofollow"">How to do an Upsert in Cassandra</a>.</p>
",['table']
26232092,29431825,2014-10-07 08:48:35,Getting Tableau to talk to Spark and Cassandra,"<p>The DataStax spark cassandra connector is great for interacting with Cassandra through Apache Spark. With Spark SQL 1.1, we can use the thrift server to interact with Spark with Tableau. Since Tableau can talk to Spark, and Spark can talk to Cassandra, there's surely some way to get Tableau talking to Cassandra through Spark (or rather Spark SQL). I can't figure out how to get this running. Ideally, I'd like to do this with Spark Standalone cluster + a cassandra cluster (i.e. without additional hadoop set up). Is this possible? Any pointers are appreciated.</p>
",<cassandra><apache-spark><tableau-api>,"<p>The HiveThriftServer has a <code>HiveThriftServer2.startWithContext(sqlContext)</code> option so you could create your sqlContext referencing C* and the appropriate table / CF and then pass that context to the thrift server.</p>

<p>So something like this:</p>

<pre><code>import  org.apache.spark.sql.hive.HiveContext
import  org.apache.spark.sql.catalyst.types._
import  java.sql.Date
val  sparkContext  =  sc
import  sparkContext._
val  sqlContext  =  new  HiveContext(sparkContext)
import  sqlContext._
makeRDD((1,""hello"") :: (2,""world"") ::Nil).toSchemaRDD.cache().registerTempTable(""t"")
import  org.apache.spark.sql.hive.thriftserver._
HiveThriftServer2.startWithContext(sqlContext)
</code></pre>

<p>So instead of starting the default thriftserver from Spark you could just lunch you cusotm one.</p>
",['table']
26279914,26295656,2014-10-09 13:37:37,Will the DataStax Cluster class ever refresh IP address from the hostname given to builder.addContactPoint() if DNS changes?,"<p>I've a problem, once set host name, cluster wouldn't update it's IP, even in DNS changes.
Or what is the recommended way of making the application resilient to the fact that more nodes can be added to DNS round robin and old nodes decomissioned ?</p>
",<dns><cassandra><datastax><datastax-java-driver>,"<p>I had same thing with Astyanax driver. For me it looks like it works this way:</p>

<ul>
<li>DNS name is used only when initial connection to cluster is created. At this point driver collects data about cluster nodes. This information is kept in terms of IP addresses already and DNS names are not used any more. Sub-sequential changes in the cluster topology are propagated into the client also using IP addresses.</li>
</ul>

<p>So, when you add more nodes to the cluster, you actually do not have to assign domain names to them. Just adding a node to the cluster propagates its IP address to the cluster topology table and this info is distributed among all cluster members and smart clients like Java Driver (some third party clients might not have this info and will use only seed nodes to pass queries to).
When you decommission node it works same way. Just all cluster nodes and smart clients receive information that node with a particular IP is not in the cluster any more. It can be even initial seed node.</p>

<p>->Domain name makes sense only for clients which hadn't established cluster connection.</p>

<p>In case you really need to switch IP you have to:</p>

<ul>
<li>Join node with new IP</li>
<li>Decommission node with old IP</li>
<li>Assign DNS name to new IP</li>
</ul>
",['table']
26307711,26308943,2014-10-10 20:39:45,Find upserted keys in Cassandra columnfamily within time interval,"<p>Is it possible to get the list of affected keys in a cassandra column family within a time interval.Cassandra does maintain column level timestamps. Can it be leveraged somehow ? </p>

<p>I have a simple columnfamily (key, value) and I want to know which keys were updated/inserted in the last one day. Unfortunately, I can't modify the column family to include a timestamp column and index it, as is generally advised. Does anyone have any other thoughts/hacks on how can this problem be tackled ?</p>
",<cassandra><cql><pycassa>,"<p>If you can't modify the table you have to scan the whole table across the cluster.
In CQL 3 and later you can write</p>

<pre><code>SELECT key, value, WRITETIME(value) FROM kvtable;
</code></pre>

<p>This is the only way and it's very onerous to the cluster</p>

<p>HTH,
Carlo</p>
",['table']
26354732,26360631,2014-10-14 07:14:07,Is it possible to use complex boolean logic in a CQL query?,"<p>In cassandra , for a table like this </p>

<pre><code>CREATE TABLE test.TestTable3 (
   PK1 int,
   PK2 int,
   CK3 int,
   CK4 int,
   CK5 text,
   CK6 int,
   CK7 int,
   Dump text,
   PRIMARY KEY ((PK1,PK2,CK3),CK4,CK5,CK6,CK7)
);
</code></pre>

<p>How to do query something like ie using or with partial or </p>

<pre><code>Select * from testtable3 where Pk1=1 and Pk2=2 and Ck3 =2 and Ck4 =4 and (( CK=5 =""ABC"" 
and  CK6=1) or ( CK=5 =""BBC"" and CK6=1))
</code></pre>

<p>regards</p>
",<cassandra><cql>,"<p>First of all, let me just say that Cassandra does not support complex, ad-hoc queries.  The Cassandra Query Language (CQL) is a <em>subset</em> (not an implementation) of SQL.  Therefore, you cannot do everything in CQL that may work in SQL.</p>

<p>Secondly, CQL does not support the <code>OR</code> operator.  However, you can use the <code>IN</code> operator in some cases.  Essentially, <code>IN</code> only works on the partitioning key.  And only on the last partitioning key, if using a composite partitioning key (as you are).  Be warned though, that as Cassandra is designed to serve reads on specific, primary key queries, it is not recommended to rely on the use of <code>IN</code>.  See <a href=""http://www.datastax.com/documentation/cql/3.1/cql/cql_reference/select_r.html"" rel=""nofollow"">SELECT: When <em>not</em> to use IN</a> for more information.</p>

<p>Third, unlike a relational database, in Cassandra it is good practice to denormalize/replicate your data while building it into multiple tables to support each desired query.  With that in-mind, you could create a query table like this:</p>

<pre><code>CREATE TABLE TestTable4 (
   PK1 int,
   PK2 int,
   CK3 int,
   CK4 int,
   CK5 text,
   CK6 int,
   CK7 int,
   Dump text,
   PRIMARY KEY ((PK1,PK2,CK3,CK4,CK6,CK5),CK7)
);
</code></pre>

<p>Note how I have extended the partition key, and changed the order of <code>CK5</code> and <code>CK6</code>.  Given this table structure, this query works:</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; SELECT * FROM testtable4 WHERE pk1=1 AND pk2=2 AND ck3=3
              ... AND ck4=4 AND ck6=1 AND ck5 IN ('ABC','BBC');

 pk1 | pk2 | ck3 | ck4 | ck6 | ck5 | ck7 | dump
-----+-----+-----+-----+-----+-----+-----+---------
   1 |   2 |   3 |   4 |   1 | ABC |   7 | row ABC
   1 |   2 |   3 |   4 |   1 | BBC |   7 | row BBC

(2 rows)
</code></pre>

<p>Note: I used this structure to show you how Cassandra query-based-modeling works.  I have no idea if this would work with your overall application or data, so take that for what it's worth.</p>
",['table']
26423038,26423147,2014-10-17 10:27:46,"Do tables share partitions (""low level rows"") in Cassandra?","<p>Let's say I have two tables (column families) defined through CQL.</p>

<pre><code>CREATE TABLE a (
    pk    uuid,
    cka   int,
    val   text,
    PRIMARY KEY (pk, cka)
);

CREATE TABLE b (
    pk    uuid,
    ckb   text,
    val1  boolean,
    val2  decimal,
    PRIMARY KEY (pk, ckb)
);
</code></pre>

<p>If I now insert a row into each table with the same partition key:</p>

<pre><code>INSERT INTO a (pk, cka, val)
     VALUES ('f47ac10b-58cc-4372-a567-0e02b2a3d379', 5, 'hi');

INSERT INTO b (pk, ckb, val1, val2)
     VALUES ('f47ac10b-58cc-4372-a567-0e02b2a3d379', 'x', 'hello', 'hey');
</code></pre>

<p>Will there now be 1 or 2 rows on the storage level?</p>
",<cassandra><cql3>,"<p>There will be 2.</p>

<p>Data in Cassandra is written into ""memtables"", and then flushed to ""SSTables"" on-disk. Both memtables and SSTables are maintained <em>on a per-column family basis</em>, so rows in different column families (tables) will always create distinct rows at the storage level.</p>

<p>See <a href=""http://www.datastax.com/docs/1.1/dml/about_writes"" rel=""nofollow"">http://www.datastax.com/docs/1.1/dml/about_writes</a></p>

<blockquote>
  <p>Cassandra writes are first written to a commit log (for durability),
  and then to an in-memory table structure called a memtable. A write is
  successful once it is written to the commit log and memory, so there
  is very minimal disk I/O at the time of write. Writes are batched in
  memory and periodically written to disk to a persistent table
  structure called an SSTable (sorted string table). Memtables and
  SSTables are maintained per column family. Memtables are organized in
  sorted order by row key and flushed to SSTables sequentially (no
  random seeking as in relational databases).</p>
</blockquote>
",['table']
26506545,26506942,2014-10-22 11:36:33,Primary key and indexing in cassandra,"<p>new to cassandra, still learning.</p>

<pre><code>create table url (
  id_website int,
  url varchar,
  data varchar,
  primary key(url, id_website)
);
</code></pre>

<p>Hi I have a table of url for a website.</p>

<p>I don't want all the url being on the same node, that's why primary key is url first, so it will be the partition key.</p>

<p>most of the time I'm going to retrieve the data for a specific url, eg : ""url = ? and id_website = ?""</p>

<p>However what about the performance when I want to retrieve a part/all the urls of a website: </p>

<pre><code>select * from url where id_website = 1 allow filtering limit XX;
</code></pre>

<p>I think this query is going to be dispatched on all the nodes, then, table scanning for id_website=  1 until limit is reach then merged and sent back to my client.</p>

<p>But is this scanning going to use an index and be effective or read the values of the column id_website one by one and compare (ineffective so) ? I did set id_website part of the primary key so I expect it to be indexed, but I really don't know.</p>

<p>Do we have some tools on cassandra like the EXPLAIN of mysql to check if a query is using index or not.</p>

<p>Thanks.</p>

<p>--</p>

<h1>EDIT</h1>

<blockquote>
  <p>Create a second table with id_website as partition key (and
  write/delete in batch)</p>
</blockquote>

<p>I don't want to use this solution because I may have one or two website which are really huge and have millions of urls (and millions of others website with little of urls). </p>

<p>If I have a partition key on id_website, and that this two or three website stay on the same node it may cause storage problem or the node handling these websites maybe too much solicited while the other get nothing. I want to spread the data over all the nodes. That's why I insisted to partition on the url.</p>

<blockquote>
  <p>You create a secondary index on id_website (which creates a table for
  you)</p>
</blockquote>

<p>What about this solution ? If I understand, each node would have a table indexing the rows it stores based on id_website (so not the rows of others nodes). So I can spread my urls across many nodes, I won't have one node handling a big indexing containing all the urls of a specific website.</p>

<p>Now when I use my query </p>

<pre><code>select * from url where id_website = 1 allow filtering limit XX;
</code></pre>

<p>Each node receive the query, but they don't have to loop through the partition (url column) this time, they can directly lookup up in the index the urls belonging to id_website, and return the rows (or nothing). Right ?</p>

<p>The contra of this solution is everytime the request is done, it's going to hit each node, however, it should be fast thanks to the new index ?</p>
",<cassandra>,"<p>You're on the right way. Using allow filtering you're just asking cassandra to scan all nodes: very ineffective. <code>id_website</code> is indexed within each partition but since you are not telling Cassandra where to go he must hit all partitions (all nodes) even those who doesn't contain information for the selected <code>id_website</code> -- once Cassandra hit a partition knows how to look for this information and does not need to scan the whole partition to get data back.</p>

<p>To solve this problem in Cassandra you have to denormalize and in this situation you can do it in two possible ways:</p>

<ol>
<li>Create a second table with id_website as partition key (and write/delete in batch)</li>
<li>You create a secondary index on id_website (which creates a table for you)</li>
</ol>

<h2>**EDIT DUE TO QUESTION EDIT**</h2>

<p>What you said is right: secondary indexes are handled as <em>""local indexes""</em> -- each node creates a local index table only for the data it owns. The following is a <a href=""http://www.wentnet.com/blog/?p=77"" rel=""nofollow"">good reading</a> about secondary indexes (that you already understood)</p>

<p>Once you created the index you have to remove <code>ALLOW FILTERING</code> from the query.</p>

<p>HTH,
Carlo</p>
",['table']
26548788,26550112,2014-10-24 13:23:39,SELECT DISTINCT cql ignores WHERE clause,"<p>Executing two identical requests but the DISTINCT keyword gives unexpected results. Without the keyword, the result is ok but with DISTINCT, it looks like the where clause is ignored. Why ?</p>

<p>Cqlsh version:</p>

<pre><code>Connected to Test Cluster at localhost:9160.
[cqlsh 4.1.1 | Cassandra 2.0.6 | CQL spec 3.1.1 | Thrift protocol 19.39.0]
</code></pre>

<p>Table considered:</p>

<pre><code>DESCRIBE TABLE events;

CREATE TABLE events (
  userid uuid,
  ""timestamp"" timestamp,
  event_type text,
  data text,
  PRIMARY KEY (userid, ""timestamp"", event_type)
) WITH
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  index_interval=128 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  default_time_to_live=0 AND
  speculative_retry='99.0PERCENTILE' AND
  memtable_flush_period_in_ms=0 AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'LZ4Compressor'};
</code></pre>

<p>Table content:</p>

<pre><code>SELECT * FROM events;

 userid                               | timestamp                | event_type | data
--------------------------------------+--------------------------+------------+------
 aaaaaaaa-be1c-44ab-a0e8-f25cf6064b0e | 1970-01-17 09:06:17+0100 |       toto | null
 4271a78f-be1c-44ab-a0e8-f25cf6064b0e | 1970-01-17 09:06:17+0100 |       toto | null
 4271a78f-be1c-44ab-a0e8-f25cf6064b0e | 1970-01-17 09:07:17+0100 |       toto | null
 4271a78f-be1c-44ab-a0e8-f25cf6064b0e | 1970-01-17 09:08:17+0100 |       toto | null
 4271a78f-be1c-44ab-a0e8-f25cf6064b0e | 1970-01-17 09:09:17+0100 |       toto | null
 4271a78f-be1c-44ab-a0e8-f25cf6064b0e | 1970-01-17 09:10:17+0100 |       toto | null

(6 rows)
</code></pre>

<p>Request1: Request without DISTINCT</p>

<pre><code>SELECT userid FROM events WHERE timestamp &gt; '1970-01-17 09:07:17+0100' ALLOW FILTERING;

 userid
--------------------------------------
 4271a78f-be1c-44ab-a0e8-f25cf6064b0e
 4271a78f-be1c-44ab-a0e8-f25cf6064b0e
 4271a78f-be1c-44ab-a0e8-f25cf6064b0e

(3 rows)
</code></pre>

<p>Request2: Same request with DISTINCT</p>

<pre><code>SELECT DISTINCT userid FROM events WHERE timestamp &gt; '1970-01-17 09:07:17+0100' ALLOW FILTERING;

 userid
--------------------------------------
 aaaaaaaa-be1c-44ab-a0e8-f25cf6064b0e
 4271a78f-be1c-44ab-a0e8-f25cf6064b0e

(2 rows)
</code></pre>

<p><strong>EDIT 1</strong><br>
here is some context.<br>
This table ""events"" is subject to a lot of writes, it receives around ~1k insertions per second and I have a batch script that checks those events every 5 minutes.<br>
This batch script has 2 needs:<br>
  1- get all userids that have been active in the last 5 minutes (i.e every userid present in the events from the last 5 minutes)<br>
  2- get all events related to those userids (not only for the last 5 minutes)</p>

<p>I used to have two different tables to handle this. One table ""activeusers"" for the first request and the ""events"" table like I have described here for the second request. My problem with that is just that it requires from my server to write in two different tables when it receives an event. So I tried this using only the events table.  </p>
",<cassandra><distinct><cql>,"<p>It happens that way because in Cassandra CQL <code>DISTINCT</code> is designed to return only the partition (row) keys of your table (column family)...which must be unique.  Therefore, the <code>WHERE</code> clause can only operate on partition keys when used with <code>DISTINCT</code> (which in your case, isn't terribly useful).  If you take the <code>DISTINCT</code> out, <code>WHERE</code> can then be used to evaluate the clustering (column) keys within each partition key (albeit, with <code>ALLOW FILTERING</code>).</p>

<p>I feel compelled to mention that <code>ALLOW FILTERING</code> is not something you should be doing a whole lot of...and definitely not in production.  If that query is one you need to run often (querying events for <code>userids</code> after a certain <code>timestamp</code>) then I would suggest partitioning your data by <code>event_type</code> instead:</p>

<pre><code>PRIMARY KEY (event_type, ""timestamp"", userid)
</code></pre>

<p>Then you'll be able to run this query without <code>ALLOW FILTERING</code>.</p>

<pre><code>SELECT userid FROM events WHERE event_type='toto' AND timestamp &gt; '1970-01-17 09:07:17+0100'
</code></pre>

<p>Without knowing anything about your application or use case, that may or may not be useful to you.  But consider it as an example, and as an indication that there may be a better way build your model to satisfy your query pattern(s).  Check out <a href=""http://planetcassandra.org/blog/getting-started-with-time-series-data-modeling/"" rel=""noreferrer"">Patrick McFadin's article on timeseries data modeling</a> for more ideas on how to model for this problem.</p>
",['table']
26584607,26602289,2014-10-27 09:37:22,Using Apache Cassandra In Coldfusion,"<p>I'm trying to use Apache Cassandra on a project I'm coding using Coldfusion. Since Coldfusion doesn't have a driver for Apache Cassandra and vice versa, I'm attempting to use Cassandra's Java drivers.</p>

<p>I'm pretty much a Java newbie so please bear with me.</p>

<p>I've managed to copy the necessary .jar files to /opt/railo/lib/ (I'm using Railo) and also managed to connect to Cassandra using Coldfusion using the code below. What I need help with is looping through the results returned by Cassandra when I run a query. I've included my very simple code below:</p>

<pre><code>&lt;cfset MyClusterInit = CreateObject(""java"", ""com.datastax.driver.core.Cluster"")&gt;
&lt;cfset MyCluster = MyClusterInit.builder().addContactPoint(""127.0.0.1"").build()&gt;
&lt;cfset MySession = MyCluster.connect(""mytest"")&gt;
&lt;cfset GetCustomer = MySession.execute(""SELECT * FROM customer"")&gt;
&lt;cfdump var=""#GetCustomer#""&gt;
</code></pre>

<p>How do I loop through the results returned? The cfdump returns the Java method ArrayBackedResultSet$SinglePage. It's not something that I can loop over using Coldfusion.</p>

<p>From the ""Getting Started With Cassandra and Java"" post, I see the code as below:</p>

<pre><code>ResultSet results = session.execute(""SELECT * FROM users WHERE lastname='Jones'"");
for (Row row : results) {
System.out.format(""%s %d\n"", row.getString(""firstname""), row.getInt(""age""));
}
</code></pre>

<p>How do I replicate the above in Coldfusion?</p>

<p>Many thanks for taking your time to help.</p>
",<java><coldfusion><cassandra><cassandra-2.0>,"<p>As pointed out by Leigh and Mark A Kruger above, using the JDBC driver was the smarter way to connect to Cassandra in Coldfusion/Railo. Just follow the instructions below:</p>

<ol>
<li><p>Download the JDBC driver at <a href=""https://code.google.com/a/apache-extras.org/p/cassandra-jdbc/"" rel=""noreferrer"">https://code.google.com/a/apache-extras.org/p/cassandra-jdbc/</a>. It works with the latest version of Cassandra (at time of writing)</p></li>
<li><p>Make sure to also download the Cassandra JDBC dependencies</p></li>
<li><p>Copy the jar files to your Coldfusion lib directory</p></li>
<li><p>Restart Coldfusion/Railo</p></li>
<li><p>Create a datasource type ""Other""</p></li>
<li><p>For the Driver Class, enter ""<strong>org.apache.cassandra.cql.jdbc.CassandraDriver</strong>""</p></li>
<li><p>The connection string should be ""<strong>jdbc:cassandra://host1--host2--host3:9160/keyspace1?primarydc=DC1&amp;backupdc=DC2&amp;consistency=QUORUM</strong>""</p></li>
<li><p>That's it! You can now query Cassandra like you query any other database</p></li>
</ol>

<p>Not only that. I managed to get the Java driver working directly and the performance of the JDBC driver was much better than calling the Java driver directly. My test was just a very simple table with 3 records and it took 50ms to connect directly using Java while using the JDBC driver took less than 5ms.</p>

<p>Thanks to Leigh and Mark A Kruger for the tip!</p>
",['table']
26640385,26640815,2014-10-29 20:46:54,Can cassandra compaction strategy be changed dynamically?,"<p>I am quite new to using Cassandra and have a basic question that I was seeking an answer to. I am using the default compaction strategy which the Size-Tiered. I am aware that this can be changed to the Levelled Compaction strategy by running a command like this:</p>

<pre><code>ALTER TABLE users WITH
compaction = { 'class' :  'LeveledCompactionStrategy'  }
</code></pre>

<p>I am however unsure if this change takes into effect at runtime on a particular keyspace, or if I would need to restart the node for the changes to take effect. I read in the Datastax documentation (<a href=""http://www.datastax.com/documentation/cassandra/1.2/cassandra/configuration/configCassandra_yaml_r.html"" rel=""noreferrer"">http://www.datastax.com/documentation/cassandra/1.2/cassandra/configuration/configCassandra_yaml_r.html</a>) that changes to global configurations in the cassandra.yaml file only takes effect after a node is restarted and would like to know if the same applies for the keyspace specific properties like compaction strategy as well.</p>

<p>Thanks in advance for your help.</p>
",<cassandra><cqlsh>,"<p>The newer documentation on <a href=""http://datastax.com/documentation/cassandra/2.1/cassandra/operations/ops_configure_compaction_t.html"" rel=""nofollow"">configuring compaction</a> indicates that the correct procedure to enable Leveled Compaction is the <code>ALTER TABLE</code> statement that you have above.  While you are correct that changes to the <code>cassandra.yaml</code> file will require a node(s) restart to take effect, table configuration changes typically do not.  It should use Leveled Compaction for that table the next time compaction is triggered.</p>

<p>Additionally, you should read through Tyler Hobbs' article titled <a href=""http://www.datastax.com/dev/blog/when-to-use-leveled-compaction"" rel=""nofollow"">When to Use Leveled Compaction</a>.  Since you are new to Cassandra, give that a look just to be sure you have an appropriate use case.</p>
",['table']
26654836,26663343,2014-10-30 13:51:32,How to auto replicate data in cassandra,"<p>I am very new to cassandra and currently in early stage of project where i am studying cassandra.</p>

<p>Now since cassandra says to de-normalize data and replicate it. So, i have a following scenario :</p>

<p>I have table, <code>user_master</code>, for users. A user has </p>

<ul>
<li>subject [type text]</li>
<li>hobbies [type list]</li>
<li>uid [type int]</li>
<li>around 40 more attributes</li>
</ul>

<p>Now, a user wants to search for another user. This search should look for all user who matches the <code>subject</code> and <code>hobbies</code> provided by user. For this reason i am planning to make a different table <code>user_discovery</code> which will have following attribute only for every user</p>

<ul>
<li>subject [type text]</li>
<li>hobbies [type list]</li>
<li>uid [type int]</li>
</ul>

<p>*other irrelevant attributes won't be part of this table.
Now my question is:</p>

<ol>
<li><p><strong>Do i need to write on both tables for every insert/update in <code>user_master</code>? Can updation of <code>user_discovery</code> be automated when their is any insert/update in <code>user_master</code>.</strong></p></li>
<li><p><strong>Even after studying a bit, i am still not so much sure that making a separate table would increase the performance.Since, number of users would be same in both table (yes, number of column would be very less in <code>user_discovery</code>). Any comment on this would be highly appreciated.</strong></p></li>
</ol>

<p>Thanks</p>
",<cassandra><data-modeling><nosql>,"<p>The idea of separate tables for queries is to have the key of the table contain what you are looking for.</p>

<p>You don't say what the key of your second table looks like, but your wording ""the following attributes for every user"" looks like you plan to have the user (Id?) as key. This would indeed have no performance advantage.</p>

<p>If you want to find users by their hobby make a table having the hobby as key, and the user id (or whatever it is you use to look up users) as columns. Write one row per hobby, listing all users having that hobby. Write the user into every row matching one of his hobbies.</p>

<p>Do the same for the subject (i.e. separate table, subject as key, user ids as columns).</p>

<p>Then, if you want to find a user having a list of specific hobbies, make one query per hobby, creating the intersection of the users. </p>

<p>To use these kind of lookup-tables you would have indeed to update all table every time you update a user. </p>

<p>Disclaimer: I used this kind of approach rather successfully in a relative complex setting managing a few hundred thousand users. However, this was two years ago, on a Cassandra 1.5 system. I haven't really looked into the new features of Cassandra 2.0, so I have no idea whether it would be possible to use a more elegant approach today.</p>
",['table']
26683942,26717327,2014-10-31 21:52:31,Spark Cassandra Connector - not able to fetch dynamic columns,"<p>I have a cassandra column family with a lot of dynamic columns. I am running a simple Spark-Cassandra connector example where I am trying to fetch all the data from this table. The issue is that it is not fetching any of the dynamic columns from my column family. </p>

<p>In my example and code snippet below, it is able to fetch the primary key and secondary index column for all the rows but not any of the other columns (It has 30+ more dynamic columns). I have a feeling the connector supports fetching of only partition and clustering keys as columns as of now, based on my reading here (<a href=""https://stackoverflow.com/questions/26001566/spark-datastax-java-api-select-statements"">Spark Datastax Java API Select statements</a>). Could someone please confirm if my understanding is correct. It would be great if someone can suggest how to fix this ?</p>

<pre><code>/**
 * Loads a cassandra column family as a spark RDD.
 */
public static CassandraJavaRDD&lt;CassandraRow&gt; getCassandraTableRDD(
        JavaSparkContext context, String keyspace, String table)
{
    return javaFunctions(context).cassandraTable(keyspace, table);
}

CREATE TABLE source_product_canonical_data_sample (
  'key' text PRIMARY KEY,
  source text
) WITH
  comment='' AND
  comparator=text AND
  read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  default_validation=text AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  replicate_on_write='true' AND
  compaction_strategy_class='SizeTieredCompactionStrategy' AND
  compression_parameters:sstable_compression='LZ4Compressor';
</code></pre>
",<cassandra><apache-spark>,"<p>Your CQL table definition is not aware of your ""dynamic columns"". There is no compound primary key with clustering columns in it. Dynamic columns / wide-rows are terms related to the old thrift data model, and in CQL they have been replaced with compound primary key.</p>

<p>See this excellent blog post by Jonathan Ellis explaining how to transition to the new data model: <a href=""http://www.datastax.com/dev/blog/does-cql-support-dynamic-columns-wide-rows"" rel=""nofollow"">http://www.datastax.com/dev/blog/does-cql-support-dynamic-columns-wide-rows</a></p>
",['table']
26708594,26717954,2014-11-03 05:56:43,Problems In Cassandra ByteOrderedPartitioner in Cluster Environment,"<p>I am using cassandra 1.2.15 with ByteOrderedPartitioner in a cluster environment of 4 nodes with 2 replicas. I want to know what are the drawbacks of using the above partitioner in cluster environment? After a long search I found one drawback. I need to know what are the consequences of such drawback?</p>

<pre><code>1) Data will not distribute evenly. 
   What type of problem will occur if data are not distributed evenly?
</code></pre>

<p>Is there is any other drawback with the above partitioner in cluster environment if so, what are the consequences of such drawbacks? Please explain me clearly.</p>

<p>One more question is, Suppose If I go with Murmur3Partitioner the data will distribute evenly. But the order will not be preserved, however this drawback can be overcome with cluster ordering (Second key in the primary keys). Whether my understanding is correct?</p>
",<cassandra><nosql>,"<p>As you are using Cassandra 1.2.15, I have found a doc pertaining to Cassandra 1.2 which illustrates the points behind why using the ByteOrderedPartitioner (BOP) is a bad idea:</p>

<p><a href=""http://www.datastax.com/documentation/cassandra/1.2/cassandra/architecture/architecturePartitionerBOP_c.html"" rel=""nofollow noreferrer"">http://www.datastax.com/documentation/cassandra/1.2/cassandra/architecture/architecturePartitionerBOP_c.html</a></p>

<blockquote>
  <ul>
  <li><p><strong>Difficult load balancing</strong> More administrative overhead is required to    load balance the cluster. An ordered partitioner
  requires    administrators to manually calculate partition ranges
  (formerly token    ranges) based on their estimates of the row key
  distribution. In    practice, this requires actively moving node
  tokens around to    accommodate the actual distribution of data once
  it is loaded.</p></li>
  <li><p><strong>Sequential writes can cause hot spots</strong> If your application tends to    write or update a sequential block of rows at a time, then the
  writes    are not be distributed across the cluster; they all go to
  one node.    This is frequently a problem for applications dealing
  with    timestamped data. </p></li>
  <li><p><strong>Uneven load balancing for multiple tables</strong> If your    application has multiple tables, chances are that those tables have different row keys and different distributions of data. An ordered<br>
  partitioner that is balanced for one table may cause hot spots and uneven distribution for another table in the same cluster.</p></li>
  </ul>
</blockquote>

<p>For these reasons, the BOP has been identified as a <a href=""http://www.datastax.com/documentation/cassandra/1.2/cassandra/architecture/architecturePlanningAntiPatterns_c.html"" rel=""nofollow noreferrer"">Cassandra anti-pattern</a>.  Matt Dennis has a <a href=""http://www.slideshare.net/mattdennis"" rel=""nofollow noreferrer"">slideshare presentation on Cassandra Anti-Patterns</a>, and his slide about the BOP looks like this:</p>

<p><img src=""https://i.stack.imgur.com/49jZR.jpg"" alt=""OPP/BOP""></p>

<p>So seriously, do not use the BOP.</p>

<p>""however this drawback can be overcome with cluster ordering (Second key in the primary keys). Whether my understanding is correct?""</p>

<p>Somewhat, yes.  In Cassandra you can dictate the order of your rows (within a partition key) by using a clustering key.  If you wanted to keep track of (for example) station-based weather data, your table definition might look something like this:</p>

<pre><code>CREATE TABLE stationreads (
  stationid uuid,
  readingdatetime timestamp,
  temperature double,
  windspeed double,
PRIMARY KEY ((stationid),readingdatetime));
</code></pre>

<p>With this structure, you could query all of the readings for a particular weather station, and order them by <code>readingdatetime</code>.  However, if you queried all of the data (ex: <code>SELECT * FROM stationreads;</code>) the results probably will not be in any discernible order.  That's because the total result set will be ordered by the (random) hashed values of the partition key (stationid in this case).  So while ""yes"" you can order your results in Cassandra, you can only do so within the context of a particular partition key. </p>

<p>Also, there have been many improvements in Cassandra since 1.2.15.  You should definitely consider using a more recent (2.x) version.</p>
","['partitioner', 'table']"
26730984,26732791,2014-11-04 08:52:42,Cassandra SELECT on 2ndary index always sorted on partition key?,"<p>Say I have the following table and secondary indices defined:</p>

<pre><code>CREATE TABLE ItemUpdates (
    time         timestamp,
    item_name    text,
    item_context text,
    item_descr   text,
    tags         map&lt;text, int&gt;,
    PRIMARY KEY ((time, item_name, item_context))
);

CREATE INDEX ItemUpdateByName
    ON ItemUpdates(item_name);

CREATE INDEX ItemUpdateByContext
    ON ItemUpdates(item_context);

CREATE INDEX ItemUpdateByTag
    ON ItemUpdates(KEYS(tags));
</code></pre>

<p>General background information on the data model: an item has a unique name within a context, so (item_name, item_context) is a natural key for items. Tags have some value associated with them.</p>

<p>A natural query in my application is ""show me all updates on item X with a certain tag"". This translates to:</p>

<pre><code>SELECT * FROM ItemUpdates
    WHERE item_name='x'
        AND item_context='a'
        AND tags CONTAINS KEY 't';
</code></pre>

<p>When I try some queries I notice that although the cluster uses the Murmur3Partitioner, the results come ordered by time. This makes sense when you consider that Cassandra stores secondary indices as wide rows, and that colums are ordered by their name.</p>

<p>(1) <em>Does Cassandra always return rows sorted by partition key when selecting on a(n) (set of) indexed column(s)?</em></p>

<p>The reason I find this interesting is that other natural queries in my application include:</p>

<ul>
<li>fetch all updates on item X, since date D</li>
<li>fetch the 300 most recent updates on item X</li>
</ul>

<p>What surprises me is that adding a clause <code>ORDER BY time DESC</code> to my select statement on ItemUpdates results in an error message ""ORDER BY with 2ndary indexes is not supported.""</p>

<p>(2) <em>(How) can I do a range query on the partition key when I narrow the query by selecting on an indexed column?</em></p>
",<indexing><cassandra><cql3><range-query>,"<p>The only natural ""auto"" sorting that you should get on cassandra is for columns in a wide row. partitions when using murmur3 are not ""sorted"" as that would mess up the random distribution (afaik). Indexes are stored on each node in a ""hidden"" table as wide rows. When on filter on an index, it's hitting that ""partition"" ""on the node"" and the values are the rows in that partition (which correspond to matching rows on that node). Try your query using different data sets and different columns. Maybe the data you have cause the results to be sorted.</p>

<p>(2) As it stands, you can only do range queries on clustering keys, not on the partition key. In general, for efficient querying, you should attempt to hit one (or a few) partitions, and filter on indexes / filter on clustering keys / range query on the clustering key. If you attempt to not hit a partition, it becomes a clusterwide operation, which isn't usually great. If you are looking to do cluster wide analysis (ala map reduce style), take a look at Apache Spark. Spark cassandra integration is quite good and is getting better.</p>
",['table']
26746846,26762607,2014-11-04 23:12:18,Why is data corruption happen in Cassandra 1.2?,"<p>I dropped a column in Cassandra 1.2 couple days ago by:
1. drop the whole table,
2. recreate the table, without the column,
3. insert insert statement (without the column).</p>

<p>The reason why I did that way is because Cassandra 1.2 doesn't support ""drop column"" operation.</p>

<p>Today I was notified by Ops team because of the data corruption issue. 
My questions: </p>

<ol>
<li>What is the root cause? </li>
<li><p>How to fix it?</p>

<p>ERROR [ReadStage:79] 2014-11-04 11:29:55,021 CassandraDaemon.java (line 191) Exception in thread Thread[ReadStage:79,5,main]
org.apache.cassandra.io.sstable.CorruptSSTableException: org.apache.cassandra.db.ColumnSerializer$CorruptColumnException: invalid column name length 0 (/data/cassandra/data/xxx/yyy/zzz-Data.db, 1799885 bytes remaining)
    at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:110)
    at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:40)
    at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
    at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
    at org.apache.cassandra.db.columniterator.SSTableSliceIterator.hasNext(SSTableSliceIterator.java:90)
    at org.apache.cassandra.db.filter.QueryFilter$2.getNext(QueryFilter.java:171)
    at org.apache.cassandra.db.filter.QueryFilter$2.hasNext(QueryFilter.java:154)
    at org.apache.cassandra.utils.MergeIterator$OneToOne.computeNext(MergeIterator.java:199)
    at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
    at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
    at org.apache.cassandra.db.filter.SliceQueryFilter.collectReducedColumns(SliceQueryFilter.java:160)
    at org.apache.cassandra.db.filter.QueryFilter.collateColumns(QueryFilter.java:136)
    at org.apache.cassandra.db.filter.QueryFilter.collateOnDiskAtom(QueryFilter.java:84)
    at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:291)
    at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:65)
    at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1398)
    at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1214)
    at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1130)
    at org.apache.cassandra.db.Table.getRow(Table.java:344)
    at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:70)
    at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:44)
    at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.lang.Thread.run(Unknown Source)
Caused by: org.apache.cassandra.db.ColumnSerializer$CorruptColumnException: invalid column name length 0 (/data/cassandra/data/xxx/yyy/zzz-Data.db, 1799885 bytes remaining)
    at org.apache.cassandra.db.ColumnSerializer$CorruptColumnException.create(ColumnSerializer.java:148)
    at org.apache.cassandra.db.OnDiskAtom$Serializer.deserializeFromSSTable(OnDiskAtom.java:86)
    at org.apache.cassandra.db.OnDiskAtom$Serializer.deserializeFromSSTable(OnDiskAtom.java:73)
    at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:106)
    ... 24 more
ERROR [ReadStage:89] 2014-11-04 11:29:58,076 CassandraDaemon.java (line 191) Exception in thread Thread[ReadStage:89,5,main]
java.lang.OutOfMemoryError: Java heap space
    at org.apache.cassandra.io.util.RandomAccessReader.readBytes(RandomAccessReader.java:376)
    at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:392)
    at org.apache.cassandra.utils.ByteBufferUtil.readWithLength(ByteBufferUtil.java:355)
    at org.apache.cassandra.db.ColumnSerializer.deserializeColumnBody(ColumnSerializer.java:108)
    at org.apache.cassandra.db.OnDiskAtom$Serializer.deserializeFromSSTable(OnDiskAtom.java:92)
    at org.apache.cassandra.db.OnDiskAtom$Serializer.deserializeFromSSTable(OnDiskAtom.java:73)
    at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:106)
    at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:40)
    at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
    at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
    at org.apache.cassandra.db.columniterator.SSTableSliceIterator.hasNext(SSTableSliceIterator.java:90)
    at org.apache.cassandra.db.filter.QueryFilter$2.getNext(QueryFilter.java:171)
    at org.apache.cassandra.db.filter.QueryFilter$2.hasNext(QueryFilter.java:154)
    at org.apache.cassandra.utils.MergeIterator$OneToOne.computeNext(MergeIterator.java:199)</p></li>
</ol>
",<cassandra>,"<p>C* 1.2 supports column deletions for cql tables - <a href=""http://www.datastax.com/documentation/cql/3.0/cql/cql_using/use_delete.html"" rel=""noreferrer"">http://www.datastax.com/documentation/cql/3.0/cql/cql_using/use_delete.html</a></p>

<p>However, I do not see anything wrong from the procedure you described to re-create a new table without your column. Here are some steps to go forward. </p>

<h2>Assumptions -</h2>

<ol>
<li><p>The corruption you are seeing is in the new table not the old one
(do they have the same name?)</p></li>
<li><p>You have a replication factor and number of nodes that are high
enough for you to be able to take this node offline</p></li>
<li><p>Your client's load balancing policy is set up appropriately so
that when the node goes down it will fail over to another node</p></li>
</ol>

<h2>Procedure -</h2>

<p>1) Take your node offline</p>

<pre><code>nodetool drain
</code></pre>

<p>This will flush memtables and make your node stop accepting requests.</p>

<p>2) Run nodetool scrub</p>

<pre><code>nodetool scrub [keyspace][table]
</code></pre>

<p>If this completes successfully then you are done, bring your node back-up by restarting cassandra and run a nodetool <code>repair keyspace table</code></p>

<p>3) If scrub errored out (probably with a corruption error), try the sstablescrub utility. ssh into your box and run: </p>

<p><code>sstablescrub &lt;keyspace&gt; &lt;table&gt;</code> </p>

<p>Note, run this using the same os user you use to start cassandra.</p>

<p>If this completes successfully then you are done, bring your node back-up by restarting cassandra and run a nodetool <code>repair keyspace table</code></p>

<p>4) If this doesn't work (again errors out with a corruption error) you will have to remove the SStable and rebuild it from your other replicas using repair:</p>

<ul>
<li>mv the culprit sstable from your data directory to a backup directory</li>
<li>restart cassandra
(delete it later once it's rebuilt)</li>
<li><code>nodetool repair keyspace cf</code> -- This repair will take time.</li>
</ul>

<h3>Please let me know if you are able to reproduce this corruption.</h3>
",['table']
26759039,26760175,2014-11-05 13:59:40,Cassandra CQL searching for element in list,"<p>I have a table that has a column of <code>list</code> type (tags):</p>

<pre><code>CREATE TABLE ""Videos"" (
    video_id UUID,
    title VARCHAR,
    tags LIST&lt;VARCHAR&gt;,
    PRIMARY KEY (video_id, upload_timestamp)
) WITH CLUSTERING ORDER BY (upload_timestamp DESC);
</code></pre>

<p>I have plenty of rows containing various values in the <code>tags</code> column, ie. <code>[""outdoor"",""funny cats"",""funny mice""]</code>.</p>

<p>I want to perform a <code>SELECT</code> query that will return all rows that contain ""funny cats"" in the <code>tags</code> column. How can I do that?</p>
",<cassandra><cql><cql3>,"<p>To directly answer your question, yes there is a way to accomplish this.  As of Cassandra 2.1 you can create a secondary index on a collection.  First, I'll re-create your column family definition (while adding a definition for <code>upload_timestamp timeuuid</code>) and put some values in it.</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; SELECT * FROM videos ;

 video_id                             | upload_timestamp                     | tags                                          | title
--------------------------------------+--------------------------------------+-----------------------------------------------+---------------------------
 2977b806-df76-4dd7-a57e-11d361e72ce1 | fc011080-64f9-11e4-a819-21b264d4c94d |             ['sci-fi', 'action', 'adventure'] |                 Star Wars
 ab696e1f-78c0-45e6-893f-430e88db7f46 | 8db7c4b0-64fa-11e4-a819-21b264d4c94d |                               ['documentary'] | The Witches of Whitewater
 15e6bc0d-6195-4d8b-ad25-771966c780c8 | 1680d120-64fa-11e4-a819-21b264d4c94d | ['dark comedy', 'action', 'language warning'] |              Pulp Fiction

(3 rows)
</code></pre>

<p>Next, I'll create a secondary index on the <code>tags</code> column:</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; CREATE INDEX ON videos (tags);
</code></pre>

<p>Now, if I want to query the videos that contain the tag ""action,"" I can accomplish this with the <code>CONTAINS</code> keyword:</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; SELECT * FROM videos WHERE tags CONTAINS 'action';

 video_id                             | upload_timestamp                     | tags                                          | title
--------------------------------------+--------------------------------------+-----------------------------------------------+--------------
 2977b806-df76-4dd7-a57e-11d361e72ce1 | fc011080-64f9-11e4-a819-21b264d4c94d |             ['sci-fi', 'action', 'adventure'] |    Star Wars
 15e6bc0d-6195-4d8b-ad25-771966c780c8 | 1680d120-64fa-11e4-a819-21b264d4c94d | ['dark comedy', 'action', 'language warning'] | Pulp Fiction

(2 rows)
</code></pre>

<p>With this all being said, I should pass along a couple of warnings:</p>

<ul>
<li>Secondary indexes do not perform well at scale.  They exist to provide convenience, not performance.  If you are expecting to have to query by tag often, then the right way to solve this would be to create a <code>videosbytag</code> query table, with the same data but keyed like this: <code>PRIMARY KEY (tag,video_id)</code></li>
<li>You don't need the double-quotes in your table name.  In fact, having it in quotes may cause you problems (ok, maybe minor irritations) down the road.</li>
</ul>
",['table']
26760585,26765895,2014-11-05 15:15:15,Cassandra keyspace for counters,"<p>I am trying to create a table for keeping counters to different hits on my APIs. I am using Cassandra 2.0.6, and aware that there have been some performance improvements to counters starting 2.1.0, but cant upgrade at this moment.<br/>
The documentation i read on datastax always starts with creating a separate keyspace like these:
<a href=""http://www.datastax.com/documentation/cql/3.0/cql/cql_using/use_counter_t.html"" rel=""nofollow"">http://www.datastax.com/documentation/cql/3.0/cql/cql_using/use_counter_t.html</a>
<a href=""http://www.datastax.com/documentation/cql/3.1/cql/cql_using/use_counter_t.html"" rel=""nofollow"">http://www.datastax.com/documentation/cql/3.1/cql/cql_using/use_counter_t.html</a></p>

<p>From documentation:</p>

<pre><code>Create a keyspace on Linux for use in a single data center, single node cluster. Use the default data center name from the output of the nodetool status command, for example datacenter1.
CREATE KEYSPACE counterks WITH REPLICATION = { 'class' : 'NetworkTopologyStrategy', 'datacenter1' : 1 };
</code></pre>

<p>Question:<br/>
1)Does it mean that i should keep my counters in a separate keyspace<br/>
2)If yes, should i declare the keyspace as defined in documentation examples, or thats just an example and i can set my own replication strategy - specifically replicate across data centers.</p>

<p>Thanks</p>
",<cassandra><cassandra-2.0><datastax><cqlsh>,"<p>Sorry you had trouble with the instructions. The instructions need to be changed to make it clear that this is just an example and improved by changing RF to 3, for example. </p>

<p>Using a keyspace for a single data center and single node cluster is not a requirement. You need to keep counters in separate tables, but not separate keyspaces; however, keeping tables in separate keyspaces gives you the flexibility to change the consistency and replication from table to table. Normally you have one keyspace per application. See related single vs mutliple keyspace discussion on <a href=""http://grokbase.com/t/cassandra/user/145bwd3va8/effect-of-number-of-keyspaces-on-write-throughput"" rel=""nofollow"">http://grokbase.com/t/cassandra/user/145bwd3va8/effect-of-number-of-keyspaces-on-write-throughput</a>.</p>
",['table']
26767224,26768427,2014-11-05 21:12:21,use select with different attributes present in where clause Cassandra,"<p>I need to create a Cassandra column family with following attributes.</p>

<pre><code>id bigint,
content varchar,
year int,
frequency int,
</code></pre>

<p>I want to get the content with highest frequency in a given year using this column family.
Also when inserting data to table, for given content and year, I need to check if an id already exist or not. How can I achieve this with Cassandra?</p>

<p>I tried creating CF using</p>

<pre><code>CREATE TABLE sinmin.word_time_inv_frequency (
id bigint,
content varchar,
year int,
frequency int,
PRIMARY KEY((year), frequency)
);
</code></pre>

<p>and then retrieved data using </p>

<pre><code>SELECT id FROM word_time_inv_frequency WHERE year = 2010 ORDER BY frequency ;
</code></pre>

<p>But when using this, I can't check if entry is already existing for the (content,year) pair in the CF.</p>
",<database-design><cassandra><datastax><denormalization><nosql>,"<p>You can use a compound partition key to be able to select by (content, year) and still be able to order by frequency:</p>
<p>with this table you'd be able to</p>
<pre><code>create table test2 (
 id bigint,
 content varchar,
 year int,
 frequency int, 
PRIMARY KEY((year, content), frequency)
);
</code></pre>
<p>Your query would work as follows:</p>
<pre><code>select * from test2 where content ='puppies' and year=2014 order by frequency ;
</code></pre>
<p>It is best practice to maintain different tables to address querying needs. You can look into some of the integrations in datastax enterpise (search/analytics) for ad-hoc queries.</p>
<p>Please check the following videos on data modeling for an in-depth look at c* data modeling:
<a href=""https://www.youtube.com/playlist?list=PL75iJfNDd0_FI-Ia_b4z8aoDdOCQTi35I"" rel=""nofollow noreferrer"">https://www.youtube.com/playlist?list=PL75iJfNDd0_FI-Ia_b4z8aoDdOCQTi35I</a></p>
<h3>By the way, is id your unique identifier for this dataset? In the table def you provided you'll overwrite your records if they have the same year and frequency. Make sure you get a unique identifier in your primary key.</h3>
<p>Also consider using <a href=""http://www.datastax.com/documentation/cql/3.0/cql/cql_reference/create_table_r.html"" rel=""nofollow noreferrer"">Clustering Order By</a> in your table definition if you are always going to be pulling data in the same order.</p>
",['table']
26783261,26784780,2014-11-06 15:33:09,CQL data model to bypass secondary index issuee,"<p>I have model which looks like </p>

<pre><code>StateChange: 
    row_id
    group_name
    timestamp
    user_id
</code></pre>

<p>I aim to query as follows:</p>

<p>Query 1 = Find all state changes with row_id = X ORDER BY Timestamp DESC
Query 2 = Find all state changes with row_id = X and group_name = Y ORDER BY Timestamp DESC</p>

<p>Using my limited CQL knowledge, the only way to do so was to create 2 query tables one for each query mentioned above</p>

<p>For query 1: </p>

<pre><code>CREATE TABLE state_change (
    row_id int,
    user_id int,
    group_name text,
    timestamp timestamp,
    PRIMARY KEY (row_id, timestamp)
)
</code></pre>

<p>For query 2: </p>

<pre><code>CREATE TABLE state_change_by_group_name (
    row_id int,
    user_id int,
    group_name text,
    timestamp timestamp,
    PRIMARY KEY ((row_id, group_name), timestamp)
)
</code></pre>

<p>This does solve the problem but I have duplicated data in Cassandra now. </p>

<p>Note: Creating an group_name index on table works but I cannot ORDER BY timestamp anymore as its is the secondary index now. </p>

<p>Looking for a solution which requires only one table. </p>
",<database><cassandra><data-modeling><cql>,"<p>Carlo is correct in that your multiple table solution is the proper approach here.</p>

<blockquote>
  <p>This does solve the problem but I have duplicated data in Cassandra now.</p>
  
  <p>...
  Looking for a solution which requires only one table.</p>
</blockquote>

<p>Planet Cassandra recently posted an article on this topic: <a href=""http://planetcassandra.org/blog/escaping-from-disco-era-data-modeling/"" rel=""nofollow"">Escaping From Disco-Era Data Modeling</a></p>

<p><em>(Full disclosure: I am the author)</em></p>

<p>But two of the last paragraphs really address your point (especially, the last sentence):</p>

<blockquote>
  <p>That is a very 1970′s way of thinking.  Relational database theory
  originated at a time when disk space was expensive.  In 1975, some
  vendors were selling disk space at a staggering eleven thousand
  dollars per megabyte (depending on the vendor and model).  Even in
  1980, if you wanted to buy a gigabyte’s worth of storage space, you
  could still expect to spend around a million dollars.  Today (2014),
  you can buy a terabyte drive for sixty bucks.  Disk space is cheap;
  operation time is the expensive part.  And overuse of secondary
  indexes will increase your operation time.</p>
  
  <p>Therefore, in Cassandra, you should take a query-based modeling
  approach.  Essentially, (Patel, 2014) model your column families
  according to how it makes sense to query your data.  This is a
  departure from relational data modeling, where tables are built
  according to how it makes sense to store the data.  <strong>Often, query-based
  modeling results in storage of redundant data</strong> (and sometimes data that
  is not dependent on its primary row key)<strong>…and that’s ok</strong>.</p>
</blockquote>
",['table']
26991287,26991496,2014-11-18 09:54:06,Cassandra saving time series for industry data sensors,"<p>I am currently developing a project and researching the best way to retrieve data from industrial factory sensors connected to PLCs (the controller of the machinery in a factory for example the control motors, speeds, switches... ). </p>

<p>I will explain the objective to achieve and I think my case could be extrapolated to so much different types of industries:</p>

<ol>
<li><p>I have several PLCs that give me a lot of different data values. (Many of these values are only booleans and other are analog values, real type for example.)</p></li>
<li><p>I will have more than 10.000 sensors in a whole factory.</p></li>
<li><p>I want to retrieve the data at least every second for the analog values (for example motor rmp, temperature, humidity....).</p></li>
<li><p>For the digital values the data will be saved with timestamp when a event appears.</p></li>
</ol>

<p>I want to use Cassandra with timeseries because it looks the most promising and faster technlogy to do that.</p>

<p><strong>My question is about storing analog values every second. Is it better to have a schema like:</strong></p>

<blockquote>
  <p>timestamp, sensor1, sensor2, sensor3, sensor4</p>
</blockquote>

<p><strong>and row and group it by parts in the factory or is it better that</strong></p>

<blockquote>
  <p>every sensor has his own table</p>
</blockquote>

<p><strong>?</strong></p>

<p>The whole system will be developed in Java and it will provide the data to an external company in order to analyse it.</p>
",<cassandra><time-series><plc>,"<p>It's not quite clear what your query is. You mention
""I want to retrieve the data at least every second for the analogic values (for example motor rmp, temperature, humidity....)"".</p>

<p>Does that mean you're querying every second for all 10K sensors? Or for a specific sensor, or for a group of sensors? In cassandra, it's vital to know what your query is before looking at data models. If you're looking for 1 second granuality, one option may be to feed incoming data streams to Spark Streaming, and have the Spark Streaming code save to a Cassandra table that suits what you want to query.</p>

<p>As for the options you mention, it's hard to say without knowing the exact nature of your queries. Having one key ronded to the second may be an option - that would mean 10K or so entries per partition, assuming a data rate or 1/s per sensor. Having a table per sensor would be weird, but you may have a partition per sensor with timestamps for each entry. It really depends on your query.</p>

<p>Perhaps if you gave us an example of how you intend to retrieve the data, we can help better?</p>
",['table']
27088251,27090428,2014-11-23 11:05:35,"After setting authenticator: PasswordAuthenticator in Cassandra.yaml, Cassandra CQL Shell does not run","<p>I'm new in Cassandra. I'm using Datastax Community edition and using only a single node in Windows 7. Trying to change my authentication, set authenticator value from <code>AllowAllAuthenticator</code> to <code>PasswordAuthenticator</code> in Cassandra.yaml. After that setting, it does not let me to run my Cassandra CQL Shell.</p>

<p>Q1. Why this is happening?</p>

<p>Q2. How to overcome it?</p>
",<windows><cassandra><cqlsh>,"<p>How are you accessing cqlsh?  If you have the password authenticator activated, then you will need to specify the default Cassandra super user with the username and password flags.</p>

<p>Linux:</p>

<pre><code>./cqlsh -u cassandra -p cassandra
</code></pre>

<p>In Windows, I'm going to guess that it's something like this:</p>

<pre><code>cqlsh -u cassandra -p cassandra
</code></pre>

<p>Note that once you get in, you'll want to create your own superuser and <a href=""http://www.datastax.com/documentation/cassandra/2.1/cassandra/security/security_config_native_authenticate_t.html"" rel=""nofollow noreferrer"">disable the default cassandra account, as described here</a>.</p>

<p>""I'm accessing cqlsh from START-> Datastax Community Edition-> Cassandra CQL Shell""</p>

<p>I wasn't aware that the Windows version now had a shortcut to cqlsh.  Try modifying that shortcut's target (<a href=""https://superuser.com/questions/358565/adding-command-line-switches-to-windows-shortcuts"">as shown here</a>), and add <code>-u cassandra -p cassandra</code> to the end.  I was able to get this to work by installing and modifying my shortcut's ""target"" property to this:</p>

<pre><code>""E:\Program Files\DataStax Community\python\python.exe"" ""e:\Program Files\DataStax Community\apache-cassandra\bin\cqlsh"" -u cassandra -p cassandra
</code></pre>

<p>Basically, put the <code>-u</code> and <code>-p</code> flags <em>outside</em> of the double quotes, and it should work.</p>
",['authenticator']
27115490,27116174,2014-11-24 22:38:13,Cassandra Performance SELECT by id or SELECT by nothing,"<p>I am wondering if C*s <code>SELECT</code> speed depends on how we select whole finite tables.</p>

<p>For example we have this table</p>

<pre><code>id | value
A  | x
A  | xx
B  | xx
C  | xxx
B  | xx
</code></pre>

<p>Would it be faster to get all the results if we would do<br>
<code>SELECT * FROM Y WHERE id='A'</code><br>
<code>SELECT * FROM Y WHERE id='B'</code><br>
<code>SELECT * FROM Y WHERE id='C'</code></p>

<p>or would it be faster if we would do<br>
<code>SELECT * FROM Y WHERE 1</code></p>

<p>or maybe would it be faster if we would do<br>
<code>SELECT * FROM Y WHERE id IN ('A', 'B', 'C')</code></p>

<p>Or would they be equally fast ( if we dismiss connection time )</p>
",<cassandra><cql><cql3>,"<p>Not sure what your column family (table) definition looks like, but your sample data could never exist like that in Cassandra.  Primary keys are unique, and if <code>id</code> is your primary key, the last write would win.  Basically, your table would look something like this:</p>

<pre><code>id | value
A  | xx
C  | xxx
B  | xx
</code></pre>

<p>As for your individual queries...</p>

<pre><code>SELECT * FROM Y WHERE 1
</code></pre>

<p>That might work well with 3 rows, but it won't when you have 3 million, all spread across multiple nodes.</p>

<pre><code>SELECT * FROM Y WHERE id IN ('A', 'B', 'C')
</code></pre>

<p>This is definitely not any faster.  <a href=""https://stackoverflow.com/questions/26999098/is-the-in-relation-in-cassandra-bad-for-queries/27000384#27000384"">See my answer here</a> as to why relying on <code>IN</code> for anything other than occasional OLAP convenience is not a good idea.</p>

<pre><code>SELECT * FROM Y WHERE id='A'
SELECT * FROM Y WHERE id='B'
SELECT * FROM Y WHERE id='C'
</code></pre>

<p>This is definitely the best way.  Cassandra is designed to be queried by a specific, unique partitioning key.  Even if you wanted to query every row in the column family (table), you're still giving it a specific partition key.  That would help your driver quickly determine which node(s) to send your query to.</p>

<p>Now, let's say you <em>do</em> have 3 million rows.  For your application, is it faster to query each individual one, or to just do a <code>SELECT *</code>?  It might be faster from a query perspective, but you will still have to iterate through each one (client side).  Which means managing them all within the constraints of your available JVM memory (which probably means paging them to some extent).  But this is a bad (extreme) example, because there's no way you should ever want to send your client application 3 million rows to deal with.</p>

<p>The bottom line, is that you'll have to negotiate these issues on your own and within the specifications of your application.  But in terms of performance, I've noticed that appropriate query based data modeling tends to outweigh query strategy or syntactical tricks.</p>
",['table']
27176054,27176400,2014-11-27 17:20:09,"Cassandra DataModel Designing, Composite Key vs Super Column","<p>while designing the datamodel in cassandra. I am stuck while designing the below scenario. </p>

<p>Like One API/Webservice can have multiple parameters(input/output). I don't know the parameters count and its column name as well. </p>

<p>How to design its cassandra datamodel. I am aware that supercolumns are not good to use and alternative good solution is using composite keys. But for my scenario I don't have fixed columns names and count that I can specify as composite keys. </p>

<p>Please see the pic below which I want to model </p>

<p><img src=""https://i.stack.imgur.com/r97Z4.png"" alt=""enter image description here""></p>

<p>Secondly how to write its create table statement so that I can specify parameter name as column name. </p>

<p>Please let me know if anything is unclear. </p>

<p>Thanks,</p>
",<cassandra><composite-key><datamodel><super-columns>,"<p>Why not use a map?</p>

<p><a href=""http://www.datastax.com/documentation/cql/3.1/cql/cql_using/use_map_t.html"" rel=""nofollow"">http://www.datastax.com/documentation/cql/3.1/cql/cql_using/use_map_t.html</a> </p>

<pre><code>create table foo(
   name text,
   owner text,
   version text,
   params map&lt;text, text&gt;,
   primary key (name, owner, version)
);
</code></pre>

<p>If you're one 2.1, you can create secondary indexes on the map keys / values, which caters to more flexibility if needed.</p>
",['table']
27178395,27178650,2014-11-27 20:40:40,Cassandra Hierachy Data Model,"<p>I'm newbie design cassandra data model and I need some help to think out the box.</p>

<p>Basically I need a hierarchical table, something pretty standard when talking about Employee.</p>

<p>You have a employee, say Big Boss, that have a list of employee under him.</p>

<p>Something like:</p>

<pre><code>create table employee(id timeuuid, name text, employees list&lt;employee&gt;, primary key(id));
</code></pre>

<p>So, is there a way to model a hierarchical model in Cassandra adding the table type itself, or even another approach?</p>

<p>When trying this line above it give me</p>

<blockquote>
  <p>Bad Request: line 1:61 no viable alternative at input 'employee'</p>
</blockquote>

<p><strong>EDITED</strong></p>

<p>I was thinking about 2 possibilities:</p>

<ol>
<li><p>Add an uuid instead and in my java application find each uuid Employee when bringing up the ""boss"".</p></li>
<li><p>Working with Map, where the uuid is the id itself and my text would be the entire Row, then in my java application get the maps, convert each ""text"" employee into a Employee entity and finally return the whole object;</p></li>
</ol>
",<cassandra><datamodel>,"<p>It really depends on your queries...one particular model would only be good for a set of queries, but not others.</p>

<p>You can store ids, and look them up again at the client side. This means n extra queries for each ""query"". This may or may not be a problem, as queries that hit a partition are <em>fast</em>. Using a map from id to name is also an option. This means you do extra work and denormalise the names into the map values. That's also valid. A third option is to use a UDT (user defined type). You could then have a list or set or even map. In cassandra 2.1, you could index the map keys/ values as well, allowing for some quite flexible querying.</p>

<p><a href=""https://www.datastax.com/documentation/cql/3.1/cql/cql_using/cqlUseUDT.html"" rel=""nofollow"">https://www.datastax.com/documentation/cql/3.1/cql/cql_using/cqlUseUDT.html</a> </p>

<p>One more approach could be to store a person's details as id, static columns for their attributes, and have ""children"" as columns in wide row format.</p>

<p>This could look like</p>

<pre><code>create table person(
  id int primary key,
  name text static,
  age int static,
  employees map&lt;int, employeeudt&gt;
);
</code></pre>

<p><a href=""http://www.datastax.com/documentation/cql/3.1/cql/cql_reference/refStaticCol.html"" rel=""nofollow"">http://www.datastax.com/documentation/cql/3.1/cql/cql_reference/refStaticCol.html</a></p>

<p>Querying this will give you rows with the static properties repeated, but on disk, it's still held once. You can resolve the rest client side.</p>
",['table']
27235061,27237513,2014-12-01 18:45:13,Error creating table in cassandra - Bad Request: Only clustering key columns can be defined in CLUSTERING ORDER directiv,"<p>I get the above error when I try to use following cql statement, not sure whats wrong with it.</p>

<pre><code>CREATE TABLE Stocks(
  id uuid,
  market text,
  symbol text,
  value text,
  time timestamp,
  PRIMARY KEY(id)
) WITH CLUSTERING ORDER BY (time DESC);
Bad Request: Only clustering key columns can be defined in CLUSTERING ORDER directive
</code></pre>

<p>But this works fine, can't I use some column which is not part of primary key to arrange my rows ?</p>

<pre><code>CREATE TABLE timeseries (
         ...   event_type text,
         ...   insertion_time timestamp,
         ...   event blob,
         ...   PRIMARY KEY (event_type, insertion_time)
         ... )
         ... WITH CLUSTERING ORDER BY (insertion_time DESC);
</code></pre>
",<cassandra><cql>,"<p>""can't I use some column which is not part of primary key to arrange my rows?""</p>

<p>No, you cannot.  From the DataStax documentation on the SELECT command:</p>

<blockquote>
  <p>ORDER BY clauses can select a single column only. That column has to be the second column in a compound PRIMARY KEY. This also applies to tables with more than two column components in the primary key.</p>
</blockquote>

<p>Therefore, for your first <code>CREATE</code> to work, you will need to adjust your PRIMARY KEY to this:</p>

<pre><code>PRIMARY KEY(id,time)
</code></pre>

<p>The second column of in a compound primary key is known as the ""clustering column.""  This is the column that determines the on-disk sort order of data <strong><em>within a partitioning key</em></strong>.  Note that last part in italics, because it is important.  When you query your Stocks column family (table) by <code>id</code>, all ""rows"" of column values for that <code>id</code> will be returned, sorted by <code>time</code>.  In Cassandra you can only specify order within a partitioning key (and not for your entire table), and your partitioning key is the first key listed in a compound primary key.</p>

<p>Of course the problem with this, is that you probably want <code>id</code> to be unique (which means that CQL will only ever return one ""row"" of column values per partitioning key).  Requiring <code>time</code> to be part of the primary key negates that, and makes it possible to store multiple values for the same id.  This is the problem with partitioning your data by a unique id.  It might be a good idea in the RDBMS world, but it can make querying in Cassandra more difficult.</p>

<p>Essentially, you are going to need to revisit your data model here.  For instance, if you wanted to query prices over time, you could name the table something like ""StockPriceEvents"" with a primary key of <code>(id,time)</code> or <code>(symbol,time)</code>.  Querying that table would give you the prices recorded for each id or symbol, sorted by time.  Now that may or may not be of any value to your use case.  Just trying to explain how primary keys and sort order work in Cassandra.</p>

<p>Note: You should really use column names that have more meaning.  Things like ""id,"" ""time,"" and ""timeseries"" are pretty vague don't really describe anything about the context in which they are used.</p>
",['table']
27280407,34050268,2014-12-03 19:50:44,"Cassandra TTL gets set to 0 on primary key if no TTL is specified on an update, but if it is, the TTL on the primary key does not change","<p>This behavior in Cassandra seems counter-intuitive and I want to know why this is happening, and possibly work around this.</p>

<hr>

<p>Imagine I have a table with three columns: <strong><code>pk</code></strong>, the primary key, a <code>text</code> type, <strong><code>foo</code></strong>, a <code>bigint</code>, and <strong><code>bar</code></strong>, another <code>text</code>.</p>

<pre><code>insert into keyspace.table (pk, foo, bar) values ('first', 1, 'test') using ttl 60;
</code></pre>

<p>This creates a row in my table that has a time-to-live of 60 seconds. Looking at it, it looks like this:</p>

<pre><code>  pk  | foo | bar
------------------
first |  1  | test
</code></pre>

<p>Now I do:</p>

<pre><code>update keyspace.table using ttl 10 set bar='change' where pk='first';
</code></pre>

<p>And then, watching the row, I see it undergo the following changes:</p>

<pre><code>  pk  | foo | bar
--------------------
first |  1  | change
first |  1  | &lt;&lt;null&gt;&gt;  // after 10 seconds
   &lt;&lt; deleted &gt;&gt;        // after the initial 60 seconds
</code></pre>

<p>All well and good. What I wanted was for <code>bar</code>'s time-to-live to change, but nothing else, especially not the primary key. This behavior was expected.</p>

<hr>

<p><em>However</em>, if my update doesn't have a <code>ttl</code> in it, or it's set to 0:</p>

<pre><code>update keyspace.table set bar='change' where pk='first';
</code></pre>

<p>Then I see this behavior over time instead.</p>

<pre><code>  pk  | foo | bar
--------------------
first |  1  | change
first |  0  | change   // after the initial 60 seconds
</code></pre>

<p>In other words, the row is never deleted. <code>foo</code> hadn't been changed, so its time-to-live was still in effect and after it passed the value was deleted (set to 0). But <code>pk</code> did have its time-to-live changed. This is totally unexpected.</p>

<p>Why does the primary key's time-to-live change only if I don't specify the time-to-live in the update? And how can I work around this so that the primary key's time-to-live will only change if I explicitly say to do so?</p>

<p><em>Edit</em> I also found that if I use a time-to-live that's higher than the initial one it also seems to change the time-to-live on the primary key.</p>

<pre><code>update keyspace.table using ttl 70 set bar='change' where pk='first';

  pk  | foo | bar
--------------------
first |  1  | change
first |  0  | change   // after the initial 60 seconds
   &lt;&lt; deleted &gt;&gt;       // after the 70 seconds
</code></pre>
",<cassandra><cql><cassandra-2.0><cql3><ttl>,"<p>The effect that you are experiencing is caused by the storage model used by Cassandra.</p>

<p>In your example, where you have a table that does not have any clustering columns, each row in the table maps to a row in the data store (often called a ""Thrift row"", because this is the storage model exposed through the Thrift API). Each of the columns in your table that are not part of the primary key (so in your example the <code>foo</code> and the <code>bar</code> columns) is mapped to a column in the Thrift row. In addition to that, an extra column that is not visible in the CQL row is created as a marker that the row exists.</p>

<p>TTL expiration happens on the level of Thrift columns, not CQL columns. When you <code>INSERT</code> a row, all the columns that you insert as well as the special marker for the row itself get the same TTL.</p>

<p>If you <code>UPDATE</code> a row, only the columns that you update get a new TTL. The row marker is not touched.</p>

<p>When running a query with <code>SELECT</code> all rows for which at least one column <strong>or</strong> the special row marker exists are returned. This means that the column with the highest TTL defines how long a CQL row is visible, unless the marker for the row itself (which is only touched when using an <code>INSERT</code> statement) has a longer TTL.</p>

<p>If you want to ensure that the row's primary key gets updated with the same TTL as the new column values, the workaround is simple: Use the <code>INSERT</code> statement when updating a row. This will have exactly the same effect as using <code>UPDATE</code>, but it will also update the TTL of the row marker.</p>

<p>The only downside of this workaround is that it does not work in combination with lightweight transactions (<code>IF</code> clause in <code>INSERT</code> or <code>UPDATE</code> statements). If you need these in combination with a TTL, you have to use a more complex workaround, but this would be a separate question, I suppose.</p>

<p>If you want to update some columns of a row, but still want the whole row to disappear once the TTL that you specified when it was inserted originally expires, this is not directly supported by Cassandra. The only way would be to find out the TTL left for the row by first querying the TTL of one of the columns and then using this TTL in the <code>UPDATE</code> operation. For example, you could use <code>SELECT TTL(foo) FROM table1 WHERE pk = 'first';</code>. However, this has performance implications because it increases the latency (you have to wait for the result of <code>SELECT</code> before you can run the <code>UPDATE</code>).</p>

<p>As an alternative, you could add a column that you only use as a ""row exists"" marker and that you only touch during the <code>INSERT</code> and never in an <code>UPDATE</code>. You could then simply ignore rows for which this column is <code>null</code>, but this filtering would need to be implemented on the client side and it will not help if you cannot specifiy a TTL in an <code>UPDATE</code> because the updated columns would never be deleted.</p>
",['table']
27281536,27282905,2014-12-03 20:59:57,Cassandra denormalization datamodel,"<p>I read that in nosql (cassandra for instance) data is often stored denormalized. For instance see this <a href=""https://stackoverflow.com/questions/12858282/structuring-cassandra-database"">SO</a> answer or this <a href=""http://maxgrinev.com/2010/07/12/do-you-really-need-sql-to-do-it-all-in-cassandra/"" rel=""nofollow noreferrer"">website</a>.</p>

<p>An example is if you have a column family of employees and departments and you want to execute a query: <code>select * from Emps where Birthdate = '25/04/1975'</code>
Then you have to make a column family birthday_Emps and store the ID of each employee as a column. So then you can query the birthday_Emps family for the key '25/04/1975' and instantly get all the ID's of the employees born on that date. You can even denormalize the employee details into birthday_Emps as well so that you also instantly have the employee names.</p>

<p>Is this really the way to do it?</p>

<ol>
<li><p>Whenever an employee is deleted or inserted then you will have to remove the employee from birthday_Emps too. And in another example someone even said that sometimes you have a situation where one delete in some table requires like 100's of deletes in other tables. Is this really common to do?</p></li>
<li><p>Is it common to do joins in application code? Do you have software that allows you create pre-written applications to join together data from different queries?</p></li>
<li><p>Are there best practices, patterns, etc for handling these data model questions?</p></li>
</ol>
",<join><cassandra><denormalization><database><nosql>,"<p>""Yes"" for the most part, taking an approach of query-based data modeling really is the best way to do it.</p>

<ol>
<li><p>That is still a good idea to do, because the speed of your query times make it worth it.  Yes, there's a little more housecleaning to do.  I haven't had to execute 100s of deletes from other column families, but occasionally there is some complicated clean-up to do.  But, you shouldn't be doing a whole lot of deleting in Cassandra anyway (anti-pattern).</p></li>
<li><p>No.  Client-side JOINs are just as bad as distributed JOINs.  The whole idea is to create a table to return data for each specific query...denormalized and/or replicated...and thus negating the need to do a JOIN at all.  The exception to this, is if you are running OLAP queries for analysis, you can use a tool like Apache Spark to execute an ad-hoc, distributed JOIN.  But it's definitely not something you'd want to do on a production system.</p></li>
<li><p>A few articles I can recommend:</p>

<ul>
<li><a href=""http://patrickmcfadin.com/2014/02/05/getting-started-with-time-series-data-modeling/"" rel=""nofollow noreferrer"">Getting Started with Cassandra Time Series Data Modeling</a> - Written by DataStax's Chief Evangelist Patrick McFadin, it covers one of the more common Cassandra use cases in a few different ways.</li>
<li><a href=""http://www.aaronstechcenter.com/escaping-disco-era-datamodeling.php"" rel=""nofollow noreferrer"">Escaping From Disco-Era Data Modeling</a> - This one talks about some of the obstacles that beginners with Cassandra can face, as well as the general approach to take in overcoming them. <em>Disclaimer: I am the author</em>.</li>
<li><a href=""http://www.ebaytechblog.com/2012/07/16/cassandra-data-modeling-best-practices-part-1/#.VH-OezHF_6M"" rel=""nofollow noreferrer"">Cassandra Data Modeling Best Practices, Part 1</a> - You can't go wrong with Jay Patel's (eBay) classic article on Cassandra modeling practices.  It's a little dated in that the examples are grounded in the pre-CQL world, but the techniques still resonate.</li>
</ul></li>
</ol>
",['table']
27295679,27297662,2014-12-04 13:48:53,how UPDATE rows in cassandra using only Partition Key?,"<p>My table looks like this<br/></p>

<pre><code>create table Notes(
    user_id varchar,
    real_time timestamp,
    insertion_time timeuuid,
    read boolean PRIMARY KEY (user_id,real_time,insertion_time)
);

create index read_index on Notes (read);
</code></pre>

<p>I want <strong>update all</strong> the <strong>rows</strong> with <strong>user_id</strong> = 'xxx' <strong>without</strong> having to specify all the clustering indexes.</p>

<pre><code>UPDATE Notes SET read = true where user_id = 'xxx'; // Says Error
</code></pre>

<p>Error: message=""Missing mandatory PRIMARY KEY part real_time</p>

<p>I have tried creating a secondary index, but its not allowed on the primary key.</p>

<p>How can i solve this?</p>

<p>I chose user_id to be in the primary key cause i want to be able to do <code>select * from Notes where user_id = 'xxx'</code> should be possible.    </p>
",<cassandra><nosql><cql><cql3>,"<p>While this might be possible with a RDBMS and SQL, it is not possible with cql in Cassandra.  From the DataStax documentation on the <a href=""http://www.datastax.com/documentation/cql/3.1/cql/cql_reference/update_r.html"" rel=""noreferrer"">UPDATE</a> command:</p>

<blockquote>
  <p>Each update statement requires a precise set of primary keys to be specified using a WHERE clause. You need to specify all keys in a table having compound and clustering columns.</p>
</blockquote>

<p>You'll probably need to write something quick in Python (or one of the other drivers) to perform this type of update.</p>
",['table']
27386223,27429916,2014-12-09 18:31:33,kafka spark-streaming data not getting written into cassandra. zero rows inserted,"<p>While writing data to cassandra from spark, data is not getting written.<br>
The flash back is:<br>
I am doing a kafka-sparkStreaming-cassandra integration.<br>
I am reading kafka messages and trying to put it in a cassandra table <code>CREATE TABLE TEST_TABLE(key INT PRIMARY KEY, value TEXT)</code>.<br>
kafka to spark-streaming is running cool, but spark to cassandra, there is some issue...data not getting written to table.<br>
I am able to create a connection with cassandra, but the data is not getting inserted into the cassandra table. The output shows its getting connected and the next second getting disconnected.<br>
The strings for <code>System.out.print()</code> is all at the output.</p>

<pre><code>+++++++++++cassandra connector created++++++++++++++++++++++++++++
+++++++++++++streaming Connection done!+++++++++++++++++++++++++++
++++++++++++++++JavaDStream&lt;TestTable&gt; created++++++++++++++++++++++++++++
</code></pre>

<p>Cassandra shell shows 0 rows.<br>
the full code and the logs and dependencies are below:</p>

<pre><code>public class SparkStream {
    static int key=0;
    public static void main(String args[]) throws Exception
    {

        if(args.length != 3)
        {
            System.out.println(""parameters not given properly"");
            System.exit(1);
        }

        Logger.getLogger(""org"").setLevel(Level.OFF);
        Logger.getLogger(""akka"").setLevel(Level.OFF);
        Map&lt;String,Integer&gt; topicMap = new HashMap&lt;String,Integer&gt;();
        String[] topic = args[2].split("","");
        for(String t: topic)
        {
            topicMap.put(t, new Integer(3));
        }

        /* Connection to Spark */
        SparkConf conf = new SparkConf();
        conf.set(""spark.cassandra.connection.host"", ""localhost"");
        JavaSparkContext sc = new JavaSparkContext(""local[4]"", ""SparkStream"",conf);
        JavaStreamingContext jssc = new JavaStreamingContext(sc, new Duration(5000));


        /* connection to cassandra */
        CassandraConnector connector = CassandraConnector.apply(sc.getConf());
        System.out.println(""+++++++++++cassandra connector created++++++++++++++++++++++++++++"");


        /* Receive Kafka streaming inputs */
        JavaPairReceiverInputDStream&lt;String, String&gt; messages = KafkaUtils.createStream(jssc, args[0], args[1], topicMap );
        System.out.println(""+++++++++++++streaming Connection done!+++++++++++++++++++++++++++"");


        /* Create DStream */                
        JavaDStream&lt;TestTable&gt; data = messages.map(new Function&lt; Tuple2&lt;String,String&gt;, TestTable &gt;() 
        {
            public TestTable call(Tuple2&lt;String, String&gt; message)
            {
                return new TestTable(new Integer(++key), message._2() );
            }
        }
        );
        System.out.println(""++++++++++++++++JavaDStream&lt;TestTable&gt; created++++++++++++++++++++++++++++"");


        /* Write to cassandra */
        javaFunctions(data).writerBuilder(""testkeyspace"", ""test_table"", mapToRow(TestTable.class)).saveToCassandra();


        jssc.start();
        jssc.awaitTermination();

    }
}

class TestTable implements Serializable
{
    Integer key;
    String value;

    public TestTable() {}

    public TestTable(Integer k, String v)
    {
        key=k;
        value=v;
    }

    public Integer getKey(){
        return key;
    }

    public void setKey(Integer k){
        key=k;
    }

    public String getValue(){
        return value;
    }

    public void setValue(String v){
        value=v;
    }

    public String toString(){
        return MessageFormat.format(""TestTable'{'key={0}, value={1}'}'"", key, value);

    }
}
</code></pre>

<p>The log is:</p>

<pre><code>+++++++++++cassandra connector created++++++++++++++++++++++++++++
+++++++++++++streaming Connection done!+++++++++++++++++++++++++++
++++++++++++++++JavaDStream&lt;TestTable&gt; created++++++++++++++++++++++++++++
14/12/09 12:07:33 INFO core.Cluster: New Cassandra host localhost/127.0.0.1:9042 added
14/12/09 12:07:33 INFO cql.CassandraConnector: Connected to Cassandra cluster: Test Cluster
14/12/09 12:07:33 INFO cql.LocalNodeFirstLoadBalancingPolicy: Adding host 127.0.0.1 (datacenter1)
14/12/09 12:07:33 INFO cql.LocalNodeFirstLoadBalancingPolicy: Adding host 127.0.0.1 (datacenter1)
14/12/09 12:07:34 INFO cql.CassandraConnector: Disconnected from Cassandra cluster: Test Cluster

14/12/09 12:07:45 INFO core.Cluster: New Cassandra host localhost/127.0.0.1:9042 added
14/12/09 12:07:45 INFO cql.CassandraConnector: Connected to Cassandra cluster: Test Cluster
14/12/09 12:07:45 INFO cql.LocalNodeFirstLoadBalancingPolicy: Adding host 127.0.0.1 (datacenter1)
14/12/09 12:07:45 INFO cql.LocalNodeFirstLoadBalancingPolicy: Adding host 127.0.0.1 (datacenter1)
14/12/09 12:07:46 INFO cql.CassandraConnector: Disconnected from Cassandra cluster: Test Cluster
</code></pre>

<p>The POM.xml dependencies are:</p>

<pre><code>   &lt;dependency&gt;
        &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
        &lt;artifactId&gt;spark-streaming-kafka_2.10&lt;/artifactId&gt;
        &lt;version&gt;1.1.0&lt;/version&gt;
    &lt;/dependency&gt;

    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
        &lt;artifactId&gt;spark-streaming_2.10&lt;/artifactId&gt;
        &lt;version&gt;1.1.0&lt;/version&gt;
    &lt;/dependency&gt;

&lt;dependency&gt;
    &lt;groupId&gt;com.datastax.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-cassandra-connector_2.10&lt;/artifactId&gt;
    &lt;version&gt;1.1.0&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;com.datastax.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-cassandra-connector-java_2.10&lt;/artifactId&gt;
    &lt;version&gt;1.1.0&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-core_2.10&lt;/artifactId&gt;
    &lt;version&gt;1.1.1&lt;/version&gt;
&lt;/dependency&gt;


    &lt;dependency&gt;
        &lt;groupId&gt;com.msiops.footing&lt;/groupId&gt;
        &lt;artifactId&gt;footing-tuple&lt;/artifactId&gt;
        &lt;version&gt;0.2&lt;/version&gt;
    &lt;/dependency&gt;   

&lt;dependency&gt;
    &lt;groupId&gt;com.datastax.cassandra&lt;/groupId&gt;
    &lt;artifactId&gt;cassandra-driver-core&lt;/artifactId&gt;
    &lt;version&gt;2.1.3&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<p>is there something wrong with the code? or cassandra configuration?</p>
",<cassandra><apache-spark><apache-kafka><spark-streaming>,"<p>solved the issue. 
the columnMapper wasnt able to access the getters and setters of class TestTable. 
So changed the access modifier to public. 
but now i had 2 public classes in one file. which is an error. 
so created another java file TestTable.java with class as </p>

<pre><code>public class TestTable implements Serializable { 
//code
}
</code></pre>

<p>now the messages are being read from kafka and getting stored in cassandra table </p>
",['table']
27391497,27392022,2014-12-10 00:35:08,dse cassandra analytics and online in same cluster,"<p>I have a cluster running datastax cassandra with 8 nodes of which 3 are analytic nodes. When I create a table in analytic nodes, it is created in non analytic nodes as well. How could this be prevented but the tables and table updates from non-analytic nodes should flow into the analytic nodes. </p>
",<cassandra><datastax-enterprise>,"<p>Make the tables in a keyspace that only specifies replication in the Analytics Datacenter. You can do this by using the NetworkTopology Strategy <a href=""http://www.datastax.com/documentation/cassandra/2.0/cassandra/architecture/architectureDataDistributeReplication_c.html"" rel=""nofollow"">(Documentation on NTS)</a>. Having the table only exist on specific nodes is only supported if you are putting those nodes in a separate datacenter.</p>
",['table']
27438113,27440651,2014-12-12 06:32:03,Cassandra: Only EQ and IN relation are supported on the partition key (unless you use the token() function),"<p>Table :</p>

<pre><code>CREATE TABLE TEST_PAYLOAD
(
  TIME_STAMP timestamp,
  TYPE text,
  PRIMARY KEY (TIME_STAMP)
);

 time_stamp           | type
--------------------------+----------
 2013-05-15 00:00:00-0700 | sometext
 2013-05-16 00:00:00-0700 | sometext
 2013-05-17 00:00:00-0700 | sometext

SELECT * FROM TEST_PAYLOAD WHERE TIME_STAMP&gt;='2013-05-15 00:00:00-0700';

code=2200 [Invalid query] message=""Only EQ and IN relation are supported on the partition key (unless you use the token() function)""
</code></pre>

<p>it doesn't work for > or any range selection while it works for = as far index is concerned it has only one primary key there is no partition key.Why it asks for token().</p>

<p>i Would like to retrieve relative range can be only date or date with time not specific timestamp exist in db.</p>
",<java><sql><cassandra><nosql>,"<p>I guess you are bit confused about the Cassandra terminology. </p>

<p>Please refer <a href=""http://www.datastax.com/documentation/cql/3.0/share/glossary/gloss_partition_key.html"">here</a></p>

<p><code>partition key: The first column declared in the PRIMARY KEY definition</code></p>

<p>ie, when you create a table like this</p>

<pre><code>CREATE TABLE table {
 key1, 
 key2,
 key3,
 PRIMARY KEY (key1, key2, key3)
}
</code></pre>

<p><code>key1</code> is called the <strong>partition key</strong> and <code>key2</code>, <code>key3</code> are called <strong>clustering keys</strong>.</p>

<p>In your case you don't have clustering keys, so the single primary key which you declared became the partition key.</p>

<p>Also range queries ( &lt; , >) should be performed on clustering keys. </p>

<p>If you don't have any other candidates for primary key, i think you should remodel your table like this</p>

<pre><code>CREATE TABLE TEST_PAYLOAD
(
  BUCKET varchar,
  TIME_STAMP timestamp,
  TYPE text,
  PRIMARY KEY (BUCKET, TIME_STAMP)
);
</code></pre>

<p>For BUCKET you can provide the year or year&amp;month combination. So your keys would look like these 2013, 2014, 06-2014, 10-2014 etc.</p>

<p>So while querying go to the desired bucket and do range scans like TIME_STAMP >= '2013-05-15 00:00:00-0700'</p>
",['table']
27439964,27446678,2014-12-12 08:53:23,Why would someone store multiple rows in one Cassandra row?,"<p>I'm looking for ways to store nested dynamic documents in Cassandra and found a presentation where they suggest to create only few rows and store in each row many JSON objects: <a href=""https://speakerdeck.com/dzello/store-json-the-hard-way?slide=112"" rel=""nofollow"">https://speakerdeck.com/dzello/store-json-the-hard-way?slide=112</a></p>

<p>I understand the idea to create column names based on the property path, and I understand that concatenating smaller objects usually results in a higher throughput, but I just think that it's a lot of overhead. </p>

<p>Would the performance in C* with a single object stored per row be really so bad that it's worth to have this mess with creating lists? Is there maybe something I am missing about C*?</p>
",<json><performance><cassandra>,"<p><em>In the Store Json the Hard Way slides it should be noted these are experts in Cassandra who are performing a great deal of ancillary compaction and tombstone clear-out outside of Cassandra's normal operations.</em></p>

<p>Data locality is key to great performance and having multiple logical rows in a single C* partition is key to that. All of the CQL Schema definitions are aimed around making it easier for the developer to specify what data goes in the same partition. </p>

<p>For an example</p>

<pre><code>CREATE TABLE soda_sold_per_store ( 
    store text, date timestamp, soda_name text, soda_count int, 
    PRIMARY KEY (store,date,soda_count,soda_name)
</code></pre>

<p>Describes a table where there is only a single Cassandra partition for every store and within that partition the information is sorted based on date then soda_count and finally on soda_name.</p>

<p>This means queries inquiring about the number of sodas sold in a particular store over time will be very fast (they only hit a single partition of contiguous data already sorted on disk.) Other queries would be very hard on this schema though, such as ""What is the total number of soda's sold in all stores on a particular day?"" A query involving all stores requires receiving data from all nodes with this table and the data is not going to be contiguous on disk. Cassandra imposes this kind of tradeoff, well modeled tables will be extremely fast but they can only be fast for certain queries. Luckily writes in Cassandra are very cheap so it is common practice to have multiple tables, each satisfying one of your high demand queries. </p>
",['table']
27441827,27446471,2014-12-12 10:40:33,cassandra filtering on an indexed column isn't working,"<p>I'm using (the latest version of) Cassandra nosql dbms to model some data.</p>

<p>I'd like to get a count of the number of active customer accounts in the last month.</p>

<p>I've created the following table:</p>

<pre><code>CREATE TABLE active_accounts
(
    customer_name   text, 
    account_name    text, 
    date            timestamp, 
    PRIMARY KEY ((customer_name, account_name))
);
</code></pre>

<p>So because I want to filter by date, I create an index on the date column:</p>

<pre><code>CREATE INDEX ON active_accounts (date);
</code></pre>

<p>When I insert some data, Cassandra automatically updates data on any existing primary key matches, so the following inserts only produce two records:</p>

<pre><code>insert into active_accounts (customer_name, account_name, date) Values ('customer2', 'account2', 1418377413000);
insert into active_accounts (customer_name, account_name, date) Values ('customer1', 'account1', 1418377413000);
insert into active_accounts (customer_name, account_name, date) Values ('customer2', 'account2', 1418377414000);
insert into active_accounts (customer_name, account_name, date) Values ('customer2', 'account2', 1418377415000);
</code></pre>

<p>This is exactly what I'd like - I won't get a huge table of data, and each entry in the table represents a unique customer account - so no need for a select distinct.</p>

<p>The query I'd like to make - is how many distinct customer accounts are active within the last month say:</p>

<pre><code> Select count(*) from active_accounts where date &gt;= 1418377411000 and date &lt;= 1418397411000 ALLOW FILTERING;
</code></pre>

<p>In response to this query, I get the following error:</p>

<pre><code>code=2200 [Invalid query] message=""No indexed columns present in by-columns clause with Equal operator""
</code></pre>

<p>What am I missing; isn't this the purpose of the Index I created?</p>
",<cassandra>,"<p>Table design in Cassandra is extremely important and it must match the kind of queries that you are trying to preform. The reason that Cassandra is trying to keep you from performing queries on the date column, is that any query along that column will be extremely inefficient.</p>
<p><strong>Table Design - Model your queries</strong></p>
<p>One of the main reasons that Cassandra can be fast is that it partitions user data so that most( 99%)
of queries can be completed without contacting all of the nodes in the cluster. This means less network traffic, less disk access, and faster response time. Unfortunately Cassandra isn't able to determine automatically what the best way to partition data. The end user must determine a schema which fits into the C* datamodel and allows the queries they want at a high speed.</p>
<pre><code>CREATE TABLE active_accounts
(
   customer_name   text, 
   account_name    text, 
   date            timestamp, 
   PRIMARY KEY ((customer_name, account_name))
);
</code></pre>
<p>This schema will only be efficient for queries that look like</p>
<p>SELECT timestamp FROM active_accounts where customer_name = ? and account_name = ?</p>
<p>This is because on the the cluster the data is actually going to be stored like</p>
<pre><code>node 1: [ ((Bob,1)-&gt;Monday), ((Tom,32)-&gt;Tuesday)]
node 2: [ ((Candice, 3) -&gt; Friday), ((Sarah,1) -&gt; Monday)]
</code></pre>
<p>The PRIMARY KEY for this table says that data should be placed on a node based on the hash of the combination of CustomerName and AccountName. This means we can only look up data quickly if we have both of those pieces of data. Anything outside of that scope becomes a batch job since it requires hitting multiple nodes and filtering over all the data in the table.</p>
<p>To optimize for different queries you need to change the layout of your table or use a distributed analytics framework like Spark or Hadoop.</p>
<p>An example of a different table schema that might work for your purposes would be something like</p>
<pre><code>CREATE TABLE active_accounts
(
    start_month     timestamp,
    customer_name   text, 
    account_name    text, 
    date            timestamp, 
    PRIMARY KEY (start_month, date, customer_name, account_name)
);
</code></pre>
<p>In this schema I would put the timestamp of the first day of the month as the partitioning key and date as the first clustering key. This means that multiple account creations that took place in the same month will end up in the same partition and on the same node. The data for a schema like this would look like</p>
<pre><code>node 1: [ (May 1 1999) -&gt; [(May 2 1999, Bob, 1), (May 15 1999,Tom,32)]
</code></pre>
<p>This places the account dates in order within each partition making it very fast for doing range slices between particular dates. Unfortunately you would have to add code on the application side to pull down the multiple months that a query might be spanning. This schema takes a lot of (dev) work so if these queries are very infrequent you should use a distributed analytics platform instead.</p>
<p>For more information on this kind of time-series modeling check out:</p>
<p><a href=""http://planetcassandra.org/getting-started-with-time-series-data-modeling/"" rel=""noreferrer"">http://planetcassandra.org/getting-started-with-time-series-data-modeling/</a></p>
<p>Modeling in general:</p>
<p><a href=""http://www.slideshare.net/planetcassandra/cassandra-day-denver-2014-40328174"" rel=""noreferrer"">http://www.slideshare.net/planetcassandra/cassandra-day-denver-2014-40328174</a>
<a href=""http://www.slideshare.net/johnny15676/introduction-to-cql-and-data-modeling"" rel=""noreferrer"">http://www.slideshare.net/johnny15676/introduction-to-cql-and-data-modeling</a></p>
<p>Spark and Cassandra:</p>
<p><a href=""http://planetcassandra.org/getting-started-with-apache-spark-and-cassandra/"" rel=""noreferrer"">http://planetcassandra.org/getting-started-with-apache-spark-and-cassandra/</a></p>
<p><strong>Don't use secondary indexes</strong></p>
<p>Allow filtering was added to the cql syntax to prevent users from accidentally designing queries that will not scale. The secondary indexes are really only for use by those do analytics jobs or those C* users who fully understand the implications. In Cassandra the secondary index lives on every node in your cluster. This means that any query that requires a secondary index necessarily will require contacting every node in the cluster. This will become less and less performant as the cluster grows and is definitely not something you want for a frequent query.</p>
",['table']
27500807,27550870,2014-12-16 08:57:12,Cassandra filtering by date with a secondary index,"<p>I have a requirement to answer the following queries:</p>

<ul>
<li>return the number of new customers per quarter (up to 36 months)</li>
<li>list the new customers per quarter (up to 36 months)</li>
</ul>

<p>I've created the following table in Cassandra to deal with this:</p>

<pre><code>CREATE TABLE first_purchase_by_shopper_date
(
    shop_id                     uuid,
    shopper_id                  uuid,
    dt_first_purchase           timestamp,

    ... (some text fields)

    PRIMARY KEY ((shop_id, shopper_id))
);
</code></pre>

<p>In order to be able to answer this query in Cassandra, I need to be able to filter this data on the dt_first_purchase field.</p>

<p>But if I add dt_first_purchase to the primary key, then it makes the row non-unique to a shopper - and therefore we get multiple entries in the table - but we only ever want 
one entry per shopper.</p>

<p>so my insert statement would look like </p>

<pre><code>Insert into first first_purchase_by_shopper_date (shop_id, shopper_id, dt_first_purchase, ... ) Values(...) If Not Exists;
</code></pre>

<p>The if not exists at the end ensures that the entry is only written if none exists already (e.g. no update is performed on an existing record.)</p>

<p>How can I filter by date on this table - is a secondary index on the dt_first_purchase column my only option - and isn't this undesirable?  </p>
",<cassandra><cql>,"<blockquote>
  <p>How can I filter by date on this table - is a secondary index on the
  dt_first_purchase column my only option - and isn't this undesirable?</p>
</blockquote>

<p>You could certainly try a secondary index on <code>dt_first_purchase</code> (and querying by range on that would also require the use of the <code>ALLOW FILTERING</code> directive).  For performance (especially with a large cluster), I do not recommend that.</p>

<p>But first and foremost, understand that Cassandra is designed around returning data for a specific key on a specific data partition.  This means that the best way for you to query your data by a date range, is to first partition your data by a key that makes sense for your model.  For instance, if you had a primary key defined like this:</p>

<pre><code>PRIMARY KEY ((shop_id), dt_first_purchase, shopper_id)
</code></pre>

<p><em>basically, a record of which shop (shop_id) recorded a first purchase (dt_first_purchase) for a particular shopper (shopper_id)</em></p>

<p>With your data partitioned by shop (shop_id) you could then query the first purchases of new shoppers <em>for a particular shop_id</em> like this:</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; SELECT * 
FROM first_purchase_by_shopper_date 
WHERE shop_id=ce1089f6-c613-4d5b-a975-5dfd677b46f9 
AND dt_first_purchase &gt;= '2014-01-01 00:00:00' 
AND dt_first_purchase &lt; '2014-04-01 00:00:00';

 shop_id                              | dt_first_purchase        | shopper_id                           | value
--------------------------------------+--------------------------+--------------------------------------+-------
 ce1089f6-c613-4d5b-a975-5dfd677b46f9 | 2014-02-12 18:33:22-0600 | a7480417-aaf8-42b1-85dd-5d9a4a30c204 | shopper1
 ce1089f6-c613-4d5b-a975-5dfd677b46f9 | 2014-03-13 11:33:22-0500 | 07db2b71-2dc7-421d-bf73-82a5f6c55f89 | shopper2

(2 rows)
</code></pre>

<p>Additionally, you could then count the number of first purchases (new shoppers) for that particular shop and date range, like this:</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; SELECT COUNT(*) FROM first_purchase_by_shopper_date
WHERE shop_id=ce1089f6-c613-4d5b-a975-5dfd677b46f9 
AND dt_first_purchase &gt;= '2014-01-01 00:00:00' 
AND dt_first_purchase &lt; '2014-04-01 00:00:00';

 count
-------
     2

(1 rows)
</code></pre>

<p>Please note that this specific example may not work for your use case.  Take it for what it is: a demonstration of how partitioning and querying work in Cassandra.</p>

<p>For more information, check out Patrick McFadin's article on <a href=""https://academy.datastax.com/resources/getting-started-time-series-data-modeling"" rel=""nofollow noreferrer"">Getting Started With Time Series Data Modeling</a>.  He discusses ways to solve a use case that is similar to yours.</p>
",['table']
27522837,27536479,2014-12-17 09:58:03,Cassandra 2.x: secondary index on a unique value,"<p>Let's say I have a user with an <code>id</code> and <code>email</code> fields, both are unique, and I want to query by both of them. <code>id</code> will be part of the primary key, but the question is what to do with <code>email</code>.</p>

<p>The first option is to create a ""manual index"", something like an <code>email_to_user</code> table. There the <code>email</code> would be the primary key, so lookups should be pretty fast. However there is some overhead in having to keep the manual index in-sync with the primary table.</p>

<p>The second option is to create a secondary index. However, as <a href=""http://www.datastax.com/documentation/cql/3.1/cql/ddl/ddl_when_use_index_c.html"" rel=""nofollow"">Cassandra's doc say</a>, secondary indexes shouldn't be used on high cardinality columns - and columns with unique values are certainly high cardinality. However, I also read that secondary indexes are implemented as a hidden table - so is there in fact any difference between the two approaches? (except having the table manually created or hidden). </p>

<p>Maybe having  a <strong>unique</strong> value for the secondary index is a different case than having a high-cardinality value? Or maybe things have changed in 2.x (most of the material on this topic in the net refers to 0.x or 1.x versions)?</p>
",<indexing><cassandra><cassandra-2.0>,"<p>The doc you mention states as well that it is fine (regarding performance) to use a secondary index on a column that holds unique data as long as the query volume on that table is moderate.</p>

<h2>Differences</h2>

<p>To answer your question in short: <strong>Yes</strong>, there are differences besides creating and maintaining the index table manually. </p>

<p>For more details, have a look at this <a href=""https://stackoverflow.com/questions/19248458/what-is-the-difference-between-a-secondary-index-and-an-inverted-index-in-cassan"">question</a>. Though it is from 2013 I think the answer is still valid. </p>

<p>In addition visit the <a href=""http://wiki.apache.org/cassandra/SecondaryIndexes"" rel=""nofollow noreferrer"">FAQ on secondary indexes</a>.</p>

<p>p.s. to give you a more sophisticated answer you might want to share more details on your specific use case such as read and write load, data volume or queries making use of the secondary index.</p>
",['table']
27566558,28323815,2014-12-19 12:49:05,What does rows_merged mean in compactionhistory?,"<p>When I issue</p>

<pre><code>$ nodetool compactionhistory
</code></pre>

<p>I get</p>

<pre><code>. . . compacted_at        bytes_in       bytes_out      rows_merged
. . . 1404936947592       8096           7211           {1:3, 3:1}
</code></pre>

<p><strong>What does <code>{1:3, 3:1}</code> mean?</strong> The only documentation I can find is <a href=""http://www.datastax.com/documentation/cassandra/2.1/cassandra/tools/toolsCompactionHistory.html"">this</a> which states</p>

<blockquote>
  <p>the number of partitions merged</p>
</blockquote>

<p>which does not explain why multiple values and what the colon means.</p>
",<cassandra><datastax-enterprise><datastax>,"<p>So basically it means {tables:rows} for example {1:3, 3:1} means 3 rows were taken from one sstable (1:3) and 1 row taken from 3 (3:1) sstables, all to make the one sstable in that compaction operation.</p>

<p>I tried it out myself so here's an example, I hope this helps:</p>

<p>create keyspace and table:</p>

<pre><code>cqlsh&gt; create keyspace space1 WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};

cqlsh&gt; create TABLE space1.tb1 ( key text, val1 text, primary KEY (key));

cqlsh&gt; INSERT INTO space1.tb1 (key, val1 ) VALUES ( 'key1','111');
cqlsh&gt; INSERT INTO space1.tb1 (key, val1 ) VALUES ( 'key2','222');
cqlsh&gt; INSERT INTO space1.tb1 (key, val1 ) VALUES ( 'key3','333');
cqlsh&gt; INSERT INTO space1.tb1 (key, val1 ) VALUES ( 'key4','444');
cqlsh&gt; INSERT INTO space1.tb1 (key, val1 ) VALUES ( 'key5','555');
cqlsh&gt; exit
</code></pre>

<p>Now we flush to create the sstable</p>

<pre><code>$ nodetool flush space1
</code></pre>

<p>We see that only one version of the table is created</p>

<pre><code>$ sudo ls -lR /var/lib/cassandra/data/space1

/var/lib/cassandra/data/space1:
total 4
drwxr-xr-x. 2 cassandra cassandra 4096 Feb  3 12:51 tb1

/var/lib/cassandra/data/space1/tb1:
total 32
-rw-r--r--. 1 cassandra cassandra   43 Feb  3 12:51 space1-tb1-jb-1-CompressionInfo.db
-rw-r--r--. 1 cassandra cassandra  146 Feb  3 12:51 space1-tb1-jb-1-Data.db
-rw-r--r--. 1 cassandra cassandra   24 Feb  3 12:51 space1-tb1-jb-1-Filter.db
-rw-r--r--. 1 cassandra cassandra   90 Feb  3 12:51 space1-tb1-jb-1-Index.db
-rw-r--r--. 1 cassandra cassandra 4389 Feb  3 12:51 space1-tb1-jb-1-Statistics.db
-rw-r--r--. 1 cassandra cassandra   80 Feb  3 12:51 space1-tb1-jb-1-Summary.db
-rw-r--r--. 1 cassandra cassandra   79 Feb  3 12:51 space1-tb1-jb-1-TOC.txt
</code></pre>

<p>check the sstable2json we see our data</p>

<pre><code>$ sudo -u cassandra /usr/bin/sstable2json /var/lib/cassandra/data/space1/tb1/space1-tb1-jb-1-Data.db
[
{""key"": ""6b657935"",""columns"": [["""","""",1422967847005000], [""val1"",""555"",1422967847005000]]},
{""key"": ""6b657931"",""columns"": [["""","""",1422967817740000], [""val1"",""111"",1422967817740000]]},
{""key"": ""6b657934"",""columns"": [["""","""",1422967840622000], [""val1"",""444"",1422967840622000]]},
{""key"": ""6b657933"",""columns"": [["""","""",1422967832341000], [""val1"",""333"",1422967832341000]]},
{""key"": ""6b657932"",""columns"": [["""","""",1422967825116000], [""val1"",""222"",1422967825116000]]}
]
</code></pre>

<p>At this point ‘notetool compactionhistory’ shows nothing for this table but lets run compact anyway to see what we get (scroll right)</p>

<pre><code>$ nodetool compactionhistory | awk 'NR == 2 || /space1/'
id                                       keyspace_name      columnfamily_name            compacted_at              bytes_in       bytes_out      rows_merged
5725f890-aba4-11e4-9f73-351725b0ac5b     space1             tb1                          1422968305305             146            146            {1:5}
</code></pre>

<p>Now lets delete two rows, and flush</p>

<pre><code>cqlsh&gt; delete from space1.tb1 where key='key1';
cqlsh&gt; delete from space1.tb1 where key='key2';
cqlsh&gt; exit

$ nodetool flush space1

$ sudo ls -l /var/lib/cassandra/data/space1/tb1/
[sudo] password for datastax: 
total 64
-rw-r--r--. 1 cassandra cassandra   43 Feb  3 12:58 space1-tb1-jb-2-CompressionInfo.db
-rw-r--r--. 1 cassandra cassandra  146 Feb  3 12:58 space1-tb1-jb-2-Data.db
-rw-r--r--. 1 cassandra cassandra  336 Feb  3 12:58 space1-tb1-jb-2-Filter.db
-rw-r--r--. 1 cassandra cassandra   90 Feb  3 12:58 space1-tb1-jb-2-Index.db
-rw-r--r--. 1 cassandra cassandra 4393 Feb  3 12:58 space1-tb1-jb-2-Statistics.db
-rw-r--r--. 1 cassandra cassandra   80 Feb  3 12:58 space1-tb1-jb-2-Summary.db
-rw-r--r--. 1 cassandra cassandra   79 Feb  3 12:58 space1-tb1-jb-2-TOC.txt
-rw-r--r--. 1 cassandra cassandra   43 Feb  3 13:02 space1-tb1-jb-3-CompressionInfo.db
-rw-r--r--. 1 cassandra cassandra   49 Feb  3 13:02 space1-tb1-jb-3-Data.db
-rw-r--r--. 1 cassandra cassandra   16 Feb  3 13:02 space1-tb1-jb-3-Filter.db
-rw-r--r--. 1 cassandra cassandra   36 Feb  3 13:02 space1-tb1-jb-3-Index.db
-rw-r--r--. 1 cassandra cassandra 4413 Feb  3 13:02 space1-tb1-jb-3-Statistics.db
-rw-r--r--. 1 cassandra cassandra   80 Feb  3 13:02 space1-tb1-jb-3-Summary.db
-rw-r--r--. 1 cassandra cassandra   79 Feb  3 13:02 space1-tb1-jb-3-TOC.txt
</code></pre>

<p>Lets check the tables contents</p>

<pre><code>$ sudo -u cassandra /usr/bin/sstable2json /var/lib/cassandra/data/space1/tb1/space1-tb1-jb-2-Data.db
[
{""key"": ""6b657935"",""columns"": [["""","""",1422967847005000], [""val1"",""555"",1422967847005000]]},
{""key"": ""6b657931"",""columns"": [["""","""",1422967817740000], [""val1"",""111"",1422967817740000]]},
{""key"": ""6b657934"",""columns"": [["""","""",1422967840622000], [""val1"",""444"",1422967840622000]]},
{""key"": ""6b657933"",""columns"": [["""","""",1422967832341000], [""val1"",""333"",1422967832341000]]},
{""key"": ""6b657932"",""columns"": [["""","""",1422967825116000], [""val1"",""222"",1422967825116000]]}
]

$ sudo -u cassandra /usr/bin/sstable2json /var/lib/cassandra/data/space1/tb1/space1-tb1-jb-3-Data.db
[
{""key"": ""6b657931"",""metadata"": {""deletionInfo"": {""markedForDeleteAt"":1422968551313000,""localDeletionTime"":1422968551}},""columns"": []},
{""key"": ""6b657932"",""metadata"": {""deletionInfo"": {""markedForDeleteAt"":1422968553322000,""localDeletionTime"":1422968553}},""columns"": []}
]
</code></pre>

<p>Now lets compact </p>

<pre><code>$ nodetool compact space1
</code></pre>

<p>Only one stable now as expected</p>

<pre><code>$ sudo ls -l /var/lib/cassandra/data/space1/tb1/
total 32
-rw-r--r--. 1 cassandra cassandra   43 Feb  3 13:05 space1-tb1-jb-4-CompressionInfo.db
-rw-r--r--. 1 cassandra cassandra  133 Feb  3 13:05 space1-tb1-jb-4-Data.db
-rw-r--r--. 1 cassandra cassandra  656 Feb  3 13:05 space1-tb1-jb-4-Filter.db
-rw-r--r--. 1 cassandra cassandra   90 Feb  3 13:05 space1-tb1-jb-4-Index.db
-rw-r--r--. 1 cassandra cassandra 4429 Feb  3 13:05 space1-tb1-jb-4-Statistics.db
-rw-r--r--. 1 cassandra cassandra   80 Feb  3 13:05 space1-tb1-jb-4-Summary.db
-rw-r--r--. 1 cassandra cassandra   79 Feb  3 13:05 space1-tb1-jb-4-TOC.txt
</code></pre>

<p>Now lets check the contents of the new stable we can see the tombstones</p>

<pre><code>$ sudo -u cassandra /usr/bin/sstable2json /var/lib/cassandra/data/space1/tb1/space1-tb1-jb-4-Data.db
[
{""key"": ""6b657935"",""columns"": [["""","""",1422967847005000], [""val1"",""555"",1422967847005000]]},
{""key"": ""6b657931"",""metadata"": {""deletionInfo"": {""markedForDeleteAt"":1422968551313000,""localDeletionTime"":1422968551}},""columns"": []},
{""key"": ""6b657934"",""columns"": [["""","""",1422967840622000], [""val1"",""444"",1422967840622000]]},
{""key"": ""6b657933"",""columns"": [["""","""",1422967832341000], [""val1"",""333"",1422967832341000]]},
{""key"": ""6b657932"",""metadata"": {""deletionInfo"": {""markedForDeleteAt"":1422968553322000,""localDeletionTime"":1422968553}},""columns"": []}
]
</code></pre>

<p>Finally lets check compaction history (scroll right)</p>

<pre><code>$ nodetool compactionhistory | awk 'NR == 2 || /space1/'
id                                       keyspace_name      columnfamily_name            compacted_at              bytes_in       bytes_out      rows_merged
5725f890-aba4-11e4-9f73-351725b0ac5b     space1             tb1                          1422968305305             146            146            {1:5}
46112600-aba5-11e4-9f73-351725b0ac5b     space1             tb1                          1422968706144             195            133            {1:3, 2:2}
</code></pre>
",['table']
27566615,27567303,2014-12-19 12:53:24,Can I force cleanup of old tombstones?,"<p>I have recently lowered <code>gc_grace_seconds</code> for a CQL table. I am running <code>LeveledCompactionStrategy</code>. Is it possible for me to force purging of old tombstones from my SSTables?</p>
",<cassandra><datastax><datastax-enterprise><tombstone>,"<h3>TL;DR</h3>

<p>Your tombstones will disappear on their own through compaction bit make sure you are running repair or they may come back from the dead.</p>

<p><a href=""http://www.datastax.com/documentation/cassandra/2.0/cassandra/dml/dml_about_deletes_c.html"" rel=""noreferrer"">http://www.datastax.com/documentation/cassandra/2.0/cassandra/dml/dml_about_deletes_c.html</a></p>

<h3>Adding some more details:</h3>

<p>Tombstones are not immediately available for deletion until both: </p>

<p>1) gc_grace_seconds has expired</p>

<p>2) they meet the requirements configured in tombstone compaction sub-properties</p>

<h3>I need to free up disk by expiring tombstones, how do I do this quickly?</h3>

<p>1) Run a repair to ensure your tombstones are consistent</p>

<p>2) Decrease gc_grace_seconds for your table (<a href=""http://docs.datastax.com/en/cql/3.1/cql/cql_reference/tabProp.html"" rel=""noreferrer"">alter table statement</a>)</p>

<p>3) Configure your <a href=""http://docs.datastax.com/en/cql/3.1/cql/cql_reference/compactSubprop.html"" rel=""noreferrer"">compaction sub properties</a> to speed up tombstone removal:</p>

<p>Decrease tombstone_compaction_interval and decrease tombstone_threshold, or set unchecked_tombstone_compaction to true to ignore both conditions and collect based purely on gc grace.</p>

<h3>Is it working?</h3>

<p>You can see statistics about tombstones in <code>nodetool cfstats</code> and by using the sstable metatdata utility found in your tools directory <code>sstablemetadata &lt;sstable filenames&gt;</code>.</p>
",['table']
27588499,27601635,2014-12-21 10:23:57,Cassandra primary key design to cater range query,"<p>I have designed a column family</p>

<p>prodgroup text, prodid int, status int, ,
PRIMARY KEY ((prodgroup), prodid, status)</p>

<p>The data model is to cater</p>

<ul>
<li>Get list of products from the product group</li>
<li>get list of products for a given range of ids</li>
<li>Get details of a specific product</li>
<li>Update status of the product acive/inactive </li>
<li>Get list of products that are active or inactive (select * from product where prodgroup='xyz' and prodid > 0 and status = 0)</li>
</ul>

<p>The design works fine, except for the last query . Cassandra not allowing to query on status unless I fix the product id. I think defining a super column family which has the key ""PRIMARY KEY((prodgroup), staus, productid)"" should work. Would like to get expert advice on other alternatives.</p>
",<cassandra><cassandra-2.0><cassandra-jdbc><nosql>,"<p>If you're providing the partition key (group id in this case), then this is an ideal use case for a secondary index. Create a secondary indea on status, and you'll be able to query for exact equality on status. Make sure you provide the partition key, coz if you don't, the query will go to every single node in your cluster, and will likely timeout. If you do provide a partition key (which you seem to be doing), then a secondary index on status should allow you to carry out your query.</p>

<p>There is another issue...you have status as part of the pk. This means that you CAN'T update it for a product. You can only create new rows for different statusses. This is likely not what you need. And if it is, you'll need to ALLOW FILTERING on your last query (which won't be much of a perf hit coz you've filtered to the partition already).</p>

<p>For your use case, this is the schema and query I'd use. I believe it covers all the use cases:</p>

<pre><code>create table products2(
    prodgroup text,
    prodid int,
    status int,
    primary key (prodgroup, prodid)
);

create index on products2 (status);

select * from products2 where prodgroup='groupname' and prodid&gt;0 and status=0;
</code></pre>

<p>Since secondary index updates are atomic and managed by cassandra, as long as you're hitting the partition, this'll work nicely.</p>

<p>Hope that helps.</p>
",['table']
27696035,27696952,2014-12-29 21:08:58,Query on timeuuid type does not return correct results via cql,"<p>I am attempting to perform a query using timeuuid to retrieve a result set.</p>

<p>Table is as such:</p>

<pre><code>CREATE TABLE mds.arguments_by_id (
  argument_id timeuuid PRIMARY KEY,
  category text,
  title text
)
</code></pre>

<p>When I select the dateOf() for all of the data in the table, I get the following:</p>

<pre><code>select dateOf(argument_id),argument_id from arguments_by_id ;

 dateOf(argument_id)      | argument_id
 -------------------------+--------------------------------------
 2014-12-29 13:50:07-0500 | 81f990c0-8f8b-11e4-abb3-5d7a44c0d8a8
 2014-12-29 14:01:43-0500 | 20def1c0-8f8d-11e4-abb3-5d7a44c0d8a8
 2014-12-29 14:01:58-0500 | 29b50f50-8f8d-11e4-abb3-5d7a44c0d8a8
 2014-12-29 14:03:01-0500 | 4f6b72c0-8f8d-11e4-bc90-abc65998337a

(4 rows)
</code></pre>

<p>The query I'd like to run needs to return results where the argument_id (date) is greater than a specified date:  </p>

<pre><code>select dateOf(argument_id),argument_id from arguments_by_id where token(argument_id) &gt; token(maxTimeuuid('2014-12-28 15:31:00-0500'));
</code></pre>

<p>However that query returns a (seemingly) incomplete result set when compared to the previous select:</p>

<pre><code> dateOf(argument_id)      | argument_id
--------------------------+--------------------------------------
 2014-12-29 14:01:43-0500 | 20def1c0-8f8d-11e4-abb3-5d7a44c0d8a8
 2014-12-29 14:01:58-0500 | 29b50f50-8f8d-11e4-abb3-5d7a44c0d8a8
 2014-12-29 14:03:01-0500 | 4f6b72c0-8f8d-11e4-bc90-abc65998337a

(3 rows)
</code></pre>

<p>My goal was to minimize the number of keys - but am wondering if I am 1) incurring a performance hit by going this route and 2) trying to do too much with the primary key.</p>
",<cassandra><cql>,"<p>In order to use a timeuuid column like this, you would need to make it a clustering column rather than a partition key (<a href=""http://www.datastax.com/documentation/cql/3.1/cql/ddl/ddl_compound_keys_c.html"" rel=""nofollow noreferrer"">docs</a>). You would need to adapt this to fit your data model, but here's an example:</p>

<pre><code>create table sample (
  id int,
  tid timeuuid,
  category text,
  title text,
  primary key (id, tid)
);
</code></pre>

<p>Now we can do a few inserts a couple seconds apart:</p>

<pre><code>insert into sample (id, tid) values (100, now());
insert into sample (id, tid) values (100, now());
insert into sample (id, tid) values (100, now());
insert into sample (id, tid) values (100, now());
</code></pre>

<p>Show all values:</p>

<pre><code>select id,tid,dateOf(tid) from sample;

 id  | tid                                  | dateOf(tid)
-----+--------------------------------------+--------------------------
 100 | df4387a0-8fa8-11e4-bd3a-97fb52c7ef8c | 2014-12-29 14:20:19-0800
 100 | e085a490-8fa8-11e4-bd3a-97fb52c7ef8c | 2014-12-29 14:20:21-0800
 100 | e2bd6c20-8fa8-11e4-bd3a-97fb52c7ef8c | 2014-12-29 14:20:24-0800
 100 | e475f190-8fa8-11e4-bd3a-97fb52c7ef8c | 2014-12-29 14:20:27-0800
</code></pre>

<p>Show just a portion using timeuuid comparison:</p>

<pre><code>select id,tid,dateOf(tid) from sample where id=100 and tid&gt;=minTimeuuid('2014-12-29 14:20:24-0800');

 id  | tid                                  | dateOf(tid)
-----+--------------------------------------+--------------------------
 100 | e2bd6c20-8fa8-11e4-bd3a-97fb52c7ef8c | 2014-12-29 14:20:24-0800
 100 | e475f190-8fa8-11e4-bd3a-97fb52c7ef8c | 2014-12-29 14:20:27-0800
</code></pre>

<p>Note if you try a select without specifying the primary key (id=100), you'll get a warning that ALLOW FILTERING would be required for that query. This is generally the wrong thing to do as it will need to do a full table scan:</p>

<pre><code>select id,tid,dateOf(tid) from sample where tid&gt;=minTimeuuid('2014-12-29 14:20:24-0800');
Bad Request: Cannot execute this query as it might involve data filtering and thus may have unpredictable performance. If you want to execute this 
query despite the performance unpredictability, use ALLOW FILTERING
</code></pre>

<p>Here's another <a href=""https://stackoverflow.com/a/24388154/9965"">SO answer</a> with a similar situation.</p>
",['table']
27752360,27755243,2015-01-03 06:27:15,What is difference between CQL for Cassandra 2.x and CQL for Cassandra 1.2,"<p>I am new to Cassandra so I have started with current version of CQL(Cassandra 2.x).So I want major difference between  CQL for Cassandra 2.x and CQL for Cassandra 1.2.</p>
",<cassandra><cql><nosql>,"<p>Based on <a href=""http://www.datastax.com/documentation/cql/3.1/cql/cql_intro_c.html"" rel=""nofollow"">http://www.datastax.com/documentation/cql/3.1/cql/cql_intro_c.html</a> (and its differences from <a href=""http://www.datastax.com/documentation/cql/3.0/cql/cql_reference/about_cql_ref_c.html"" rel=""nofollow"">http://www.datastax.com/documentation/cql/3.0/cql/cql_reference/about_cql_ref_c.html</a>) the main difference(s) are:</p>

<ul>
<li>Lightweight transactions using the IF keyword in INSERT and UPDATE
statements.</li>
<li>Initial support for triggers.</li>
<li>The ALTER TABLE DROP command, which had been removed in the earlier release.</li>
<li>Column aliases, similar to aliases in RDBMS SQL, in a SELECT statement.</li>
<li>Indexing of any part, partition key or clustering columns, portion of a compound primary key.</li>
<li>CQL for Cassandra 2.0 deprecated super columns. Cassandra continues to support apps that query super columns, translating super columns on the fly into CQL constructs and results.</li>
<li>The ASSUME command has been removed.</li>
<li>The COPY command supports collections.</li>
<li>New CQL table attributes: <code>default_time_to_live</code>, <code>memtable_flush_period_in_ms</code>, <code>populate_io_cache_on_flush</code>, <code>speculative_retry</code></li>
</ul>
",['table']
27778670,27779449,2015-01-05 11:32:32,Cassandra two nodes with redundancy,"<p>I have setup two servers to both run Cassandra by following the documentation in the DataStax website. My current setup is to have</p>

<blockquote>
  <p>1 seed node (configured in both yamls)</p>
</blockquote>

<p>When running, both nodes are up (when testing via nodetool) and both seem to have the data replicated correctly but I've noticed that when I bring down the seed node, the other node doesn't allow client connections (neither via their API or by connecting to cqlsh) which is a problem.</p>

<p>My requirement is to have two servers which are perfect replicas of each other and in case one server is temporary down (due to disk space failures for instance), the other server can take over the requests until the broken server comes back online.</p>

<p>Given this requriement, I have the following questions:</p>

<ol>
<li>Do I need to setup both nodes as ""seed"" nodes?</li>
<li>How would I make sure everything is replicated across both servers? Does this happen automatically or is there some setting somewhere I need to set?</li>
</ol>

<p>Many thanks in advance,</p>
",<cassandra>,"<p>Cassandra does not do master-slave replication. There is no master in cassandra. Rather, data is distributed across the cluster. The distribution mechanism depends on a number of things.</p>

<p>Data is stored on nodes in partitions. Remember cassandra is a partitioned row store? That's where partitions come in. Data is stored in partitions. All rows for a partition are stored together in a single node (and replicas). How many replicas depends on the table's replication factor. If the replication factor is 3 for a table, each partition for that table (and as such, all rows in that partition) are stored in two additional replicas. It's like saying - ""I want 3 copies of this data"". </p>

<p>During writing, clients can specify a consistency level (CL). This is the number of nodes that must acknowledge for a write to be successful. Clients can specify a CL for reading too. Cassandra issues read requests to n=CL nodes, and takes the most recent value as the query result.</p>

<p>By tuning read and write CLs, you control consistency. If Read CL + Write CL > Replication factor (RF), you get full consistency. </p>

<p>In terms of fault tolerance, you can tweak CLs and RF to be what you need. For example, if you have RF=3, Read CL=2, Write CL = 2, then you have full consistency, and you can tolerate one node going down. For RF=5, Read CL=3, Write CL=3, you have the same, but can tolerate 2 nodes going down.</p>

<p>A two node cluster, is not really a good idea. You can set RF=2 (all data replicated), write CL=2 and read CL =1. However, this will mean if a node is down, you can only read but not write. You can set read CL=2 and write CL=1, in which case, if a node goes down, you can write but not read. Realistically, you should go for at least 5 (at the very least 4) nodes with a RF=3. Any lower than that, and you're asking for trouble.</p>
",['table']
27786942,27787105,2015-01-05 20:00:59,Cassandra : Timeseries data and secondary indexes,"<p>Lets say I have 100 K users spread over 10 K towns/localities.  I am recv time series data for them say every 5 minutes (for each user)
I dont have town as part of the key.</p>

<p>Is it a good practice to create secondary index on town for such case.</p>

<p>regards</p>
",<cassandra>,"<p>10,000 different keys for secondary indices is definitely not an ideal scenario as that would be a pretty high cardinality.  I would recommend reading Richard Low's article on <a href=""http://www.wentnet.com/blog/?p=77"" rel=""nofollow"">'The sweet spot for Cassandra secondary indexing'</a>.   Read performance would probably be less than ideal as an index scan would need to happen on a replica in each partition.</p>

<p>In your case I would suggest denormalizing by creating a separate table called 'users_by_town' that would allow you to search for users by town.</p>

<p>You could always try both cases and use <a href=""http://www.datastax.com/documentation/cql/3.1/cql/cql_reference/tracing_r.html"" rel=""nofollow"">request tracing</a> to understand the costs of secondary indexes in this particular scenario.</p>
",['table']
27840329,27841262,2015-01-08 12:38:17,Cassandra: selecting first entry for each value of an indexed column,"<p>I have a table of events and would like to extract the first timestamp (column <code>unixtime</code>) for each user.
Is there a way to do this with a single Cassandra query?</p>

<p>The schema is the following:</p>

<pre><code>CREATE TABLE events (
 id VARCHAR,
 unixtime bigint,
 u bigint,
 type VARCHAR,
 payload map&lt;text, text&gt;, 
 PRIMARY KEY(id)
);

CREATE INDEX events_u
  ON events (u);

CREATE INDEX events_unixtime
  ON events (unixtime);

CREATE INDEX events_type
  ON events (type);
</code></pre>
",<cassandra><cql>,"<p>According to your schema, each user will have a single time stamp. If you want one event per entry, consider:</p>

<pre><code>PRIMARY KEY (id, unixtime).
</code></pre>

<p>Assuming that is your schema, the entries for a user will be stored in ascending unixtime order. Be careful though...if it's an unbounded event stream and users have lots of events, the partition for the id will grow and grow. It's recommended to keep partition sizes to tens or hundreds of megs. If you anticipate larger, you'll need to start some form of bucketing.</p>

<p>Now, on to your query. In a word, no. If you don't hit a partition (by specifying the partition key), your query becomes a cluster wide operation. With little data it'll work. But with lots of data, you'll get timeouts. If you do have the data in its current form, then I recommend you use the Cassandra Spark connector and Apache Spark to do your query. An added benefit of the spark connectory is that if you have cassandra nodes as spark worker nodes, due to locality, you can efficiently hit a secondary index without specifying the partition key (which would normally cause a cluster wide query with timeout issues, etc.). You could even use Spark to get the required data and store it into another cassandra table for fast querying.</p>
",['table']
27854151,27859125,2015-01-09 05:05:40,An Approach to Cassandra Data Model,"<p>Please note that I am first time using NoSQL and pretty much every concept is new in this NoSQL world, being from RDBMS for long time!!</p>

<p>In one of my heavy used applications, I want to use NoSQL for some part of the data and move out from MySQL where transactions/Relational model doesn't make sense. What I would get is, C<strong>AP</strong> [Availability and Partition Tolerance].</p>

<p>The present data model is simple as this</p>

<pre><code>ID (integer) |  ENTITY_ID (integer)  |  ENTITY_TYPE (String)  | ENTITY_DATA (Text)  | CREATED_ON (Date) | VERSION (interger)|
</code></pre>

<p>We can safely assume that this part of application is similar to Logging of the Activity!
I would like to move this to NoSQL as per my requirements and separate from Performance Oriented MySQL DB.</p>

<p>Cassandra says, everything in it is simple <code>Map&lt;Key,Value&gt; type</code>! Thinking in terms of Map level,
I can use <code>ENTITY_ID|ENTITY_TYPE|ENTITY_APP</code> as key and store the rest of the data in values!</p>

<p>After reading through User Defined Types in Cassandra, can I use <code>UserDefinedType</code> as value which essentially leverage as One Key and multiple values! Otherwise, Use it as normal column level without <code>UserDefinedType</code>! One idea is to use the same model for different applications across systems where it would be simple logging/activity data can be pushed to the same, since the key varies from application to application and within application each entity will be unique! </p>

<p>No application/business function to access this data without Key, or in simple terms no requirement to get data randomly!</p>

<p>References: <a href=""http://www.ebaytechblog.com/2012/07/16/cassandra-data-modeling-best-practices-part-1/"" rel=""nofollow"">http://www.ebaytechblog.com/2012/07/16/cassandra-data-modeling-best-practices-part-1/</a></p>
",<cassandra><user-defined-types><datamodel><nosql>,"<p>Let me explain the cassandra data model a bit (or at least, a part of it). You create tables like so:</p>

<pre><code>create table event(
   id uuid,
   timestamp timeuuid,
   some_column text,
   some_column2 list&lt;text&gt;,
   some_column3 map&lt;text, text&gt;,
   some_column4 map&lt;text, text&gt;,
   primary key (id, timestamp .... );
</code></pre>

<p>Note the primary key. There's multiple columns specified. The first column is the partition key. All ""rows"" in a partition are stored together. Inside a partition, data is ordered by the second, then third, then fourth... keys in the primary key. These are called clustering keys. To query, you almost always hit a partition (by specifying equality in the where clause). Any further filters in your query are then done on the selected partition. If you don't specify a partition key, you make a cluster wide query, which may be slow or most likely, time out. After hitting the partition, you can filter with matches on subsequent keys in order, with a range query on the last clustering key specified in your query. Anyway, that's all about querying.</p>

<p>In terms of structure, you have a few column types. Some primitives like text, int, etc., but also three collections - sets, lists and maps. Yes, maps. UDTs are typically more useful when used in collections. e.g. A Person may have a map of addresses: map. You would typically store info in columns if you needed to query on it, or index on it, or you know each row will have those columns. You're also free to use a map column which would let you store ""arbitrary"" key-value data; which is what it seems you're looking to do.</p>

<p>One thing to watch out for... your primary key is unique per records. If you do another insert with the same pk, you won't get an error, it'll simply overwrite the existing data. Everything in cassandra is an upsert. And you won't be able to change the value of any column that's in the primary key for any row.</p>

<p>You mentioned querying is not a factor. However, if you do find yourself needing to do aggregations, you should check out Apache Spark, which works very well with Cassandra (and also supports relational data sources....so you should be able to aggregate data across mysql and cassandra for analytics).</p>

<p>Lastly, if your data is time series log data, cassandra is a very very good choice.</p>
",['table']
27854431,27867017,2015-01-09 05:33:58,Remove all data Cassandra?,"<p>I have a eight node cassandra setup. I am saving data with 3 days TTL. But the data is useless after I take a summary (using my java script, count of things etc). I want to delete all the data in a table. I can stop cassandra for sometime to do the deletion. So the data is removed from all nodes. 
Should I run truncate and nodetool repair afterwards or should I flush first then delete. Whats the proper way to do it.</p>
",<cassandra><cql><cassandra-2.0><cql3>,"<p>You can drop the tables or truncate them... but keep in mind that Cassandra will snapshot your tables by default, so you'll also need to run nodetool clearsnapshot on all of your nodes afterwards. There is no need to stop Cassandra while you do this delete. </p>

<p>I don't know that there is a right way per se... but when I do when I need to clear a table is, first, I run truncate on the table using cqlsh.  Then I run nodetool clearsnapshot on my nodes using pssh (<a href=""https://code.google.com/p/parallel-ssh/"" rel=""nofollow"">https://code.google.com/p/parallel-ssh/</a>). </p>

<p>Hope this helps</p>
",['table']
27868907,27872557,2015-01-09 20:41:46,To what extent is denormalization necessary in Cassandra?,"<p>I'm in charge of migrating an application from MySQL to Cassandra. And I'm curious, to what extent is denormalization necessary in this process?</p>

<p>For example, if the program searches for an index in table A, then looks up that value's information in table B, is this not allowed in Cassandra, or just not optimal? There are no joins in the application, just several lookups like this.</p>

<p>The resources I've found online confuse me. Do I <em>need</em> to denormalize the data by combining these tables together, or is this just something that speeds up performance in Cassandra?</p>
",<mysql><database><optimization><cassandra><database-normalization>,"<p>Usually in a relational database like MySQL you design your tables to efficiently store your data, and then you normalize those tables to eliminate redundant information, to save storage space, and to prevent having inconsistent data (like having different addresses for a person in different rows).  Then almost as an afterthought, you can figure out what queries you want to do against those normalized tables by doing joins and adding indexes on any column to make those queries fast.</p>

<p>With Cassandra you approach it first by figuring out what queries you need to do, and then designing your schema to do those queries efficiently.  The query options in Cassandra are far more limited than in MySQL, since all you really have to work with is the partition key and the clustering columns.  You can't easily do joins, you can't easily aggregate, and search options are very limited.  You can create secondary indexes, but using them is not efficient like RDBMS indexes, so generally you want to avoid them and rely mostly on the compound primary key.</p>

<p>So no, you do not <em>need</em> to denormalize your data completely, but it is a useful tool in the toolbox for making frequently used queries efficient.  It's basically a way of grouping a lot of related information into one bucket that you can access quickly by the key.  Storage is considered cheap, so generally we don't care if we have some redundant information in multiple tables (within reason).</p>

<p>When you say a program ""searches"" for an index in table A, that sounds inefficient since you can't easily search for things in Cassandra tables.  What you want is for the program to know the key for what it is looking for so that Cassandra can go directly to the place where that information is stored.  For example, if a user logs into a system, you use their userid to access the bucket of information that tells everything about them.</p>

<p>Now it is perfectly acceptable to have a foreign key in table A that you use to look up other related information in table B, since that is just two key reads, one for table A and then one for table B.  But if rather than doing this two step look up occasionally for individual rows, you actually need to join all the rows of table A and B for generating a report, well then you'd be better off combining them into one denormalized table.</p>
",['table']
27934698,27946633,2015-01-14 02:00:27,"Create an index, or add to the primary key and create a new table?","<p>In Cassandra, I have a table with columns <code>(a,b,c)</code>. I either need to query <code>SELECT * FROM {table} WHERE a = ? and b = ?</code> and <code>SELECT * FROM {table} WHERE a = ? and c = ?</code>.</p>

<p>In this case, what should I make the primary key? Could I make two tables with <code>PRIMARY KEY(a,b)</code> and <code>PRIMARY KEY(a,c)</code>, because Cassandra needs the entirety of the partition key and/or non-partition keys in the order they are listed? Or could I do something like <code>PRIMARY KEY(a)</code> and create an <code>INDEX</code> on <code>b</code> and <code>c</code>? </p>

<p>Basically, should the primary key only contain the minimum number of values required for uniqueness (and choosing an appropriate partition key from these values)? Will performance improve if I add other columns to the primary key because I need to query them?</p>
",<database-design><cassandra><database><nosql>,"<p>As noted above, a well-grounded answer can only be given if you provide more information about the cardinality of the a, b and c columns. Also make sure you understand the meaning of partitioning key and clustering key - they are both part of primary key, and have a huge impact on your design.</p>

<p>If you have enough distinct values in column a, you can make it a partition key, and choose one from the following two approaches:</p>

<p>1) separate table for each query</p>

<pre><code>CREATE TABLE table1_by_ab (
  a int, b int, c int, 
  PRIMARY KEY (a, b));

CREATE TABLE table1_by_ac (
  a int, b int, c int, 
  PRIMARY KEY (a, c));
</code></pre>

<p>2) one table for the more frequent query, and index for the other column:</p>

<pre><code>CREATE TABLE table2 (
  a int, b int, c int, 
  PRIMARY KEY (a, b));

CREATE INDEX ON table2 (c);
</code></pre>

<p>In both cases you can execute your queries on (a,b) and (a,c). Usually it is recommended to avoid secondary indexes, but in case 2) your query on (a,c) pre-selects the partition key (field a), so the secondary index can be executed on a single node, and its performance will not be bad.</p>

<p>If you don't have enough distinct values in column a, then you cannot make it a partitioning key, you will need to duplicate your tables, both with a compound partitioning key:</p>

<pre><code>CREATE TABLE table3_by_ab (
  a int, b int, c int, 
  PRIMARY KEY ((a, b)));

CREATE TABLE table3_by_ac (
  a int, b int, c int, 
  PRIMARY KEY ((a, c)));
</code></pre>

<p>Hope this helps</p>
",['table']
27939234,27944273,2015-01-14 09:11:15,Cassandra ByteOrderedPartitioner,"<p>I want to execute some range queries on a table that is structured like:</p>

<pre><code>CREATE TABLE table(

num int,
val1 int,
val2 float,
val3 text,
...
PRIMARY KEY(num)
)
</code></pre>

<p>A range query should look like:</p>

<pre><code>SELECT num, val1, val2 FROM table WHERE num&gt;100 AND num&lt;1000;
</code></pre>

<p>I read ths post: <a href=""https://stackoverflow.com/questions/24919732/performing-range-queries-for-cassandra-table"">Performing range queries for cassandra table</a> and now I have problems with using the ByteOrderedPartitoner. </p>

<p>I use the OPSCenter Web Interface and try to create a new cluster. I change the Partitioner and the following error appears:</p>

<p>Error provisioning cluster: A token_map argument of the form {ip: token} is required when not using RandomPartitioner or Murmur3Partitioner</p>

<p>I can not find a token_map argument. What am I doing wrong? What else do I have to do to enable the query?</p>

<p>I hope somebody can help me. Thank you!</p>
",<cassandra><cql>,"<blockquote>
  <p>What am I doing wrong?</p>
</blockquote>

<p>You are using the Byte Ordered Partitioner.  Its use has been identified as a Cassandra anti-pattern...for a while now. Matt Dennis has a <a href=""http://www.slideshare.net/mattdennis"" rel=""noreferrer"">slideshare presentation on Cassandra Anti-Patterns</a>, and it contains a slide concerning the BOP:
<img src=""https://i.stack.imgur.com/wOJyk.jpg"" alt=""Don&#39;t use the Byte Ordered Partitioner""></p>

<p>So while the above slide is meant to be humorous, seriously, do not use the Byte Ordered Partitioner.  It is still included with Cassanra, so that those who used it back in 2011 have an upgrade path.  <strong>No new clusters should be built with the BOP.</strong>  The (default) Murmur3 partitioner is what you should use.</p>

<p>As for how to solve your problem with the Murmur3 partitioner, the question/answer you linked above refers to Patrick McFadin's article on <a href=""http://planetcassandra.org/blog/post/getting-started-with-time-series-data-modeling/"" rel=""noreferrer"">Getting Started With Time Series Data Modeling</a>.  In that article, there are three modeling patterns demonstrated.  They should be able to help you come up with an appropriate data model.  Basically, you can order your data with a clustering key and then read it with a range query...just not by your <em>current</em> partitioning key.</p>

<pre><code>CREATE TABLE tableorderedbynum(
 num int,
 val1 int,
 val2 float,
 val3 text,
 someotherkey text,
...
PRIMARY KEY((someotherkey),num)
);
</code></pre>

<p>Examine your data model, and see if you can find another key to help partition your data.  Then, if you create a query table (like I have above) using the other key as your partitioning key, and num as your clustering key; then this range query will work:</p>

<pre><code>SELECT num, val1, val2 
FROM tableorderedbynum WHERE someotherkey='yourvalue' AND num&gt;100 AND num&lt;1000;
</code></pre>
","['partitioner', 'table']"
27942152,27944519,2015-01-14 11:43:19,Only date range scanning Cassandra CQL timestamp,"<p>I have a table like given below.</p>

<pre><code>CREATE TEST(
 HOURLYTIME TIMESTAMP,
 FULLTIME TIMESTAMP,
 DATA TEXT,
 PRIMARY KEY(HOURLYTIME,FULLTIME)
)
</code></pre>

<p>I inserted the record <code>(2014-12-12 00:00:00,2014-12-12 00:00:01,'Hello World')</code></p>

<p>I would like to search based on date time range in HOURLYTIME field which holds hourly records.When i tried with token() like</p>

<p><code>select * from TEST where token(HOURLYTIME)=token('2014-12-12')</code> </p>

<p>to get all the records for that date it returns only for one hour record i.e for</p>

<pre><code> 2014-12-12 **00:00:00**
</code></pre>

<p>If i add date range </p>

<pre><code>select * from TEST where token(HOURLYTIME)&gt;=token('2014-12-12') AND token(HOURLYTIME)&lt;=token('2014-12-14');
</code></pre>

<p>It gives the error : <em>More than one restriction was found for the start bound</em>. </p>

<p>How to resolve this issue.</p>

<p>I am able to scan using FULLTIME but i need to provide ALLOW FILTERING which will scan whole records &amp; inefficient.</p>
",<java><cassandra><cassandra-cli><nosql>,"<p>You are not allowed to restrict the primary key by a range without explicitly demanding it with allow filterting . This prevents queries which require a full table scan which as you note are slow and will not scale for true big data sizes. The reason for this is that the primary key values are randomly hashed so specifying a range of primary key values is basically the same as providing two loosely coupled random numbers. For example in your case dates most likely are not monotonically hashed. This means saying you want dates that hash to a value less that the hash of another value will return a  completely random set of data.</p>

<p>The issue here is that your table setup does not allow the queries that you actually want to perform. You need to model your tables so that the information you want can be obtained from a single partition.</p>
",['table']
27956084,28772651,2015-01-15 02:53:13,eqs on Option[UUID] type? Phantom + Cassandra + Scala,"<p>I'm using Phantom framework to work with Cassandra and I'm trying to do a eqs on a Option field eg.</p>

<pre><code>Address.select.where(_.id eqs Some(uuid)).one()
</code></pre>

<p>Then I get ""value eqs is not a member of object""</p>

<p>is there a way to accomplish that? I can't figure out...</p>

<p>The id field is an Option[UUID], because it must be null when I'm receiving a POST request in Play Framework, but I don't know how to do this assert in phantom</p>

<p>I also opened an issue on github.</p>

<p><a href=""https://github.com/websudos/phantom/issues/173"" rel=""nofollow"">https://github.com/websudos/phantom/issues/173</a></p>
",<scala><cassandra><phantom-dsl>,"<p>Phantom relies on a series of implicit conversions to provide most of the functionality. A very simple way to fix most of the errors you get from compiling phantom tables is to make sure the relevant import is in scope.</p>

<p>Before phantom 1.7.0</p>

<pre><code>import com.websudos.phantom.Implicits._
</code></pre>

<p>After 1.7.0</p>

<pre><code>import com.websudos.phantom.dsl._
</code></pre>

<p>Beyond the implicit mechanism, phantom will also help you with aliases to a vast number of useful objects in Cassandra:</p>

<ul>
<li>Phantom connectors</li>
<li>Cassandra Consistency Levels</li>
<li>Keyspaces </li>
</ul>

<p>Using a potentially <code>null</code> value as part of a <code>PRIMARY KEY</code> in CQL is also wrong as no part of the CQL primary can be <code>null</code>. It's a much better idea to move processing logic outside of phantom.</p>

<p>Traditionally, a tables -> db service -> api controller -> api approach is the way to build modular applications with better separation of concerns. It's best to keep simple I/O at table level, app level consistency at db service level, and all processing logic at a higher level.</p>

<p>Hope this helps.</p>
",['table']
27974911,27975779,2015-01-15 23:29:29,Not enough replica available for query at consistency ONE (1 required but only 0 alive),"<p>I have a Cassandra cluster with three nodes, two of which are up. They are all in the same DC. When my Java application goes to write to the cluster, I get an error in my application that seems to be caused by some problem with Cassandra:</p>

<p>Caused by: com.datastax.driver.core.exceptions.UnavailableException: Not enough replica available for query at consistency ONE (1 required but only 0 alive)
    at com.datastax.driver.core.exceptions.UnavailableException.copy(UnavailableException.java:79)</p>

<p>The part that doesn't make sense is that ""1 required but only 0 alive"" statement. There are two nodes up, which means that one should be ""alive"" for replication.</p>

<p>Or am I misunderstanding the error message?</p>

<p>Thanks.</p>
",<cassandra>,"<p>You are likely getting this error because the Replication Factor of the keyspace the table you are querying belongs to has a Replication Factor of one, is that correct?</p>

<p>If the partition you are reading / updating does not have enough available replicas (nodes with that data) to meet the consistency level, you will get this error.</p>

<p>If you want to be able to handle more than 1 node being unavailable, what you could do is look into <a href=""http://www.datastax.com/documentation/cql/3.1/cql/cql_reference/alter_keyspace_r.html"" rel=""noreferrer"">altering your keyspace</a> to set a higher replication factor, preferably three in this case, and then running a <a href=""http://www.datastax.com/documentation/cassandra/2.0/cassandra/tools/toolsRepair.html"" rel=""noreferrer"">nodetool repair</a> on each node to get all of your data on all nodes.   With this change, you would be able to survive the loss of 2 nodes to read at a consistency level of one.</p>

<p><a href=""http://www.ecyrd.com/cassandracalculator/"" rel=""noreferrer"">This cassandra parameters calculator</a> is a good reference for understanding the considerations of node count, replication factor, and consistency levels.</p>
",['table']
27990460,27992113,2015-01-16 18:29:50,Cassandra 2.1 CQL error creating table with tuple: no viable alternative at input '>',"<p>It's probably something really stupid... but I can't create a table with the new tuple type:</p>

<pre><code>cqlsh:ta&gt; CREATE TABLE tuple_test (k int PRIMARY KEY, v frozen &lt;tuple&lt;int, int&gt;&gt; );
Bad Request: line 1:68 no viable alternative at input '&gt;'
cqlsh:ta&gt; 
</code></pre>

<p>I've pretty much copied the table creation statement from the DataStax docs... What am I missing?</p>

<p>thanks!</p>

<p>Update - based on help from BryceAtNetwork23 and RossS:</p>

<p>Yes, you are right - I had DataStax Enterprise which had Cassandra 2.0.</p>

<p>I have installed DataStax Community with Cassandra 2.1 and all worked fine! </p>

<p>One note:  skipping the 'frozen' keyword does not work with the DSC's Cassandra distribution - but having the frozen in does work. Thanks for your help!</p>

<pre><code>[cqlsh 4.1.1 | Cassandra 2.1.2 | DSE  | CQL spec 3.1.1 | Thrift protocol 19.39.0]
cqlsh&gt; CREATE TABLE ta.tuple_test (k int, v tuple&lt;int, int&gt;,PRIMARY KEY(k) );
Bad Request: Non-frozen tuples are not supported, please use frozen&lt;&gt;
cqlsh&gt; CREATE TABLE ta.tuple_test (k int, v frozen &lt;tuple&lt;int, int&gt;&gt;,PRIMARY KEY(k) );
cqlsh&gt; 
</code></pre>
",<cassandra><cql><cassandra-2.0>,"<p>That <em>is</em> weird.  I get the same error.  I did manage to get it to work with a slight modification or two.  I then did a <code>desc</code> just to make sure that it created ok:</p>

<pre><code>aploetz@cqlsh&gt; CREATE TABLE stackoverflow.tuple_test (k int, v tuple&lt;int, int&gt;,PRIMARY KEY(k) );

aploetz@cqlsh&gt; use stackoverflow ;
aploetz@cqlsh:stackoverflow&gt; desc table tuple_test ;

CREATE TABLE stackoverflow.tuple_test (
    k int PRIMARY KEY,
    v frozen&lt;tuple&lt;int, int&gt;&gt;
) WITH bloom_filter_fp_chance = 0.01
    AND caching = '{""keys"":""ALL"", ""rows_per_partition"":""NONE""}'
    AND comment = ''
    AND compaction = {'min_threshold': '4', 'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32'}
    AND compression = {'sstable_compression': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99.0PERCENTILE'
</code></pre>

<p>The main thing, is that I didn't specify <code>frozen</code> in my <code>CREATE</code>, but when you <code>desc</code> the table, you can see that it knew to put it there.</p>

<p>Edit- Here is my <code>cqlsh</code> spec:</p>

<pre><code>[cqlsh 5.0.1 | Cassandra 2.1.0-rc5-SNAPSHOT | CQL spec 3.2.0 | Native protocol v3]
</code></pre>

<blockquote>
  <p>Cassandra 2.0.11.83</p>
</blockquote>

<p>Hmm...based on this, I don't know that you're actually on Cassandra 2.1.  And I'm pretty sure that the Tuple type is a 2.1 and higher feature.  Double check your Cassandra version once.  Also, if you're on DSE (which means you have support) I'd open up a ticket with them, describing the error that you're seeing.</p>

<p>Edit- FYI, I have upgraded my 2.1.0-rc5 version to 2.1.2, and run your original <code>CREATE</code>, and it works as-is:</p>

<pre><code>Connected to PermanentWaves at 127.0.0.1:9042.
[cqlsh 5.0.1 | Cassandra 2.1.2 | CQL spec 3.2.0 | Native protocol v3]
Use HELP for help.
aploetz@cqlsh&gt; use stackoverflow ;
aploetz@cqlsh:stackoverflow&gt; CREATE TABLE tuple_test (k int PRIMARY KEY, v frozen &lt;tuple&lt;int, int&gt;&gt; );
aploetz@cqlsh:stackoverflow&gt; desc table tuple_test ;

CREATE TABLE stackoverflow.tuple_test (
    k int PRIMARY KEY,
    v frozen&lt;tuple&lt;int, int&gt;&gt;
)...
</code></pre>
",['table']
27998820,28003215,2015-01-17 11:08:14,how to do a query with cassandradb counter table,"<p>i have a table in Cassandradb as mentioned below:</p>

<pre><code>CREATE TABLE remaining (owner varchar,buddy varchar,remain counter,primary key(owner,buddy));
</code></pre>

<p>generally i do some inc/dec operations on REMAIN field ,using cql like below: </p>

<pre><code>update remaining set remain=remain + 1 where owner='userA' and buddy='userB';
update remaining set remain=remain + 1 where owner='userA' and buddy='userC';
....
</code></pre>

<p>and now i need to find out all buddies for userA which it's REMAIN field greater then 1. when i using:</p>

<pre><code>select buddy,remain from remaining where owner='userA' and remain &gt; 0;
</code></pre>

<p>gives me an error:</p>

<pre><code>No indexed columns present in by-columns clause with Equal operator
</code></pre>

<p>how to do this in a cassandradb way?</p>
",<cassandra><cassandra-2.0><cql3>,"<p>The short answer to this is that you cannot do queries with conditionals on counter columns in Cassandra. </p>

<p>The reason behind this is that all Cassandra queries need to be modeled around the primary key of that particular table. Counter columns are not allowed as parts of the primary key of a table (their changing values would cause constant reorganization of the dat on disk). Counter columns are more used for tracking the state of a known piece of data, for example number of times a photo has been up-voted. This could be quickly recalled as long as we knew which photo we were interested in. To actually sort photos by numbers of votes you would need to perform an analytics style query using spark or Hadoop.</p>
",['table']
28039626,28041782,2015-01-20 07:14:51,Scala - Unable to write Scala object into Cassandra,"<p>I am trying to write Scala case class objects into Cassandra using Spark. But I am getting an exception while running the code. I guess I am unable to map my case class objects to my Cassandra rows. My Scala code looks like this</p>

<p><strong>CassandraPerformerClass.scala</strong></p>

<pre><code>object CassandraPerformerClass extends App
{
override def main(args: Array[String]) 
{
 val keyspace = ""scalakeys1""
 val tablename = ""demotable1""
 val conf = new SparkConf().setAppName(""CassandraDemo"") .setMaster(""spark://ct-0015:7077"") .setJars(SparkContext.jarOfClass(this.getClass).toSeq)
 conf.set(""spark.cassandra.connection.host"", ""192.168.50.103"")
 conf.set(""spark.cassandra.connection.native.port"", ""9041"")
 conf.set(""spark.cassandra.connection.rpc.port"", ""9160"")
 val sc = new SparkContext(conf);
 CassandraConnector(conf).withSessionDo 
 { session =&gt;
        session.execute(""DROP KEYSPACE IF EXISTS ""+keyspace+"" ;"");
        session.execute(""CREATE KEYSPACE ""+ keyspace +"" WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 3};"");
        session.execute(""CREATE TABLE ""+keyspace+"".""+tablename+"" (keyval bigint, rangef bigint, arrayval text, PRIMARY KEY (rangef, keyval));"");
        session.execute(""CREATE INDEX index_11 ON ""+keyspace+"".""+tablename+"" (keyval) ;"");
  }

 val data = Seq(new Data(1, 10, ""string1""), new Data(2, 20, ""string2""));
 val collection = sc.parallelize(data)    
 collection.saveToCassandra(keyspace, tablename)
}

case class Data(kv : Long, rf : Long, av : String) extends Serializable
 {
   private var keyval : Long = kv
   private var rangef : Long = rf
   private var arrayval : String = av

   def setKeyval (kv : Long)
   {
     keyval = kv
   }
   def setRangef (rf : Long)
   {
     rangef = rf
   }
   def setArrayval (av : String)
   {
     arrayval = av
   }
   def getKeyval = keyval
   def getRangef = rangef
   def getArrayval = arrayval
   override def toString = keyval + "","" + rangef + "","" + arrayval
 }
}
</code></pre>

<p><strong>Exception</strong></p>

<blockquote>
  <p>Exception in thread ""main"" java.lang.IllegalArgumentException: Some primary key columns are missing in RDD or have not been selected: rangef, keyval
      at com.datastax.spark.connector.writer.DefaultRowWriter.checkMissingPrimaryKeyColumns(DefaultRowWriter.scala:44)
      at com.datastax.spark.connector.writer.DefaultRowWriter.(DefaultRowWriter.scala:71)
      at com.datastax.spark.connector.writer.DefaultRowWriter$$anon$2.rowWriter(DefaultRowWriter.scala:109)
      at com.datastax.spark.connector.writer.DefaultRowWriter$$anon$2.rowWriter(DefaultRowWriter.scala:107)
      at com.datastax.spark.connector.writer.TableWriter$.apply(TableWriter.scala:170)
      at com.datastax.spark.connector.RDDFunctions.saveToCassandra(RDDFunctions.scala:23)
      at com.cleartrail.spark.scala.cassandra.poc.CassandraPerformerClass$.main(CassandraPerformerClass.scala:33)
      at com.cleartrail.spark.scala.cassandra.poc.CassandraPerformerClass.main(CassandraPerformerClass.scala)</p>
</blockquote>

<p>Please tell me how to map my case class object to Cassandra row. </p>
",<scala><cassandra><apache-spark>,"<p>The Scala-based connector for Spark does not expect a java-bean like case class with getters for the fields. (That's a bad practice anyway -  case classes are an immutable alternative to bean-like data containers and have default accessors for the fields and no mutators).</p>

<p>Creating a <code>case class</code> with the same names and types as the Cassandra table will just work:</p>

<pre><code>case class Data(keyval: Long, rangef:Long , arrayval: String) extends Serializable
</code></pre>
",['table']
28120834,28157910,2015-01-24 00:14:03,"Cassandra ""default_time_to_live"" property is not deleting data","<p>I've created a table like:</p>

<pre><code>CREATE TABLE IF NOT EXISTS metrics_second(
  timestamp timestamp,
  value counter,
  PRIMARY KEY ((timestamp))
) WITH default_time_to_live=1;
</code></pre>

<p>And inserted some data like:</p>

<pre><code>UPDATE metrics_second SET value = value + 1 WHERE timestamp = '2015-01-22 17:43:55-0800';
</code></pre>

<p>When executing <code>SELECT * FROM metrics_second</code> I always see the data, even after a minute or so, although the <code>default_time_to_live</code> property of the table is set to one second. Why is that?</p>
",<database><cassandra><cql><ttl>,"<p>As @RussS confirmed, unfortunately Cassandra doesn't support TTL on tables or rows when there are counters. </p>

<p>Even if <code>default_time_to_live</code> is being set when creating the table and no error is being returned, Cassandra won't enforce the TTL.</p>
",['table']
28147078,28175835,2015-01-26 08:58:48,Cassandra eats up all the disk space,"<p>I have a single node cassandra cluster, I use the current minute as partition key and insert rows with TTL of 12 hours.</p>

<p>I see a couple of issue I can't explain</p>

<ol>
<li>The <code>/var/lib/cassandra/data/&lt;key_space&gt;/&lt;table_name&gt;</code> contains multiple files, lots of them are really old (way older then 12 hours, something like 2 days)</li>
<li>When I try to perform a query in cqlsh I get a lot of logs that seem to indicate that my table contain lots of tombstones</li>
</ol>

<p>log:</p>

<pre><code>WARN  [SharedPool-Worker-2] 2015-01-26 10:51:39,376 SliceQueryFilter.java:236 - Read 0 live and 1571042 tombstoned cells in &lt;table_name&gt;_name (see tombstone_warn_threshold). 100 columns was requested, slices=[-], delInfo={deletedAt=-9223372036854775808, localDeletion=2147483647}
WARN  [SharedPool-Worker-2] 2015-01-26 10:51:40,472 SliceQueryFilter.java:236 - Read 0 live and 1557919 tombstoned cells in &lt;table_name&gt; (see tombstone_warn_threshold). 100 columns was requested, slices=[-], delInfo={deletedAt=-9223372036854775808, localDeletion=2147483647}
WARN  [SharedPool-Worker-2] 2015-01-26 10:51:41,630 SliceQueryFilter.java:236 - Read 0 live and 1589764 tombstoned cells in &lt;table_name&gt; (see tombstone_warn_threshold). 100 columns was requested, slices=[-], delInfo={deletedAt=-9223372036854775808, localDeletion=2147483647}
WARN  [SharedPool-Worker-2] 2015-01-26 10:51:42,877 SliceQueryFilter.java:236 - Read 0 live and 1582163 tombstoned cells in &lt;table_name&gt; (see tombstone_warn_threshold). 100 columns was requested, slices=[-], delInfo={deletedAt=-9223372036854775808, localDeletion=2147483647}
WARN  [SharedPool-Worker-2] 2015-01-26 10:51:44,081 SliceQueryFilter.java:236 - Read 0 live and 1550989 tombstoned cells in &lt;table_name&gt; (see tombstone_warn_threshold). 100 columns was requested, slices=[-], delInfo={deletedAt=-9223372036854775808, localDeletion=2147483647}
WARN  [SharedPool-Worker-2] 2015-01-26 10:51:44,869 SliceQueryFilter.java:236 - Read 0 live and 1566246 tombstoned cells in &lt;table_name&gt; (see tombstone_warn_threshold). 100 columns was requested, slices=[-], delInfo={deletedAt=-9223372036854775808, localDeletion=2147483647}
WARN  [SharedPool-Worker-2] 2015-01-26 10:51:45,582 SliceQueryFilter.java:236 - Read 0 live and 1577906 tombstoned cells in &lt;table_name&gt; (see tombstone_warn_threshold). 100 columns was requested, slices=[-], delInfo={deletedAt=-9223372036854775808, localDeletion=2147483647}
WARN  [SharedPool-Worker-2] 2015-01-26 10:51:46,443 SliceQueryFilter.java:236 - Read 0 live and 1571493 tombstoned cells in &lt;table_name&gt; (see tombstone_warn_threshold). 100 columns was requested, slices=[-], delInfo={deletedAt=-9223372036854775808, localDeletion=2147483647}
WARN  [SharedPool-Worker-2] 2015-01-26 10:51:47,701 SliceQueryFilter.java:236 - Read 0 live and 1559448 tombstoned cells in &lt;table_name&gt; (see tombstone_warn_threshold). 100 columns was requested, slices=[-], delInfo={deletedAt=-9223372036854775808, localDeletion=2147483647}
WARN  [SharedPool-Worker-2] 2015-01-26 10:51:49,255 SliceQueryFilter.java:236 - Read 0 live and 1574936 tombstoned cells in &lt;table_name&gt; (see tombstone_warn_threshold). 100 columns was requested, slices=[-], delInfo={deletedAt=-9223372036854775808, localDeletion=2147483647}
</code></pre>

<p>I've tried multiple compaction strategies, multithreaded compaction, I've tried running compaction manually with nodetool, also, I've tried forcing garbage collection with jmx.</p>

<p>One of my guesses is that the compaction doesn't delete tombstones files</p>

<p>Any ideas how to avoid disk space from getting too big, my biggest concern is running out of space, I'd rather store less (by making the ttl smaller but currently that doesn't help)</p>
",<cassandra><cassandra-2.0>,"<p>I'm assuming you are using the timestamp as a clustering column within each partition when you say you are using the minute as the partition key, along with a TTL of 12 hours when you do the insert.  This will build up tombstones in each partition since you are never deleting the entire row (i.e. a whole minute partition).</p>

<p>Suppose your keyspace is called k1 and your table is called t2, then you can run:</p>

<pre><code>nodetool flush k1 t2
nodetool compact k1 t2
sstable2json /var/lib/cassandra/data/k1/t2/k1-t2-jb-&lt;last version&gt;-Data.db
</code></pre>

<p>then you'll see all the tombstones like this (marked with a ""d"" for deleted):</p>

<pre><code>{""key"": ""00000003"",""columns"": [[""4:"",""54c7b514"",1422374164512000,""d""], [""5:"",""54c7b518"",1422374168501000,""d""], [""6:"",""54c7b51b"",1422374171987000,""d""]]}
</code></pre>

<p>Now if you go and delete that row (i.e. delete from k1.t2 where key=3;), then do the flush, compact, and sstable2json again, you'll see it change to:</p>

<pre><code>{""key"": ""00000003"",""metadata"": {""deletionInfo"": {""markedForDeleteAt"":1422374340312000,""localDeletionTime"":1422374340}},""columns"": []}
</code></pre>

<p>So you see all the tombstones are gone and Cassandra only has to remember that the whole row was deleted at a certain time instead of little bits and pieces of the row being deleted at certain times.</p>

<p>Another way to get rid of the tombstones is to truncate the whole table.  When you do that, Cassandra only needs to remember that the whole table was truncated at a certain time, and so no longer needs to keep tombstones prior to that time (since tombstones are used to tell other nodes that certain data was deleted, and if you can say the whole table was emptied at time x, then the details prior to that no longer matter).</p>

<p>So how could you apply this in your situation you ask.  Well, you could use the hour and minute as your partition key, and then once an hour run a cron job to delete all the rows from 13 hours ago.  Then on the next compaction, all the tombstones for that partition would be removed.</p>

<p>Or keep a separate table for each hour, and then truncate the table from 13 hours ago each hour using a cron job.</p>

<p>Another strategy that is sometimes useful is to ""re-use"" clustering keys.  For example, if you were inserting data once per second, instead of having a high resolution timestamp as a clustering key, you could use the time modulo 60 seconds as the clustering key and keep the more unique timestamp as just a data field.  So within each minute partition you would be changing tombstones (or outdated information) from yesterday back into live rows today, and then you wouldn't accumulate tombstones over many days.</p>

<p>So hopefully that gives you some ideas for things to try.  Usually when you run into a tombstone problem, it's a sign that you need to re-think your schema a little bit.</p>
",['table']
28149480,28212547,2015-01-26 11:45:27,Tombstone warning with SELECT LIMIT 1,"<pre><code>CREATE TABLE test (
    ck INT, 
    pk INT, 
    PRIMARY KEY (ck, pk)
);

for (int i = 1; i &lt; 10000; i++) {
    sessionRW.execute(QueryBuilder.insertInto(""test"").value(""ck"", 1).value(""pk"", i));
}

root@cqlsh:ks&gt; select * from test limit 5;

 ck | pk
----+----
  1 |  1
  1 |  2
  1 |  3
  1 |  4
  1 |  5

(5 rows)


root@cqlsh:ks&gt; delete from test where ck = 1;

root@cqlsh:ks&gt; insert into test(ck,pk) values (1, 0); -- new minimal value
root@cqlsh:ks&gt; select * from test limit 1;

 ck | pk
----+-------
  1 | 0

(1 rows)

WARN  11:37:39 Read 1 live and 9999 tombstoned cells in ks.test (see tombstone_warn_threshold). 1 columns was reque
</code></pre>

<p>Why when I do a SELECT with ""LIMIT 1"" I get the tombstone warning ?</p>

<p>The rows are order by pk ASC and the lower pk value of this table (0) is the first row and is not deleted.</p>

<p>I don't understand why cassandra keep scanning my table for other results (hence fetching a lot of tombstone) because the first row match and I specified I just want one row.</p>

<p>I could have understood the warning If I didn't specified LIMIT. But what's the point of scanning the whole table when first row match with LIMIT 1?</p>
",<cassandra>,"<p>OK so I think I found the answer, the answer is cassandra is doing one more lookup after limit 1 (like if you did limit 2).</p>

<p>Just insert one more row: </p>

<pre><code>insert into test(ck,pk) values (1, 1);
</code></pre>

<p>and now <code>select * from test limit 1;</code> won't trigger a tombstone error.</p>

<p>However, if you do LIMIT 2, it will trigger a tombstone error even if we have 2 valid rows, first in the table order.</p>

<p>Why cassandra is doing (limit+1) lookup is the question.</p>
",['table']
28249131,28252288,2015-01-31 07:00:02,How to delete Counter columns in Cassandra?,"<p>I know Cassandra rejects TTL for counter type. So, what's the best practice to delete old counters? e.g. old view counters.</p>

<p>Should I create cron jobs for deleting old counters?</p>
",<cassandra><ttl>,"<p>It's probably not a good practice to delete individual clustered rows or partitions from a counter table, since the key you delete cannot be used again.  That could give rise to bugs if the application tries to increment a counter in a deleted row, since the increment won't happen.  If you use a unique key whenever you create a new counter, then maybe you could get away with it.</p>

<p>So a better approach may be to truncate or drop the entire table, so that afterwards you can re-use keys.  To do this you'd need to separate your counters into multiple tables, such as one per month for example, so that you could truncate or drop an entire table when it was no longer relevant.  You could have a cron job that runs periodically and drops the counter table from x months ago.</p>
",['table']
28291136,28291329,2015-02-03 03:59:06,Cassandra internal working without joins,"<p>How Cassandra works without joins and how it solve joins problem and sub queries.
I mean SQL uses joins so we can get result from different tables easily but how Cassandra do this job of getting rows from different tables because they say that Cassandra does not have join and sub queries.</p>
",<cassandra>,"<p>The short answer is that you do not get data from multiple tables.</p>

<p>The long answer is that you plan your data model such that your tables all answer specific questions of your application. This makes it so that no query ever requires data from more than one table. So if for example your application requires data about particular users you would build a table that is structured around looking up information given a particular user. The analogy in the RDBMS world is materialized views. </p>

<p>Look into these videos for more information on denormalization and data modeling the Cassandra way.</p>

<p><a href=""http://www.youtube.com/watch?v=HdJlsOZVGwM"" rel=""nofollow"">C* Summit 2013: The World's Next Top Data Model - YouTube</a>
<a href=""https://www.youtube.com/watch?v=CY5-bWpqAVA"" rel=""nofollow"">The Weather Channel: CQL (Cassandra Query Language) Under the Hood</a></p>
",['table']
28303928,28308552,2015-02-03 16:30:07,range query in Cassandra,"<p>I'm using Cassandra 2.1.2 with the corresponding DataStax Java driver and the Object mapping provided by DataStax.</p>

<p>following table definition:</p>

<pre><code>CREATE TABLE IF NOT EXISTS ses.tim (id text PRIMARY KEY, start bigint, cid int);
</code></pre>

<p>the mapping: </p>

<pre><code>@Table(keyspace = ""ses"", name = ""tim"")
class MyObj {
    @PartitionKey
    private String id;
    private Long start;
    private int cid;
}
</code></pre>

<p>the accessor</p>

<pre><code>@Accessor
interface MyAccessor {
    @Query(""SELECT * FROM ses.tim WHERE id = :iid"")
    MyObj get(@Param(""iid"") String id);

    @Query(SELECT * FROM ses.tim WHERE start &lt;= :sstart"")
    Result&lt;MyObj&gt; get(@Param(""sstart"") long start);
}
</code></pre>

<p>as indicated within the accessor I want to do a query that returns everything where 'start' is smaller or equal than a specific value. </p>

<p>With this definition of the table it's simply not possible. Therefore I tried creating a secondary index:</p>

<pre><code>CREATE INDEX IF NOT EXISTS myindex ON ses.tim (start);
</code></pre>

<p>this seems to be not working as well (I read a lot of explanations why its decided to not support this, but I still don't understand why somebody would give such restrictions, anyhow..)</p>

<p>so, as far as I understandd, we have to have at least one equals in the WHERE clause</p>

<pre><code>@Query(SELECT * FROM ses.tim WHERE cid = :ccid AND start &lt;= :sstart"")

CREATE INDEX IF NOT EXISTS myindex2 ON ses.tim (cid);
</code></pre>

<p>if this would work I would have to know ALL possible values for cid, and query them separately and do the rest on the client... but the error I get is</p>

<pre><code>Cannot execute this query as it might involve data filtering and thus may have unpredictable performance
</code></pre>

<p>then I tried </p>

<pre><code>id text, start bigint, cid int, PRIMARY KEY (id, start, cid)
</code></pre>

<p>with </p>

<pre><code>@Table(keyspace = ""ses"", name = ""tim"")
class MyObj {
    @PartitionKey
    private String id;
    @ClusteringColumn(0)
    private Long start;
    @ClusteringColumn(1)
    private int cid;
}
</code></pre>

<p>but still without luck.</p>

<p>furthermore, I tried to set 'start' as PartitionKey, but it's only possible to query with Equals again...</p>

<p>what am I missing? how can I achieve getting results for this type of query?</p>

<p>EDIT: version updated to correct one</p>
",<java><cassandra><datastax-java-driver>,"<p>You could consider denormalizing your data if you have different query-ability needs for the same set of data.   Based on your question, it sounds like you want the following:</p>

<ul>
<li>Query by <code>id</code></li>
<li>Query by <code>start</code> &lt; X</li>
</ul>

<p>The first query works fine as you indicated with your current schema.  The second query however cannot work as is without a secondary index which will be slow for reasons you have already investigated (I always point to <a href=""http://www.wentnet.com/blog/?p=77"">this blog post</a> with respect to secondary indexes.</p>

<p>You indicated that you did not want to partition on <code>cid</code> since you would need to know all possible values for <code>cid</code>.</p>

<p>Three ideas I can think of:</p>

<ul>
<li><p>Create a separate table with a dummy primary key so all of your data is stored in the same partition.  This can be problematic though if you have many entries creating a super-wide partition and hotspots on whatever nodes hold that data.  How many do you plan on having?</p>

<pre><code>create table if not exists tim (
    dummy int, 
    start bigint, 
    id text, 
    cid int, 
    primary key (dummy, start)
);
</code></pre>

<p>You could then make queries like:</p>

<pre><code>select * from tim where dummy=0 and start &lt;= 10;
</code></pre></li>
<li><p>The other option is to use ALLOW FILTERING on your original table which will still do an expensive range query and filter through the data.</p>

<pre><code>select * from tim where start &lt;= 10 ALLOW FILTERING;
</code></pre></li>
<li><p>Another option is to use something like the <a href=""https://github.com/datastax/spark-cassandra-connector"">spark-connector</a> to create a spark job that makes the query.  The connector will break up an expensive range query into smaller tasks and map the data to RDDs, allowing you flexibility to make more complex queries with good performance.</p></li>
</ul>
",['table']
28327945,28328300,2015-02-04 17:45:37,Can an index be created on a UUID Column?,"<p>Is it possible to create an index on a UUID/TIMEUUID column in Cassandra?  I'm testing out a model design which would have an index on a UUID column, but queries on that column always return 0 rows found.</p>

<p>I have a table like this:</p>

<pre><code>create table some_data (site_id int, user_id int, run_id uuid, value int, primary key((site_id, user_id), run_id));
</code></pre>

<p>I create an index with this command:</p>

<pre><code>create index idx on some_data (run_id) ;
</code></pre>

<p>No errors are thrown by CQL when I create this index.</p>

<p>I have a small bit of test data in the table:</p>

<pre><code> site_id | user_id | run_id                               | value
---------+---------+--------------------------------------+-----------------
       1 |       1 | 9e118af0-ac92-11e4-81ae-8d1bc921f26d |               3
</code></pre>

<p>However, when I run the query:</p>

<pre><code>select * from some_data where run_id = 9e118af0-ac92-11e4-81ae-8d1bc921f26d
</code></pre>

<p>CQLSH just returns:  <code>(0 rows)</code></p>

<p>If I use an <code>int</code> for the <code>run_id</code> then the index behaves as expected.</p>
",<cassandra><cql><cassandra-2.0>,"<p>Yes, you can create a secondary index on a UUID.  The real question is ""should you?""</p>

<p>In any case, I followed your steps, and got it to work.</p>

<pre><code>Connected to Test Cluster at 192.168.23.129:9042.
[cqlsh 5.0.1 | Cassandra 2.1.2 | CQL spec 3.2.0 | Native protocol v3]
Use HELP for help.
aploetz@cqlsh&gt; use stackoverflow ;
aploetz@cqlsh:stackoverflow&gt; create table some_data (site_id int, user_id int, run_id uuid, value int, primary key((site_id, user_id), run_id));
aploetz@cqlsh:stackoverflow&gt; create index idx on some_data (run_id) ;
aploetz@cqlsh:stackoverflow&gt; INSERT INTO some_data (site_id, user_id, run_id, value) VALUES (1,1,9e118af0-ac92-11e4-81ae-8d1bc921f26d,3);
aploetz@cqlsh:stackoverflow&gt; select * from usr_rec3 where run_id = 9e118af0-ac92-11e4-81ae-8d1bc921f26d;
code=2200 [Invalid query] message=""unconfigured columnfamily usr_rec3""
aploetz@cqlsh:stackoverflow&gt; select * from some_data where run_id = 9e118af0-ac92-11e4-81ae-8d1bc921f26d;

 site_id | user_id | run_id                               | value
---------+---------+--------------------------------------+-------
       1 |       1 | 9e118af0-ac92-11e4-81ae-8d1bc921f26d |     3

(1 rows)
</code></pre>

<p>Notice though, that when I ran this command, it failed:</p>

<pre><code>select * from usr_rec3 where run_id = 9e118af0-ac92-11e4-81ae-8d1bc921f26d
</code></pre>

<p>Are you sure that you didn't mean to select from <code>some_data</code> instead?</p>

<p>Also, creating secondary indexes on high-cardinality columns (like a UUID) is generally not a good idea.  If you need to query by <code>run_id</code>, then you should revisit your data model and come up with an appropriate query table to serve that.</p>

<p><strong>Clarification:</strong></p>

<ul>
<li>Using secondary indexes in general is not considered good practice.  In the new book <a href=""https://rads.stackoverflow.com/amzn/click/com/1783989122"" rel=""nofollow noreferrer"" rel=""nofollow noreferrer"">Cassandra High Availability</a>, Robbie Strickland identifies their use as an anti-pattern, due to poor performance.</li>
<li>Just because a column is of the UUID data type doesn't necessarily make it high-cardinality.  That's more of a data model question for you.  But knowing the nature of UUIDs and their underlying purpose toward being unique, is setting off red flags.</li>
<li>Put these two points together, and there isn't anything about creating an index on a UUID that sounds appealing to me.  If it were my cluster, and (more importantly) I had to support it later, I wouldn't do it.</li>
</ul>
",['table']
28420129,28420370,2015-02-09 21:53:41,Order by with Cassandra No Sql Db,"<p>I'm starting to using Cassandra but I'm getting some problems on ""ordering"" or ""selecting"".</p>

<pre><code>CREATE TABLE functions (
id_function int,
sort int,
id_subfunction int,
php_class varchar,
php_function varchar,
PRIMARY KEY (id_function, sort, id_subfunction)
);
</code></pre>

<p>This is my table.</p>

<p>If I execute this query</p>

<pre><code>SELECT * FROM functions WHERE id_subfunction = 0 ORDER BY sort;
</code></pre>

<p>this is what I get.</p>

<pre><code>Bad Request: ORDER BY is only supported when the partition key is restricted by an EQ or an IN.
</code></pre>

<p>Where I'm doing wrong?</p>

<p>Thanks</p>
",<cassandra><cql><nosql>,"<pre><code>PRIMARY KEY (id_function, sort, id_subfunction)
</code></pre>

<p>In Cassandra CQL the columns in a compound PRIMARY KEY are either <em>partitioning</em> keys or <em>clustering</em> keys.  In your case, <code>id_function</code> (the first key listed) is the partitioning key.  This is the key value that is hashed so that your data for that key can be evenly distributed on your cluster.</p>

<p>The remaining columns (<code>sort</code> and <code>id_subfunction</code>) are known as clustering columns, which determine the sort order of your data <em>within a partition</em>.  This essentially means that your data will only be sorted by your clustering key(s) when a partitioning key is first designated in your <code>WHERE</code> clause.</p>

<p>You have two options:</p>

<p>1) Query this table by <code>id_function</code> instead:</p>

<pre><code>SELECT * FROM functions WHERE id_function= 0 ORDER BY sort;
</code></pre>

<p>This will technically work, although I'm guessing that it won't give you the results that you are looking for.</p>

<p>2) The better option, is to create a ""query table.""  This is a table designed to specifically handle your query by <code>id_subfunction</code>.  It only differs from the original <code>functions</code> table in that the PRIMARY KEY is defined with <code>id_subfunction</code> as the partitioning key:</p>

<pre><code>CREATE TABLE functionsbysubfunction (
id_function int,
sort int,
id_subfunction int,
php_class varchar,
php_function varchar,
PRIMARY KEY (id_subfunction, sort, id_function)
);
</code></pre>

<p>This query table will allow this query to function as expected:</p>

<pre><code>SELECT * FROM functionsbysubfunction WHERE id_subfunction = 0;
</code></pre>

<p>And you shouldn't need to indicate<code>ORDER BY</code>, unless you want to specify either ASCending or DESCending order.</p>

<p>Remember with Cassandra, it is important to design your data model according to how you want to query your data.  And that may not necessarily be the way that it originally makes sense to store it.</p>
",['table']
28498210,28498822,2015-02-13 11:06:23,"What data structure should I use to mimic ""order by counter"" in Cassandra?","<p>Let's say I currently have a table like this</p>

<pre><code>create table comment_counters
{ 
    contentid uuid,
    commentid uuid,
    ...
    liked counter,
    PRIMARY_KEY(contentid, commentid)
 };
</code></pre>

<p>This purpose of this table is to track the comments and the number of times individual comments have been ""liked"". </p>

<p>What I would like to do is to  get the top comments (let's say 20 top comments) determined by their number of likes from this table for each content.</p>

<p>I know there's no way to order by counters so what I would like to know is, are there any other ways to do this in Cassandra, by restructuring my tables or tracking more/different information for instance, or am I left with no choice but to do this in an RDBMS? </p>

<p>Sorting in client is not really an option I would like to consider at this stage.</p>
",<cassandra>,"<p>Unfortunately there's now way to do this type of aggregations using plain Cassandra queries. The best option for doing this kind of data analysis would be to use an external tool such as <a href=""http://www.spark-project.org/"" rel=""nofollow"">Spark</a>. 
Using Spark you can start periodical jobs that would read and aggregate all counters from the comment_counters table and afterwards write the results (such as top 20 comments) to a different table that you can use to query directly afterwards. 
See <a href=""http://planetcassandra.org/getting-started-with-apache-spark-and-cassandra/"" rel=""nofollow"">here</a> to get started with Cassandra and Spark.</p>
",['table']
28565470,28568169,2015-02-17 15:52:22,cassandra primary key column cannot be restricted,"<p>I am using Cassandra for the first time in a web app and I got a query problem.
Here is my tab : </p>

<pre><code>CREATE TABLE vote (
    doodle_id uuid,
    user_id uuid,
    schedule_id uuid,
    vote int,
    PRIMARY KEY ((doodle_id), user_id, schedule_id)
);
</code></pre>

<p>On every request, I indicate my partition key, doodle_id.
For example I can make without any problems : </p>

<pre><code>select * from vote where doodle_id = c4778a27-f2ca-4c96-8669-15dcbd5d34a7 and user_id = 97a7378a-e1bb-4586-ada1-177016405142;
</code></pre>

<p>But on the last request I made :</p>

<pre><code>select * from vote where doodle_id = c4778a27-f2ca-4c96-8669-15dcbd5d34a7 and schedule_id = c37df0ad-f61d-463e-bdcc-a97586bea633;
</code></pre>

<p>I got the following error :</p>

<pre><code>Bad Request: PRIMARY KEY column ""schedule_id"" cannot be restricted (preceding column ""user_id"" is either not restricted or by a non-EQ relation)
</code></pre>

<p>I'm new with Cassandra, but correct me if I'm wrong, in a composite primary key, the first part is the PARTITION KEY which is mandatory to allow Cassandra to know where to look for data.
Then the others parts are CLUSTERING KEY to sort data.</p>

<p>But I still don't get why my first request is working and not the second one ?</p>

<p>If anyone could help it will be a great pleasure.</p>
",<cassandra><cql><cqlsh>,"<p>In Cassandra, you should design your data model to suit your queries.  Therefore the proper way to support your second query (queries by <code>doodle_id</code> and <code>schedule_id</code>, but not necessarilly with <code>user_id</code>), is to create a new table to handle that specific query.  This table will be pretty much the same, except the PRIMARY KEY will be slightly different:</p>

<pre><code>CREATE TABLE votebydoodleandschedule (
    doodle_id uuid,
    user_id uuid,
    schedule_id uuid,
    vote int,
    PRIMARY KEY ((doodle_id), schedule_id, user_id)
);
</code></pre>

<p>Now this query will work:</p>

<pre><code>SELECT * FROM votebydoodleandschedule 
WHERE doodle_id = c4778a27-f2ca-4c96-8669-15dcbd5d34a7 
AND schedule_id = c37df0ad-f61d-463e-bdcc-a97586bea633;
</code></pre>

<p>This gets you around having to specify <code>ALLOW FILTERING</code>.  Relying on <code>ALLOW FILTERING</code> is never a good idea, and is certainly not something that you should do in a production cluster.</p>
",['table']
28585519,28603428,2015-02-18 14:02:03,Using Insert with timestamp in Cassandra,"<p>I am trying to INSERT (also UPDATE and DELETE) data in Cassandra using timestamp, but no change occur to the table. Any help please?</p>

<pre><code>BEGIN BATCH
  INSERT INTO transaction_test.users(email,age,firstname,lastname) VALUES ('1',null,null,null) USING TIMESTAMP 0;
  INSERT INTO transaction_test.users(email,age,firstname,lastname) VALUES ('2',null,null,null) USING TIMESTAMP 1;
  INSERT INTO transaction_test.users(email,age,firstname,lastname) VALUES ('3',null,null,null) USING TIMESTAMP 2;         
APPLY BATCH;
</code></pre>
",<cassandra>,"<p>I think you're falling into Cassandra's &quot;<em>control of timestamps</em>&quot;. Operations in C* are (in effect<sup>1</sup>) executed only if the timestamp of the new operation is &quot;<em>higher</em>&quot; than previous one.</p>
<p>Let's see an example. Given the following insert</p>
<pre><code>INSERT INTO test (key, value ) VALUES ( 'mykey', 'somevalue') USING TIMESTAMP 1000;
</code></pre>
<p>You expect this as output:</p>
<pre><code>select key,value,writetime(value) from test where key='mykey';

 key   | value     | writetime(value)
-------+-----------+------------------
 mykey | somevalue |             1000
</code></pre>
<p>And it should be like this unless someone before you didn't perform an operation on this information with a higher timestamp. For instance, if you now write</p>
<pre><code>INSERT INTO test (key, value ) VALUES ( 'mykey', '999value') USING TIMESTAMP 999;
</code></pre>
<p>Here's the output</p>
<pre><code>select key,value,writetime(value) from test where key='mykey';

 key   | value     | writetime(value)
-------+-----------+------------------
 mykey | somevalue |             1000
</code></pre>
<p>As you can see neither the value nor the timestamp have been updated.</p>
<hr />
<p><sup>1</sup> That's a slight simplification. Unless you are doing a specialised 'compare-and-set' write, Cassandra doesn't read anything from the table before it writes and it doesn't know if there is existing data or what its timestamp is. So you end up with two versions of the row, with different timestamps. But when you read the row back you always get the one with the latest timestamp. Normally Cassandra will compact such duplicate rows after a while, which is when the older timestamp row gets discarded.</p>
",['table']
28616595,28616923,2015-02-19 20:48:42,Get Date Range for Cassandra - Select timeuuid with IN returning 0 rows,"<p>I'm trying to get data from a date range on Cassandra, the table is like this:</p>
<pre><code>CREATE TABLE test6 (
  time timeuuid,
  id text,
  checked boolean,
  email text,
  name text,
  PRIMARY KEY ((time), id)
)
</code></pre>
<p>But when I select a data range I get nothing:</p>
<pre><code>SELECT * FROM teste WHERE time IN ( minTimeuuid('2013-01-01 00:05+0000'), now() );

(0 rows)
</code></pre>
<p>How can I get a date range from a Cassandra Query?</p>
",<cassandra><cql>,"<p>The <code>IN</code> condition is used to specify multiple keys for a <a href=""http://www.datastax.com/documentation/cql/3.1/cql/cql_reference/select_r.html"" rel=""noreferrer"">SELECT</a> query.  To run a date range query for your table, (you're close) but you'll want to use greater-than and less-than.</p>

<p>Of course, you can't run a greater-than/less-than query on a partition key, so you'll need to flip your keys for this to work.  This also means that you'll need to specify your <code>id</code> in the WHERE clause, as well:</p>

<pre><code>CREATE TABLE teste6 (
  time timeuuid,
  id text,
  checked boolean,
  email text,
  name text,
  PRIMARY KEY ((id), time)
)

INSERT INTO teste6 (time,id,checked,email,name)
VALUES (now(),'B26354',true,'rdeckard@lapd.gov','Rick Deckard');

SELECT * FROM teste6 
WHERE id='B26354'
AND time &gt;= minTimeuuid('2013-01-01 00:05+0000')
AND time &lt;= now();

 id     | time                                 | checked | email             | name
--------+--------------------------------------+---------+-------------------+--------------
 B26354 | bf0711f0-b87a-11e4-9dbe-21b264d4c94d |    True | rdeckard@lapd.gov | Rick Deckard

(1 rows)
</code></pre>

<p>Now while this will technically work, partitioning your data by <code>id</code> might not work for your application.  So you may need to put some more thought behind your data model and come up with a better partition key.</p>

<p><strong>Edit:</strong></p>

<p>Remember with Cassandra, the idea is to get a handle on what kind of queries you need to be able to fulfill.  Then build your data model around that.  Your original table structure might work well for a relational database, but in Cassandra that type of model actually makes it difficult to query your data in the way that you're asking.</p>

<p>Take a look at the modifications that I have made to your table (basically, I just reversed your partition and clustering keys).  If you still need help, Patrick McFadin (DataStax's Chief Evangelist) wrote a really good article called <a href=""http://planetcassandra.org/blog/getting-started-with-time-series-data-modeling/"" rel=""noreferrer"">Getting Started with Time Series Data Modeling</a>.  He has three examples that are similar to yours.  In fact his first one is very similar to what I have suggested for you here.</p>
",['table']
28621721,28627598,2015-02-20 04:38:39,Cassandra best practice for querying data only by date range?,"<p>I'm planning storing log records in Cassandra, and primarily need to be able query them by date range.  My primary key is a time based UUID.  I've seen lots of examples that allow filtering by date range in addition to some key, but is there any way to efficiently query just by a date range, without such a key, and without using an Ordered Partitioner?</p>
",<cassandra>,"<p>No, the partition key (first element of the primary key) allows queries to be routed to the appropriate node and not scan the whole cluster. Yet if the partition is still the same then data won't be distributed over the cluster and a few nodes will get the workload. You could create a table like:</p>

<pre><code>create table log (
   log_type text,
   day text, -- In format YYYY-MM-DD for instance
   id timeuuid,
   message text,
   primary key ((log_type, day), id)
)
</code></pre>

<p>Then from your date range, you can determine the day values and the possible partition keys. Add a condition on timeuiid to finish:</p>

<pre><code>select * from log where log_type='xxx' and day='2014-02-19' and dateOf(id)&gt;? and dateOf(id)&lt;?
select * from log where log_type='xxx' and day='2014-02-20' and dateOf(id)&gt;? and dateOf(id)&lt;?
select * from log where log_type='xxx' and day='2014-02-21' and dateOf(id)&gt;? and dateOf(id)&lt;?
</code></pre>

<p>Another option could be to use the <code>ALLOW FILTERING</code> clause, but this will do a full cluster scan. So it's a good idea only if you know that at least 90% of partition keys will contain interesting data.</p>

<pre><code>select * from log where dateOf(id)&gt;? and dateOf(id)&lt;? allow filtering
</code></pre>
",['table']
28645815,28649698,2015-02-21 12:30:34,COPY cassandra table from csv file,"<p>I'm setting up a demo landscape for Cassandra, Apache Spark and Flume on my Mac (Mac OS X Yosemite with Oracle jdk1.7.0_55). The landscape shall work as a proof of concept for a new analytics platform and therefore I need some test data in my cassandra db. I am using cassandra 2.0.8. </p>

<p>I created some demo data in excel and exported that as a CSV file. The structure is like this:</p>

<pre><code>ProcessUUID;ProcessID;ProcessNumber;ProcessName;ProcessStartTime;ProcessStartTimeUUID;ProcessEndTime;ProcessEndTimeUUID;ProcessStatus;Orderer;VorgangsNummer;VehicleID;FIN;Reference;ReferenceType
0F0D1498-D149-4FCC-87C9-F12783FDF769;AbmeldungKl‰rfall;1;Abmeldung Kl‰rfall;2011-02-03 04:05+0000;;2011-02-17 04:05+0000;;Finished;SIXT;4278;A-XA 1;WAU2345CX67890876;KLA-BR4278;internal
</code></pre>

<p>I then created a keyspace and a column family in cqlsh using:</p>

<pre><code>CREATE KEYSPACE dadcargate 
WITH REPLICATAION  = { 'class' : 'SimpleStrategy', 'replication_factor' : '1' };

use dadcargate;

CREATE COLUMNFAMILY Process (
  ProcessUUID uuid, ProcessID varchar, ProcessNumber bigint, ProcessName varchar, 
  ProcessStartTime timestamp, ProcessStartTimeUUID timeuuid, ProcessEndTime timestamp, 
  ProcessEndTimeUUID timeuuid, ProcessStatus varchar, Orderer varchar,
  VorgangsNummer varchar, VehicleID varchar, FIN varchar, Reference varchar,
  ReferenceType varchar, 
PRIMARY KEY (ProcessUUID))
WITH COMMENT='A process is like a bracket around multiple process steps';
</code></pre>

<p>The column family name and all columns in it are created with all lower case - will have to investigate into this as well some day, but that is not so relevant at the moment.</p>

<p>Now I take my CSV file, which has around 1600 entries and want to import that in my table named <code>process</code> like this:</p>

<pre><code>cqlsh:dadcargate&gt; COPY process (processuuid, processid, processnumber, processname, 
processstarttime, processendtime, processstatus, orderer, vorgangsnummer, vehicleid,
fin, reference, referencetype) 
FROM 'Process_BulkData.csv' WITH DELIMITER = ';' AND HEADER = TRUE;
</code></pre>

<p>It gives the following error:</p>

<pre><code>Record #0 (line 1) has the wrong number of fields (15 instead of 13).
0 rows imported in 0.050 seconds.
</code></pre>

<p>Which is essentially true, As I do NOT have the timeUUID Fields in my cvs-export.</p>

<p>If I try the COPY command without explicit column-names like this (given the fact, that I actually do miss two fields):</p>

<pre><code>cqlsh:dadcargate&gt; COPY process from 'Process_BulkData.csv' 
WITH DELIMITER = ';' AND HEADER = TRUE;
</code></pre>

<p>I end up with another error:</p>

<pre><code>Bad Request: Input length = 1
Aborting import at record #0 (line 1). Previously-inserted values still present.
0 rows imported in 0.009 seconds.
</code></pre>

<p>Hm. Kinda strange, but okay. Maybe the COPY command does not like the fact that there are two fields missing. I still think this to be strange, as the missing fields are of course there (from a structural point of view) but only empty. </p>

<p>I still have another shot: I deleted the missing columns in excel, exported the file again as cvs and try to import WITHOUT header line in my csv BUT explicit column names, like this:</p>

<pre><code>cqlsh:dadcargate&gt; COPY process (processuuid, processid, processnumber, processname, 
processstarttime, processendtime, processstatus, orderer, vorgangsnummer, vehicleid, 
fin, reference, referencetype) 
FROM 'Process_BulkData-2.csv' WITH DELIMITER = ';' AND HEADER = TRUE;
</code></pre>

<p>I get this error:</p>

<pre><code>Bad Request: Input length = 1
Aborting import at record #0 (line 1). Previously-inserted values still present.
0 rows imported in 0.034 seconds.
</code></pre>

<p>Can ANYONE tell me what I'm doing wrong here? According to the <a href=""http://www.datastax.com/documentation/cql/3.0/cql/cql_reference/copy_r.html"" rel=""nofollow noreferrer"">documentation of copy-command</a>, the way I setup my commands, should work for at least two of them. Or so I would think. </p>

<p>But nah, I'm obviously missing something important here.</p>
",<csv><cassandra><copy><cqlsh>,"<p>cqlsh's <code>COPY</code> command can be touchy.  However, in the <a href=""http://www.datastax.com/documentation/cql/3.1/cql/cql_reference/copy_r.html""><code>COPY</code> documentation</a> is this line:</p>

<blockquote>
  <p>The number of columns in the CSV input is the same as the number of columns in the Cassandra table metadata.</p>
</blockquote>

<p>Keeping that in-mind, I did manage to get your data to import with a <code>COPY FROM</code>, by naming the empty fields (<code>processstarttimeuuid</code> and <code>processendtimeuuid</code>, respectively):</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; COPY process (processuuid, processid, processnumber, 
processname, processstarttime, processstarttimeuuid, processendtime, 
processendtimeuuid, processstatus, orderer, vorgangsnummer, vehicleid, fin, reference, 
referencetype) FROM 'Process_BulkData.csv' WITH DELIMITER = ';' AND HEADER = TRUE;

1 rows imported in 0.018 seconds.
aploetz@cqlsh:stackoverflow&gt; SELECT * FROM process ;

 processuuid                          | fin               | orderer | processendtime            | processendtimeuuid | processid         | processname        | processnumber | processstarttime          | processstarttimeuuid | processstatus | reference  | referencetype | vehicleid | vorgangsnummer
--------------------------------------+-------------------+---------+---------------------------+--------------------+-------------------+--------------------+---------------+---------------------------+----------------------+---------------+------------+---------------+-----------+----------------
 0f0d1498-d149-4fcc-87c9-f12783fdf769 | WAU2345CX67890876 |    SIXT | 2011-02-16 22:05:00+-0600 |               null | AbmeldungKl‰rfall | Abmeldung Kl‰rfall |             1 | 2011-02-02 22:05:00+-0600 |                 null |      Finished | KLA-BR4278 |      internal |    A-XA 1 |           4278

(1 rows)
</code></pre>
",['table']
28663644,28663904,2015-02-22 21:43:21,Cassandra read before write/update,"<p>What is the fastest way to read before write?
I've a Import script that's load about 10-15K emails and I need to check if they exists or not, Is there a way to write and see if it's overwrite other data?
Or I just need to do read before write?</p>

<p>Thanks.</p>
",<cassandra><database><nosql>,"<p>If you don't care to overwrite an email then you dont need any reads as insert and update are synonyms. 
If you do then you can use lightweight transactions (<code>INSERT ... IF NOT EXISTS</code>). If a record with a given key exists then it will not be overwritten. You can add a column to the table where an application will pass a unique value. Once you insert data you call <code>SELECT</code> and compare the value of that column with the value that was passed. If they match then this record was created by your call. If not then it was created by some other process</p>
",['table']
28666089,28677780,2015-02-23 02:26:14,How to populate related table in Cassandra using CQL?,"<p>I am trying to practice Cassandra using <a href=""http://www.datastax.com/docs/1.1/ddl/column_family"" rel=""nofollow noreferrer"">this example</a> (under <strong>Composite Columns</strong> paragraph):</p>

<p>So, I have created table <strong>tweets</strong> and it looks like following:</p>

<pre><code>cqlsh:twitter&gt; SELECT * from tweets;

 tweet_id                             | author      | body
--------------------------------------+-------------+--------------
 73954b90-baf7-11e4-a7d0-27983e9e7f51 | gwashington | I chopped...

(1 rows)
</code></pre>

<p>Now I am trying to populate timeline, which is a related table using CQL and I am not sure how to do it. I have tried SQL approach, but it did not work:</p>

<pre><code>cqlsh:twitter&gt; INSERT INTO timeline (user_id, tweet_id, author, body) SELECT 'gmason', 73954b90-baf7-11e4-a7d0-27983e9e7f51, author, body FROM tweets WHERE tweet_id = 73954b90-baf7-11e4-a7d0-27983e9e7f51;
Bad Request: line 1:55 mismatched input 'select' expecting K_VALUES
</code></pre>

<p>So I have two questions:</p>

<ol>
<li>How to populate <strong>timeline</strong> table with SQL, so that it would relate to <strong>tweets</strong>?</li>
<li>How do I make sure that <em>Timeline Physical Layout</em> will be created as shown in that example?</li>
</ol>

<p>Thanks.</p>

<p>EDIT:</p>

<p>This is explanation for my question #2 above (the picture is taken from <a href=""http://www.datastax.com/docs/1.1/ddl/column_family"" rel=""nofollow noreferrer"">here</a>):</p>

<p><img src=""https://i.stack.imgur.com/17fft.jpg"" alt=""This is explanation for my question #2 above:""></p>
",<cassandra><cql><cassandra-2.0><composite-primary-key><nosql>,"<p><strong>tldr;</strong></p>

<ol>
<li><p>Use cqlsh <code>COPY</code> to export <code>tweets</code>, modify the file, use <code>COPY</code> to import <code>timeline</code>.</p></li>
<li><p>Use cassandra-cli to verify the physical structure.</p></li>
</ol>

<p><em>Long version...</em></p>

<ol>
<li>I'll go a different way on this one, and suggest that it will probably be easier using the native <code>COPY</code> command in cqlsh.</li>
</ol>

<p>I followed the similar <a href=""http://www.datastax.com/dev/blog/schema-in-cassandra-1-1"" rel=""nofollow"">examples found here</a>.  After creating the <code>tweets</code> and <code>timeline</code> tables in cqlsh, I inserted rows into <code>tweets</code> as indicated.  My <code>tweets</code> table then looked like this:</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; SELECT * FROM tweets;

 tweet_id                             | author      | body
--------------------------------------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------
 05a5f177-f070-486d-b64d-4e2bb28eaecc |      gmason | Those gentlemen, who will be elected senators, will fix themselves in the federal town, and become citizens of that town more than of your state.
 b67fe644-4dbe-489b-bc71-90f809f88636 |    jmadison |                                                                                  All men having power ought to be distrusted to a certain degree.
 819d95e9-356c-4bd5-9ad0-8cd36a7aa5e1 | gwashington |                                                                    To be prepared for war is one of the most effectual means of preserving peace.
</code></pre>

<p>I then exported them like this:</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; COPY tweets TO '/home/aploetz/tweets_20150223.txt' 
WITH DELIMITER='|' AND HEADER=true;

3 rows exported in 0.052 seconds.
</code></pre>

<p>I then edited the <code>tweets_20150223.txt file</code>, adding a <code>user_id</code> column on the front and copying a couple of rows, like this:</p>

<pre><code>userid|tweet_id|author|body
gmason|05a5f177-f070-486d-b64d-4e2bb28eaecc|gmason|Those gentlemen, who will be elected senators, will fix themselves in the federal town, and become citizens of that town more than of your state.
jmadison|b67fe644-4dbe-489b-bc71-90f809f88636|jmadison|All men having power ought to be distrusted to a certain degree.
gwashington|819d95e9-356c-4bd5-9ad0-8cd36a7aa5e1|gwashington|To be prepared for war is one of the most effectual means of preserving peace.
jmadison|819d95e9-356c-4bd5-9ad0-8cd36a7aa5e1|gwashington|To be prepared for war is one of the most effectual means of preserving peace.
ahamilton|819d95e9-356c-4bd5-9ad0-8cd36a7aa5e1|gwashington|To be prepared for war is one of the most effectual means of preserving peace.
ahamilton|05a5f177-f070-486d-b64d-4e2bb28eaecc|gmason|Those gentlemen, who will be elected senators, will fix themselves in the federal town, and become citizens of that town more than of your state.
</code></pre>

<p>I saved that file as <code>timeline_20150223.txt</code>, and imported it into the <code>timeline</code> table, like this:</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; COPY timeline FROM '/home/aploetz/timeline_20150223.txt' 
WITH DELIMITER='|' AND HEADER=true;

6 rows imported in 0.016 seconds.
</code></pre>

<ol start=""2"">
<li>Yes, <code>timeline</code> will be a wide-row table, partitioning on <code>user_id</code> and then clustering on <code>tweet_id</code>.  I verified the ""under the hood"" structure by running the cassandra-cli tool, and <code>list</code>ing the <code>timeline</code> column family (table).  Here you can see how the rows are partitioned by <code>user_id</code>, and each column has the <code>tweet_id</code> uuid as a part of its name:</li>
</ol>

<p>-</p>

<pre><code>[default@stackoverflow] list timeline;
Using default limit of 100
Using default cell limit of 100
-------------------
RowKey: ahamilton
=&gt; (name=05a5f177-f070-486d-b64d-4e2bb28eaecc:, value=, timestamp=1424707827585904)
=&gt; (name=05a5f177-f070-486d-b64d-4e2bb28eaecc:author, value=676d61736f6e, timestamp=1424707827585904)
=&gt; (name=05a5f177-f070-486d-b64d-4e2bb28eaecc:body, value=54686f73652067656e746c656d656e2c2077686f2077696c6c20626520656c65637465642073656e61746f72732c2077696c6c20666978207468656d73656c76657320696e20746865206665646572616c20746f776e2c20616e64206265636f6d6520636974697a656e73206f66207468617420746f776e206d6f7265207468616e206f6620796f75722073746174652e, timestamp=1424707827585904)
=&gt; (name=819d95e9-356c-4bd5-9ad0-8cd36a7aa5e1:, value=, timestamp=1424707827585715)
=&gt; (name=819d95e9-356c-4bd5-9ad0-8cd36a7aa5e1:author, value=6777617368696e67746f6e, timestamp=1424707827585715)
=&gt; (name=819d95e9-356c-4bd5-9ad0-8cd36a7aa5e1:body, value=546f20626520707265706172656420666f7220776172206973206f6e65206f6620746865206d6f73742065666665637475616c206d65616e73206f662070726573657276696e672070656163652e, timestamp=1424707827585715)
-------------------
RowKey: gmason
=&gt; (name=05a5f177-f070-486d-b64d-4e2bb28eaecc:, value=, timestamp=1424707827585150)
=&gt; (name=05a5f177-f070-486d-b64d-4e2bb28eaecc:author, value=676d61736f6e, timestamp=1424707827585150)
=&gt; (name=05a5f177-f070-486d-b64d-4e2bb28eaecc:body, value=54686f73652067656e746c656d656e2c2077686f2077696c6c20626520656c65637465642073656e61746f72732c2077696c6c20666978207468656d73656c76657320696e20746865206665646572616c20746f776e2c20616e64206265636f6d6520636974697a656e73206f66207468617420746f776e206d6f7265207468616e206f6620796f75722073746174652e, timestamp=1424707827585150)
-------------------
RowKey: gwashington
=&gt; (name=819d95e9-356c-4bd5-9ad0-8cd36a7aa5e1:, value=, timestamp=1424707827585475)
=&gt; (name=819d95e9-356c-4bd5-9ad0-8cd36a7aa5e1:author, value=6777617368696e67746f6e, timestamp=1424707827585475)
=&gt; (name=819d95e9-356c-4bd5-9ad0-8cd36a7aa5e1:body, value=546f20626520707265706172656420666f7220776172206973206f6e65206f6620746865206d6f73742065666665637475616c206d65616e73206f662070726573657276696e672070656163652e, timestamp=1424707827585475)
-------------------
RowKey: jmadison
=&gt; (name=819d95e9-356c-4bd5-9ad0-8cd36a7aa5e1:, value=, timestamp=1424707827585597)
=&gt; (name=819d95e9-356c-4bd5-9ad0-8cd36a7aa5e1:author, value=6777617368696e67746f6e, timestamp=1424707827585597)
=&gt; (name=819d95e9-356c-4bd5-9ad0-8cd36a7aa5e1:body, value=546f20626520707265706172656420666f7220776172206973206f6e65206f6620746865206d6f73742065666665637475616c206d65616e73206f662070726573657276696e672070656163652e, timestamp=1424707827585597)
=&gt; (name=b67fe644-4dbe-489b-bc71-90f809f88636:, value=, timestamp=1424707827585348)
=&gt; (name=b67fe644-4dbe-489b-bc71-90f809f88636:author, value=6a6d616469736f6e, timestamp=1424707827585348)
=&gt; (name=b67fe644-4dbe-489b-bc71-90f809f88636:body, value=416c6c206d656e20686176696e6720706f776572206f7567687420746f206265206469737472757374656420746f2061206365727461696e206465677265652e, timestamp=1424707827585348)

4 Rows Returned.
Elapsed time: 35 msec(s).
</code></pre>
",['table']
28666712,28667971,2015-02-23 03:58:25,CRUD in Cassandra,"<p>I'm trying Cassandra with simple CRUD operations and don't understand how should I model the data.</p>

<p>Let's say, we need to manage simple user data:</p>

<p><code>UserId | Email | Name</code></p>

<p>We want to be able to GET information by either <code>UserId</code> or <code>Email</code>. Also we want to be able to change user info, i.e. <code>Email</code> and <code>Name</code>.</p>

<p>That leads me to a dilemma: to query by <code>Email</code>, I should add it to PRIMARY KEY. But if I index it, I won't be able to UPDATE it.</p>

<p>How should I change the data model or indexing to be able to UPDATE the data?</p>

<p>From what I've read, secondary indexes are evil in Cassandra and I shouldn't use them to keep Cassandra's performance on a good level.</p>
",<cassandra><cql3>,"<p>Indeed you should not use secondary indexes unless you absolutely have to. But if you need to search by an email, you can create another table with 2 columns - <code>Email</code> and <code>UserId</code>. The primary key will be <code>Email</code> and that is how you will be searching for a <code>UserId</code> by <code>Email</code>. Think of it as of an index in a traditional relational database. Since the <code>Email</code> value should be unique - the lookup table approach should be more efficient than a secondary index.</p>

<p>Once you found <code>UserId</code> by <code>Email</code> you can use it in the queries to the main table.</p>
",['table']
28671882,28674506,2015-02-23 10:43:26,Cassandra - filtering results on a boolean,"<p>I have a table looking like this:</p>

<pre><code>CREATE TABLE table(
  user text,
  gender boolean,
  ...,
  PRIMARY KEY(user)
);
</code></pre>

<p>where <code>gender</code> stands for <code>true</code> for a male and <code>false</code> for a female.</p>

<p>I'd like to make 3 types of queries: </p>

<ul>
<li><ol>
<li>retrieve all the users, no matter the gender</li>
</ol></li>
<li><ol start=""2"">
<li>retrieve all males</li>
</ol></li>
<li><ol start=""3"">
<li>retrieve all females</li>
</ol></li>
</ul>

<p>I have thought of adding the <code>gender</code> parameter to the primary key. This would make queries 2 and 3 straightforward and to retrieve all users, I would need to make queries 2 and 3 and then concatenate the results.</p>

<p>The problem is that my user table get really big and I have to paginate the results. This poses a problem when <code>gender</code> is part of the primary key as I have to pass both <code>pageState</code> in my API response to enable the user to query for more results.</p>

<p>Is there a better way to enable this kind of requests making paging relatively easy and straightforward?</p>

<p>Many thanks</p>
",<cassandra>,"<p>There are 2 questions in one.</p>

<p>First, the querying one. If you need these 3 types of queries then denormalize and create 3 tables: users, male_users and female_users.</p>

<p>Second, the paging one. If you're listing all users (or male/Female users) by querying at the table level results will be unsorted because multiple nodes will participate in elaborating the result and that partitions are hashed. If you're talking about paging at the UI level (only the first 100 users are fetched and displayed), it will be <a href=""http://www.datastax.com/documentation/cql/3.1/cql/cql_using/paging_c.html"" rel=""nofollow"">tricky</a>. If you're talking about paging at the driver level (all users are fetched by chunks/pages), then it may be easier.</p>
",['table']
28733228,28734213,2015-02-26 02:02:22,No indexed columns present in by-columns clause with Equal operator,"<p>I'm running CQL3 with Cassandra 2.1.2, and here's what happens (I have a keyspace called default): </p>

<pre><code>cqlsh&gt; CREATE TABLE default.test (x int, y int) PRIMARY KEY (x);
cqlsh&gt; CREATE TABLE default.test (x int PRIMARY KEY, y int);
cqlsh&gt; INSERT INTO default.test (x, y) VALUES (1, 2);
cqlsh&gt; INSERT INTO default.test (x, y) VALUES (1, 0);
cqlsh&gt; SELECT * FROM default.test WHERE x=1 AND y &gt; 1;
code=2200 [Invalid query] message=""No indexed columns present in by-columns clause with Equal operator""
</code></pre>

<p>What happened? I read the related questions on here and they said I could have a > filter on the non-primary key as long as I had an = filter on the primary key.</p>
",<cassandra><cql><cql3>,"<p>Ok, there are a few things going on here, so I'll tackle them one at a time.</p>

<ol>
<li>Your first <code>CREATE TABLE</code> statement is not syntactically correct.</li>
</ol>

<p>-</p>

<pre><code>CREATE TABLE default.test (x int, y int) PRIMARY KEY (x);
</code></pre>

<p>Your PRIMARY KEY definition needs to be inside of your column definition, like this:</p>

<pre><code>CREATE TABLE default.test (x int, y int, PRIMARY KEY (x));
</code></pre>

<p>Ironically, this is close to what you need to support your query.</p>

<ol start=""2"">
<li>PRIMARY KEYS in Cassandra are unique.  This becomes apparent when I follow your steps above and <code>INSERT</code> two rows:</li>
</ol>

<p>-</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; INSERT INTO default.test (x, y) VALUES (1, 2);
aploetz@cqlsh:stackoverflow&gt; INSERT INTO default.test (x, y) VALUES (1, 0);
aploetz@cqlsh:stackoverflow&gt; SELECT * FROm test;
x | y
---+---
1 | 0

(1 rows)
</code></pre>

<p>As <code>x</code> is your only PRIMARY KEY, the values of x = 1 and y = 2 are <code>INSERT</code>ed first...and then next <code>INSERT</code> promptly overwrites <code>y</code> with the value of 0.  And not only are PRIMARY KEYS unique, <code>INSERT</code>s and <code>UPDATE</code>s are treated the same by Cassandra.</p>

<ol start=""3"">
<li>Ironically, the solution to support your query is the same as the solution to support both rows.</li>
</ol>

<blockquote>
  <p>I read the related questions on here and they said I could have a > filter on the non-primary key as long as I had an = filter on the primary key.</p>
</blockquote>

<p>Not entirely true.  You are allowed to filter by > or &lt; only on clustering columns, and then only if the partition key is restricted by equals.  As you have a single PRIMARY KEY, <code>x</code> is your partition key, and you <em>do not have a clustering column defined</em>.  Therefore to support this query, <code>y</code> must also be defined as part of the PRIMARY KEY, like this:</p>

<pre><code>CREATE TABLE default.test (x int, y int, PRIMARY KEY (x,y));
</code></pre>

<p>Including <code>y</code> as a part of your PRIMARY KEY also helps to ensure uniqueness, which will allow your table to contain two rows:</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; INSERT INTO default.test (x, y) VALUES (1, 2);
aploetz@cqlsh:stackoverflow&gt; INSERT INTO default.test (x, y) VALUES (1, 0);
aploetz@cqlsh:stackoverflow&gt; SELECT * FROm test;
x | y
---+---
1 | 0
1 | 2

(2 rows)
</code></pre>

<p>With all of that done, this will now work:</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; SELECT * FROM test WHERE x=1 AND y &gt; 1;

 x | y
---+---
 1 | 2

(1 rows)
</code></pre>

<p>Here is a link to the most-recent doc detailing the CQL <a href=""http://www.datastax.com/documentation/cql/3.1/cql/cql_reference/select_r.html"" rel=""nofollow"">SELECT</a> statement.  You should definitely give that a read, a well as this one that explains <a href=""http://www.datastax.com/documentation/cql/3.1/cql/ddl/ddl_compound_keys_c.html"" rel=""nofollow"">Compound Keys and Clustering</a>.</p>
",['table']
28753656,28753871,2015-02-26 22:04:57,"ORDER BY reloaded, cassandra","<p>A given column family I would like to sort and to this I am trying to create a table with the option CLUSTERING ORDER BY. I always encounter the following errors:</p>

<p>1.) Variant A resulting in
<strong>Bad Request: Missing CLUSTERING ORDER for column userid</strong>
Statement:</p>

<pre><code>CREATE TABLE test.user (
  userID timeuuid,
  firstname varchar,
  lastname varchar,
  PRIMARY KEY (lastname, userID)
)WITH CLUSTERING ORDER BY (lastname desc);
</code></pre>

<p>2.) Variant B resulting in
<strong>Bad Request: Only clustering key columns can be defined in CLUSTERING ORDER directive</strong>
Statement:</p>

<pre><code>CREATE TABLE test.user (
  userID timeuuid,
  firstname varchar,
  lastname varchar,
  PRIMARY KEY (lastname, userID)
)WITH CLUSTERING ORDER BY (lastname desc, userID asc);
</code></pre>

<p>As far as I can see in the manual this is the correct syntax for creating a table for which I would like to run queries as ""SELECT .... FROM user WHERE ... ORDER BY lastname"". <strong>How could I achieve this?</strong> (The column 'lastname' I would like to keep as the first part of the primary key, so that I could use it in delete statements with the WHERE-clause.)</p>

<p>Thanks a lot, Tamas</p>
",<cassandra><cassandra-cli>,"<p>Clustering would be limited to whats defined in partitioning key, in your case (lastName + userId). So cassandra would store result in sorted order whose (lastName+userId) combination.   Thats why u nned to give both for retrieval purpose. Its still not useful schema if you want to sort all data in table as  last name as userId is unique(timeuuid) so clustering key would be of no use.</p>

<pre><code>CREATE TABLE test.user (
  userID timeuuid,
  firstname varchar,
  lastname varchar,
  bucket int,
  PRIMARY KEY (bucket)
)WITH CLUSTERING ORDER BY (lastname desc);
</code></pre>

<p>Here if u provide buket value say 1 for all user records then , all user would go in same bucket and hense it would retrieve all rows in sorted order of last name. (By no mean this is a good design, just to give you an idea). </p>

<p>Revised :</p>

<pre><code>CREATE TABLE user1 (
  userID uuid,
  firstname varchar,
  lastname varchar,
  bucket int,
  PRIMARY KEY ((bucket), lastname,userID)
)WITH CLUSTERING ORDER BY (lastname desc);
</code></pre>
",['table']
28763627,28768011,2015-02-27 11:07:14,What's read-before-write in NoSQL?,"<p>I read in a book : ""Cassandra is an NoSQL database and promotes read-before-write instead of relational model"".</p>

<p>What does ""read-before-write"" means in a NoSQL context?</p>
",<cassandra><nosql>,"<p>Read before write means that you are checking the value of a cell before modifying it.</p>

<p>Read-Before write is a huge anti-pattern in Cassandra. Any book you read that encourages doing this should be looked at with suspicion. Normally Cassandra writes are performed without having any information about the current state of the database. One of the side effects of this is that all writes to Cassandra are actually update operations. This is allows for extremely fast writes but does have some limitations.</p>

<p>If you really need to check the state of the database before writing, Cassandra provides ""Check and Set""(CAS) operations which use PAXOS to establish database state prior to modifying the record. These are written like <code>update table set x = 3 if y = 1</code>. CAS queries are orders of magnitude slower than a normal write in C* and should be used sparingly.</p>
",['table']
28767452,28768755,2015-02-27 14:31:39,Cassandra Static Column design,"<p>How are static columns stored internally in cassandra?  Can someone please post an example discussing the design implementation of static column in cassandra?</p>
",<cassandra><cassandra-2.0>,"<p>Why don't we take a look at the structure of a table with static columns on disk and find out?</p>

<pre><code>cqlsh:test&gt; CREATE TABLE test (k int, v int, s int static, d int, PRIMARY KEY(k,v))

cqlsh:test&gt; INSERT INTO test (k, v, s, d) VALUES ( 1, 1 ,20, 1 );
cqlsh:test&gt; INSERT INTO test (k, v, s, d) VALUES ( 1, 3 ,21, 2 );
cqlsh:test&gt; INSERT INTO test (k, v, s, d) VALUES ( 1, 2 ,21, 2 );
</code></pre>

<p>Exit out of C* and run <code>nodetool flush</code> to make our sstables. Run <code>sstable2json</code> on the .db file that was created in the data directory.</p>

<pre><code>[
{""key"": ""1"", &lt;--- K=1 Partition
 ""cells"": [["":s"",""21"",1425050917842350], &lt;---- Our Static Column
           [""1:"","""",1425050906896717], &lt; --- C=1 row
           [""1:d"",""1"",1425050906896717], &lt; --- C=1, D=1 value
           [""2:"","""",1425050917842350], &lt; --- C=2 row
           [""2:d"",""2"",1425050917842350], &lt; --- C=2, D=2 value
           [""3:"","""",1425050912874025], &lt;--- C=3 Row
           [""3:d"",""2"",1425050912874025]]} &lt;--- C=3, D=2 Value
]
</code></pre>

<p>You can see that in Cassandra this static column is held in a cell with the title ""Blank:ColumnName"" at the very beginning of our partition. Unlike all the other cells there is no information about <em>c</em>(our clustering column) in the cell name, so all values of <em>c</em> will still modify the same static column <em>s</em></p>

<p>For more details on why this is, check out the JIRA at
<a href=""https://issues.apache.org/jira/browse/CASSANDRA-6561"" rel=""noreferrer"">https://issues.apache.org/jira/browse/CASSANDRA-6561</a>
and the blog post at
<a href=""http://www.datastax.com/dev/blog/cql-in-2-0-6"" rel=""noreferrer"">http://www.datastax.com/dev/blog/cql-in-2-0-6</a></p>
",['table']
28777840,28778097,2015-02-28 03:40:08,Cassandra nodejs DataStax driver don't return newly added columns via prepared statement execution,"<p>After adding a pair of columns in schema, I want to select them via <code>select *</code>. Instead <code>select *</code> returns old set of columns and none new.</p>

<p>By documentation recommendation, I use {prepare: true} to smooth JavaScript floats and Cassandra ints/bigints difference (I don't really need the prepared statement here really, it is just to resolve this <a href=""https://stackoverflow.com/questions/26682873/responseerror-expected-4-or-0-byte-int"">ResponseError : Expected 4 or 0 byte int</a> issue and I also don't want to bother myself with <code>query hints</code>).</p>

<p>So on first execution of <code>select *</code> I had 3 columns. After this, I added 2 columns to schema. <code>select *</code> still returns 3 columns if is used with <code>{prepare: true}</code> and 5 columns if used without it.</p>

<p>I want to have a way to reliably refresh this cache or make cassandra driver prepare statements on each app start. </p>

<p>I don't consider restarting database cluster a reliable way.</p>
",<node.js><cassandra><prepared-statement>,"<p>This is actually an issue in Cassandra that was fixed in 2.1.3 (<a href=""https://issues.apache.org/jira/browse/CASSANDRA-7910"" rel=""nofollow"">CASSANDRA-7910</a>).  The problem is that on schema update, the prepared statements are not evicted from the cache on the Cassandra side.  If you are running a version less than 2.1.3 (which is likely since 2.1.3 was released last week), there really isn't a way to work around this unless you create another separate prepared statement that is slightly different (like extra spaces or something to cause a separate unique statement).</p>

<p>When running with 2.1.3 and changing the table schema, C* will properly evict the relevant prepared statements from the cache, and when the driver sends another query using that statement, Cassandra will respond with an 'UNPREPARED' message, which should provoke the nodejs driver to reprepare the query and resend the request for you.</p>

<p>On the Node.js driver, you can <a href=""https://datastax-oss.atlassian.net/browse/NODEJS-45"" rel=""nofollow"">programatically clear the prepared statement metadata</a>:</p>

<pre><code>client.metadata.clearPrepared(); 
</code></pre>
",['table']
28804626,28812501,2015-03-02 07:29:51,use cql copy for map data,"<p>there is one post for copying multi-value data like list and set from csv to cassandra table, </p>

<p><a href=""https://stackoverflow.com/questions/18140645/import-csv-with-multi-valued-collection-attributes-to-cassandra"">Import CSV with multi-valued (collection) attributes to Cassandra</a></p>

<p>has any one done this for map type data?</p>
",<csv><cassandra><cql>,"<p>Sure, I'll give you a simple example that uses both a <code>map</code> and a <code>list</code>.  Consider the following table designed to hold starship data, as well as a list of crew and passengers:</p>

<pre><code>CREATE TABLE shipregistry (
  name text,
  class text,
  hullnumber text,
  crew map&lt;text,text&gt;,
  passengers list&lt;text&gt;,
  PRIMARY KEY ((name))
);
</code></pre>

<p>I will load this table's data from the following text file (<code>/home/aploetz/shipregistry_20150302.csv</code>), which has a header and a single row of pipe-delimited data:</p>

<pre><code>name|class|crew|hullnumber|passengers
Serenity|03-K64-Firefly|{'1st Officer': 'Zoey Washburne', 'captain': 'Malcolm Reynolds', 'engineer': 'Kaylee Frye', 'pilot': 'Hoban Washburne'}|G-82659|['Simon Tam', 'River Tam', 'Derial Book', 'Inara Serra']
</code></pre>

<p>I will then import it using the cqlsh <code>COPY</code> command:</p>

<pre><code>aploetz@cqlsh:presentation&gt; COPY shipregistry FROM
'/home/aploetz/shipregistry_20150302.csv' WITH HEADER=true and DELIMITER='|';

1 rows imported in 0.017 seconds.
</code></pre>

<p>Important to note, is that the <code>list</code> data for the <code>passengers</code> column, is comma-delimited and enclosed in brackets:</p>

<pre><code>['Simon Tam', 'River Tam', 'Derial Book', 'Inara Serra']
</code></pre>

<p>Likewise, the <code>map</code> data for the <code>crew</code> column, is a comma-delimited list of key-value pairs and enclosed in braces:</p>

<pre><code>{'1st Officer': 'Zoey Washburne', 'captain': 'Malcolm Reynolds', 'engineer': 'Kaylee Frye', 'pilot': 'Hoban Washburne'}
</code></pre>

<p>I chose a pipe as a delimiter to make this easier on myself and avoid potential clashes with commas in the collection types.</p>
",['table']
28824143,29580038,2015-03-03 04:33:27,Are there any major disadvantages to having multiple clustering columns in cassandra?,"<p>I'm designing a cassandra table where I need to be able able to retrieve rows by their geohash.  I have something that works, but I'd like to avoid range queries more so than I'm currently able to.</p>

<p>The current table schema is this, with geo_key containing the first five characters of the geohash string.  I query using the geo_key, then range filter on the full geohash, allowing me to prefix search based on a 5 or greater length geohash:  </p>

<pre><code>CREATE TABLE georecords (geo_key text,geohash text, data text) PRIMARY KEY (geo_key, geohash))
</code></pre>

<p>My idea is that I could instead store the characters of the geohash as seperate columns, allowing me to specify as many caracters as I wanted, to do a prefix match on the geohash.  My concern is what impact using multiple clustering columns might have:</p>

<pre><code>CREATE TABLE georecords (g1 text,g2 text,g3 text,g4 text,g5 text,g6 text,g7 text,g8 text,geohash text, data text) PRIMARY KEY (g1,g2,g3,g4,g5,g6,g7,g8,geohash,pid))
</code></pre>

<p>(I'm not really concerned about the cardinality of the partition key - g1 would have minimum 30 values, and I have other workarounds for it as well)</p>

<p>Other that cardinality of the partition key, and extra storage requirements, what should I be aware of if I used the many cluster column approach?</p>
",<cassandra><geohashing>,"<blockquote>
  <p>Other that cardinality of the partition key, and extra storage requirements, what should I be aware of if I used the many cluster column approach?</p>
</blockquote>

<p>This seemed like an interesting problem to help out with, so I built a few CQL tables of differing PRIMARY KEY structure and options.  I then used <a href=""http://geohash.org/"" rel=""nofollow"">http://geohash.org/</a> to come up with a few endpoints, and inserted them.</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; SELECT g1, g2, g3, g4, g5, g6, g7, g8, geohash, pid, data FROm georecords3;

 g1 | g2 | g3 | g4 | g5 | g6 | g7 | g8 | geohash      | pid  | data
----+----+----+----+----+----+----+----+--------------+------+---------------
  d |  p |  8 |  9 |  v |  c |  n |  e |  dp89vcnem4n | 1001 |    Beloit, WI
  d |  p |  8 |  c |  p |  w |  g |  v |    dp8cpwgv3 | 1003 |   Harvard, IL
  d |  p |  c |  8 |  g |  e |  k |  t | dpc8gektg8w7 | 1002 | Sheboygan, WI
  9 |  x |  j |  6 |  5 |  j |  5 |  1 |    9xj65j518 | 1004 |    Denver, CO

(4 rows)
</code></pre>

<p>As you know, Cassandra is designed to return data with a specific, precise key.  Using multiple clustering columns <em>helps</em> in that approach, in that you are helping Cassandra quickly identify the data you wish to retrieve.</p>

<p>The only thing I would think about changing, is to see if you can do without either <code>geohash</code> or <code>pid</code> in the PRIMARY KEY.  My gut says to get rid of <code>pid</code>, as it really isn't anything that you would query by.  The only value it provides is that of uniqueness, which you will need if you plan on storing the same geohashes multiple times.</p>

<p>Including <code>pid</code> in your PRIMARY KEY leaves you with one non-key column, and that allows you to use the <code>WITH COMPACT STORAGE</code> directive.  Really the only true edge that gets you, is in saving disk space as the clustering column names are not stored with the value.  This becomes apparent when looking at the table from within the <code>cassandra-cli</code> tool:</p>

<p><em>Without</em> compact storage:</p>

<pre><code>[default@stackoverflow] list georecords3;
Using default limit of 100
Using default cell limit of 100
-------------------
RowKey: d
=&gt; (name=p:8:9:v:c:n:e:dp89vcnem4n:1001:, value=, timestamp=1428766191314431)
=&gt; (name=p:8:9:v:c:n:e:dp89vcnem4n:1001:data, value=42656c6f69742c205749, timestamp=1428766191314431)
=&gt; (name=p:8:c:p:w:g:v:dp8cpwgv3:1003:, value=, timestamp=1428766191382903)
=&gt; (name=p:8:c:p:w:g:v:dp8cpwgv3:1003:data, value=486172766172642c20494c, timestamp=1428766191382903)
=&gt; (name=p:c:8:g:e:k:t:dpc8gektg8w7:1002:, value=, timestamp=1428766191276179)
=&gt; (name=p:c:8:g:e:k:t:dpc8gektg8w7:1002:data, value=536865626f7967616e2c205749, timestamp=1428766191276179)
-------------------
RowKey: 9
=&gt; (name=x:j:6:5:j:5:1:9xj65j518:1004:, value=, timestamp=1428766191424701)
=&gt; (name=x:j:6:5:j:5:1:9xj65j518:1004:data, value=44656e7665722c20434f, timestamp=1428766191424701)

2 Rows Returned.
Elapsed time: 217 msec(s).
</code></pre>

<p><em>With</em> compact storage:</p>

<pre><code>[default@stackoverflow] list georecords2;
Using default limit of 100
Using default cell limit of 100
-------------------
RowKey: d
=&gt; (name=p:8:9:v:c:n:e:dp89vcnem4n:1001, value=Beloit, WI, timestamp=1428765102994932)
=&gt; (name=p:8:c:p:w:g:v:dp8cpwgv3:1003, value=Harvard, IL, timestamp=1428765717512832)
=&gt; (name=p:c:8:g:e:k:t:dpc8gektg8w7:1002, value=Sheboygan, WI, timestamp=1428765102919171)
-------------------
RowKey: 9
=&gt; (name=x:j:6:5:j:5:1:9xj65j518:1004, value=Denver, CO, timestamp=1428766022126266)

2 Rows Returned.
Elapsed time: 39 msec(s).
</code></pre>

<p>But, I would recommend <em>against</em> using <code>WITH COMPACT STORAGE</code> for the following reasons:</p>

<ul>
<li>You cannot add or remove columns after table creation.</li>
<li>It prevents you from having multiple non-key columns in the table.</li>
<li>It was really intended to be used in the old (deprecated) thrift-based approach to column family (table) modeling, and really shouldn't be used/needed anymore.</li>
<li>Yes, it saves you disk space, but disk space is cheap so I'd consider this a very small benefit.</li>
</ul>

<p>I know you said ""other than cardinality of the partition key"", but I am going to mention it here anyway.  You'll notice in my sample data set, that almost all of my rows are stored with the <code>d</code> partition key value.  If I were to create an application like this for myself, tracking geohashes in the Wisconsin/Illinois stateline area, I would definitely have the problem of most of my data being stored in the same partition (creating a hotspot in my cluster).  So knowing my use case and potential data, I would probably combine the first three or so columns into a single partition key.</p>

<p>The other issue with storing everything in the same partition key, is that each partition can store a max of about 2 billion columns.  So it would also make sense to put some though behind whether or not your data could ever eclipse that mark.  And obviously, the higher the cardinality of your partition key, the less likely you are to run into this issue.</p>

<p>By looking at your question, it appears to me that you <em>have</em> looked at your data and you understand this...definite ""plus.""  And 30 unique values in a partition key should provide sufficient distribution.  I just wanted to spend some time illustrating how big of a deal that could be.</p>

<p>Anyway, I also wanted to add a ""nicely done,"" as it sounds like you are on the right track.</p>

<p><strong>Edit</strong></p>

<blockquote>
  <p>The still unresolved question for me is which approach will scale better, in which situations.</p>
</blockquote>

<p>Scalability is more tied to how many R replicas you have across N nodes.  As <a href=""http://techblog.netflix.com/2011/11/benchmarking-cassandra-scalability-on.html"" rel=""nofollow"">Cassandra scales linearly</a>; the more nodes you add, the more transactions your application can handle.  Purely from a data distribution scenario, your first model will have a higher cardinality partition key, so it will distribute much more evenly than the second.  However, the first model presents a much more restrictive model in terms of query flexibility.</p>

<p>Additionally, if you are doing range queries within a partition (which I believe you said you are) then the second model will allow for that in a <em>very</em> performant manner.  All data within a partition is stored on the same node.  So querying multiple results for <code>g1='d' AND g2='p'</code>...etc...will perform extremely well.</p>

<blockquote>
  <p>I may just have to play with the data more and run test cases.</p>
</blockquote>

<p>That is a great idea.  I think you will find that the second model is the way to go (in terms of query flexibility and querying for multiple rows).  If there is a performance difference between the two when it comes to single row queries, my suspicion is that it should be negligible.</p>
",['table']
28847886,28848610,2015-03-04 06:24:08,how to update data in cassandra using IN operator,"<p>I have a table with the following schema.</p>

<pre><code>CREATE TABLE IF NOT EXISTS group_friends(
groupId timeuuid,
friendId bigint,
time bigint,
PRIMARY KEY(groupId,friendId));
</code></pre>

<p>I need to keep a track of time if any changes happen in a group (such changing the group name or adding a new friend in table etc.). So I need to update the value of time field by groupId every time there is any change in any related table.</p>

<p>As update in cassandra requires mentioning all primary keys in where clause this query will not run.</p>

<pre><code>update group_friends set time = 123456 where groupId = 100;
</code></pre>

<p>So I can do something like this.</p>

<pre><code>update group_friends set time=123456 where groupId=100 and friendId in (...);
</code></pre>

<p>But it is showing the following error--></p>

<pre><code>[Invalid query] message=""Invalid operator IN for PRIMARY KEY part friendid""
</code></pre>

<p>Is there any way to perform an update operation using IN operator in clustering column? If not then what are the possible ways to do this?</p>

<p>Thanks in advance.</p>
",<cassandra><cassandra-2.0><cql3>,"<p>Since friendId is a clustering column, a batch operation is probably a reasonable and well performing choice in this case since all updates would be made in the same partition (assuming you are using the same group id for the update).   For example, with the java driver you could do the following:</p>

<pre class=""lang-java prettyprint-override""><code>Cluster cluster = new Cluster.Builder().addContactPoint(""127.0.0.1"").build();
Session session = cluster.connect(""friends"");

PreparedStatement updateStmt = session.prepare(""update group_friends set time = ? where groupId = ? and friendId = ?"");

long time = 123456;
UUID groupId = UUIDs.startOf(0);
List&lt;Long&gt; friends = Lists.newArrayList(1L, 2L, 4L, 8L, 22L, 1002L);
BatchStatement batch = new BatchStatement(BatchStatement.Type.UNLOGGED);
for(Long friendId : friends) {
    batch.add(updateStmt.bind(time, groupId, friendId));
}
session.execute(batch);
cluster.close();
</code></pre>

<p>The other advantage of this is that since the partition key can be inferred from the BatchStatement, the driver will use token-aware routing to send a request to a replica that would own this data, skipping a network hop.</p>

<p>Although this will effectively be a single write, be careful with the size of your batches.  You should take care not to make it too large.</p>

<p>In the general case, you can't really go wrong by executing each statement individually instead of using a batch.   The CQL transport allows many requests on a single connection and are asynchronous in nature, so you can have many requests going on at a time without the typical performance cost of a request per connection.</p>

<p>For more about writing data in batch see: <a href=""https://medium.com/@foundev/cassandra-batch-loading-without-the-batch-keyword-40f00e35e23e"" rel=""nofollow"">Cassandra: Batch loading without the Batch keyword</a> </p>

<p>Alternatively, there may be an even easier way to accomplish what you want.  If what you are really trying to accomplish is to maintain a group update time and you want it to be the same for all friends in the group, you can make time a <a href=""http://www.datastax.com/documentation/cql/3.1/cql/cql_reference/refStaticCol.html"" rel=""nofollow"">static column</a>.  This is a new feature in Cassandra 2.0.6.  What this does is shares the column value for all rows in the groupId partition.  This way you would only have to update time once, you could even set the time in the query you use to add a friend to the group so it's done as one write operation.</p>

<pre><code>CREATE TABLE IF NOT EXISTS friends.group_friends(
  groupId timeuuid,
  friendId bigint,
  time bigint static,
  PRIMARY KEY(groupId,friendId)
);
</code></pre>

<p>If you can't use Cassandra 2.0.6+ just yet, you can create a separate table called group_metadata that maintains the time for a group, i.e.:</p>

<pre><code>CREATE TABLE IF NOT EXISTS friends.group_metadata(
  groupId timeuuid,
  time bigint,
  PRIMARY KEY(groupId)
);
</code></pre>

<p>The downside here being that whenever you want to get at this data you need to select from this table, but that seems manageable.</p>
",['table']
28856303,28865801,2015-03-04 13:56:45,Cassandra preventing duplicates,"<p>I have a simple table distributed by <code>userId</code>:</p>

<pre><code>create table test (
  userId uuid,
  placeId uuid,
  visitTime timestamp,
  primary key(userId, placeId, visitTime)
) with clustering order by (placeId asc, visitTime desc);
</code></pre>

<p>Each pair <code>(userId, placeId)</code> can have either 1 or none visits. <code>visitTime</code> is just some data associated with it, used for sorting in queries like <code>select * from test where userId = ? order by visitTime desc</code>.</p>

<p>How can I require <code>(userId, placeId)</code> to be unique? I need to make sure that</p>

<pre><code>insert into test (userId, placeId, timeVisit) values (?, ?, ?)
</code></pre>

<p>won't insert 2nd visit to <code>(userId, placeId)</code> with different time. Checking for existence before inserting isn't atomic, is there a better way?</p>
",<cassandra><cql><cql3>,"<p>Let me understand -- if the couple <code>(userId, placeId)</code> should be unique, (meaning that you don't have to put two rows with this pair of data) what is the <code>timeVisit</code> useful for in the primary key? Why would you perform a query using <code>order by visitTime desc</code> if this will have only one row?</p>

<p>If what you need is to prevent duplication you have 2 ways.</p>

<p>1 - Lightweight transaction -- this, using <code>IF NOT EXISTS</code> will do what you want. But as I explained <a href=""https://stackoverflow.com/questions/24986578/consistency-level-of-cassandra-lightweight-transactions"">here</a> lightweight transactions are really slow due to a particular handling by cassandra</p>

<p>2 - <code>USING TIMESTAMP</code> Writetime enforcement - (be careful with it!***) The '<em>trick</em>' is to force a decreasing <code>TIMESTAMP</code> </p>

<p>Let me give an example:</p>

<pre><code>INSERT INTO users (uid, placeid , visittime , otherstuffs ) VALUES ( 1, 2, 1000, 'PLEASE DO NOT OVERWRITE ME') using TIMESTAMP 100;
</code></pre>

<p>This produces this output</p>

<pre><code>select * from users;

 uid | placeid | otherstuffs                | visittime
-----+---------+----------------------------+-----------
   1 |       2 | PLEASE DO NOT OVERWRITE ME |      1000
</code></pre>

<p>Let's now decrease the <code>timestamp</code></p>

<pre><code>INSERT INTO users (uid, placeid , visittime , otherstuffs ) VALUES ( 1, 2, 2000, 'I WANT OVERWRITE YOU') using TIMESTAMP 90;
</code></pre>

<p><strong>Now data in the table have not been updated</strong>, since there is a higher TS operation (100) for the couple <code>(uid, placeid)</code> -- in fact here the output has not changed</p>

<pre><code>select * from users;

 uid | placeid | otherstuffs                | visittime
-----+---------+----------------------------+-----------
   1 |       2 | PLEASE DO NOT OVERWRITE ME |      1000
</code></pre>

<p>If performance matters then use solution 2, if performance doesn't matter then use solution 1. For solution 2 you could calculate a decreasing timestamp for each write using a fixed number minus the system time millis</p>

<p>eg:</p>

<pre><code>Long decreasingTimestamp = 2_000_000_000_000L - System.currentTimeMillis();
</code></pre>

<p>*** this solution might lead to unexpected behaviour if, for instance, you want delete and then reinsert data. It is important to know that once you delete data you will be able to write them again only if the write operation will have a higher timestamp of the deletion one (if not specified, the timestamp used is the one of the machine)</p>

<p>HTH,<br/>
Carlo</p>
",['table']
28857504,28858301,2015-03-04 14:51:38,cassandra error when using select and where in cql,"<p>I have a <code>cassandra</code> table defined like this:    </p>

<pre><code>CREATE TABLE test.test(
id text,
time bigint,
tag text,
mstatus boolean,
lonumb  int,
PRIMARY KEY (id, time, tag)
)
</code></pre>

<p>And I want to select one column using <code>select</code>.
I tried:</p>

<pre><code>select * from test where lonumb = 4231;    
</code></pre>

<p>It gives:</p>

<pre><code>code=2200 [Invalid query] message=""No indexed columns present in by-columns clause with Equal operator""
</code></pre>

<p>Also I cannot do</p>

<pre><code>select * from test where mstatus = true;
</code></pre>

<p>Doesn't <code>cassandra</code> support <code>where</code> as a part of CQL? How to correct this?</p>
",<cassandra><cql><cqlsh>,"<p>Jny is correct in that <code>WHERE</code> is only valid on columns in the PRIMARY KEY, or those where a secondary index has been created for.  One way to solve this issue is to create a specific query table for <code>lonumb</code> queries.</p>

<pre><code>CREATE TABLE test.testbylonumb(
  id text,
  time bigint,
  tag text,
  mstatus boolean,
  lonumb  int,
  PRIMARY KEY (lonumb, time, id)
)
</code></pre>

<p>Now, this query will work:</p>

<pre><code>select * from testbylonumb where lonumb = 4231; 
</code></pre>

<p>It will return all CQL rows where <code>lonumb</code> = 4231, sorted by <code>time</code>.  I put <code>id</code> on the PRIMARY KEY to ensure uniqueness.</p>

<pre><code>select * from test where mstatus = true;
</code></pre>

<p>This one is trickier.  Indexes and keys on low-cardinality columns (like booleans) are generally considered a bad idea.  See if there's another way you could model that.  Otherwise, you could experiment with a secondary index on <code>mstatus</code>, but only use it when you specify a partition key (<code>lonumb</code> in this case), like this:</p>

<pre><code>select * from testbylonumb where lonumb = 4231 AND mstatus = true;
</code></pre>

<p>Maybe that wouldn't perform too badly, as you are restricting it to a specific partition.  But I definitely wouldn't ever do a <code>SELECT *</code> on <code>mstatus</code>.</p>
",['table']
28888865,28890467,2015-03-05 22:16:59,ALLOW FILTERING implementation @Cassandra,"<p>I have a table like below:</p>

<pre><code>CREATE TABLE tab(
    categoryid text,
    id text,
    name text,
    author text,
    desc text,
    PRIMARY KEY (categoryid , id)
) WITH CLUSTERING ORDER BY (id ASC);

CREATE INDEX ON tab (name);
CREATE INDEX ON tab (author);
</code></pre>

<p>When I execute the below queries:</p>

<pre><code>select * from tab ALLOW FILTERING;  ---1
select * from tab where id = 'id01' ALLOW FILTERING;  ---2
select * from tab where categoryid = 'cid01' ALLOW FILTERING;  ---3
</code></pre>

<p>What is happening in the back end for the three queries?</p>

<p>Is it going to completely neglect the key indices on id and categoryid.</p>

<p>Appreciate the reply.
Thanks</p>
",<cassandra><datastax-java-driver><cassandra-cli>,"<p>By specifying <code>ALLOW FILTERING</code> you are basically telling Cassandra that you are ok with it retrieving all rows from your table, examining each of them one-by-one, and returning only the ones that match your <code>WHERE</code> clause.  Depending on your <code>WHERE</code> clause, it may or may not need to do this.  This can be a painfully slow operation on a table that is large or where multiple nodes must be queried to retrieve all of the data.</p>

<pre><code>select * from tab ALLOW FILTERING;
</code></pre>

<p>For this query, you do not have a <code>WHERE</code> clause specified, so it will return all of the rows in the <code>tabs</code> table.  <code>ALLOW FILTERING</code> shouldn't alter performance in any noticeable way on this one.</p>

<pre><code>select * from tab where id = 'id01' ALLOW FILTERING;
</code></pre>

<p>On this query, Cassandra will retrieve all rows from the <code>tab</code> table, and just return the ones where <code>id='id01'</code>.  The fact that <code>id</code> is a clustering key really won't make much of a difference here.  This is because the partition key is what is important for data lookup, and clustering keys are typically use for enforcing row uniqueness and ordering.  And incidentally, since you are not querying by partition key (<code>categoryid</code>) it can't even enforce a clustering order on <code>id</code>, so your result set will not be sorted by <code>id</code>.</p>

<pre><code>select * from tab where categoryid = 'cid01' ALLOW FILTERING;
</code></pre>

<p>Queries like this make Cassandra happy!  Yes, the fact that <code>categoryid</code> is your partitioning key will be respected, and your result set will contain all CQL rows where <code>categoryid='cid01'</code>.  And as an added bonus, in this case your clustering order will be enforced.</p>

<p>DataStax's Developer Blog has a decent article on this topic that might also be of some further help to you: <a href=""http://www.datastax.com/dev/blog/allow-filtering-explained-2"">ALLOW FILTERING Explained</a>.</p>

<p>In my opinion, I would avoid queries that require ALLOW FILTERING.  And I would certainly not run one in production or in an OLTP environment.</p>
",['table']
28909408,28909469,2015-03-06 23:30:17,Cassandra - multiple counters based on timeframe,"<p>I am building an application and using Cassandra as my datastore.  In the app, I need to track event counts per user, per event source, and need to query the counts for different windows of time.  For example, some possible queries could be:</p>

<ul>
<li>Get all events for user A for the last week.</li>
<li>Get all events for all users for yesterday where the event source is source S.</li>
<li>Get all events for the last month.</li>
</ul>

<p>Low latency reads are my biggest concern here. From my research, the best way I can think to implement this is a different counter tables for each each permutation of source, user, and predefined time.  For example, create a count_by_source_and_user table, where the partition key is a combination of source and user ID, and then create a count_by_user table for just the user counts.</p>

<p>This seems messy. What's the best way to do this, or could you point towards some good examples of modeling these types of problems in Cassandra?  </p>
",<cassandra><datastax>,"<p>You are right. If latency is your main concern, and it should be if you have already chosen Cassandra, you need to create a table for each of your queries. This is the recommended way to use Cassandra: optimize for read and don't worry about redundant storage. And since within every table data is stored sequentially according to the index, then you cannot index a table in more than one way (as you would with a relational DB). I hope this helps. Look for the ""Data Modeling"" presentation that is usually given in ""Cassandra Day"" events. You may find it on ""Planet Cassandra"" or John Haddad's blog.</p>
",['table']
28949790,35898544,2015-03-09 19:01:23,Prettifying results of cqlsh commands in Linux terminal,"<p>Is there any way to prettify the results of cql commands in the Linux terminal while using the cqlsh utility (cql version of Mongo .pretty())? It becomes quite difficult to read the results when the output is displayed normally, especially when there are nested documents and arrays</p>
",<cassandra><cql3><cqlsh>,"<p>Perhaps you are interested in the EXPAND command?</p>
<p>Usage: <code>EXPAND ON;</code></p>
<p>From the documentation over at Datastax:</p>
<blockquote>
<p>This command lists the contents of each row of a table vertically, providing a more convenient way to read long rows of data than the default horizontal format. You scroll down to see more of the row instead of scrolling to the right. Each column name appears on a separate line in column one and the values appear in column two.</p>
</blockquote>
<p>Source: <a href=""https://docs.datastax.com/en/dse/5.1/cql/cql/cql_reference/cqlsh_commands/cqlshExpand.html"" rel=""noreferrer"">https://docs.datastax.com/en/dse/5.1/cql/cql/cql_reference/cqlsh_commands/cqlshExpand.html</a></p>
",['table']
29012661,29013689,2015-03-12 14:37:15,"Querying with ""contains"" on a list of user defined type (UDT)","<p>For data model like:</p>

<pre><code>create type city (
   name text,
   code int
);

create table user (
    id uuid,
    name text,
    cities list&lt;FROZEN&lt;city&gt;&gt;,
    primary key ( id )
);

create index user_city_index on user(cities);
</code></pre>

<p>Querying as</p>

<pre><code>select id, cities from user where cities contains {name:'My City', code: 10};
</code></pre>

<p>is working fine. But is it possible to query </p>

<pre><code>select id, cities from user where cities contains {name:'My City'};
</code></pre>

<p>and discard the <code>code</code> attribute, i.e. <code>code=&lt;any&gt;</code>?</p>

<p>Can this be achieved with the utilization of Spark?</p>
",<apache-spark><cassandra><nosql>,"<blockquote>
  <p>But is it possible to query: <code>select id, cities from user where cities contains {name:'My City'};</code></p>
</blockquote>

<p>No, it is not.  The documentation on <a href=""http://www.datastax.com/documentation/cql/3.1/cql/cql_using/cqlUseUDT.html"" rel=""nofollow"">using a UDT</a> states (for a UDT column <code>name</code>): </p>

<blockquote>
  <ol start=""8"">
  <li>Filter data on a column of a user-defined type. Create an index and then run a conditional query. In Cassandra 2.1.x, you need to list <strong>all components</strong> of the <code>name</code> column in the <code>WHERE</code> clause.</li>
  </ol>
</blockquote>

<p>So querying your <code>cities</code> UDT collection will require all components of the <code>city</code> type.</p>

<p>I'm sure there's a way to query this in Spark, but I'll give you a Cassandra based answer.  Basically, create an additional list column defined/indexed just to hold the list of city names, and run your <code>CONTAINS</code> on that.  Even better, would be to denormalize the <code>city</code> type into a query table (<code>usersbycity</code>) with a PRIMARY KEY definition like <code>PRIMARY KEY(cityname, citycode, userid)</code> and use that in addition to your <code>user</code> table to support queries by city name and code (or just city name).</p>

<p>Remember, Cassandra works best when the tables are specifically designed to suit your query patterns.  Secondary indexes are meant for convenience, not performance.  Trying to augment one table to support multiple queries is a RDBMs approach to data modeling (which typically doesn't work well in Cassandra).  And instead of one table that serves one query well, you usually end up with one table that serves multiple queries poorly.</p>

<p>Edit for your questions:</p>

<p>1) ""Is it acceptable to have long clustering keys?""</p>

<p>I cannot find a definitive statement on this at the moment, but I think the bigger issue here is in how clustering keys are stored/used ""under the hood.""  Essentially, each clustering key value is appended to each column value (for quicker retrieval).  Obviously, if you have a lot of them, that's going to eat disk space (not too big of a concern these days...if it is you can counter that with the <code>COMPACT STORAGE</code> directive).</p>

<p>If you have many of them, it may eventually impact performance.  I can double-check on this one and get back to you.  I wouldn't go with...say...100 clustering keys.  But I don't think 10 is a big deal.  I know that I've created models using 7 or 8, and they perform just fine.</p>

<p>2) ""If there are other denormalized tables (like usersbyhobby, usersbybookread etc.) related to user, how can I combine filtering from these tables to filters from usersbycity into one query, since there is no JOINs in c*?""</p>

<p>You cannot combine them at query-time.  What you <em>can</em> do, is if you find that you have a query that needs data from usersbyhobby, usersbybookread, and usersbycity all at once; is to create a denormalized table containing all of that data.  Depending on your query needs, you may need to order the PRIMARY KEY different ways, in which case you would need to create as many tables as you have specific queries to serve.</p>

<p>The other alternative, would be to make individual queries and manage them client-side.  Client-side JOINs are considered to be a Cassandra anti-pattern, so I would use that with caution.  It all depends on the needs of your application, and whether you want to spend the majority of your time working on data modeling/management or in processing on the client side.  Honestly, I prefer to keep the client side as simple as I can.</p>
",['table']
29037733,32582517,2015-03-13 16:49:37,Cassandra 2.1: Recursion by nesting UDT's,"<p>I was playing around with the user-defined types and found out you can do something like this:</p>

<pre><code>cqlsh:test&gt; CREATE TYPE ping(time timestamp);
cqlsh:test&gt; CREATE TYPE pong(time timestamp, ping frozen &lt;ping&gt;);
cqlsh:test&gt; ALTER TYPE ping ADD pong frozen &lt;pong&gt;;


cqlsh:test&gt; DESC TYPE ping ;

CREATE TYPE test.ping (
    time timestamp,
    pong frozen&lt;pong&gt;
);

cqlsh:test&gt; DESC TYPE pong ;

CREATE TYPE test.pong (
    time timestamp,
    ping frozen&lt;ping&gt;
);
</code></pre>

<p>Is this relevant for any use case?</p>
",<cassandra><cassandra-2.0>,"<p>Just came across this while working on a closely related <a href=""https://datastax-oss.atlassian.net/browse/JAVA-908"" rel=""nofollow"">Java driver ticket</a>.</p>

<p>The schema appears to be recursive, but that doesn't work when you actually try to insert data:</p>

<pre><code>// (using int instead of time for the sake of clarity)
cqlsh:test&gt; create type ping(pingid int);
cqlsh:test&gt; create type pong(pongid int, ping frozen&lt;ping&gt;);
cqlsh:test&gt; alter type ping ADD pong frozen&lt;pong&gt;;
cqlsh:test&gt; create table foo(ping frozen&lt;ping&gt; primary key);

// These are OK:

cqlsh:test&gt; insert into foo(ping) values( {pingid:1} );

cqlsh:test&gt; insert into foo(ping) values(
    { pingid:1, 
      pong: { pongid:2, 
              ping: {pingid: 3}}} );

// But notice what happens when you nest one more level:

cqlsh:test&gt; insert into foo(ping) values(
    { pingid:1, 
      pong: { pongid:2, 
              ping: {pingid: 3, 
                     pong: {pongid: 4}}}} );
InvalidRequest: code=2200 [Invalid query] message=""Unknown field 'pong' in value of user
defined type ping""
</code></pre>

<p>Looks like the <code>ping</code> that was used at the time <code>pong</code> was defined is a ""copy"" that didn't see the effect of the ALTER statement. So my guess is that recursivity is not allowed and there is a missing check. I'll update my answer when I get confirmation from the Cassandra developers.</p>

<p>One interesting side-effect is that you can delete neither <code>ping</code> nor <code>pong</code> after that :-)</p>

<hr>

<p><strong>EDIT:</strong> this is indeed something that <code>ALTER TYPE</code> should not allow. See <a href=""https://issues.apache.org/jira/browse/CASSANDRA-10339"" rel=""nofollow"">CASSANDRA-10339</a>.</p>
",['table']
29064850,29069437,2015-03-15 18:59:51,Inserting only few columns into cassandra table,"<p>I am new to cassandra, is it possible to insert into only few columns in a table and leaving other columns for future filling?</p>

<p>Thanks in advance</p>
",<cassandra><cassandra-2.0>,"<p>Yes. The syntax of the <a href=""http://www.datastax.com/documentation/cql/3.0/cql/cql_reference/insert_r.html"" rel=""nofollow"">CQL INSERT</a> gives you what you'd expect:</p>

<p><code>
INSERT INTO table (col_1, col_5) VALUES (val1, val5)
</code></p>
",['table']
29078486,29087801,2015-03-16 13:50:29,Cassandra throw NoHostAvailableException,"<p>I am using the following code to connect my .net client (CQL based) to 3 node Cassandra cluster. I am getting the data (from RabbitMQ) 30 records/sec and they get stored in cassandra upto 800-900 rows smoothly. But after that i am getting this follwing exception. Can anyone please tell me what are the optimization/changes i can make to avoid this exception. I could't find specific solution to this problem anywhere.</p>

<p><strong>Error:</strong>   <code>ERROR ErrorLog - error in Cassandra GetCWCRow Function Connection :None of the hosts tried for query are available (tried: X.X.X.201:9042, X.X.X.200:9042, X.X.X.X:9042)</code></p>

<p><strong>Code :</strong></p>

<pre><code>using Cassandra;
using Consumer;
using System;
using System.Collections.Generic;
using System.Configuration;
using System.Data;
using System.Linq;
using System.Text;
using System.Text.RegularExpressions;
using System.Threading.Tasks;



namespace RabbitMqCarWaleUserTracking
{
    class DataAccessCassandra
    {

        public bool InsertCookieLogData(string cwc, string page_uri)
        {
            try
            {
                Logs.WriteInfoLog(""Cassandra InsertCookieLogData a Function called"");
                Cluster cluster = Cluster.Builder().AddContactPoints(ConfigurationManager.AppSettings[""cassandraCluster""].ToString().Split(',')).Build();
                ISession session = cluster.Connect(ConfigurationManager.AppSettings[""cassandraKeySpace""].ToString());
                string pageCategory = string.Empty;
                try
                {

                    if ((Regex.IsMatch(page_uri, ""/newcars/upcomingcars"", RegexOptions.IgnoreCase)))
                    {
                        pageCategory = ""upcomingCars"";
                    }
                    else
                        if ((Regex.IsMatch(page_uri, ""/newcars/dealers/newCarDealerShowroom"", RegexOptions.IgnoreCase)) || (Regex.IsMatch(page_uri, ""/newcars/dealers/listnewcardealersbycity"", RegexOptions.IgnoreCase)
                            || (Regex.IsMatch(page_uri, ""/newcars/dealers/dealerdetails"", RegexOptions.IgnoreCase))))
                        {
                            pageCategory = ""newcarsDealers"";
                        }
                        else
                            if ((Regex.IsMatch(page_uri, ""/offers"", RegexOptions.IgnoreCase)) || (Regex.IsMatch(page_uri, ""/alloffers"", RegexOptions.IgnoreCase)))
                            {
                                pageCategory = ""offers"";
                            }
                            else
                                if ((Regex.IsMatch(page_uri, ""/dealer/testdrive"", RegexOptions.IgnoreCase)))
                                {
                                    pageCategory = ""dealerTestDrive"";
                                }

                    if (pageCategory != string.Empty)
                    {
                        Row result = session.Execute(""select logdate from pageWiseCookieLog where cwc ='"" + cwc + ""' and page_uri ='"" + pageCategory + ""' and logdate= '"" + DateTime.Today.ToString(""yyyy-MM-dd"") + ""'"").FirstOrDefault();
                        if (result == null)
                        {
                            session.Execute(""insert into pageWiseCookieLog (cwc, page_uri, logdate) values ('"" + cwc + ""' , '"" + pageCategory + ""' , '"" + DateTime.Now.ToString(""yyyy-MM-dd"") + ""' )"");
                            session.Execute(""insert into pageWiseCookieLogByld (cwc, page_uri, logdate) values ('"" + cwc + ""' , '"" + pageCategory + ""' , '"" + DateTime.Now.ToString(""yyyy-MM-dd"") + ""' )"");
                            session.Dispose();
                            cluster.Dispose();
                            return true;
                        }
                    }
                    else
                    {
                        //don't want to store the data for rest of the page category but need to return true  
                        session.Dispose();
                        cluster.Dispose();
                        return true;
                    }
                }
                catch (Exception ex)
                {
                    string subject = string.Concat(ex.Source, "" : "", Environment.MachineName);
                    Logs.WriteErrorLog(""error in Cassandra InsertCookieLogData function with cwc :"" + cwc + ""error is :"" + ex.Message);
                    SendMail.HandleException(ex, subject);
                    session.Dispose();
                    cluster.Dispose();
                }
            }
            catch (Exception ex)
            {
                string subject = string.Concat(ex.Source, "" : "", Environment.MachineName);
                Logs.WriteErrorLog(""error in Cassandra InsertCookieLogData function connection :"" + ex.Message);
                SendMail.HandleException(ex, subject);              
            }

            return false;
        }

        public string GetCWCRow(string cwc, int index, string mobileId)
        {
            try
            {
                Logs.WriteInfoLog(""Cassandra GetCWCRow Function called"");
                Cluster cluster = Cluster.Builder().AddContactPoints(ConfigurationManager.AppSettings[""cassandraCluster""].ToString().Split(',')).Build();
                ISession session = cluster.Connect(ConfigurationManager.AppSettings[""cassandraKeySpace""].ToString());
                try
                {
                    Row result = session.Execute(""select cur_visit_id from usertracking where cwc ='"" + cwc + ""'"").FirstOrDefault();

                    if (result != null)
                    {
                        session.Dispose();
                        cluster.Dispose();
                        return result[0].ToString();
                    }

                }
                catch (Exception ex)
                {
                    string subject = string.Concat(ex.Source, "" : "", Environment.MachineName);
                    Logs.WriteErrorLog(""error in Cassandra GetCWCRow function with cwc :"" + cwc + ""error is :"" + ex.Message);
                    SendMail.HandleException(ex, subject);
                    session.Dispose();
                    cluster.Dispose();
                }

            }
            catch (Exception ex)
            {
                string subject = string.Concat(ex.Source, "" : "", Environment.MachineName);
                Logs.WriteErrorLog(""error in Cassandra GetCWCRow Function Connection :"" + ex.Message);
                SendMail.HandleException(ex, subject);
            }

            return string.Empty;
        }

        public bool InsertCWCRecords(string cwv, Cut_Case caseType, int index, string mobileId)
        {
            try
            {
                Logs.WriteInfoLog(""Cassandra InsertCWCRecords function called for case:"" + caseType);
                Cluster cluster = Cluster.Builder().AddContactPoints(ConfigurationManager.AppSettings[""cassandraCluster""].ToString().Split(',')).Build();
                ISession session = cluster.Connect(ConfigurationManager.AppSettings[""cassandraKeySpace""].ToString());
                try
                {

                    bool _isProcessed = false;
                    string visitCount = """";
                    string[] leadParameters = cwv.Split('.');
                    string cwc = leadParameters[0];
                    string visitId = leadParameters[1];
                    string visitStartTime = leadParameters[2];
                    string visitPrevPageTime = leadParameters[3];
                    string visitLastPageTime = leadParameters[4];

                    if (leadParameters.Length == 6)
                    {
                        visitCount = leadParameters[5];
                    }
                    string TOT_TIME_SPENT = (Convert.ToInt64(visitLastPageTime) - Convert.ToInt64(visitPrevPageTime)).ToString();

                    if ((int)caseType == 1) //to enter new cwc data in summary table
                    {
                        session.Execute(""insert into usertracking (cwc, cur_visit_id, cur_visit_last_ts,  tot_page_view, tot_time_spent, tot_visit_count, cur_visit_datetime) values ('"" + cwc + ""' , '"" + visitId + ""' ,"" + visitStartTime + "","" + ""1"" + "","" + TOT_TIME_SPENT + "","" + ""1"" + "", '"" + DateTime.Today.ToString(""yyyy-MM-dd"") + ""' )"");
                        _isProcessed = true;
                    }
                    if ((int)caseType == 2) //if cwc exits and visit id is same
                    {
                        Row result = session.Execute(""select tot_page_view, tot_time_spent from usertracking where cwc ='"" + cwc + ""'"").FirstOrDefault();
                        int page_cnt_val = int.Parse(result[0].ToString()) + 1;
                        Int64 time_spt_val = Int64.Parse(result[1].ToString()) + Convert.ToInt64(visitLastPageTime) - Convert.ToInt64(visitPrevPageTime);
                        session.Execute(""update usertracking SET cur_visit_last_ts = "" + visitLastPageTime + "", tot_page_view = "" + page_cnt_val + "", tot_time_spent = "" + time_spt_val + "" WHERE cwc = '"" + cwc.Trim() + ""'"");
                        _isProcessed = true;
                    }
                    if ((int)caseType == 3) //if cwc exits ans visit id is different
                    {
                        Row result = session.Execute(""select tot_page_view, tot_time_spent, tot_visit_count, cur_visit_last_ts, cur_visit_datetime from usertracking where cwc = '"" + cwc + ""'"").First();
                        int page_cnt_val = int.Parse(result[0].ToString()) + 1;
                        Int64 time_spt_val = Int64.Parse(result[1].ToString()) + Convert.ToInt64(visitLastPageTime) - Convert.ToInt64(visitPrevPageTime);
                        int visit_val = int.Parse(result[2].ToString()) + 1;
                        Int64 prev_visit_ts_val = Int64.Parse(result[3].ToString());
                        String prev_visit_datetime_val = Convert.ToDateTime(result[4].ToString()).ToString(""yyyy-MM-dd"");

                        session.Execute(""update usertracking SET  cur_visit_id = '"" + visitId + ""' , tot_visit_count= "" + visit_val
                            + "" , prev_visit_last_ts= "" + prev_visit_ts_val + "", prev_visit_datetime = '"" + prev_visit_datetime_val
                            + ""' , cur_visit_last_ts = "" + visitLastPageTime
                            + "", tot_page_view = "" + page_cnt_val + "", tot_time_spent = "" + time_spt_val
                            + "", cur_visit_datetime='"" + DateTime.Today.ToString(""yyyy-MM-dd"")
                            + ""' WHERE cwc = '"" + cwc.Trim() + ""'"");
                        _isProcessed = true;
                    }
                    session.Dispose();
                    cluster.Dispose();
                    return _isProcessed;
                }
                catch (Exception ex)
                {
                    string subject = string.Concat(ex.Source, "" : "", Environment.MachineName);
                    Logs.WriteErrorLog(""error in Cassandra InsertCWCRecords function with cwv :"" + cwv + ""error is :"" + ex.Message);
                    SendMail.HandleException(ex, subject);
                    session.Dispose();
                    cluster.Dispose();
                    return false;
                }
            }
            catch (Exception ex)
            {
                string subject = string.Concat(ex.Source, "" : "", Environment.MachineName);
                Logs.WriteErrorLog(""error in Cassandra InsertCWCRecords function connection :"" + ex.Message);
                SendMail.HandleException(ex, subject);
                return false;
            }

        }

        public bool UpdateReferrerTimeSpent(string cwc, int referrerCategoryId, double referrerTimeSpent, int index, string mobileId)
        {
            bool _isUpdated = false;

            try
            {              
                Logs.WriteInfoLog(""Cassandra UpdateReferrerTimeSpent function called"");
                Cluster cluster = Cluster.Builder().AddContactPoints(ConfigurationManager.AppSettings[""cassandraCluster""].ToString().Split(',')).Build();
                ISession session = cluster.Connect(""cw"");

                try
                {
                    Row result = session.Execute(""select time_spent_in_sec from userTimeSpentPage WHERE cwc = '"" + cwc.Trim() + ""' And logdate = '"" + DateTime.Today.ToString(""yyyy-MM-dd"") + ""' And page_category_id ="" + referrerCategoryId).FirstOrDefault();
                    if (result != null)
                    {
                        if (result[0].ToString().Trim() != string.Empty)
                        {

                            Int64 page_time_spent_val = Int64.Parse(result[0].ToString());
                            Int64 tot_time_spt_val = page_time_spent_val + Int64.Parse(referrerTimeSpent.ToString());
                            session.Execute(""update userTimeSpentPage set time_spent_in_sec= "" + tot_time_spt_val + ""WHERE cwc = '"" + cwc.Trim() + ""' And logdate = '"" + DateTime.Today.ToString(""yyyy-MM-dd"") + ""' And page_category_id="" + referrerCategoryId);
                        }
                    }
                    else
                    {
                        session.Execute(""insert into userTimeSpentPage (cwc, page_category_id, time_spent_in_sec, logdate) values ('"" + cwc + ""' ,"" + referrerCategoryId + "","" + referrerTimeSpent + "", '"" + DateTime.Now.ToString(""yyyy-MM-dd"") + ""' )"");
                    }
                    _isUpdated = true;
                    session.Dispose();
                    cluster.Dispose();
                    return _isUpdated;
                }
                catch (Exception ex)
                {
                    string subject = string.Concat(ex.Source, "" : "", Environment.MachineName);
                    Logs.WriteErrorLog(""error in Cassandra UpdateReferrerTimeSpent function with cwc:"" + cwc + ""error is :"" + ex.Message);
                    SendMail.HandleException(ex, subject);
                    session.Dispose();
                    cluster.Dispose();
                    return _isUpdated;
                }
            }
            catch (Exception ex)
            {
                string subject = string.Concat(ex.Source, "" : "", Environment.MachineName);
                Logs.WriteErrorLog(""error in Cassandra UpdateReferrerTimeSpent function connection"" + ex.Message);
                SendMail.HandleException(ex, subject);
                return _isUpdated;
            }

        }
    }
}
</code></pre>

<p><strong>Edited Question:</strong></p>

<pre><code>output of netstat -an | awk '/^tcp/ {print $NF}' | sort | uniq -c | sort -rn

On Machine 1  While cassandra running :
      773 ESTABLISHED
      36 LISTEN
      1 CLOSE_WAIT

After cassandra stopped :
      274 ESTABLISHED
      36 LISTEN
      1 CLOSE_WAIT

Machine 2 while cassandra running :
       3941 ESTABLISHED
       26 LISTEN
       7 CLOSE_WAIT

After cassandra stopped :
       26 LISTEN
       9 ESTABLISHED

On machine 3 while cassandra running :
       500 ESTABLISHED
       21 LISTEN

After cassandra stopped :    
      21 LISTEN
      13 ESTABLISHED
</code></pre>
",<cassandra><cassandra-2.0>,"<p>The NoHostAvailableException can be thrown for many reasons. However this is all about the problem described in the <a href=""http://www.datastax.com/drivers/java/2.0/com/datastax/driver/core/exceptions/NoHostAvailableException.html"" rel=""nofollow"">driver documentation</a>:</p>

<blockquote>
  <p>Exception thrown when a query cannot be performed because no host are
  available. This exception is thrown if </p>
  
  <ul>
  <li>either there is no host live in
  the cluster at the moment of the query </li>
  <li>all host that have been tried
  have failed due to a connection problem</li>
  </ul>
</blockquote>

<p>Now why is this happening - there could be several reasons for this. </p>

<ol>
<li>Possibility of simultaneous major Garbage collection on all 3 nodes. I personally don't think it is the case, but you should definitely read on this and see if it may apply to your case. Here is a link to a very nice documednt describing how to <a href=""http://tech.shift.com/post/74311817513/cassandra-tuning-the-jvm-for-read-heavy-workloads"" rel=""nofollow"">tune GC in Cassandra</a>. The fact that you create cluster and session objects practically for any call instead of storing them as singletons and just reusing them, may make things even worse.</li>
<li>After looking at the function that throws an error, I am more or less convinced that the problem is that after so many inserts, your nodes just timeout while reading the wide row during this statement: <code>Row result = session.Execute(""select cur_visit_id from usertracking where cwc ='"" + cwc + ""'"").FirstOrDefault();</code>. Further down the road you're performing a lot of updates for the same clustering key <code>cws</code> which, due to immutable nature of the SSTables, makes a lot of versioned data, resulting in added data retrieval time, since the cluster needs to combine all of this data for each your request.</li>
</ol>

<p>It is hard to make recommendations without any table schema, and reverse engineering won't help much either, but I would recommend somehow utilizing a composite primary key for faster lookups. Tune your JVMs, and make session and cluster singletons and reuse them in your code. See if this helps.</p>

<p>Read through the Cassandra cluster logs, focusing on the times around when the issues happen. See if any clues are in these logs, like Garbage collection activity, or timeout errors.</p>

<p>HTH</p>

<p>Roman</p>
",['table']
29081027,29081843,2015-03-16 15:43:41,Delete rows vs Delete Columns performance,"<p>I'm creating the datamodel for a timeseries application on Cassandra 2.1.3. We will be preserving X amount of data for each user of the system and I'm wondering what is the best approach to design for this requirement.</p>

<h2>Option1:</h2>

<p>Use a 'bucket' in the partition key, so data for X period goes into the same row. Something like this:</p>

<pre><code>((id, bucket), timestamp) -&gt; data
</code></pre>

<p>I can delete a single row at once at the expense of maintaining this bucket concept. It also limits the range I can query on <code>timestamp</code>, probably resulting in several queries.</p>

<h2>Option2:</h2>

<p>Store all the data in the same row. N deletes are per column.</p>

<pre><code>(id, timestamp) -&gt; data
</code></pre>

<p>Range queries are easy again. But what about performance after many column deletes?</p>

<p>Given that we plan to use TTL to let the data expire, which of the two models would deliver the best performance? Is the tombstone overhead of Option1 &lt;&lt; Option2 or will there be a tombstone per column on both models anyway?</p>

<p>I'm trying to avoid to bury myself in the tombstone graveyard.</p>
",<performance><cassandra><tombstone>,"<p>I think it will all depend on how much data you plan on having for the given partition key you end up choosing, what your TTL is and what queries you are making.</p>

<p>I typically lean towards option #1, especially if your TTL is the same for all writes.  In addition if you are using LeveledCompactionStrategy or DataTieredCompactionStrategy, Cassandra will do a great job keeping data from the same partition in the same SSTable, which will greatly improve read performance. </p>

<p>If you use Option #2, data for the same partition could likely be spread across multiple levels (if using LCS) or just in general multiple sstables, which may cause you to read from a lot of SSTables, depending on the nature of your queries.  There is also the issue of hotspotting, where you could overload particular cassandra nodes if you have a really wide partition.</p>

<p>The other benefit of #1 (which you allude to), is that you can easily delete the entire partition, which creates a single tombstone marker which is much cheaper.  Also, if you are using the same TTL, data within that partition will expire pretty much at the same time.</p>

<p>I do agree that it is a bit of a pain to have to make multiple queries to read across multiple partitions as it pushes some complexity into the application end.  You may also need to maintain a separate table to keep track of the buckets for the given id if they can not be determined implicitly.</p>

<p>As far as performance goes, do you see it as likely that you will need to read cross-partitions when your application makes queries?  For example, if you have a query for 'the most recent 1000 records' and a partition typically is wider than that, you may only need to make 1 query for Option #1.   However, if you want to have a query like 'give me all records', Option #2 may be better as otherwise you'll need to a make queries for each bucket.</p>
",['table']
29087090,29087407,2015-03-16 21:17:41,Connecting to Cassandra with Spark,"<p>First, I have bought the new O'Reilly Spark book and tried those Cassandra setup instructions. I've also found other stackoverflow posts and various posts and guides over the web. None of them work as-is. Below is as far as I could get.</p>

<p>This is a test with only a handful of records of dummy test data. I am running the most recent Cassandra 2.0.7 Virtual Box VM provided by plasetcassandra.org linked from the main Cassandra project page.</p>

<p>I downloaded Spark 1.2.1 source and got the latest Cassandra Connector code from github and built both against Scala 2.11. I have JDK 1.8.0_40 and Scala 2.11.6 setup on Mac OS 10.10.2.</p>

<p>I run the spark shell with the cassandra connector loaded:</p>

<pre><code>bin/spark-shell --driver-class-path ../spark-cassandra-connector/spark-cassandra-connector/target/scala-2.11/spark-cassandra-connector-assembly-1.2.0-SNAPSHOT.jar
</code></pre>

<p>Then I do what should be a simple row count type test on a test table of four records:</p>

<pre><code>import com.datastax.spark.connector._
sc.stop
val conf = new org.apache.spark.SparkConf(true).set(""spark.cassandra.connection.host"", ""192.168.56.101"")
val sc = new org.apache.spark.SparkContext(conf)
val table = sc.cassandraTable(""mykeyspace"", ""playlists"")
table.count
</code></pre>

<p>I get the following error. What is confusing is that it is getting errors trying to find Cassandra at 127.0.0.1, but it also recognizes the host name that I configured which is 192.168.56.101.</p>

<pre><code>15/03/16 15:56:54 INFO Cluster: New Cassandra host /192.168.56.101:9042 added
15/03/16 15:56:54 INFO CassandraConnector: Connected to Cassandra cluster: Cluster on a Stick
15/03/16 15:56:54 ERROR ServerSideTokenRangeSplitter: Failure while fetching splits from Cassandra
java.io.IOException: Failed to open thrift connection to Cassandra at 127.0.0.1:9160
&lt;snip&gt;
java.io.IOException: Failed to fetch splits of TokenRange(0,0,Set(CassandraNode(/127.0.0.1,/127.0.0.1)),None) from all endpoints: CassandraNode(/127.0.0.1,/127.0.0.1)
</code></pre>

<p>BTW, I can also use a configuration file at conf/spark-defaults.conf to do the above without having to close/recreate a spark context or pass in the --driver-clas-path argument. I ultimately hit the same error though, and the above steps seem easier to communicate in this post.</p>

<p>Any ideas?</p>
",<cassandra><apache-spark><cassandra-2.0>,"<p>Check the rpc_address config in your cassandra.yaml file on your cassandra node.  It's likely that the spark connector is using that value from the system.local/system.peers tables and it may be set to 127.0.0.1 in your cassandra.yaml.</p>

<p>The spark connector uses thrift to get token range splits from cassandra.  Eventually I'm betting this will be replaced as C* 2.1.4 has a new table called system.size_estimates (<a href=""https://issues.apache.org/jira/browse/CASSANDRA-7688"" rel=""nofollow"">CASSANDRA-7688</a>).  It looks like it's getting the host metadata to find the nearest host and then making the query using thrift on port 9160.</p>
","['rpc_address', 'table']"
29128575,29131550,2015-03-18 17:18:00,How to save data in cassandra conditionally only if properties did not change,"<p>We have data model of article with lot of properties. Here is our table model:</p>

<pre><code>CREATE TABLE articles (
    organization_id bigint,
    gtin text,
    barcodes text,
    code text,
    brand text,
    season text,
    name text,
    option text,
    style text,
    color text,
    sizes text,
    supplier text,
    category text,
    prices text,
    last_updated timeuuid,
    content_hash uuid,
    markdown boolean,
    PRIMARY KEY (organization_id, gtin)
) WITH COMMENT='Articles';
</code></pre>

<p>Where gtin uniquely identifies article and we save all articles of organization in one row. We have constraint to update each article only if something has changed. This is important since if article is changed, we update <code>last_updated</code> field and external devices know which articles to synchronizes since they have information when they synchronized last time.</p>

<p>We added one more table for that:</p>

<pre><code>CREATE TABLE articles_by_last_updated (
    organization_id bigint,
    gtin text,
    barcodes text,
    code text,
    brand text,
    season text,
    name text,
    option text,
    style text,
    color text,
    sizes text,
    supplier text,
    category text,
    prices text,
    last_updated timeuuid,
    content_hash uuid,
    markdown boolean,
    PRIMARY KEY (organization_id, last_updated)
) WITH CLUSTERING ORDER BY (last_updated ASC) AND COMMENT='Articles by last updated field';
</code></pre>

<p>So we can easily return all articles updated after certain point in time. This table must be cleared from duplicates per gtin since we import articles each day and sync is done from mobile devices so we want to keep dataset small (in theory we could save everything in that table, and overwrite with latest info but that created large datasets between syncs so we started deleting from that table, and to delete we needed to know <code>last_updated</code> from first table)</p>

<p>Problems we are facing right now are:</p>

<ol>
<li>In order to check if article fields are updated we need to do read before write (we partially solved that with <code>content_hash</code> field which is hash over all fields so we read and compare hash of incoming article with value in DB)</li>
<li>We are deleting and inserting in second table since we need unique <code>gtins</code> there (need only latest change to send to devices, not duplicate articles) which produces awful lot of tombstones</li>
<li>We have feature to add to search by many different combinations of fields </li>
</ol>

<p><strong>Questions:</strong>  </p>

<ol>
<li>Is cassandra good choice for this kind of data or we should move it to some other storage (or even have <code>elasticsearch</code> and <code>cassandra</code> in combination where we can index changes after time and cassandra can hold only master data per <code>gtin</code>)?</li>
<li>Can data be modeled better for our use case to avoid read before write or deletes in second table?</li>
</ol>

<p><strong>Update</strong>
Just to clarify use case: other devices are syncing with pagination (sending <code>last_sync_date</code> and <code>skip</code> and <code>count</code>) so we need table with all article information, sorted by <code>last_updated</code> without duplicates and searchable by <code>last_updated</code></p>
",<cassandra><data-modeling><cassandra-2.1><nosql>,"<p>If you are using Cassandra 2.1.1 and later, then you can use the ""not equal"" comparison in the IF part of the UPDATE statement (see <a href=""https://issues.apache.org/jira/browse/CASSANDRA-6839"" rel=""nofollow"">CASSANDRA-6839</a> JIRA issue) to make sure you update data only if anything has changed. Your statement would look something like this:</p>

<pre><code>UPDATE articles 
SET 
  barcodes = &lt;barcodes&gt;, 
  ... = &lt;...&gt;, 
  last_updated = &lt;last_updated&gt; 
WHERE 
  organization_id = &lt;organization_id&gt; 
  AND gtin = &lt;gtin&gt; 
IF content_hash != &lt;content_hash&gt;;
</code></pre>

<p>For your second table, you don't need to duplicate entire data from the first table as you can do the following:</p>

<p>create your table like this:</p>

<pre><code>CREATE TABLE articles_by_last_updated (
    organization_id bigint,
    last_updated timeuuid,
    gtin text,
    PRIMARY KEY (organization_id, last_updated)
) WITH CLUSTERING ORDER BY (last_updated ASC) AND COMMENT='Articles by last updated field';
</code></pre>

<p>Once you've updated the first table, you can read the <code>last_updated</code> value for that <code>gtin</code> again and if it is equal or greater than the <code>last_updated</code> value you passed in, then you know that the update was successful (by your or another process), so you can now go ahead and insert that retrieved <code>last_updated</code> value into the second table. You don't need to delete the records for this update. I assume you can create distinct updated <code>gtin</code> list on the application side, if you do polling (using a range query) on a regular basis, which I assume pulls a reasonable amount of data. You can TTL these new records after a few poll cycles to remove a necessity to do manual deletes for example. Then, after you found the <code>gtins</code> affected, then you do a second query where you pull all of the data from the first table. You can then run a second sanity check on the updated dates to avoid sending anything that is supposed to be sent on the next update (if it is necessary of course).</p>

<p>HTH.</p>
",['table']
29133678,29134982,2015-03-18 22:11:50,"add cassandra node fails with ConfigurationException: For input string: ""None""","<p>I have a dse cluster with 4 nodes already. I am adding the fifth node using opscenter. </p>

<p>I installed datastax-agent on this last node which seems to run properly, then using the opscenter, I proceeded to add the node. It first reports that it is loading new software onto the node, then errors out.</p>

<p>in /var/log/cassandra/system.out I see this error</p>

<p>ERROR [main] 2015-03-18 15:04:27,080 DatabaseDescriptor.java (line 117) Fatal configuration error
org.apache.cassandra.exceptions.ConfigurationException: For input string: ""None""
        at org.apache.cassandra.dht.Murmur3Partitioner$1.validate(Murmur3Partitioner.java:178)
        at org.apache.cassandra.config.DatabaseDescriptor.applyConfig(DatabaseDescriptor.java:447)</p>

<p>I can't find out where this configuration comes from and how to fix it. </p>

<p>Can someone help?</p>
",<cassandra><opscenter>,"<p>Sounds like the Murmur partitioner did not like the token value from your configuration. Exception is thrown out of <a href=""http://grepcode.com/file/repo1.maven.org/maven2/org.apache.cassandra/cassandra-all/2.0.12/org/apache/cassandra/dht/Murmur3Partitioner.java?av=f"" rel=""nofollow"">Murmur3Partitioner.java</a></p>

<pre><code>170       public void validate(String token) throws ConfigurationException
171        {
172            try
173            {
174                Long i = Long.valueOf(token);
175            }
176            catch (NumberFormatException e)
177            {
178                throw new ConfigurationException(e.getMessage());
179            }
180        }
</code></pre>

<p>Check what that value is in your cassandra.yaml. I suspect you should see ""None"" in place for the token value.</p>
",['partitioner']
29146583,29147815,2015-03-19 13:58:33,CQL with a wide row - how to get most recent set?,"<p>How would I write the CQL to get the most recent set of data from each row?</p>

<p>I'm investigating transitioning from MSSQL to Cassandra and am starting to grasp the concepts. Lots of research has help tremendously, but I haven't found answer to this (I know there must be a way):</p>

<pre><code>CREATE TABLE WideData {
 ID text,
 Updated timestamp,
 Title text,
 ReportData text,
 PRIMARY KEY (ID, Updated)
} WITH CLUSTERING ORDER (Updated DESC) 

INSERT INTO WideData (ID, Updated, Title, ReportData) VALUES ('aaa', NOW, 'Title', 'Blah blah blah blah')
INSERT INTO WideData (ID, Updated, Title, ReportData) VALUES ('bbb', NOW, 'Title', 'Blah blah blah blah')
</code></pre>

<p>wait 1 minute:</p>

<pre><code>INSERT INTO WideData (ID, Updated, Title, ReportData) VALUES ('bbb', NOW, 'Title 2', 'Blah blah blah blah')
</code></pre>

<p>wait 3 minutes:</p>

<pre><code>INSERT INTO WideData (ID, Updated, Title, ReportData) VALUES ('aaa', NOW, 'Title 2', 'Blah blah blah blah')
</code></pre>

<p>wait 5 minutes:</p>

<pre><code>INSERT INTO WideData (ID, Updated, Title, ReportData) VALUES ('aaa', NOW, 'Title 3', 'Blah blah blah blah')
</code></pre>

<p>How would I write the CQL to get the most recent set of data from each row?</p>

<p>SELECT ID, Title FROM WideRow - gives me 5 rows, as it pivots the data for me.</p>

<p>Essentially I want the results for (SELECT ID, Title FROM WideRow WHERE .....) to be:</p>

<pre><code>ID   Title
aaa, Title3
bbb, Title2
</code></pre>

<p>Also, is there a way to get a count of the number of data sets in a wide row?</p>

<p>Essentially the equivalent of TSQL: SELECT ID, Count(*) FROM Table GROUP BY ID</p>

<pre><code>ID   Count
aaa  3
bbb  2
</code></pre>

<p>Thanks</p>

<p>Also, any references to learn more about these types of queries would also be appreciated.</p>
",<cassandra><cql><cql3><cassandra-2.1>,"<p>With your current data model, you can only query the most-recent row by partition key.  In your case, that is <code>ID</code>.</p>

<pre><code>SELECT ID, Title FROM WideData WHERE ID='aaa' LIMIT 1
</code></pre>

<p>Since you have indicated your clustering order on <code>Updated</code> in DESCending order, the row with the most-recent <code>Updated</code> timestamp will be returned first.</p>

<p>Given your desired results, I'll go ahead and assume that you do not want to query each partition key individually.  Cassandra only maintains CQL result set order by partition key.  Also Cassandra does not support aggregation.  So there really is no way to get the ""most recent"" for all of your <code>ID</code>s together at once, nor is there a way to get a report of how many updates each <code>ID</code> has.</p>

<p>With Cassandra data modeling, you need to build your tables to suit your queries.  Query ""planning"" is not really a strong point of Cassandra (as you are finding out).  To get the most-recent updates by <code>ID</code>, you would need to build an additional query table designed to store only the most-recent update for each ID.  Likewise, to get the count of updates for each <code>ID</code> you could create an additonal query table using <a href=""http://www.datastax.com/documentation/cql/3.1/cql/cql_using/use_counter_t.html"" rel=""nofollow"">counter coulmns</a> to suit that query.</p>

<p><strong>tl;dr</strong></p>

<p>In Cassandra, denormalization and redundant data storage is the key.  For some applications, you might have one table for each query you need to support...and that's ok.</p>
",['table']
29166566,29166684,2015-03-20 12:31:00,Cassandra - join two tables and save result to new table,"<p>I am working on a self-bi application where users can upload their own datasets which are stored in Cassandra tables that are created dynamically. The data is extracted from files that the user can upload. So, each dataset is written into its own Cassandra table modeled based on column headers in the uploaded file while indexing the dimensions.</p>

<p>Once the data is uploaded, the users are allowed to build reports, analyze, etc., from within the application. I need a way to allow users to merge/join data from two or more datasets/tables based on matching keys and write the result into a new Cassandra table. Once a dataset/table is created, it will stay immutable and data is only read from it.</p>

<p><strong>user table 1</strong></p>

<ol>
<li>username</li>
<li>email</li>
<li>employee id</li>
</ol>

<p><strong>user table 2</strong></p>

<ol>
<li>employee id</li>
<li>manager</li>
</ol>

<p>I need to merge data in <em>user table 1</em> and <em>user table 2</em> on matching <em>employee id</em> and write to <em>new table</em> that is created dynamically.</p>

<p><strong>new table</strong></p>

<ol>
<li>username</li>
<li>email</li>
<li>employee id</li>
<li>manager</li>
</ol>

<p>What would be the best way to do this?</p>
",<cassandra><cql3><datastax-java-driver>,"<p>The only option that you have is to do the join in your application code. There are just few details to suggest a proper solution.</p>

<p>Please add details about table keys, usage patterns... in general, in cassandra you model from usage point of view, i.e. starting with queries that you'll execute on data.</p>

<p>In order to merge 2 tables on this pattern, you have to do it into application, creating the third table (target table ) and fill it with data from both tables. You have to make sure that you read the data in pages to not OOM, it really depends on size of the data. </p>

<p>Another alternative is to build the joins into Spark, but maybe is too over-engineering in your case.</p>
",['table']
29197007,29199706,2015-03-22 16:39:52,How do I change location of Cassandra storage files?,"<p>I am running a single node test instance of Apache Cassandra. I would like to change the location of where Cassandra stores its files to an external disk.</p>

<p>How can I change it?</p>
",<database><cassandra><cassandra-2.0>,"<p>You should have a good look at the main <strong>cassandra.yaml</strong> configuration file. Actually, it's good practice to try and understand as much of it as you can, since it reflects Cassandra's model and inner workings nicely.</p>

<p>You want to change this parameter (<a href=""https://www.datastax.com/documentation/cassandra/2.1/cassandra/configuration/configCassandra_yaml_r.html?scroll=reference_ds_qfg_n1r_1k__data_file_directories"" rel=""noreferrer"">as per the documentation</a>):</p>

<pre><code>data_file_directories¶
    (Default: /var/lib/cassandra/data ) The directory location where table data
    (SSTables) is stored.
</code></pre>
",['table']
29232121,29240677,2015-03-24 11:59:55,Cassandra (CQL) schema/tables look the same as RDBMS for my scenario,"<p>I have looked at the Twissandra examples. I asked a similar question regarding this a few days back and received some tips I implemented here. However, by looking at the tables (column families) I see barely any difference between this and a relational database.</p>

<p>My scenario:
A simple address book where a user can create his own contacts and group them (one contact can be placed in many groups, one group can contain many contacts). A contact may have multiple addresses for example.</p>

<p>I want to retrieve all the contacts who live in address x and are placed in group y. Therefore, I did the following:</p>

<pre><code>CREATE TABLE if not exists User (user_id uuid, contact_id uuid, type varchar, email varchar, PRIMARY KEY(id));
CREATE TABLE if not exists Contact (contact_id uuid, firstname varchar,lastname varchar, photo blob, imagelength int, note varchar, PRIMARY KEY (id));
CREATE TABLE if not exists Address (address_id uuid, contact_id uuid, street varchar, number int, zipcode varchar, country varchar, PRIMARY KEY(address_id));
CREATE TABLE if not exists Group (group_id uuid, user_id, groupname varchar, PRIMARY KEY(group_id));
CREATE TABLE if not exists Group_Contact (group_id uuid, contact_id, PRIMARY KEY(id, contact_id));
</code></pre>

<p>However, based on this, this is literally exactly the same as a relational database, well, except that I believe Cassandra is putting this data in a different way than a RDBMS on disk. I don't see how this can be made better in Cassandra and whether I even modeled this the right way. It just feels as a plain relational database.
I feel that I did something wrong since I <strong><em>have</em></strong> to use application level joins to get the address of the contacts. I really don't know how I can de-normalize this to allow multiple addresses (and maybe even phones, emails).</p>

<p>Any suggestions to improve this scenario would be greatly appreciated!</p>
",<database><cassandra><database-schema><cql><schema-design>,"<p>As jny indicated, data duplication, denormalization and query-based modeling are keys to building good Cassandra data models.  If I wanted to take your tables above, and build a table to support address/contact queries based-on country, I could do it like this:</p>

<p>First, I'll create a <a href=""https://www.datastax.com/documentation/cql/3.1/cql/cql_using/cqlUseUDT.html"" rel=""nofollow"">user defined type</a> for the contact's address.</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; CREATE TYPE contactAddress (
             ...   street varchar, 
             ...   city varchar,
             ...   zip_code varchar,
             ...   country varchar);
</code></pre>

<p>Next, I'll create a table called <code>UserContactsByCountry</code> to store user contact info, as well as any user contact addresses:</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; CREATE TABLE UserContactsByCountry (
             ...   country varchar,
             ...   user_id uuid,
             ...   type varchar,
             ...   email varchar,
             ...   firstname varchar,
             ...   lastname varchar,
             ...   photo blob,
             ...   imagelength int,
             ...   note varchar,
             ...   addresses map&lt;text, frozen &lt;contactAddress&gt;&gt;,
             ...   PRIMARY KEY ((country),user_id));
</code></pre>

<p>A couple of things to note here:</p>

<ul>
<li>I am using <code>country</code> as a partitioning key for querying, and addding <code>user_id</code> as a clustering key for uniqueness.</li>
<li>Technically, <code>country</code> is being stored multiple in each row.  Once as the partiiton key, and again with each address.  Note that the <code>country</code> partition key is the one which allows us to run our query.</li>
<li>I assume that user contacts can have multiple addresses, so I'll store them in a map of type text (varchar), contactAddress (type created above).</li>
</ul>

<p>Next, I'll insert three user contacts, each with two addresses, two from the USA and one from Great Britain.</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; INSERT INTO usercontactsbycountry (country, user_id, type, email, firstname, lastname, note, addresses)
VALUES ('USA',uuid(),'Tech','brycelynch@network23.com','Bryce','Lynch','Head of R&amp;D at Network 23',{'work':{street:'101 Big Network Drive',city:'New York',zip_code:'10023',country:'USA'},'home':{street:'8192 N. 42nd St.',city:'New York',zip_code:'10025',country:'USA'}});
aploetz@cqlsh:stackoverflow&gt; INSERT INTO usercontactsbycountry (country, user_id, type, email, firstname, lastname, note, addresses)
VALUES ('USA',uuid(),'Reporter','edisoncarter@network23.com','Edison','Carter','Reporter at Network 23',{'work':{street:'101 Big Network Drive',city:'New York',zip_code:'10023',country:'USA'},'home':{street:'76534 N. 62nd St.',city:'New York',zip_code:'10024',country:'USA'}});
aploetz@cqlsh:stackoverflow&gt; INSERT INTO usercontactsbycountry (country, user_id, type, email, firstname, lastname, note, addresses)
VALUES ('GBR',uuid(),'Reporter','theorajones@network23.com','Theora','Jones','Controller at Network 23',{'work':{street:'101 Big Network Drive',city:'New York',zip_code:'10023',country:'USA'},'home':{street:'821 Wembley St.',city:'London',zip_code:'W11 2BQ',country:'GBR'}});
</code></pre>

<p>Now I can query that table for all user contacts in the USA:</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; SELECT * FROM usercontactsbycountry WHERE country ='USA';
 country | user_id                              | addresses                                                                                                                                                                                    | email                      | firstname | imagelength | lastname | note                      | photo | type
---------+--------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------+-----------+-------------+----------+---------------------------+-------+----------
     USA | 2dee94e2-4887-4988-8cf5-9aee5fd0ea1e |  {'home': {street: '8192 N. 42nd St.', city: 'New York', zip_code: '10025', country: 'USA'}, 'work': {street: '101 Big Network Drive', city: 'New York', zip_code: '10023', country: 'USA'}} |   brycelynch@network23.com |     Bryce |        null |    Lynch | Head of R&amp;D at Network 23 |  null |     Tech
     USA | b92612dd-dbaa-42f2-8ff2-d36b6c601aeb | {'home': {street: '76534 N. 62nd St.', city: 'New York', zip_code: '10024', country: 'USA'}, 'work': {street: '101 Big Network Drive', city: 'New York', zip_code: '10023', country: 'USA'}} | edisoncarter@network23.com |    Edison |        null |   Carter |    Reporter at Network 23 |  null | Reporter

(2 rows)
</code></pre>

<p>There are probably other ways in which this could be modeled, but this is one that I hoped to use to help you understand some of the techniques available.</p>
",['table']
29258500,29259492,2015-03-25 14:29:11,Is it possible to keep tomb stones after tables compaction?,"<p>I understand that entries get marked with tombstones when a deletion in requested in C*. This way, a soft deletion is performed and it is made effective <a href=""http://www.datastax.com/docs/1.1/operations/tuning#tuning-compaction"" rel=""nofollow"">during compaction</a>:</p>

<blockquote>
  <p>In addition to consolidating SSTables, the compaction process merges
  keys, combines columns, <strong>discards tombstones</strong>, and creates a new index
  in the merged SSTable.</p>
</blockquote>

<p>Is it possible to avoid tombstones discard in order to keep them stored for ever? I know that would be against efficiency. I just wonder if it is possible.</p>
",<cassandra><soft-delete>,"<p>You can use a higher <code>gc_grace_seconds</code> value in your table properties to increase the time until tombstones will be effectively deleted. </p>
",['table']
29272691,29274182,2015-03-26 06:58:29,Get current date in cassandra cql select,"<p>In SQL, I am able to do:</p>

<pre><code>select getdate(), getdate() - 7
</code></pre>

<p>Which returns the current date as well as current date - 7 days. I want to achieve the same in Cassandra CQL. I tried:</p>

<pre><code>select dateof(now())
</code></pre>

<p>But that does not work. It works only on insert and not in select. How can I get the same? Any help would be appreciated.</p>
",<cassandra><cql><cql3>,"<pre><code>select dateof(now())
</code></pre>

<p>On its own, you are correct, that does not work.  But if you have a table that you know only has one row (like <code>system.local</code>):</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; SELECT dateof(now()) FROM system.local ;

 dateof(now())
--------------------------
 2015-03-26 03:18:39-0500

(1 rows)
</code></pre>

<p>Unfortunately, Cassandra CQL does not (yet? <a href=""https://issues.apache.org/jira/browse/CASSANDRA-5505"" rel=""noreferrer"">CASSANDRA-5505</a>) include support for arithmetic operations, let alone date arithmetic.  So subtracting 7 days from that value is something that you would have to do in your application level.</p>

<p><strong>Edit 20200422</strong></p>

<p>The newer syntax uses the <code>toTimestamp()</code> function instead:</p>

<pre><code>aploetz@cqlsh&gt; SELECT toTimestamp(now()) FROM system.local;

 system.totimestamp(system.now())
----------------------------------
  2020-04-22 13:22:04.752000+0000

(1 rows)
</code></pre>

<p>Both syntaxes work as of 20200422.</p>
",['table']
29277096,29280756,2015-03-26 11:18:59,Can I do a multi query on collections in Cassandra 2.1?,"<p>IN Cassandra 2.1, we can query on collections by creating a secondary index on the column.</p>

<pre><code>cqlsh:play&gt; select * from songs where tags contains 't1';

 id                                   | tags         | title

--------------------------------------+--------------+-------

 e99f8f30-d212-11e4-bc9e-5d1b1922b94d | {'t1', 't2'} | Song1
</code></pre>

<p>But I want to query on multiple values - like this:</p>

<pre><code>select * from songs where tags contains 't1|t2';
</code></pre>

<p>Is this possible?</p>
",<cassandra><cql>,"<blockquote>
  <p>Is this possible?</p>
</blockquote>

<p>Sort of, yes.  Should you do it?  No, not really.  Let me explain...</p>

<p>While Carlo is correct in that CQL does not support <code>OR</code>, this can be made to work with <code>AND</code>.  That is, you want to query on the presence of both tags, you can do this:</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; SELECT * FROM songs
  WHERE tags CONTAINS 't2' AND tags CONTAINS 't1' ALLOW FILTERING;

 id                                   | tags         | title
--------------------------------------+--------------+-------
 75e46eb2-292a-42d0-8330-510fb35c635b | {'t1', 't2'} | Song1

(1 rows)
</code></pre>

<p>While this <em>technically</em> works, it is a <strong>TERRIBLE IDEA</strong></p>

<ul>
<li>Multi-key querying has been identified as an anti-pattern.  Using simultaneous, asynchronous queries is typically faster than using <code>IN</code> or <code>CONTAINS</code> to bring back rows for multiple keys.  DataStax has a section of the <code>SELECT</code> documentation titled <a href=""http://www.datastax.com/documentation/cql/3.1/cql/cql_reference/select_r.html?scroll=reference_ds_d35_v2q_xj__selectInNot"" rel=""nofollow"">When Not To Use In</a> that you should read through.</li>
<li>Secondary indexes do not perform well, and it would be only logical that secondary indexes on collections would perform even worse than their single-valued counterparts.  In fact the documentation has a whole section on <a href=""http://www.datastax.com/documentation/cql/3.1/cql/ddl/ddl_when_use_index_c.html"" rel=""nofollow"">When Not To Use An Index</a> that you should really read before using them.</li>
<li>To make the <code>AND</code> operator work on the collection twice, <code>ALLOW FILTERING</code> is required.  <code>ALLOW FILTERING</code> essentially brings back every row (from every node) you have, and <em>then</em> filters the results.  If you have a large dataset and/or several nodes, you should <strong>never</strong> use a query that requires <code>ALLOW FILTERING</code> to complete.</li>
</ul>

<p>The <em>right</em> way to do this, is to build an additional query table with <code>tag</code> as the partition key (and <code>id</code> as a clustering key for uniqueness).</p>

<pre><code>CREATE TABLE songsByTag (
  tag text,
  title text,
  id uuid,
  PRIMARY KEY ((tag),id));
</code></pre>

<p>This will allow you to query songs by a particular tag, without requiring a secondary index.  And while that would allow you to then use <code>IN</code> (which is essentially an <code>OR</code>), multiple, asynchronous queries for each key (tag) would still be faster.</p>
",['table']
29320986,29321157,2015-03-28 18:32:05,java.util.NoSuchElementException: Column not found ID in table demo.usertable in Cassandra Spark,"<p>I am try to write RDD[CassandraRow] into existing Cassandra Table using Spark-cassandra-Connector. Here is my piece of code</p>

<pre><code>val conf = new SparkConf().setAppName(getClass.getSimpleName)
            .setMaster(""local[*]"")
            .set(""spark.cassandra.connection.host"", host)
        val sc = new SparkContext(""local[*]"", keySpace, conf)
val rdd = sc.textFile(""hdfs://hdfs-host:8020/Users.csv"")
val columns = Array(""ID"", ""FirstName"", ""LastName"", ""Email"", ""Country"")
val types = Array(""int"", ""string"", ""string"", ""string"", ""string"")
val crdd=rdd.map(p =&gt; {
            var tokens = p.split("","")
            new CassandraRow(columns,tokens)
        })
val targetedColumns = SomeColumns.seqToSomeColumns(columns)
crdd.saveToCassandra(keySpace, tableName, targetedColumns,  WriteConf.fromSparkConf(conf))
</code></pre>

<p>When I run this code I get following exception</p>

<pre><code>Exception in thread ""main"" java.util.NoSuchElementException: Column not found ID in table demo.usertable
</code></pre>

<p>here is actual schema of table </p>

<pre><code>CREATE TABLE usertable (
  id int,
  country text,
  email text,
  firstname text,
  lastname text,
  PRIMARY KEY ((id))
)
</code></pre>

<p>Any suggestion?
Thanks</p>
",<scala><cassandra><apache-spark>,"<p>Keyspace, table and column names are case sensitive in Cassandra.  Have you tried adjusting your code to use the same case as the table definition? (For example use 'id' instead of 'ID').</p>
",['table']
29365074,29374576,2015-03-31 09:23:49,Real time analytic using Apache Spark,"<p>I am using Apache Spark to analyse the data from Cassandra and will insert the data back into Cassandra by designing new tables in Cassandra as per our queries. I want to know that whether it is possible for spark to analyze in real time? If yes then how? I have read so many tutorials regarding this, but found nothing.</p>

<p>I want to perform the analysis and insert into Cassandra whenever a data comes into my table instantaneously. </p>
",<java><cassandra><apache-spark><bigdata><cql3>,"<p>This is possible with Spark Streaming, you should take a look at the demos and documentation which comes packaged with the Spark Cassandra Connector. </p>

<p><a href=""https://github.com/datastax/spark-cassandra-connector"" rel=""nofollow"">https://github.com/datastax/spark-cassandra-connector</a></p>

<p>This includes support for streaming, as well as support for creating new tables on the fly.</p>

<p><a href=""https://github.com/datastax/spark-cassandra-connector/blob/master/doc/8_streaming.md"" rel=""nofollow"">https://github.com/datastax/spark-cassandra-connector/blob/master/doc/8_streaming.md</a></p>

<blockquote>
  <p>Spark Streaming extends the core API to allow high-throughput,
  fault-tolerant stream processing of live data streams. Data can be
  ingested from many sources such as Akka, Kafka, Flume, Twitter,
  ZeroMQ, TCP sockets, etc. Results can be stored in Cassandra.</p>
</blockquote>

<p><a href=""https://github.com/datastax/spark-cassandra-connector/blob/master/doc/5_saving.md#saving-rdds-as-new-tables"" rel=""nofollow"">https://github.com/datastax/spark-cassandra-connector/blob/master/doc/5_saving.md#saving-rdds-as-new-tables</a></p>

<blockquote>
  <p>Use saveAsCassandraTable method to automatically create a new table
  with given name and save the RDD into it. The keyspace you're saving
  to must exist. The following code will create a new table words_new in
  keyspace test with columns word and count, where word becomes a
  primary key:</p>
  
  <p>case class WordCount(word: String, count: Long) val collection =
  sc.parallelize(Seq(WordCount(""dog"", 50), WordCount(""cow"", 60)))
  collection.saveAsCassandraTable(""test"", ""words_new"",
  SomeColumns(""word"", ""count""))</p>
</blockquote>
",['table']
29365747,29488052,2015-03-31 09:56:29,Cassandra- Data modelling for UserProfilie,"<p>I've a user model, having attributes as follow:-</p>

<pre><code>class User(Model):
    user_id = columns.Integer(primary_key=True)
    username = columns.Text()
    email = columns.Text()
    fname = columns.Text()
    lname = columns.Text()
    age = columns.Text()
    state = columns.Text()
    city = columns.Text()
    country = columns.Text()
    gender = columns.Text()
    phone = columns.Text()
    school_name = columns.Text()
    created_at = columns.Text()
    race = columns.boolean()
</code></pre>

<p>This is my normal RDBMS model. My queries are as follow:-</p>

<pre><code>1) Get all users with city = 'something'

2) Get a user with email = 'something' 

3) Get a user with username = 'something' 

4) Get all users with phones IN ('something' )

5) Get all users with state = 'something' 

6) Get all users with age &gt; something

7) Get all users with gender = 'something' 

8) Get all users with race = 'something' 

9) Get count(*),school_name users Group By schoolname

10) Get all users with created_date &gt; 'something' LIMIT 1000

11) Get all users with username IN ('something') AND age IN ('something') AND phone IN ('something') AND state IN ('something') AND so on  LIMIT 1000
</code></pre>

<p>I can get the above results for queries with a simple Select queries in RDBMS, but the problem lies in Cassandra.</p>

<p>Since, to get the result for the above queries in Cassandra, it is recommended to have a different model per query, which will speed up the reading capability.  In this day and age disk is WAY cheaper than it used to be. That being said, I understand that it isn't always easy to just throw more disk at a problem. The bigger problem I see is adjusting the DAO layer of your application to keep 10 different tables in-sync. (Also, my inner instinct is not convinced to have 10 models for different queries. :P )</p>

<p>Can please someone explain me the proper model in Cassandra to get the result for these queries?</p>

<p>PS: The actions on the above model can be Read/Write/Update/Delete. <strong>Query 11</strong> is the most important query.</p>

<p>The most important is to make these queries really fast on large amounts of data, considering that the information about a particular user can be  updated.</p>
",<cassandra>,"<p>You are facing a real Cassandra limitation: if you are sure to go with Cassandra you need to follow the ""Cassandra rules"". Among these there are</p>

<ul>
<li>Denormalize</li>
<li>Choose indexes wisely</li>
</ul>

<p>So let's start. Each user should have unique id, username, email and phone. This means that these columns are not good candidate for indexing (<a href=""http://docs.datastax.com/en/cql/3.1/cql/ddl/ddl_when_use_index_c.html"" rel=""nofollow"">read here why</a>), so denormalization is the right way.</p>

<p>From your queries you will have a user_by_username, user_by_email and user_by_phones. You might think that repeating data each time can be onerous in terms of updating and disk usage: so you can have a compromise by creating each of these containing as value only the ID of the user. e.g:</p>

<pre><code> user_email     | user_id 
--------------+-------------------
 some@thing.com | 123-456-7aa |    
 girl@hello.org | efg-123-ghi | 
</code></pre>

<p>In another table inside the KS you need a table that by id will retrieve all informations concerning the user. This will solve the <code>update problem</code>, if you need to update an email address or a phone you can update only couples of tables instead of N. The dark side is that you have to perform two queries to have your data.</p>

<p>Let's go on.</p>

<p><code>state</code>, <code>gender</code> and <code>race</code> are good candidate for being indexed for the following reasons:</p>

<ol>
<li><em>Low Cardinality</em></li>
<li><em>Many rows will contain these values</em></li>
</ol>

<p>By indexing you will solve some other queries. The hardest part are the queries like </p>

<pre><code>select * from users where age &gt; xyz
</code></pre>

<p>This kind of query is not allowed in Cassandra since you need to perform the <em>""!equals""</em> operations on clustering part. To do this you need to ""organize"" users by some kind of common key: like the state or a ""state-group"" -- this means that to know all users with certain age you will have to query for each partition.</p>

<p>Take care: I am not providing a solution and this is not my goal -- what I'm trying to do is to provide an approach to solve this problem with Cassandra.</p>

<p>HTH,<br/>
Carlo</p>
",['table']
29405069,29413484,2015-04-02 04:39:04,Cassandra time range query,"<p>Before you downvote I would like to state that I looked at all of the similar questions but I am still getting the dreaded ""PRIMARY KEY column cannot be restricted"" error.</p>

<p>Here's my table structure:</p>

<pre><code>CREATE TABLE IF NOT EXISTS events (
    id text,
    name text,
    start_time timestamp,
    end_time timestamp,
    parameters blob,
    PRIMARY KEY (id, name, start_time, end_time)
);
</code></pre>

<p>And here's the query I am trying to execute:</p>

<pre><code>SELECT * FROM events WHERE name = ? AND start_time &gt;= ? AND end_time &lt;= ?;
</code></pre>

<p>I am really stuck at this. Can anyone tell me what I am doing wrong?</p>

<p>Thanks,
Deniz</p>
",<cassandra><cql><cql3>,"<p>This is a query you need to remodel your data for, or use a distributed analytics platform (like spark). Id describes how your data is distributed through the database. Since it is not specified in this query a full table scan will be required to determine the necessary rows. The Cassandra design team has decided that they would rather you not do a query at all rather than do a query which will not scale. </p>

<p>Basically whenever you see ""COLUMN cannot be restricted"" It means that the query you have tried to perform cannot be done efficiently on the table you created.</p>
",['table']
29459962,29461298,2015-04-05 17:14:29,secondary indexes for low cardinality columns cassandra,"<p>we have a table with 15 million records, and ours is a 10 node cassandra cluster. We have a column which has close to 20 repeatable values. Is it advisable to build secondary index on this column?</p>
",<cassandra><cql><datastax-java-driver><cql3>,"<p>Assuming completely uniform distribution on that column, then each column value would map to 750,000 rows.  Now while the DataStax doc on <a href=""http://docs.datastax.com/en/cql/3.1/cql/ddl/ddl_when_use_index_c.html"">When To Use An Index</a> states that...</p>

<blockquote>
  <p>built-in indexes are best on a table having many rows that contain the indexed value.</p>
</blockquote>

<p>750,000 rows certainly qualifies as ""many.""  But even given that, remember that you're also talking about 14,250,000 rows that Cassandra has to <em>ignore</em> when fulfilling your query.</p>

<p>Also, unless you have a RF of 10 (and I doubt that you would with 10 nodes), you are going to incur network time as Cassandra works between all of the different nodes required to fulfill your query.  For 750,000 rows, that's probably going to timeout.</p>

<p>The only way I think this could be efficient, would be to first restrict your query by a partition key.  Using the secondary index while also restricting with a partition key will help Cassandra find your rows more quickly.  Even so, with a dataset that big, I would re-evaluate your data model and try to figure out a different table to fulfill that query without requiring a secondary index.</p>
",['table']
29473747,29474345,2015-04-06 14:58:23,Cassandra query on Map - Contains Clause,"<p>I am trying to query a table containing Map. Is it possible to apply contains clause on map data type table?</p>

<pre><code>CREATE TABLE data.Table1 (
   fetchDataMap map&lt;text, frozen&lt;Config&gt;&gt;,
   userId text ,
   PRIMARY KEY(userId)
);
</code></pre>

<p>Getting following Error:</p>

<pre><code>cqlsh&gt; SELECT * FROM data.Table1 WHERE fetchDataMap CONTAINS '233322554843924';
InvalidRequest: code=2200 [Invalid query] message=""No secondary indexes on
    the restricted columns support the provided operators: ""
</code></pre>

<p>Please enlighten me with better query approach on this requirement.</p>
",<cassandra><cql>,"<p>For this to work, you have to create a secondary index on the map.  But, you first have to ask yourself if you want to index your map keys or values (cannot do both).  Given your CQL statement, I'll assume that you want to index your map key (and we'll go from there).</p>

<pre><code>CREATE INDEX table1_fetchMapKey ON table1(KEYS(fetchDataMap));
</code></pre>

<p>After inserting some data (making a guess as to what your <code>Config</code> UDT looks like), I can SELECT with a slightly modified version of your CQL query above:</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; SELECT * FROm table1 WHERE
    fetchDataMap CONTAINS KEY '233322554843924';

 userid | fetchdatamap
--------+------------------------------------------------------------
 B26354 | {'233322554843924': {key: 'location', value: '~/scripts'}}

(1 rows)
</code></pre>

<p>Note that I cannot in good conscience provide you with this solution, without passing along a link to the DataStax doc <a href=""http://docs.datastax.com/en/cql/3.1/cql/ddl/ddl_when_use_index_c.html"" rel=""nofollow"">When To Use An Index</a>.  Secondary indexes are known to not perform well.  So I can only imagine that a secondary index on a <em>collection</em> would perform worse, but I suppose that really depends on the relative cardinality.  If it were me, I would re-model my table to avoid using a secondary index, if at all possible.</p>
",['table']
29502979,29546956,2015-04-07 23:08:53,Backups folder in Opscenter keyspace growing really huge,"<p>We have a 10 node Cassandra cluster. We configured a repair in Opscenter. We find there is a backups folder created for every table in Opscenter keyspace. It keeps growing huge. Is there a solution to this, or do we manually delete the data in each backups folder?</p>
",<cassandra><cql><datastax><datastax-enterprise><opscenter>,"<p>First off, Backups are different from snapshots - you can take a look at the backup <a href=""http://www.datastax.com/2015/03/datastax-opscenter-5-1-real-world-test-simulation-of-visual-backup-and-restore-services"" rel=""nofollow noreferrer"">documentation</a> for OpsCenter to learn more.</p>
<h2>Incremental backups:</h2>
<p>From the datastax <a href=""http://docs.datastax.com/en/cassandra/2.0/cassandra/operations/ops_backup_incremental_t.html"" rel=""nofollow noreferrer"">docs</a> -</p>
<blockquote>
<p>When incremental backups are enabled (disabled by default), Cassandra
hard-links each flushed SSTable to a backups directory under the
keyspace data directory. This allows storing backups offsite without
transferring entire snapshots. Also, incremental backups combine with
snapshots to provide a dependable, up-to-date backup mechanism.
...
As with snapshots, Cassandra does not automatically clear
incremental backup files. DataStax recommends setting up a process to
clear incremental backup hard-links each time a new snapshot is
created.</p>
</blockquote>
<p>You must have turned on incremental backups by setting incremental_backups to true in cassandra yaml.</p>
<p>If you are interested in a backup strategy, I recommend you use <a href=""http://docs.datastax.com/en/opscenter/5.1/opsc/online_help/services/opscBackupCluster.html"" rel=""nofollow noreferrer"">the OpsCenter Backup Service</a> instead. That way, you're able to control granularly which keyspace you want to back up and push your files to S3.</p>
<h2>Snapshots</h2>
<p>Snapshots are hardlinks to old (no longer used) SSTables. Snapshots protect you from yourself. For example you accidentally truncate the wrong keyspace, you'll still have a snapshot for that table that you can bring back. There are some cases when you have too many snapshots, there's a couple of things you can do:</p>
<h3>Don't run Sync repairs</h3>
<p>This is related to repairs because synchronous repairs generate a Snapshot each time they run. In order to avoid this, you should run parallel repairs instead (-par flag or by setting the number of repairs in the opscenter config file note below)</p>
<h3>Clear your snapshots</h3>
<p>If you have too many snapshots and need to free up space (maybe once you have backed them up to S3 or glacier or something) go ahead and use nodetool clearsnapshots to delete them. This will free up space. You can also go in and remove them manually from your file system but nodetool clearsnapshots removes the risk of rm -rf ing the wrong thing.</p>
<p><strong>Note:</strong> You may also be running repairs too fast if you don't have a ton of data (check my response to this <a href=""https://stackoverflow.com/questions/28021344/high-load-on-cassandra-nodes"">other SO question</a> for an explanation and the repair service config levers).</p>
","['table', 'incremental_backups']"
29524769,29870730,2015-04-08 20:30:26,cleaning up cassandra tombstones from memtable,"<p>When I delete a row from cassandra and the data still lives in memtable (no SSTable has been created yet), it looks like that deleted row is never getting cleaned up in memtable since the tombstone cleanup is only done by compaction and compaction only applies to SSTables. Is there anyway I can completely cleanup that deleted row from memtable itself, before flushing it to SSTable? Updates are in place but looks like deletes are not.</p>

<p>We are using Cassandra 2.0.8.</p>

<p>Thanks</p>
",<cassandra><tombstone>,"<p>If you have RF > 1, the tombstones still need to be persisted to disk to ensure that the deletion was safely transmitted to all replicas. For example, consider the following:</p>

<p>RF = 3
N = 3</p>

<p>You have a table of employees to fire at the end of the month.
You add John Smith to the list of employees to terminate.
Two minutes later, John Smith does something awesome, and you want to remove him from the list. You delete his entry, but one of the 3 nodes is offline - John Smith is still in the list of employees to fire for that offline node.</p>

<p>When the memtable flushes on one of the ""up"" nodes, it will persist the tombstone indicating that John Smith should not be fired, because when that offline server comes up, it needs to know that John Smith's job is safe.</p>

<p>Compaction will eventually remove the tombstone after gc_grace_seconds, but the underlying behavior is correct: if you write a cell, and then immediately delete it, you still have to save the tombstone to disk in order to make sure all replicas properly delete that cell.</p>
",['table']
29575571,29585129,2015-04-11 08:02:43,How to map static columns differently from other columns?,"<p>How can I map <a href=""http://www.datastax.com/dev/blog/cql-in-2-0-6"" rel=""nofollow"">the example given here</a> to the below class, using datastax Java object mapping?</p>

<pre><code>public class User {
    private int user;
    private int balance;

    private List&lt;Bill&gt; bills;
}

public class Bill {
    private String description;
    private int amount;
}
</code></pre>
",<cassandra><cassandra-2.0><datastax-java-driver>,"<p>With regards to the mapping module in the java-driver, a static column does not need to be treated any differently then a non-static column.  One concern you will have though is you want some consistency in that the balance is updated only if it is an expected value, so using the Mapper's save method alone will not be adequate.  Rather you will do a batch with a conditional update of the balance, and an update with the expense in the same batch.</p>

<p>To make this convenient and still make use of the Mapper, you could use an <a href=""http://docs.datastax.com/en/developer/java-driver/2.1/common/drivers/reference/accessorAnnotatedInterfaces.html"" rel=""nofollow"">Accessor-annotated interface</a> to define your queries and map them back to your objects.  You can then create a data access object for interfacing with Cassandra using your mapper object and some other methods.</p>

<p>This will take some work, but I think it provides you a nice clean way to abstract your solution away from Cassandra while still using it in an idiomatic way.  Another option is look into <a href=""https://github.com/doanduyhai/Achilles"" rel=""nofollow"">Achilles</a> which is a more advanced object persistence manager for Cassandra.  <a href=""https://github.com/impetus-opensource/Kundera"" rel=""nofollow"">Kundera</a> and <a href=""https://github.com/spring-projects/spring-data-cassandra.git"" rel=""nofollow"">Spring Data</a> are other possible options.</p>

<p>First, lets look at your classes and map them to the table defined in the example from the blog:</p>

<pre><code>  CREATE TABLE bills (
     user text,
     balance int static,
     expense_id int,
     amount int,
     description text,
     paid boolean,
     PRIMARY KEY (user, expense_id)
  );
</code></pre>

<p>From your example, I suspect that you may wish to use an User-defined type instead of separate columns for a bill,  but since you tagged this post 'cassandra-2.0' and UDTs are not introduced until 2.1, I will not cover this, but if you'd like me to expound more on that I can.</p>

<p>Lets define our class <code>Bill</code>:</p>

<pre class=""lang-java prettyprint-override""><code>@Table(name=""bills"")
public class Bill {

    @PartitionKey
    private String user;

    private int balance;

    @ClusteringColumn
    @Column(name=""expense_id"")
    private int expenseId;

    private int amount;

    private String description;

    private boolean paid;

    public String getUser() {
        return user;
    }

    public void setUser(String user) {
        this.user = user;
    }

    public int getBalance() {
        return balance;
    }

    public void setBalance(int balance) {
        this.balance = balance;
    }

    public int getExpenseId() {
        return expenseId;
    }

    public void setExpenseId(int expenseId) {
        this.expenseId = expenseId;
    }

    public int getAmount() {
        return amount;
    }

    public void setAmount(int amount) {
        this.amount = amount;
    }

    public String getDescription() {
        return description;
    }

    public void setDescription(String description) {
        this.description = description;
    }

    public boolean isPaid() {
        return paid;
    }

    public void setPaid(boolean paid) {
        this.paid = paid;
    }
}
</code></pre>

<p>Lets also define a <code>BillAccessor</code> for interacting with our Bills in cassandra mapping them back to <code>Bill</code> objects.  This should cover all queries from the blog post:</p>

<pre class=""lang-java prettyprint-override""><code>@Accessor
public interface BillAccessor {

    @Query(""INSERT INTO bills (user, balance) VALUES (?, ?) IF NOT EXISTS"")
    BoundStatement addUser(String user, int balance);

    @Query(""UPDATE bills SET balance = :newBalance WHERE user = :user IF balance = :currentBalance"")
    BoundStatement updateBalance(@Param(""user"") String user, @Param(""currentBalance"") int currentBalance,
                            @Param(""newBalance"") int newBalance);

    @Query(""SELECT balance from bills where user=?"")
    ResultSet getBalance(String user);

    @Query(""INSERT INTO bills (user, expense_id, amount, description, paid) values (?, ?, ?, ?, false) IF NOT EXISTS"")
    BoundStatement addBill(String user, int expenseId, int amount, String description);

    @Query(""UPDATE bills set paid=true where user=? and expense_id=? IF paid=false"")
    BoundStatement markBillPaid(String user, int expenseId);

    @Query(""SELECT * from bills where user=?"")
    Result&lt;Bill&gt; getBills(String user);
}
</code></pre>

<p>Next we'll create a DAO for interfacing with your bills using the <code>Bill</code> class and <code>BillAccessor</code>:</p>

<pre class=""lang-java prettyprint-override""><code>public class BillDao {

    private final Session session;

    private final Mapper&lt;Bill&gt; mapper;

    private final BillAccessor accessor;

    public BillDao(Session session) {
        this.session = session;
        MappingManager manager = new MappingManager(session);
        this.mapper = manager.mapper(Bill.class);
        this.accessor = manager.createAccessor(BillAccessor.class);
    }

    public Integer getBalance(String user) {
        ResultSet result = accessor.getBalance(user);
        Row row = result.one();
        if(row == null) {
            return null;
        } else {
            return row.getInt(0);
        }
    }

    public Iterable&lt;Bill&gt; getBills(String user) {
        return accessor.getBills(user);
    }

    public Bill getBill(String user, int expenseId) {
        return mapper.get(user, expenseId);
    }

    public int addBill(String user, int expenseId, int amount, String description) throws UpdateException {
        BatchStatement batch = new BatchStatement();

        Integer balance = getBalance(user);
        if (balance == null) {
            balance = 0;
            // we need to create the user.
            batch.add(accessor.addUser(user, balance - amount));
        } else {
            // we need to update the users balance.
            batch.add(accessor.updateBalance(user, balance, balance - amount));
        }
        batch.add(accessor.addBill(user, expenseId, amount, description));
        ResultSet result = session.execute(batch);

        if (result.wasApplied()) {
            return balance - amount;
        } else {
            throw new UpdateException(""Failed applying bill, conditional update failed."");
        }
    }

    public int payForBill(Bill bill) throws UpdateException {
        Integer balance = getBalance(bill.getUser());
        if(balance == null) {
            throw new UpdateException(""Failed paying for bill, user doesn't exist!"");
        }
        BatchStatement batch = new BatchStatement();
        batch.add(accessor.updateBalance(bill.getUser(), balance, bill.getAmount() + balance));
        batch.add(accessor.markBillPaid(bill.getUser(), bill.getExpenseId()));

        ResultSet result = session.execute(batch);

        if(result.wasApplied()) {
            return bill.getAmount() + balance;
        } else {
            throw new UpdateException(""Failed paying for bill, conditional update failed."");
        }
    }

    public class UpdateException extends Exception {
        public UpdateException(String msg) {
            super(msg);
        }
    }
}
</code></pre>

<p>Note that we check whether or not a change was applied by checking <a href=""http://www.datastax.com/drivers/java/2.0/com/datastax/driver/core/ResultSet.html#wasApplied()"" rel=""nofollow"">ResultSet.wasApplied()</a>.  Since we are doing conditional updates, the change may not be applied if our conditions do not hold.  The DAO will simply throw an <code>UpdateException</code> if the change wasn't applied, but you could choose a different strategy like retrying an arbitrary number of times in the DAO.</p>

<p>Finally lets write some code to exercise the DAO:</p>

<pre class=""lang-java prettyprint-override""><code>Cluster cluster = Cluster.builder().addContactPoint(""127.0.0.1"").build();
try {
    Session session = cluster.connect(""readtest"");
    BillDao billDao = new BillDao(session);

    String user = ""chandru"";

    // Create a bill, should exercise user create logic.
    int balance = billDao.addBill(user, 1, 10, ""Sandwich"");
    System.out.format(""Bill %s/%d created, current balance is %d.%n"", user, 1, balance);

    // Create another bill, should exercise balance update logic.
    balance = billDao.addBill(user, 2, 6, ""Salad"");
    System.out.format(""Bill %s/%d created, current balance is %d.%n"", user, 2, balance);

    // Pay for all the bills!
    for(Bill bill : billDao.getBills(user)) {
        balance = billDao.payForBill(bill);
        System.out.format(""Paid for %s/%d, current balance is %d.%n"", user, bill.getExpenseId(), balance);

        // Ensure bill was paid.
        Bill newBill = billDao.getBill(user, bill.getExpenseId());
        System.out.format(""Is %s/%d paid for?: %b.%n"", user, newBill.getExpenseId(), newBill.isPaid());
    }

    // Try to add another bill with an already used expense id.
    try {
        billDao.addBill(user, 1, 1, ""Diet Coke"");
    } catch(BillDao.UpdateException ex) {
        System.err.format(""Could not add bill %s/%d: %s"", user, 1, ex.getMessage());
    }

} finally {
    cluster.close();
}
</code></pre>

<p>If all goes well, you should observe the following output:</p>

<pre><code>Bill chandru/1 created, current balance is -10.
Bill chandru/2 created, current balance is -16.
Paid for chandru/1, current balance is -6.
Is chandru/1 paid for?: true.
Paid for chandru/2, current balance is 0.
Is chandru/2 paid for?: true.
Could not add bill chandru/1: Failed applying bill, conditional update failed.
</code></pre>

<p>And the state of your table will be:</p>

<pre><code>cqlsh:readtest&gt; select * from bills;

 user    | expense_id | balance | amount | description | paid
---------+------------+---------+--------+-------------+------
 chandru |          1 |       0 |     10 |    Sandwich | True
 chandru |          2 |       0 |      6 |       Salad | True
</code></pre>
",['table']
29583586,29583904,2015-04-11 22:02:38,Is it possible to shut down cassandra cluster and restart it back without loosing data?,"<p>Im running 2 node cassandra cluster on virtual box (one as a seed). I need to shut down the cluster and turn it on again. But when i tried to restart, it is throwing an error ""cannot change the number of tokens from 1 to 256"", even though i dint change any configuration. But i am able to restart it after deleting the data. Is there any way to restart the cluster without removing the data ?</p>
",<cassandra><cluster-computing><datastax>,"<p>You must have changed your cassandra.yaml to include num_tokens to 256 on a cluster that was using single tokens.</p>

<p>Comment out num_tokens and try again.</p>
",['num_tokens']
29589311,29592830,2015-04-12 12:11:15,Prevent UnAuhtorize Node to Join Cassandra Cluster,"<p>I'm using Apache Cassandra 2.1.2 and create 5 nodes cluster.
in <code>cassandra.yaml</code> config file, authorizer &amp; authenticator properties are set to <code>CassandraAuthorizer</code> and <code>PasswordAuthenticator</code>.</p>

<p>It's works prefect but when some of these nodes config file changed to connect without authorization, this node can query all keyspaces and read all secure data easily!</p>

<p>What can do to secure Cassandra keyspaces and data from being accessed from authorized client (node)?</p>
",<cassandra><authorization><cql><cassandra-2.0>,"<p>If you are concerned about config files being reset, you should really consider using configuration management software like <a href=""https://puppetlabs.com/"" rel=""nofollow"">Puppet</a> or <a href=""https://www.chef.io/"" rel=""nofollow"">Chef</a> to keep your configuration in sync between the nodes in your cluster.</p>

<p>I'd also strongly recommend setting up <a href=""http://docs.datastax.com/en/cassandra/2.0/cassandra/security/secureSSLNodeToNode_t.html"" rel=""nofollow"">node-to-node encryption</a> and set up a trust store on your cassandra nodes that only allows other nodes to join the ring if they provide a certificate that is trusted.</p>

<p>This way you can't have unauthorized nodes joining your ring that you don't know about.   Also, chances are if your config files are changed, the server_encryption_options will be invalidated as well and your node won't be able to connect.</p>

<p>If you are concerned about securing your cluster.  I'd also recommend using <a href=""http://docs.datastax.com/en/cassandra/2.0/cassandra/security/secureSSLClientToNode_t.html"" rel=""nofollow"">client-to-node encryption</a> when using authorization as without SSL the credentials are passed in the clear when a client connects.</p>
",['server_encryption_options']
29605744,29607461,2015-04-13 12:47:43,Cassandra DB: What ultimately does 'replication_factor' controls?,"<p>I want to verify and test the 'replication_factor' and the consistency level ONE of Cassandra DB.</p>

<p>And I specified a Cluster: <strong>'MyCluster01' with three nodes in two data center: DC1(node1, node2) in RAC1, DC2(node3) in RAC2</strong>.</p>

<p>Structure shown as below:</p>

<pre><code>[root@localhost ~]# nodetool status
Datacenter: DC1
===============
Status=Up/Down |/ State=Normal/Leaving/Joining/Moving

--  Address        Load       Tokens  Owns    Host ID                               Rack

UN  10.0.0.62  409.11 KB  256     ?       59bf9a73-45cc-4f9b-a14a-a27de7b19246  RAC1

UN  10.0.0.61  408.93 KB  256     ?       b0cdac31-ca73-452a-9cee-4ed9d9a20622  RAC1
Datacenter: DC2
===============
Status=Up/Down |/ State=Normal/Leaving/Joining/Moving

--  Address        Load       Tokens  Owns    Host ID                               Rack

UN  10.0.0.63  336.34 KB  256     ?       70537e0a-edff-4f48-b5db-44f623ec6066  RAC2
</code></pre>

<p>Then, I created a keyspace and table like following: </p>

<pre><code>CREATE KEYSPACE my_check1 WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'};
create table replica_test(id uuid PRIMARY KEY);

    After I inserted one record into that table: 
insert into replica_test(id) values (uuid());
select * from replica_test;
 id
--------------------------------------
 5e6050f1-8075-4bc9-a072-5ef24d5391e5
</code></pre>

<p>I got that record. </p>

<p>But when I <strong>stopped node1</strong> and queried again in either node 2 and node 3, 
none of the query succeeded.</p>

<pre><code>select * from replica_test;

Traceback (most recent call last):   File ""/usr/bin/cqlsh"", line 997,
in perform_simple_statement
    rows = self.session.execute(statement, trace=self.tracing_enabled)   File
""/usr/share/cassandra/lib/cassandra-driver-internal-only-2.1.3.post.zip/cassandra-driver-2.1.3.post/cassandra/cluster.py"",
line 1337, in execute
    result = future.result(timeout)   File ""/usr/share/cassandra/lib/cassandra-driver-internal-only-2.1.3.post.zip/cassandra-driver-2.1.3.post/cassandra/cluster.py"",
line 2861, in result
    raise self._final_exception Unavailable: code=1000 [Unavailable exception] message=""Cannot achieve consistency level ONE""
info={'required_replicas': 1, 'alive_replicas': 0, 'consistency':
'ONE'}
</code></pre>

<p>While the 'nodetool status' command returned: </p>

<pre><code>UN  10.0.0.62  409.11 KB  256     ?       59bf9a73-45cc-4f9b-a14a-a27de7b19246  RAC1

DN  10.0.0.61  408.93 KB  256     ?       b0cdac31-ca73-452a-9cee-4ed9d9a20622  RAC1

UN  10.0.0.63  336.34 KB  256     ?       70537e0a-edff-4f48-b5db-44f623ec6066  RAC2
</code></pre>

<p>And when I tried stopping node 2, keeping node 1 and 3 alive; or stopping node 3, keeping node 1 and 2 alive; The error occurred as well. </p>

<p><strong>Then what 's the problem, since I think I 've already satisfied the consistency level, and where exactly does this record exists?</strong></p>
",<cassandra><consistency>,"<blockquote>
  <p>What ultimately does 'replication_factor' controls?</p>
</blockquote>

<p>To directly answer the question, replication factor (RF) controls the number of replicas of each data partition that exist in a cluster or data center (DC).  In your case, you have 3 nodes and a RF of 1.  That means that when a row is written to your cluster, that it is only stored on 1 node.  This also means that your cluster cannot withstand the failure of a single node.</p>

<p>In contrast, consider a RF of 3 on a 3 node cluster.  Such a cluster could withstand the failure of 1 or 2 nodes, and still be able to support queries for all of its data.</p>

<p>With all of your nodes up and running, try this command:</p>

<pre><code>nodetool getendpoints my_check1 replica_test 5e6050f1-8075-4bc9-a072-5ef24d5391e5
</code></pre>

<p>That will tell you on which node the data for key <code>5e6050f1-8075-4bc9-a072-5ef24d5391e5</code> resides.  My first thought, is that you are dropping the only node which has this key, and then trying to query it.</p>

<p>My second thought echoes what Carlo said in his answer.  You are using 2 DCs, <a href=""http://docs.datastax.com/en/cql/3.1/cql/cql_reference/create_keyspace_r.html?scroll=reference_ds_ask_vyj_xj__example-of-setting-the-simplestrategy-class"" rel=""nofollow"">which is really not supported with the <code>SimpleStrategy</code></a>.  Using <code>SimpleStrategy</code> with multiple DCs could produce unpredictable results.  Also with multiple DCs, you need to be using the <code>NetworkTopologyStrategy</code> and something other than the default <code>SimpleSnitch</code>.  Otherwise Cassandra may fail to find the proper node to complete an operation.</p>

<p>First of all, <a href=""http://docs.datastax.com/en/cql/3.1/cql/cql_using/create_keyspace_c.html"" rel=""nofollow"">re-create your keyspace and table with the <code>NetworkTopologyStrategy</code></a>.  Then change your snitch (in the <code>cassandra.yaml</code>) to a network-aware snitch, restart your nodes, and try this exercise again.</p>
",['table']
29615391,29678567,2015-04-13 21:25:43,Best Data Model for Unique Data Selection and Assignment in Cassandra,"<p>What's the best Cassandra data model and query for the following situation?</p>

<p>Our system is responsible for uniquely assigning serial numbers to our toasters, when each one is created at our factory. </p>

<ul>
<li>We have multiple models of toasters, each type uniquly identified by a different UPC. </li>
<li>A serial number can <em>never</em> be assigned to more then one toaster model (UPC specific). </li>
<li>It's not crucial to assign serial numbers in order. </li>
<li>At least once a day, we need to find out how many un-assigned serial numbers we still have for each UPC. </li>
<li>Our system is supplied by serial numbers from time to time by another system in large batches by their UPC.</li>
</ul>

<p>Performance Requirements:</p>

<ul>
<li>Finding an un-assigned serial number for a UPC <em>must be fast</em>. </li>
<li>Inserting new serial numbers does <em>not</em> need to be fast. </li>
<li>Counting does <em>not</em> need to be fast. </li>
</ul>

<p>Our dataset is about 10M serial numbers today growing about 1M annually.</p>

<p>We are currently using Cassandra 2.0.x and will soon migrate to 2.1.x.</p>
",<cassandra><data-modeling><datastax><denormalization><nosql>,"<p>Ok, I put a bit of thought behind this one.  There are a couple things that are tricky about this scenario:</p>

<ul>
<li><p>Serial numbers come in, and are assigned (but not right away).  With growth of 1 million per year, that works out to ~2800 new serial numbers per day.  Queuing those up (upserting them when they come in and deleting them when they are assigned) will create a lot of tombstones (essentially 2800 per day).</p></li>
<li><p>With 50 UPCs relating to 10 million serial numbers, that works out to 200k serial numbers per UPC (assuming even distribution).  This means that we can't store the UPC-to-serial numbers relationship in a collection (max size of 65536 items).</p></li>
</ul>

<p>I assume that you'll want to be able to figure out which serial numbers are tied to which models, and which models have which serial numbers.  For that, I'd go with two lookup tables:</p>

<pre><code>CREATE TABLE serialNumbersByUPC (
  modelUPC uuid,
  insertTime timeuuid,
  serialNumber text,
  PRIMARY KEY (modelUPC,insertTime))
WITH CLUSTERING ORDER BY (insertTime DESC);

CREATE TABLE UPCsBySerialNumbers (
  modelUPC uuid
  insertTime timeuuid,
  serialNumber text,
  PRIMARY KEY (serialNumber));
</code></pre>

<p>Note that you could also key <code>serialNumbersByUPC</code> with <code>serialNumber</code> as a clustering key (instead of <code>insertTime</code>).  But timeuuids are unique (so there won't be collisions on <code>serialNumbers</code>), and clustering by <code>insertTime</code> has the added benefit of allowing you to sort by date/time.  Of course, you'll want to make sure that you upsert to both of those tables when assigning a serial number to a UPC.</p>

<p>For the unassigned serial numbers, it might be best to use a queuing system like <a href=""http://hornetq.jboss.org/"" rel=""nofollow"">HornetQ</a> or <a href=""https://www.rabbitmq.com/"" rel=""nofollow"">RabbitMQ</a>.  That way you could just pull the new serial numbers off of the queue, and assign them as-needed.  The reason I suggest this, is that using Cassandra to queue-up transient data has been identified as an anti-pattern.</p>

<p>Of course you could decide not to heed the above warning, and insist on using Cassandra for that functionality.  If so, then <em>this</em> is how I would store unassigned serial numbers in Cassandra:</p>

<pre><code>CREATE TABLE unassignedSerialNumbers (
  dateBucket bigint,
  serialNumber text,
  insertTime timeuuid,
  PRIMARY KEY ((dateBucket),insertTime))
WITH compaction = {'class': 'org.apache.cassandra.db.compaction.DateTieredCompactionStrategy'}
AND gc_grace_seconds = 86400;
</code></pre>

<p>A few things about this solution:</p>

<ul>
<li><p>I'm partitioning on <code>datebucket</code> as I'm not sure how fast you assign the 2800 serial numbers that come in each day.  It's possible that you may want to query just the numbers that came in today, or yesterday.  I've created it as a <code>bigint</code>, but you can use whatever size bucket you wish (ex: ""20150416"" would partition serial numbers that came-in on April 16th, 2015 together).</p></li>
<li><p>If you find that you assign serial numbers fast enough that you don't need to partition by <code>datebucket</code>, then I wouldn't worry about that table getting big enough to hinder query performance.  Sure, your deletes will create tombstones that you query will have to contend with, but that should be helped with my last two points.</p></li>
<li><p>I'm clustering on <code>insertTime</code> for the same reason as I did in the <code>serialNumbersByUPC</code> table.</p></li>
<li><p>I'm using the <code>DateTieredCompactionStrategy</code> for this table.  This strategy will keep rows written at the same time in the same SSTABLE files on-disk.  This becomes important for performance as you delete and write new data.</p></li>
<li><p><code>gc_grace_seconds</code> is being set to 1 day instead of 10.  This will force tombstoned rows to be garbage-collected every day.  The drawback of this setting, is that if you have a node go down you need to bring it back within 1 day of it going down to pick-up the deletes.  If you don't you'll need to run a full repair or risk your deleted serial numbers ""coming back from the dead.""</p></li>
</ul>

<p>You'll also want to read up on the <a href=""http://www.datastax.com/dev/blog/datetieredcompactionstrategy"" rel=""nofollow"">DateTieredCompactionStrategy</a>.  There may be some other options with it that might make sense for you to set.</p>

<p>Let me know if you have any questions, or if I missed anything.</p>
",['table']
29635372,29636188,2015-04-14 18:59:56,How does Cassandra partitioning work when replication factor == cluster size?,"<h2>Background:</h2>

<p>I'm new to Cassandra and still trying to wrap my mind around the internal workings.</p>

<p>I'm thinking of using Cassandra in an application that will only ever have a limited number of nodes (less than 10, most commonly 3).  Ideally each node in my cluster would have a complete copy of all of the application data.  So, I'm considering setting replication factor to cluster size.  When additional nodes are added, I would alter the keyspace to increment the replication factor setting (nodetool repair to ensure that it gets the necessary data).</p>

<p>I would be using the NetworkTopologyStrategy for replication to take advantage of knowledge about datacenters.</p>

<p>In this situation, how does partitioning actually work?  I've read about a combination of nodes and partition keys forming a ring in Cassandra.  If all of my nodes are ""responsible"" for each piece of data regardless of the hash value calculated by the partitioner, do I just have a ring of one partition key?</p>

<p>Are there tremendous downfalls to this type of Cassandra deployment?  I'm guessing there would be lots of asynchronous replication going on in the background as data was propagated to every node, but this is one of the design goals so I'm okay with it.</p>

<p>The consistency level on reads would probably generally be ""one"" or ""local_one"".</p>

<p>The consistency level on writes would generally be ""two"".</p>

<h2>Actual questions to answer:</h2>

<ol>
<li>Is replication factor == cluster size a common (or even a reasonable) deployment strategy aside from the obvious case of a cluster of one?</li>
<li>Do I actually have a ring of one partition where all possible values generated by the partitioner go to the one partition?</li>
<li>Is each node considered ""responsible"" for every row of data?</li>
<li>If I were to use a write consistency of ""one"" does Cassandra always write the data to the node contacted by the client?</li>
<li>Are there other downfalls to this strategy that I don't know about?</li>
</ol>
",<cassandra><replication><partitioning><distributed-system>,"<blockquote>
<p>Do I actually have a ring of one partition where all possible values
generated by the partitioner go to the one partition?</p>
<p>Is each node considered &quot;responsible&quot; for every row of data?</p>
<p>If all of my nodes are &quot;responsible&quot; for each piece of data regardless
of the hash value calculated by the partitioner, do I just have a ring
of one partition key?</p>
</blockquote>
<p>Not exactly, C* nodes still have token ranges and c* still assigns a primary replica to the &quot;responsible&quot; node. But all nodes will also have a replica with RF = N (where N is number of nodes). So in essence the implication is the same as what you described.</p>
<blockquote>
<p>Are there tremendous downfalls to this type of Cassandra deployment?
Are there other downfalls to this strategy that I don't know about?</p>
</blockquote>
<p>Not that I can think of, I guess you might be more susceptible than average to inconsistent data so use C*'s anti-entropy mechanisms to counter this (repair, read repair, hinted handoff).</p>
<p>Consistency level quorum or all would start to get expensive but I see you don't intend to use them.</p>
<blockquote>
<p>Is replication factor == cluster size a common (or even a reasonable)
deployment strategy aside from the obvious case of a cluster of one?</p>
</blockquote>
<p>It's not common, I guess you are looking for super high availability and all your data fits on one box. I don't think I've ever seen a c* deployment with RF &gt; 5. Far and wide RF = 3.</p>
<blockquote>
<p>If I were to use a write consistency of &quot;one&quot; does Cassandra always
write the data to the node contacted by the client?</p>
</blockquote>
<p>This depends on your load balancing policies at the driver. Often we select token aware policies (assuming you're using one of the Datastax drivers), in which case requests are routed to the primary replica automatically. You could use round robin in your case and have the same effect.</p>
",['partitioner']
29654067,29654197,2015-04-15 15:15:04,Cassandra partial partition key,"<pre><code>CREATE TABLE footable (
    column1 text,
    column2 text,
    column3 text,
    column4 text,
    PRIMARY KEY ((column1, column2))
)
</code></pre>

<p>In the example above which I got from <a href=""https://stackoverflow.com/questions/27277025/querying-cassandra-by-a-partial-partition-key"">Querying Cassandra by a partial partition key</a>, is it possible to use condition on the 1st partition key and select all condition on 2nd partition key?</p>

<p>Example cql statement may look like this:</p>

<pre><code>select * from footable where column1 = 'name' and column2 ALL;
</code></pre>

<p>Is there some sort of querying like this in Cassandra? </p>
",<cassandra><primary-key><cql>,"<blockquote>
  <p>is it possible to use condition on the 1st partition key and select all condition on 2nd partition key?</p>
</blockquote>

<p>No.  To support that query, (in your table definition) you would have to modify the PRIMARY KEY to only use only <code>column1</code> as the partition key, and designate <code>column2</code> as a clustering key:</p>

<pre><code>PRIMARY KEY ((column1), column2)
</code></pre>

<p>Then this query would return your desired results:</p>

<pre><code>select * from footable where column1 = 'name';
</code></pre>
",['table']
29654972,29913772,2015-04-15 15:51:29,Cassandra delta/relative querying,"<p>I have to orchestrate a batch which copy the delta of a table each day. This table is only written, never updated. I use java with jdbc and I wonder if there is a metadata or something on the table which can be queried to get all the rows added after a certains date.</p>

<hr>

<p>Why a metadata ?
Because with my table looking like :</p>

<pre><code>CREATE TABLE aTable (
  aTable_id timeuuid,
  ...
  PRIMARY KEY ((aTable_id))
) WITH
...
</code></pre>

<p>I can't put the timeuuid key in the where clause like :</p>

<pre><code>SELECT * FROM aTable WHERE aTable_id &gt; minTimeuuid(?)
</code></pre>

<p>And the token function, even if the <code>aTable_id</code> is correctly ordered give me wrong results :</p>

<pre><code>SELECT * FROM aTable WHERE token(aTable_id) &gt; token(minTimeuuid(?))
</code></pre>

<p>In a nutshell, my question is : how to get aTables newer than a certain date ?</p>
",<select><cassandra><cql><database-partitioning><delta>,"<p>So I end up with a solution, found in a meetup introducing cassandra 3.0.</p>

<p>Remember that the schema was set for another request and the keys were not set for a delta request.</p>

<p>The aim for me was to query just the updated row from the previous batch and here the way I did :</p>

<ul>
<li>Create a index table, partitioned on date hour (the minutes, seconds and millis are truncated). This table is fed by a gobal index from the main table.</li>
<li>In java, query the index by hour (<code>loop on a calendar</code>) and select the main table with a <code>IN</code> query.</li>
<li>Job done !</li>
</ul>
",['table']
29659564,29796305,2015-04-15 19:51:21,Validating row at client side better than secondary index with whole primary key?,"<p>In cassandra, it's well known that secondary indexes should be used very sparingly.</p>

<p>If I have a table for example:</p>

<pre><code>User(username, usertype, email, etc..)
</code></pre>

<p>Here username is partition key. Now I want to support operation which returns a specific user(username will be given) if and only if usertype is a specific value X.</p>

<p>There are two ways I can do it:</p>

<p>One:
Create a secondary index on usertype, possible values ('A', 'B', 'C')
and username is partition key.</p>

<pre><code>SELECT * FROM user WHERE username='something' AND usertype='A';
</code></pre>

<p>Two:</p>

<p>I can just fetch the row with username to client and then check if usertype is A.</p>

<p><strong>Which approach is better?</strong> Please also consider a wide row(not so big, 10s) scenario where not all rows of a partition might have the given value (which requires some client side filtering).</p>

<p>What I'm not clear about secondary indexes is how data is looked up in a particular node. </p>

<p>Ex: <code>SELECT * FROM user WHERE username='something' AND usertype='A'</code></p>

<p>For example usertype hidden CF has data 'A'-> 'jhon', 'miller', 'chris',...etc, 100 usernames </p>

<p>And the query with partition key is given along with usertype does it scan through all these 100 usernames to match with the username 'something' or does it just fetches by username first and sees the usertype column if it matches with 'A'? <strong>How exactly it does that searching? How does the query fares  given the index is on low cardinality data and each one is mapped to many rows?</strong></p>

<p>I'm using java as client if that matters.</p>

<p><strong>Update:</strong>
I understand that I can use clustering (usertype) key for this particular example but I wanted to know the trade off I've asked. My original tables are much more complex.</p>
",<java><cassandra><secondary-indexes>,"<p>For this example, let's say I create a table to keep track of crew members by ship and id:</p>

<pre><code>CREATE TABLE crewByShip (
  ship text,
  id int,
  firstname text,
  lastname text,
  gender text,
  PRIMARY KEY(ship,id));
</code></pre>

<p>And I'll create an index on <code>gender</code>:</p>

<pre><code>CREATE INDEX crewByShipG_idx ON crewByShip(gender);
</code></pre>

<p>After inserting some data, my table looks like this:</p>

<pre><code> ship     | id | firstname | gender | lastname
----------+----+-----------+--------+-----------
 Serenity |  1 |     Hoban |      M | Washburne
 Serenity |  2 |      Zoey |      F | Washburne
 Serenity |  3 |   Malcolm |      M |  Reynolds
 Serenity |  4 |    Kaylee |      F |      Frye
 Serenity |  5 |  Sheppard |      M |      Book
 Serenity |  6 |     Jayne |      M |      Cobb
 Serenity |  7 |     Simon |      M |       Tam
 Serenity |  8 |     River |      F |       Tam
 Serenity |  9 |     Inara |      F |     Serra
</code></pre>

<p>Now I'll turn tracing on, and query a distinct row with the PRIMARY KEY, but also restrict by our index on <code>gender</code>.</p>

<pre><code>aploetz@cqlsh:stackoverflow2&gt; tracing on;
aploetz@cqlsh:stackoverflow2&gt; SELECT * FROM crewByShip WHERE ship='Serenity' AND id=3 AND gender='M';

 ship     | id | firstname | gender | lastname
----------+----+-----------+--------+----------
 Serenity |  3 |   Malcolm |      M | Reynolds

(1 rows)

Tracing session: 34ea1840-e8e1-11e4-9cb7-21b264d4c94d

 activity                                                                                                                                                                                                                                                                                                       | timestamp                  | source         | source_elapsed
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------+----------------+----------------
                                                                                                                                                                                                                                                                                             Execute CQL3 query | 2015-04-22 06:17:48.102000 | 192.168.23.129 |              0
                                                                                                                                                                                                          Parsing SELECT * FROM crewByShip WHERE ship='Serenity' AND id=3 AND gender='M'; [SharedPool-Worker-1] | 2015-04-22 06:17:48.114000 | 192.168.23.129 |           3715
                                                                                                                                                                                                                                                                      Preparing statement [SharedPool-Worker-1] | 2015-04-22 06:17:48.116000 | 192.168.23.129 |           4846
                                                                                                                                                                                                                                                Executing single-partition query on users [SharedPool-Worker-2] | 2015-04-22 06:17:48.118000 | 192.168.23.129 |           5730
                                                                                                                                                                                                                                                             Acquiring sstable references [SharedPool-Worker-2] | 2015-04-22 06:17:48.118000 | 192.168.23.129 |           5757
                                                                                                                                                                                                                                                              Merging memtable tombstones [SharedPool-Worker-2] | 2015-04-22 06:17:48.119000 | 192.168.23.129 |           5793
                                                                                                                                                                                                                                                              Key cache hit for sstable 1 [SharedPool-Worker-2] | 2015-04-22 06:17:48.119000 | 192.168.23.129 |           5848
                                                                                                                                                                                                                                              Seeking to partition beginning in data file [SharedPool-Worker-2] | 2015-04-22 06:17:48.120000 | 192.168.23.129 |           5856
                                                                                                                                                                                                                Skipped 0/1 non-slice-intersecting sstables, included 0 due to tombstones [SharedPool-Worker-2] | 2015-04-22 06:17:48.120000 | 192.168.23.129 |           7056
                                                                                                                                                                                                                                               Merging data from memtables and 1 sstables [SharedPool-Worker-2] | 2015-04-22 06:17:48.121000 | 192.168.23.129 |           7080
                                                                                                                                                                                                                                                       Read 1 live and 0 tombstoned cells [SharedPool-Worker-2] | 2015-04-22 06:17:48.122000 | 192.168.23.129 |           7143
                                                                                                                                                                                                                                                                Computing ranges to query [SharedPool-Worker-1] | 2015-04-22 06:17:48.122000 | 192.168.23.129 |           7578
 Candidate index mean cardinalities are CompositesIndexOnRegular{columnDefs=[ColumnDefinition{name=gender, type=org.apache.cassandra.db.marshal.UTF8Type, kind=REGULAR, componentIndex=1, indexName=crewbyshipg_idx, indexType=COMPOSITES}]}:0. Scanning with crewbyship.crewbyshipg_idx. [SharedPool-Worker-1] | 2015-04-22 06:17:48.122000 | 192.168.23.129 |           7742
                                                                                                                                                                                              Submitting range requests on 1 ranges with a concurrency of 1 (0.0 rows per range expected) [SharedPool-Worker-1] | 2015-04-22 06:17:48.122000 | 192.168.23.129 |           7807
                                                                                                                                                                                                                                  Submitted 1 concurrent range requests covering 1 ranges [SharedPool-Worker-1] | 2015-04-22 06:17:48.122000 | 192.168.23.129 |           7851
                                                                                                                                                                                                                                          Executing indexed scan for [Serenity, Serenity] [SharedPool-Worker-2] | 2015-04-22 06:17:48.123000 | 192.168.23.129 |          10848
 Candidate index mean cardinalities are CompositesIndexOnRegular{columnDefs=[ColumnDefinition{name=gender, type=org.apache.cassandra.db.marshal.UTF8Type, kind=REGULAR, componentIndex=1, indexName=crewbyshipg_idx, indexType=COMPOSITES}]}:0. Scanning with crewbyship.crewbyshipg_idx. [SharedPool-Worker-2] | 2015-04-22 06:17:48.123000 | 192.168.23.129 |          10936
 Candidate index mean cardinalities are CompositesIndexOnRegular{columnDefs=[ColumnDefinition{name=gender, type=org.apache.cassandra.db.marshal.UTF8Type, kind=REGULAR, componentIndex=1, indexName=crewbyshipg_idx, indexType=COMPOSITES}]}:0. Scanning with crewbyship.crewbyshipg_idx. [SharedPool-Worker-2] | 2015-04-22 06:17:48.123000 | 192.168.23.129 |          11007
                                                                                                                                                                                                                           Executing single-partition query on crewbyship.crewbyshipg_idx [SharedPool-Worker-2] | 2015-04-22 06:17:48.123000 | 192.168.23.129 |          11130
                                                                                                                                                                                                                                                             Acquiring sstable references [SharedPool-Worker-2] | 2015-04-22 06:17:48.123000 | 192.168.23.129 |          11139
                                                                                                                                                                                                                                                              Merging memtable tombstones [SharedPool-Worker-2] | 2015-04-22 06:17:48.124000 | 192.168.23.129 |          11155
                                                                                                                                                                                                                Skipped 0/0 non-slice-intersecting sstables, included 0 due to tombstones [SharedPool-Worker-2] | 2015-04-22 06:17:48.124000 | 192.168.23.129 |          11253
                                                                                                                                                                                                                                               Merging data from memtables and 0 sstables [SharedPool-Worker-2] | 2015-04-22 06:17:48.124000 | 192.168.23.129 |          11262
                                                                                                                                                                                                                                                       Read 1 live and 0 tombstoned cells [SharedPool-Worker-2] | 2015-04-22 06:17:48.127000 | 192.168.23.129 |          11281
                                                                                                                                                                                                                                           Executing single-partition query on crewbyship [SharedPool-Worker-2] | 2015-04-22 06:17:48.130000 | 192.168.23.129 |          11369
                                                                                                                                                                                                                                                             Acquiring sstable references [SharedPool-Worker-2] | 2015-04-22 06:17:48.131000 | 192.168.23.129 |          11375
                                                                                                                                                                                                                                                              Merging memtable tombstones [SharedPool-Worker-2] | 2015-04-22 06:17:48.131000 | 192.168.23.129 |          11383
                                                                                                                                                                                                                Skipped 0/0 non-slice-intersecting sstables, included 0 due to tombstones [SharedPool-Worker-2] | 2015-04-22 06:17:48.133000 | 192.168.23.129 |          11409
                                                                                                                                                                                                                                               Merging data from memtables and 0 sstables [SharedPool-Worker-2] | 2015-04-22 06:17:48.134000 | 192.168.23.129 |          11415
                                                                                                                                                                                                                                                       Read 1 live and 0 tombstoned cells [SharedPool-Worker-2] | 2015-04-22 06:17:48.138000 | 192.168.23.129 |          11430
                                                                                                                                                                                                                                                             Scanned 1 rows and matched 1 [SharedPool-Worker-2] | 2015-04-22 06:17:48.138000 | 192.168.23.129 |          11490
                                                                                                                                                                                                                                                                                               Request complete | 2015-04-22 06:17:48.115679 | 192.168.23.129 |          13679
</code></pre>

<p>Now, I'll re-run the same query, but without the superfluous index on <code>gender</code>.</p>

<pre><code>aploetz@cqlsh:stackoverflow2&gt; SELECT * FROM crewByShip WHERE ship='Serenity' AND id=3;

 ship     | id | firstname | gender | lastname
----------+----+-----------+--------+----------
 Serenity |  3 |   Malcolm |      M | Reynolds

(1 rows)

Tracing session: 38d7f440-e8e1-11e4-9cb7-21b264d4c94d

 activity                                                                                        | timestamp                  | source         | source_elapsed
-------------------------------------------------------------------------------------------------+----------------------------+----------------+----------------
                                                                              Execute CQL3 query | 2015-04-22 06:17:54.692000 | 192.168.23.129 |              0
          Parsing SELECT * FROM crewByShip WHERE ship='Serenity' AND id=3; [SharedPool-Worker-1] | 2015-04-22 06:17:54.695000 | 192.168.23.129 |             87
                                                       Preparing statement [SharedPool-Worker-1] | 2015-04-22 06:17:54.696000 | 192.168.23.129 |            246
                                 Executing single-partition query on users [SharedPool-Worker-3] | 2015-04-22 06:17:54.697000 | 192.168.23.129 |           1185
                                              Acquiring sstable references [SharedPool-Worker-3] | 2015-04-22 06:17:54.698000 | 192.168.23.129 |           1197
                                               Merging memtable tombstones [SharedPool-Worker-3] | 2015-04-22 06:17:54.698000 | 192.168.23.129 |           1215
                                               Key cache hit for sstable 1 [SharedPool-Worker-3] | 2015-04-22 06:17:54.700000 | 192.168.23.129 |           1249
                               Seeking to partition beginning in data file [SharedPool-Worker-3] | 2015-04-22 06:17:54.700000 | 192.168.23.129 |           1278
 Skipped 0/1 non-slice-intersecting sstables, included 0 due to tombstones [SharedPool-Worker-3] | 2015-04-22 06:17:54.701000 | 192.168.23.129 |           3309
                                Merging data from memtables and 1 sstables [SharedPool-Worker-3] | 2015-04-22 06:17:54.701000 | 192.168.23.129 |           3333
                                        Read 1 live and 0 tombstoned cells [SharedPool-Worker-3] | 2015-04-22 06:17:54.702000 | 192.168.23.129 |           3368
                            Executing single-partition query on crewbyship [SharedPool-Worker-2] | 2015-04-22 06:17:54.702000 | 192.168.23.129 |           4607
                                              Acquiring sstable references [SharedPool-Worker-2] | 2015-04-22 06:17:54.704000 | 192.168.23.129 |           4633
                                               Merging memtable tombstones [SharedPool-Worker-2] | 2015-04-22 06:17:54.704000 | 192.168.23.129 |           4643
 Skipped 0/0 non-slice-intersecting sstables, included 0 due to tombstones [SharedPool-Worker-2] | 2015-04-22 06:17:54.705000 | 192.168.23.129 |           4678
                                Merging data from memtables and 0 sstables [SharedPool-Worker-2] | 2015-04-22 06:17:54.705000 | 192.168.23.129 |           4683
                                        Read 1 live and 0 tombstoned cells [SharedPool-Worker-2] | 2015-04-22 06:17:54.706000 | 192.168.23.129 |           4697
                                                                                Request complete | 2015-04-22 06:17:54.697676 | 192.168.23.129 |           5676
</code></pre>

<p>As you can see, the ""source_elapsed"" for the query with the secondary index was more than twice what it was for the same query (which returned the same row) without the index.</p>

<p>I think we can definitely say that using a secondary index on a low-cardinality column in a wide-row table will <strong>not</strong> perform well.  Now while I won't say that filtering client-side is a good idea, <strong>in this case</strong>, with a small result set, it probably would be the better option.</p>
",['table']
29665503,29666313,2015-04-16 04:44:39,DSE cassandra and spark map collections type: how to perform get operation,"<p>For example I have the following table named ""example"":</p>

<pre><code>  name      |       age       |       address

  'abc'     |       12        | {'street':'1', 'city':'kl', 'country':'malaysia'}
  'cab'     |       15        | {'street':'5', 'city':'jakarta', 'country':'indonesia'}
</code></pre>

<p>In Spark I can do this:</p>

<p>scala> val test = sc.cassandraTable (""test"",""example"")</p>

<p>and this:</p>

<p>scala> test.first.getString</p>

<p>and this:</p>

<p>scala> test.first.getMapString, String</p>

<p>which gives me all the fields of the address in the form of a map</p>

<p><strong>Question 1</strong>: But how do I use the ""get"" to access ""city"" information?
<strong>Question 2</strong>: Is there a way to falatten the entire table?
<strong>Question 3</strong>: how do I go about counting number of rows where ""city"" = ""kl""?</p>

<p>Thanks</p>
",<dictionary><collections><cassandra><apache-spark>,"<h3>Question 3 : How do we count the number of rows where city == something</h3>
<p>I'll answer 3 first because this may provide you an easier way to work with the data. Something like</p>
<pre><code>sc.cassandraTable[(String,Map[String,String],Int)](&quot;test&quot;,&quot;example&quot;)
 .filter( _._2.getOrElse(&quot;city&quot;,&quot;NoCity&quot;) == &quot;kl&quot; )
 .count
</code></pre>
<p>First, I use the type parameter <code>[(String,Map[String,String],Int)]</code> on my <code>cassandraTable</code> call to transform the rows into tuples. This gives me easy access to the Map without any casting. (The order is just how it appears when I made the table in my test environment you may have to change the ordering)</p>
<p>Second I say I would like to filter based on the <code>_._2</code> which is shorthand for the second element of the incoming tuple. <code>getOrElse</code> returns the value for the key &quot;city&quot; if the key exists and &quot;NoCity&quot; otherwise. The final equivalency checks what city it is.</p>
<p>Finally, I call <code>count</code> to find out the number of entries in the city.</p>
<h3>1 How do we access the map?</h3>
<p>So the answer to 2 is that once you have a Map, you can call get(&quot;key&quot;) or getOrElse(&quot;key&quot;) or any of the standard Scala operations to get a value out of the map.</p>
<h3>2 How to flatten the entire table.</h3>
<p>Depending on what you mean by &quot;flatten&quot; this can be a variety of things. For example if you want to return the entire table as an array to the driver (Not recommended since your RDD should be very big in production.) You can call <code>collect</code></p>
<p>If you want to flatten the elements of your map into a tuple you can always do something like calling <code>toSeq</code> and you will end up with a list of <code>(key,value)</code> tuples.  Feel free to ask another question if I haven't answered what you want with &quot;flattening.&quot;</p>
",['table']
29682559,29683485,2015-04-16 17:59:02,Query Cassandra with Both Primary Key and Secondary Key Constraints,"<p>I have a table in Cassandra defined as</p>

<pre><code>CREATE TABLE foo (""A"" text, ""B"" text, ""C"" text,
    ""D"" text, ""E"" text, ""F"" text,
    PRMIARY KEY (""A"", ""B""),
    INDEX (""C""))
</code></pre>

<p>I inserted billions of records into this table. And now I want to query the table with CQL</p>

<pre><code>SELECT * FROM foo WHERE ""A""='abc' AND ""B""='def' AND ""C""='ghi'
</code></pre>

<p>I keep receiving 1200 error saying that</p>

<blockquote>
  <p>ReadTimeout: code=1200 [Coordinator node timed out waiting for replica
  nodes' responses] message=""Operation timed out - received only 0
  responses."" info={'received_responses': 0, 'required_responses': 1,
  'consistency': 'ONE'}</p>
</blockquote>

<p>After googling, I suspect the reason of this error is that the query is directed to some partitions that does not hold any data.</p>

<p>My questions are</p>

<ol>
<li>Is there any constraint querying CQL with both primary key and secondary key specified?</li>
<li>If I specified the partition key in my CQL, here ""A""='abc' (correct me if wrong), why C* still tries other partition that apparently does not hold the data?</li>
<li>Any hints to solve this timeout problem?</li>
</ol>

<p>Thank you!</p>
",<cassandra><cql><cql3>,"<p><em>Note:  For my examples, I got rid of the double-quotes around the column names.  It really doesn't do anything other than preserve case in the column names (not the values) and only just serves to muck-up the works.</em></p>

<blockquote>
  <p>Is there any constraint querying CQL with both primary key and secondary key specified?</p>
</blockquote>

<p>First of all, I need to clear-up what, exactly, your ""primary key"" and ""secondary key"" are.  If you are referring to <code>C</code> as a ""secondary key,"" then ""yes"" you can, with some restrictions.  If you mean your <em>partition</em> key (<code>A</code>) and your <em>cluster</em> key (<code>B</code>), then yes, you can.</p>

<p>Querying by your partition and clustering keys (or even just your partition key(s) works:</p>

<pre><code>aploetz@cqlsh:stackoverflow2&gt; SELECT * FROM foo WHERe A='abc' AND B='def';

 a   | b   | c   | d   | e   | f
-----+-----+-----+-----+-----+-----
 abc | def | ghi | jkl | mno | pqr

(1 rows)
aploetz@cqlsh:stackoverflow2&gt; SELECT * FROM foo WHERe A='abc';

 a   | b   | c   | d   | e   | f
-----+-----+-----+-----+-----+-----
 abc | ddd | ghi | jkl | mno | pqr
 abc | def | ghi | jkl | mno | pqr

(2 rows)
</code></pre>

<p>When I create your table and index, insert a few rows, and run your query:</p>

<pre><code>aploetz@cqlsh:stackoverflow2&gt; SELECT * FROM foo WHERE A='abc' AND B='def' AND C='ghi';

 a   | b   | c   | d   | e   | f
-----+-----+-----+-----+-----+-----
 abc | def | ghi | jkl | mno | pqr

(1 rows)
</code></pre>

<p>That works.</p>

<blockquote>
  <p>If I specified the partition key in my CQL, here ""A""='abc' (correct me if wrong), why C* still tries other partition that apparently does not hold the data?</p>
</blockquote>

<p>I don't believe that is the problem.  You <em>are</em> restricting it to a single partition, so it should only query data off of the <code>abc</code> partition.</p>

<blockquote>
  <p>I inserted billions of records into this table.</p>
</blockquote>

<p>What you are seeing, is the reason that secondary index usage is considered to be an ""anti-pattern"" in Cassandra.  Secondary indexes do not work the same way that they do in the relational world.  They just do not scale well to large clusters or data sets.</p>

<blockquote>
  <p>Any hints to solve this timeout problem?</p>
</blockquote>

<p>Yes.  Recreate your table with <code>C</code> as a second clustering key.  And do <strong>not</strong> create an index on <code>C</code>.</p>

<pre><code>CREATE TABLE foo (A text, B text, C text, D text, E text, F text,
  PRMIARY KEY (A, B, C));
</code></pre>

<p>Reload your data, and then this should work for you:</p>

<pre><code>aploetz@cqlsh:stackoverflow2&gt; SELECT * FROM foo WHERE A='abc' AND B='def' AND C='ghi';
</code></pre>

<p>Not only should it work, but it should not timeout and it should be fast.</p>
",['table']
29692738,29701067,2015-04-17 07:19:53,How do secondary indexes work in Cassandra?,"<p>Suppose I have a column family:</p>
<pre><code>CREATE TABLE update_audit (
  scopeid bigint,
  formid bigint,
  time timestamp,
  record_link_id bigint,
  ipaddress text,
  user_zuid bigint,
  value text,
  PRIMARY KEY ((scopeid, formid), time)
  ) WITH CLUSTERING ORDER BY (time DESC)
</code></pre>
<p>With two secondary indexes, where <code>record_link_id</code> is a high-cardinality column:</p>
<pre><code>CREATE INDEX update_audit_id_idx ON update_audit (record_link_id);

CREATE INDEX update_audit_user_zuid_idx ON update_audit (user_zuid);
</code></pre>
<p>According to my knowledge Cassandra will create two hidden column families like so:</p>
<pre><code>CREATE TABLE update_audit_id_idx(
    record_link_id bigint,
    scopeid bigint,
    formid bigint,
    time timestamp
    PRIMARY KEY ((record_link_id), scopeid, formid, time)
);

CREATE TABLE update_audit_user_zuid_idx(
    user_zuid bigint,
    scopeid bigint,
    formid bigint,
    time timestamp
    PRIMARY KEY ((user_zuid), scopeid, formid, time)
);
</code></pre>
<p>Cassandra secondary indexes are implemented as local indexes rather than being distributed like normal tables. Each node only stores an index for the data it stores.</p>
<p>Consider the following query:</p>
<pre><code>select * from update_audit where scopeid=35 and formid=78005 and record_link_id=9897;
</code></pre>
<ol>
<li>How will this query execute 'under the hood' in Cassandra?</li>
<li>How will a high-cardinality column index (<code>record_link_id</code>) affect its performance?</li>
<li>Will Cassandra touch all nodes for the above query? <em>Why?</em></li>
<li>Which criteria will be executed first, base table partition_key or secondary index partition_key? How will Cassandra intersect these two results?</li>
</ol>
",<cassandra><cql><cassandra-2.0><cql3>,"<pre><code>select * from update_audit where scopeid=35 and formid=78005 and record_link_id=9897;
</code></pre>
<blockquote>
<p>How the above query will work internally in cassandra?</p>
</blockquote>
<p>Essentially, all data for partition <code>scopeid=35</code> and <code>formid=78005</code> will be returned, and then filtered by the <code>record_link_id</code> index.  It will look for the <code>record_link_id</code> entry for <code>9897</code>, and attempt to match-up entries that match the rows returned where <code>scopeid=35</code> and <code>formid=78005</code>.  The intersection of the rows for the partition keys and the index keys will be returned.</p>
<blockquote>
<p>How high-cardinality column (record_link_id)index will affect the query performance for the above query?</p>
</blockquote>
<p>High-cardinality indexes essentially create a row for (almost) each entry in the main table.  Performance is affected, because Cassandra is designed to perform sequential reads for query results.  An index query essentially forces Cassandra to perform <em>random</em> reads.  As cardinality of your indexed value increases, so does the time it takes to find the queried value.</p>
<blockquote>
<p>Does cassandra will touch all nodes for the above query? WHY?</p>
</blockquote>
<p>No.  It should only touch a node that is responsible for the <code>scopeid=35</code> and <code>formid=78005</code> partition.  Indexes likewise are stored locally, only contain entries that are valid for the local node.</p>
<blockquote>
<p>creating index over high-cardinality columns will be the fastest and best data model</p>
</blockquote>
<p>The problem here is that approach does not scale, and will be slow if <code>update_audit</code> is a large dataset.  MVP Richard Low has a great article on secondary indexes(<a href=""http://www.wentnet.com/blog/?p=77"" rel=""noreferrer"">The Sweet Spot For Cassandra Secondary Indexing</a>), and particularly on this point:</p>
<blockquote>
<p>If your table was significantly larger than memory, a query would be very slow even to return just a few thousand results. Returning potentially millions of users would be disastrous even though it would appear to be an efficient query.</p>
<p>...</p>
<p>In practice, this means indexing is most useful for returning tens, maybe hundreds of results. Bear this in mind when you next consider using a secondary index.</p>
</blockquote>
<p>Now, your approach of first restricting by a specific partition will help (as your partition should certainly fit into memory).  But I feel the better-performing choice here would be to make <code>record_link_id</code> a clustering key, instead of relying on a secondary index.</p>
<p><strong>Edit</strong></p>
<blockquote>
<p>How does having index on low cardinality index when there are millions of users scale even when we provide the primary key</p>
</blockquote>
<p>It will depend on how wide your rows are.  The tricky thing about extremely low cardinality indexes, is that the % of rows returned is usually greater.  For instance, consider a wide-row <code>users</code> table.  You restrict by the partition key in your query, but there are still 10,000 rows returned.  If your index is on something like <code>gender</code>, your query will have to filter-out about half of those rows, which won't perform well.</p>
<p>Secondary indexes tend to work best on (for lack of a better description) &quot;middle of the road&quot; cardinality.  Using the above example of a wide-row <code>users</code> table, an index on <code>country</code> or <code>state</code> should perform much better than an index on <code>gender</code> (assuming that most of those users don't all live in the same country or state).</p>
<p><strong>Edit 20180913</strong></p>
<blockquote>
<p>For your answer to 1st question &quot;How the above query will work internally in cassandra?&quot;, do you know what's the behavior when query with pagination?</p>
</blockquote>
<p>Consider the following diagram, taken from the <a href=""https://docs.datastax.com/en/developer/java-driver/3.6/manual/paging/"" rel=""noreferrer"">Java Driver documentation</a> (v3.6):</p>
<p><a href=""https://i.stack.imgur.com/br20s.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/br20s.png"" alt=""enter image description here"" /></a></p>
<p>Basically, paging will cause the query to break itself up and return to the cluster for the next iteration of results.  It'd be less likely to timeout, but performance will trend downward, proportional to the size of the total result set and the number of nodes in the cluster.</p>
<p>TL;DR; The more requested results spread over more nodes, the longer it will take.</p>
",['table']
29700146,29702012,2015-04-17 13:09:49,Cassandra database design - 1000 columns or dynamically created tables,"<p>I wanted to hear your advice about a potential solution for an advertise agency database.</p>

<p>We want to build a system that will be able to track users in a way that we know
what they did on the ads, and where.</p>

<p>There are many type of ads, and some of them also FORMS, so user can fill data.
Each form is different but we dont want to create table per form.</p>

<p>We thought of creating a very WIDE table with 1k columns, dozens for each type, and store the data.</p>

<p>In short:</p>

<ol>
<li>Use Cassandra;</li>
<li>Create daily tables so data will be stored on a daily table;</li>
<li>Each table will have 1000 cols (100 for datetime, 100 for int, etc).</li>
</ol>

<p>Application logic will map the data into relevant cols so we will be able to search and update those later.</p>

<p>What do you think of this ?</p>
",<cassandra>,"<p>Be careful with generating tables dynamically in Cassandra. You will start to have problems when you have too many tables because there is a per table memory overhead. Per <a href=""https://stackoverflow.com/questions/11256881/what-is-the-maximum-number-of-keyspaces-in-cassandra"">Jonathan Ellis</a>:</p>

<blockquote>
  <p>Cassandra will reserve a minimum of 1MB for each CF's memtable: <a href=""http://www.datastax.com/dev/blog/whats-new-in-cassandra-1-0-performance"" rel=""nofollow noreferrer"">http://www.datastax.com/dev/blog/whats-new-in-cassandra-1-0-performance</a></p>
</blockquote>

<p>Even daily tables are not a good idea in Cassandra (tables per form is even worse). I recommend you build a table that can hold all your data and you know will scale well -- verify this with <a href=""http://www.datastax.com/dev/blog/improved-cassandra-2-1-stress-tool-benchmark-any-schema"" rel=""nofollow noreferrer"">cassandra-stress</a>.</p>

<p>At this point, heed mikea's advice and start thinking about your access patterns (see Patrick's <a href=""http://www.datastax.com/resources/data-modeling"" rel=""nofollow noreferrer"">video series</a>), you may have to build additional tables to meet your querying needs.</p>

<p><strong>Note:</strong> For anyone wishing for a schemaless option in c*:
<a href=""https://blog.compose.io/schema-less-is-usually-a-lie/"" rel=""nofollow noreferrer"">https://blog.compose.io/schema-less-is-usually-a-lie/</a>
<a href=""http://rustyrazorblade.com/2014/07/the-myth-of-schema-less/"" rel=""nofollow noreferrer"">http://rustyrazorblade.com/2014/07/the-myth-of-schema-less/</a></p>
",['table']
29793438,29795048,2015-04-22 09:36:43,"cassandra, select via a non primary key","<p>I'm new with cassandra and I met a problem. I created a keyspace demodb and a table users. This table got 3 columns: id (int and primary key), firstname (varchar), name (varchar).
this request send me the good result:</p>

<pre><code>SELECT * FROM demodb.users WHERE id = 3;
</code></pre>

<p>but this one:</p>

<pre><code>SELECT * FROM demodb.users WHERE firstname = 'francois';
</code></pre>

<p>doesn't work and I get the following error message:</p>

<pre><code>InvalidRequest: code=2200 [Invalid query] message=""No secondary indexes on the restricted columns support the provided operators: ""
</code></pre>

<p>This request also doesn't work:</p>

<pre><code>SELECT * FROM users WHERE firstname  = 'francois'  ORDER BY id DESC LIMIT 5;
InvalidRequest: code=2200 [Invalid query] message=""ORDER BY with 2ndary indexes is not supported.""
</code></pre>

<p>Thanks in advance.</p>
",<cassandra><cql><cqlsh>,"<blockquote>
  <p>This request also doesn't work:</p>
</blockquote>

<p>That's because you are mis-understanding how sort order works in Cassandra.  Instead of using a secondary index on <code>firstname</code>, create a table specifically for this query, like this:</p>

<pre><code>CREATE TABLE usersByFirstName (
  id int,
  firstname text,
  lastname text,
  PRIMARY KEY (firstname,id));
</code></pre>

<p>This query should now work:</p>

<pre><code>SELECT * FROM usersByFirstName WHERE firstname='francois'
ORDER BY id DESC LIMIT 5;
</code></pre>

<p>Note, that I have created a compound primary key on <code>firstname</code> and <code>id</code>.  This will partition your data on <code>firstname</code> (allowing you to query by it), while also clustering your data by <code>id</code>.  By default, your data will be clustered by <code>id</code> in ascending order.  To alter this behavior, you can specify a <code>CLUSTERING ORDER</code> in your table creation statement:</p>

<pre><code>WITH CLUSTERING ORDER BY (id DESC)
</code></pre>

<p>...and then you won't even need an <code>ORDER BY</code> clause.</p>

<p>I recently wrote an article on how clustering order works in Cassandra (<a href=""http://planetcassandra.org/blog/we-shall-have-order/"">We Shall Have Order</a>).  It explains this, and covers some ordering strategies as well.</p>
",['table']
29798137,29825858,2015-04-22 12:50:26,What is the graveyard compaction in cassandra?,"<p>Cassandra noob here. While going through the documentation etc, I'm repetitevely finding references to graveyard compaction (and occasionally tombstones), example:</p>

<pre><code>$ cassandra-cli 
[default@unknown] help truncate;
...
A snapshot of the data is created, which is deleted asyncronously during a
'graveyard' compaction.
</code></pre>

<p>I've tried googling around, but that just turns up more places where graveyard compaction is referred, examples <a href=""http://www.ukessays.com/essays/engineering/structure-and-apis-for-cassandra-engineering-essay.php"" rel=""nofollow"">this</a>, <a href=""http://marc.info/?l=cassandra-user&amp;m=133291986931370"" rel=""nofollow"">this</a> and <a href=""https://www.mail-archive.com/commits@cassandra.apache.org/msg100761.html"" rel=""nofollow"">this</a>.</p>

<p>What exactly is the graveyard compaction? And what do people mean when they refer to tombstones?</p>
",<cassandra><cassandra-2.0><cassandra-cli>,"<p>A Graveyard compaction is a compaction that deletes SSTables that are no longer needed.</p>

<p>For instance during a drop, after the snapshot is made, the sstables of the respective table are deleted by a graveyard compaction.</p>

<pre><code>A snapshot of the data is created, which is deleted asyncronously during a
'graveyard' compaction.
</code></pre>

<p>What you found here is just an error in the documentation. It should say:</p>

<pre><code>A snapshot of the data is created, 
then the data is deleted asyncronously during a 'graveyard' compaction.
</code></pre>
",['table']
29933879,29934057,2015-04-29 03:36:29,Cassandra Clustering Key not able to order by,"<p>I am new to cassandra and I am trying to figure out why I cannot order my logs by the created_at date.
The following are the table description, the select result and the select statement I am trying to create.</p>

<pre><code>    cassandra@cqlsh:mytable&gt; DESCRIBE TABLE mytable.log;

    CREATE TABLE mytable.log (
        id uuid,
        created_at timestamp,
        deleted boolean,
        level text,
        message text,
        obj text,
        obj_name text,
        origin text,
        user int,
        PRIMARY KEY (id, created_at)
    ) WITH CLUSTERING ORDER BY (created_at DESC)
        AND bloom_filter_fp_chance = 0.01
        AND caching = '{""keys"":""ALL"", ""rows_per_partition"":""NONE""}'
        AND comment = ''
        AND compaction = {'min_threshold': '4', 'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32'}
        AND compression = {'sstable_compression': 'org.apache.cassandra.io.compress.LZ4Compressor'}
        AND dclocal_read_repair_chance = 0.1
        AND default_time_to_live = 0
        AND gc_grace_seconds = 864000
        AND max_index_interval = 2048
        AND memtable_flush_period_in_ms = 0
        AND min_index_interval = 128
        AND read_repair_chance = 0.0
        AND speculative_retry = '99.0PERCENTILE';
    CREATE INDEX deleted_idx ON mytable.log (deleted);
    CREATE INDEX level_idx ON mytable.log (level);
    CREATE INDEX message_idx ON mytable.log (message);
    CREATE INDEX origin_idx ON mytable.log (origin);
    CREATE INDEX user_idx ON mytable.log (user);

    cassandra@cqlsh:mytable&gt; SELECT *  FROM mytable.log  WHERE ""created_at"" &lt;= '2015-04-29 00:00:00' AND ""user"" = 20 LIMIT 10;

     id                                   | created_at               | deleted | level | message | obj                                                                                                                                                                                                                                                                                                                                                | obj_name | origin          | user
    --------------------------------------+--------------------------+---------+-------+---------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+-----------------+------
     a98a98d5-5710-431b-a23d-d78ece882763 | 2015-04-28 19:18:34-0400 |   False |  net | updated |  {'prio': None, 'type_id': u'A', 'auth': None, 'is_free': False, 'ttl': 300L, 'active': True, 'domain_id': 32L, 'ordername': None, 'name': u'myrecord.mytable.net', 'created': '2015-04-14 17:44:23+00:00', 'modified': '2015-04-29 03:18:34.159619+00:00', 'id': 143L, 'content': u'192.213.216.16', 'change_date': 1430277514, 'owner_id': 20L} |   Record | update_a_record |   20
     893e9600-3d57-4b82-bdfd-41586023a90f | 2015-04-28 19:21:01-0400 |   False |  net | updated | {'prio': None, 'type_id': u'A', 'auth': None, 'is_free': False, 'ttl': 300L, 'active': True, 'domain_id': 32L, 'ordername': None, 'name': u'myrecord.mytable.net', 'created': '2015-04-14 17:44:23+00:00', 'modified': '2015-04-29 03:21:01.414393+00:00', 'id': 143L, 'content': u'192.213.15.16', 'change_date': 1430277661, 'owner_id': 20L} |   Record | update_a_record |   20
     f951b3ec-092a-4e9e-95c5-a6dce3363c29 | 2015-04-28 19:18:35-0400 |   False |  net | updated | {'prio': None, 'type_id': u'A', 'auth': None, 'is_free': False, 'ttl': 300L, 'active': True, 'domain_id': 32L, 'ordername': None, 'name': u'myrecord.mytable.net', 'created': '2015-04-14 17:44:23+00:00', 'modified': '2015-04-29 03:18:35.199869+00:00', 'id': 143L, 'content': u'192.213.15.16', 'change_date': 1430277515, 'owner_id': 20L} |   Record | update_a_record |   20
     db60ac52-39e9-4b46-accb-28a34b10579c | 2015-04-28 19:18:37-0400 |   False |  net | updated | {'prio': None, 'type_id': u'A', 'auth': None, 'is_free': False, 'ttl': 300L, 'active': True, 'domain_id': 32L, 'ordername': None, 'name': u'myrecord.mytable.net', 'created': '2015-04-14 17:44:23+00:00', 'modified': '2015-04-29 03:18:37.650135+00:00', 'id': 143L, 'content': u'192.213.15.16', 'change_date': 1430277517, 'owner_id': 20L} |   Record | update_a_record |   20
     336acc47-6a93-4ff9-a6c5-d29d3b2c4e35 | 2015-04-28 19:23:24-0400 |   False |  net | updated | {'prio': None, 'type_id': u'A', 'auth': None, 'is_free': False, 'ttl': 300L, 'active': True, 'domain_id': 32L, 'ordername': None, 'name': u'myrecord.mytable.net', 'created': '2015-04-14 17:44:23+00:00', 'modified': '2015-04-29 03:23:24.146505+00:00', 'id': 143L, 'content': u'192.213.15.16', 'change_date': 1430277804, 'owner_id': 20L} |   Record | update_a_record |   20
     4ca66f70-36cb-47cc-9324-6a5747d6a592 | 2015-04-28 19:18:48-0400 |   False |  net | updated | {'prio': None, 'type_id': u'A', 'auth': None, 'is_free': False, 'ttl': 300L, 'active': True, 'domain_id': 32L, 'ordername': None, 'name': u'myrecord.mytable.net', 'created': '2015-04-14 17:44:23+00:00', 'modified': '2015-04-29 03:18:48.242689+00:00', 'id': 143L, 'content': u'192.213.15.16', 'change_date': 1430277528, 'owner_id': 20L} |   Record | update_a_record |   20
     dbfda8bc-f6f2-4b97-b3c1-ccaff21338bb | 2015-04-28 19:18:32-0400 |   False |  net | updated | {'prio': None, 'type_id': u'A', 'auth': None, 'is_free': False, 'ttl': 300L, 'active': True, 'domain_id': 32L, 'ordername': None, 'name': u'myrecord.mytable.net', 'created': '2015-04-14 17:44:23+00:00', 'modified': '2015-04-29 03:18:32.857508+00:00', 'id': 143L, 'content': u'192.213.15.16', 'change_date': 1430277512, 'owner_id': 20L} |   Record | update_a_record |   20
     6c05779a-d3b8-40ac-84ee-af91a3bf6b15 | 2015-04-28 19:18:47-0400 |   False |  net | updated |  {'prio': None, 'type_id': u'A', 'auth': None, 'is_free': False, 'ttl': 300L, 'active': True, 'domain_id': 32L, 'ordername': None, 'name': u'myrecord.mytable.net', 'created': '2015-04-14 17:44:23+00:00', 'modified': '2015-04-29 03:18:47.181657+00:00', 'id': 143L, 'content': u'192.213.216.16', 'change_date': 1430277527, 'owner_id': 20L} |   Record | update_a_record |   20
     a037fb9d-cb58-4994-baad-88c441429199 | 2015-04-28 19:18:31-0400 |   False |  net | updated |  {'prio': None, 'type_id': u'A', 'auth': None, 'is_free': False, 'ttl': 300L, 'active': True, 'domain_id': 32L, 'ordername': None, 'name': u'myrecord.mytable.net', 'created': '2015-04-14 17:44:23+00:00', 'modified': '2015-04-29 03:18:31.680786+00:00', 'id': 143L, 'content': u'192.213.216.16', 'change_date': 1430277511, 'owner_id': 20L} |   Record | update_a_record |   20
     66ee42af-6770-4ef8-a300-764246ccc8ff | 2015-04-28 19:20:33-0400 |   False |  net | updated | {'prio': None, 'type_id': u'A', 'auth': None, 'is_free': False, 'ttl': 300L, 'active': True, 'domain_id': 32L, 'ordername': None, 'name': u'myrecord.mytable.net', 'created': '2015-04-14 17:44:23+00:00', 'modified': '2015-04-29 03:20:33.336544+00:00', 'id': 143L, 'content': u'192.213.15.16', 'change_date': 1430277633, 'owner_id': 20L} |   Record | update_a_record |   20
</code></pre>

<p>What I don't understand is that it doesn't order by the created_at column in a descending order.
My end goal is to store the logs of my app in this table and then be able to only show a few of them in a dashboard that is why I do a limit of 10.</p>

<p>What am I doing wrong here?
Regards</p>
",<cassandra><data-modeling><cql>,"<blockquote>
  <p>What I don't understand is that it doesn't order by the created_at column in a descending order. </p>
</blockquote>

<p>Because Cassandra will only enforce a clustering order within a partition key.  Your partition key is <code>id</code>.  But that looks like it has an almost unique level of cardinality.  So unique, that if you partition on it you won't have any data to within to make sorting worthwhile.</p>

<pre><code>SELECT *  FROM mytable.log  
WHERE ""created_at"" &lt;= '2015-04-29 00:00:00' AND ""user"" = 20 LIMIT 10;
</code></pre>

<p>To satisfy this query, you should create a separate query table partitioned by <code>user</code>, such as <code>logByUser</code>.  You'll want that table to have the same columns, but with a PRIMARY KEY definition like this:</p>

<pre><code>PRIMARY KEY (user, created_at, id)
</code></pre>

<p>This PRIMARY KEY definition will allow the following query to function as you expect:</p>

<pre><code>SELECT *  FROM mytable.logByUser  
WHERE ""created_at"" &lt;= '2015-04-29 00:00:00' AND ""user"" = 20 LIMIT 10;
</code></pre>

<p>Also, I'd like to point out two things:</p>

<ol>
<li><p>Cassandra functions best when you design your data model to fit your query patterns.  That may mean creating a table for each query.  As crazy as this might sound, creating five or six tables to suit each of your potential queries will perform <strong>much</strong> better than adding 5 secondary indexes to one table.</p></li>
<li><p>Secondary indexes are meant for convenience, not performance.  Their use is a known Cassandra anti-pattern.  Using them on low-cardinality columns (especially booleans) is asking for trouble.  They are not intended as a ""magic bullet"" to bridge the shortcomings of your data model.</p></li>
</ol>
",['table']
29976998,29993081,2015-04-30 20:14:48,How to Connect to Cassandra Remotely Using DevCenter,"<p>I setup the DataStax Cassandra Sandbox on Azure using their image. I was able to run OpsCenter locally on the server without any issues. The install is Ubuntu which I am very new to. </p>

<p>Per this post <a href=""https://stackoverflow.com/questions/12236898/apache-cassandra-remote-access"">Apache Cassandra remote access</a>, <strong>I should be able to set my rpc_address to 0.0.0.0 to allow remote access to my database</strong>. However it says unable to connect when attempting a connection from DevCenter on my local Windows 8 PC.</p>

<p>Here are my settings:
<img src=""https://i.stack.imgur.com/9MlTP.png"" alt=""enter image description here""></p>

<p>The contact host address is the virtual ip address shown in Azure for my VM. The port is the same one shown in the cassandra.yaml config file. <strong>I haven't configured any authorization and from what I have read I should just be able to connect using .NET or the management tools but neither works.</strong></p>

<p><img src=""https://i.stack.imgur.com/f8krr.png"" alt=""enter image description here""></p>

<p>I also checked to see if the ports are open which they are as far as I can tell:
<img src=""https://i.stack.imgur.com/wGKJW.png"" alt=""enter image description here""></p>

<p>Far I know it would be either 9160 or 9042.</p>
",<.net><azure><cassandra><datastax>,"<p>Thanks to everyone who helped me figured this out. Ultimately the issue was that when setting up an Azure VM, the Virtual IP that is assigned is for the cloud service itself and not the virtual machine. Therefore even though it appears that the proper ports are exposed you are not able to access them from an external computer.</p>

<p>More <a href=""http://blogs.technet.com/b/canitpro/archive/2014/10/28/step-by-step-assign-a-public-ip-to-a-vm-in-azure.aspx"" rel=""nofollow"" title=""more info here"">info about it here</a> <strong>(but read my instructions below first since this is much easier to do in the Azure Management Console)</strong>.</p>

<p><strong>You will notice when setting up your virtual machine that Azure automatically creates an endpoint for your SSH connection such as 55xxx. You won't be able to connect to the configured port of 22 as shown on the box itself, but instead will have to use the endpoint port of 55xxx, etc.</strong> </p>

<p><strong>This is important to note because the same goes for the Cassandra ports 8888 (OpsCenter), and 9042 (native transport).</strong> </p>

<p>So you can either:</p>

<ol>
<li>Create endpoints for these ports and use them when connecting
remotely.</li>
<li>Create a public IP address that points to the VM itself
rather than the cloud service.</li>
</ol>

<p>I couldn't get the endpoints to work at first, <em>but later got them working</em>. This lead me to setup a public ip address. I did it the hard way using the Azure Powershell. This was painful and a lot of research. <strong>But, after spending the time to do this, I realized it can now be done in the preview console. Simply go to the IP Addresses settings on your VM and enable the ""Instance IP Address"" option.</strong> </p>

<p>Then you should be able to connect remotely to OpsCenter using the ip address that is returned after the setup is complete via your browser: (The New IP Address):8888 </p>

<p>...then in DevCenter using the new ip address and port 9042. </p>

<p>If you used endpoints instead of setting up a public static ip <strong>(which you will want to do for security reasons and enable user access control via ip filters)</strong>, then you will want to use those newly created port numbers instead along with your virtual ip address.</p>

<p>Secondly... you will need to set rpc_address to 0.0.0.0 in the cassandra.yaml file.</p>
",['rpc_address']
30073253,30079778,2015-05-06 09:52:34,Cassandra datastax driver ResultSet sharing in multiple threads for fast reading,"<p>I've huge tables in cassandra, more than 2 billions rows and increasing. The rows have a date field and it is following date bucket pattern so as to limit each row.</p>

<p>Even then, I've more than a million entries for a particular date. </p>

<p>I want to read and process rows for each day as fast as possible. What I am doing is that getting instance of <code>com.datastax.driver.core.ResultSet</code> and obtain iterator from it and share that iterator across multiple threads.</p>

<p>So, essentially I want to increase the read throughput. Is this the correct way? If not, please suggest a better way.</p>
",<cassandra><cassandra-2.0><datastax-java-driver>,"<p>Unfortunately you cannot do this as is.  The reason why is that a ResultSet provides an <a href=""https://github.com/datastax/java-driver/blob/2.0/driver-core/src/main/java/com/datastax/driver/core/PagingState.java"">internal paging state</a> that is used to retrieve rows 1 page at a time.</p>

<p>You do have options however.  Since I imagine you are doing range queries (queries across multiple partitions), you can use a strategy where you submit multiple queries across token ranges at a time using the token directive.  A good example of this is documented in <a href=""http://docs.datastax.com/en/cql/3.0/cql/cql_using/paging_c.html"">Paging through unordered partitioner results</a>.</p>

<p>java-driver 2.0.10 and 2.1.5 each provide a mechanism for retrieving token ranges from Hosts and <a href=""https://github.com/datastax/java-driver/blob/2.1.5/driver-core/src/main/java/com/datastax/driver/core/TokenRange.java#L65-L90"">splitting them</a>.  There is an example of how to do this in the java-driver's integration tests in <a href=""https://github.com/datastax/java-driver/blob/2.1/driver-core/src/test/java/com/datastax/driver/core/TokenIntegrationTest.java#L96-L127"">TokenRangeIntegrationTest.java#should_expose_token_ranges()</a>:</p>

<pre class=""lang-java prettyprint-override""><code>    PreparedStatement rangeStmt = session.prepare(""SELECT i FROM foo WHERE token(i) &gt; ? and token(i) &lt;= ?"");

    TokenRange foundRange = null;
    for (TokenRange range : metadata.getTokenRanges()) {
        List&lt;Row&gt; rows = rangeQuery(rangeStmt, range);
        for (Row row : rows) {
            if (row.getInt(""i"") == testKey) {
                // We should find our test key exactly once
                assertThat(foundRange)
                    .describedAs(""found the same key in two ranges: "" + foundRange + "" and "" + range)
                    .isNull();
                foundRange = range;
                // That range should be managed by the replica
                assertThat(metadata.getReplicas(""test"", range)).contains(replica);
            }
        }
    }
    assertThat(foundRange).isNotNull();
}
...
private List&lt;Row&gt; rangeQuery(PreparedStatement rangeStmt, TokenRange range) {
    List&lt;Row&gt; rows = Lists.newArrayList();
    for (TokenRange subRange : range.unwrap()) {
        Statement statement = rangeStmt.bind(subRange.getStart(), subRange.getEnd());
        rows.addAll(session.execute(statement).all());
    }
    return rows;
}
</code></pre>

<p>You could basically generate your statements and submit them in async fashion, the example above just iterates through the statements one at a time.</p>

<p>Another option is to use the <a href=""https://github.com/datastax/spark-cassandra-connector"">spark-cassandra-connector</a>, which essentially does this under the covers and in a very efficient way.  I find it very easy to use and you don't even need to set up a spark cluster to use it.  See <a href=""https://github.com/datastax/spark-cassandra-connector/blob/master/doc/7_java_api.md"">this document</a> for how to use the Java API.</p>
",['partitioner']
30101184,30109268,2015-05-07 12:32:25,Partition key column in Cassandra,"<p>I want to understand exactly what will improve my performance if I decide to go with following strategy for partition </p>

<p>Lets say I have a table for songs and I want to define artists as the partition key. This table is going to grow gradually. Today I have 25 artists and 5 songs each for those 25 artists (so total 125 rows). But over a period of time i foresee 500 artists and 5 songs per artists (so total 2500) rows. I want to make artist id as partition key because in CQL it is necessary to mention partition key in where clause and in my ui this is the unique value based on which i can show those 5 songs. </p>

<p>Also, what if I start with 2 cassandra nodes today and eventually grow to 4 nodes and then later 10 nodes. Can I continue to have the same partition key as I grow?</p>

<p>Here is my table structure :</p>

<pre><code>ArtistId (partition key)  |  SongId  |  Song
--------------------------------------------
1                         | 1        |  abc
1                         | 2        |  cde
1                         | 3        |  fgh
2                         | 4        |  ijk
2                         | 5        |  lmn
1                         | 6        |  opq
1                         | 7        |  rst
</code></pre>
",<cassandra><cql><datastax><datastax-enterprise><cql3>,"<blockquote>
  <p>Also, what if I start with 2 cassandra nodes today and eventually grow to 4 nodes and then later 10 nodes. Can I continue to have the same partition key as I grow?</p>
</blockquote>

<p>Yes, you can keep your partition key.</p>

<blockquote>
  <p>I want to understand exactly what will improve my performance if I decide to go with following strategy for partition</p>
</blockquote>

<p>Clarifying primary keys can be a single column, or compound, when compound can have a partition key and clustering key[s].</p>

<p>Since you are saying partition key over artist, that would be your row key and I am assuming song would be your clustering key. </p>

<p>Partition keys are used to distribute across different nodes and your clustering keys the order in which they are stored. </p>

<p>Per the <a href=""http://cassandra.apache.org/doc/cql3/CQL.html#createTablepartitionClustering"" rel=""nofollow"">cql documentation</a>:</p>

<blockquote>
  <p>all the rows sharing the same partition key (even across table in fact) are stored on the same physical node</p>
</blockquote>

<p>That would be very efficient to search, since doesn't require a quorum on all nodes, instead it would be find them faster. </p>
",['table']
30156068,30156246,2015-05-10 20:00:35,No viable alternative at input 'mytable1' cassanda cql python session.execute error,"<p>I'm trying to execute a simple cql query in python and I keep getting an error.</p>

<pre><code>table1 = ""mytable1""
table2 = ""mytable2""

query1 = ""SELECT * FROM %s""
table1Rows = session.execute(query1, (table1,))
table2Rows = session.execute(query1, (table2,))
</code></pre>

<p>The table variables are actually passed in as arguments but I just made my own as an example. I get this error:</p>

<pre><code>cassandra.protocol.SyntaxException: &lt;ErrorMessage code=2000 [Syntax error in CQL query] message=""line 1:14 no viable alternative at input 'mytable1' (SELECT * FROM ['mytable]...)""&gt;
</code></pre>

<p>I can't figure out what's wrong with my syntax. Please help. Thanks!</p>
",<python><session><cassandra><cql><execute>,"<p>Parametrized queries do not support providing the table name as a parameter. You can achieve it by constructing the query string via string concatenation. Just make sure that the input variable is in a whitelist of allowed values, to guard against SQL injection.</p>
",['table']
30164572,30172567,2015-05-11 09:54:31,Store countByKey result into Cassandra,"<p>I want to count the number of IndicatePresence messages for each user for any given day (out of a Cassandra table), and then store this in a separate Cassandra table to drive some dashboard pages. I managed to get the 'countByKey' working, but now cannot figure out how to use the Spark-Cassandra 'saveToCassandra' method with a Map (it only takes RDD).</p>

<pre><code>    JavaSparkContext sc = new JavaSparkContext(conf);
    CassandraJavaRDD&lt;CassandraRow&gt; indicatePresenceTable = javaFunctions(sc).cassandraTable(""mykeyspace"", ""indicatepresence"");
    JavaPairRDD&lt;UserDate, CassandraRow&gt; keyedByUserDate = indicatePresenceTable.keyBy(new Function&lt;CassandraRow, UserDate&gt;() {
        private static final long serialVersionUID = 1L;
        @Override
        public UserDate call(CassandraRow cassandraIndicatePresenceRow) throws Exception {
            SimpleDateFormat sdf = new SimpleDateFormat(""yyyy-MM-dd"");
            return new UserDate(cassandraIndicatePresenceRow.getString(""userid""), sdf.format(cassandraIndicatePresenceRow.getDate(""date"")));
        }
    });

    Map&lt;UserDate, Object&gt; countByKey = keyedByUserDate.countByKey();

    writerBuilder(""analytics"", ""countbykey"", ???).saveToCassandra();
</code></pre>

<p>Is there a way use a Map directly in a writerBuilder? Or should I write my own custom reducer, that returns an RDD, but essentially does the same thing as the countByKey method? Or, should I convert each entry in the Map into a new POJO (eg UserDateCount, with user, date, and count) and use 'parallelize' to turn the list into an RDD and then store that?</p>
",<cassandra><apache-spark>,"<p>The best thing to do would be to never return the result to the driver (by using countByKey). Instead do a reduceByKey to get another RDD back in the form of <code>(key, count)</code>. Map that RDD to the row format of your table and then call <code>saveToCassandra</code> on that. </p>

<p>The most important strength of this approach is we never serialize the data back to the driver application. All the information is kept on the cluster and saved from their directly to C* rather than running through the bottleneck of the driver application.</p>

<p>Example (Very Similar to a Map Reduce Word Count):</p>

<ol>
<li>Map each element to (key, 1) </li>
<li>Call reduceByKey to change (key, 1) -> (key, count) </li>
<li>Map each element to something writeable to C* (key,count)-> WritableObject </li>
<li>Call save to C*</li>
</ol>

<p>In Scala this would be something like</p>

<pre><code>keyedByUserDate
  .map(_.1, 1)                               // Take the Key portion of the tuple and replace the value portion with 1
  .reduceByKey( _ + _ )                      // Combine the value portions for all elements which share a key
  .map{ case (key, value) =&gt; your C* format} // Change the Tuple2 to something that matches your C* table
  .saveToCassandra(ks,tab)                   // Save to Cassandra
</code></pre>

<p>In Java it is a little more convoluted (Insert your types in for K and V)</p>

<pre><code>.mapToPair(new PairFunction&lt;Tuple2&lt;K,V&gt;,K,Long&gt;&gt;, Tuple2&lt;K, Long&gt;(){
    @Override
    public Tuple2&lt;K, Long&gt; call(Tuple2&lt;K, V&gt; input) throws Exception {
      return new Tuple2(input._1(),1)
    }
}.reduceByKey(new Function2(Long,Long,Long)(){
    @Override
    public Long call(Long value1, Long value2) throws Exception {
      return value1 + value2
    }
}.map(new Function1(Tuple2&lt;K, Long&gt;, OutputTableClass)(){  
    @Override
    public OutputTableClass call(Tuple2&lt;K,Long&gt; input) throws Exception {
    //Do some work here
    return new OutputTableClass(col1,col2,col3 ... colN)
   }
}.saveToCassandra(ks,tab, mapToRow(OutputTableClass.class))
</code></pre>
",['table']
30168264,30170040,2015-05-11 12:55:51,Using secondary indexes to update rows in Cassandra 2.1,"<p>I'm using Cassandra 2.1 and have a model that roughly looks as follows:</p>

<pre><code>CREATE TABLE events (
  client_id bigint,
  bucket int,
  timestamp timeuuid,
  ...
  ticket_id bigint,
  PRIMARY KEY ((client_id, bucket), timestamp)
);
CREATE INDEX events_ticket ON events(ticket_id);
</code></pre>

<p>As you can see, I've created a secondary index on <code>ticket_id</code>. This index works ok. <code>events</code> contains around 100 million rows, while only 5 million of these rows have around 50,000 distinct tickets. So a ticket - on average - has 100 events.</p>

<p>Querying the secondary index works without supplying the partition key, which is convenient in our situation. As the <code>bucket</code> column is sometimes hard to determine beforehand (i.e. you should know the date of the events, <code>bucket</code> is currently the date).</p>

<pre><code>cqlsh&gt; select * from events where ticket_id = 123;

 client_id | bucket | timestamp | ... | ticket_id
-----------+--------+-----------+-----+-----------

(0 rows)
</code></pre>

<p>How do I solve the problem when all events of a ticket should be moved to another ticket? I.e. the following query won't work:</p>

<pre><code>cqlsh&gt; UPDATE events SET ticket_id = 321 WHERE ticket_id = 123;
InvalidRequest: code=2200 [Invalid query] message=""Non PRIMARY KEY ticket_id found in where clause""
</code></pre>

<p>Does this imply secondary indexes cannot be used in <code>UPDATE</code> queries?</p>

<p>What model should I use to support these changes?</p>
",<cassandra><cassandra-2.1>,"<p>First of all, <code>UPDATE</code> and <code>INSERT</code> operations are treated the same in Cassandra.  They are colloquially known as ""UPSERTs.""</p>

<blockquote>
  <p>Does this imply secondary indexes cannot be used in UPDATE queries?</p>
</blockquote>

<p>Correct.  You cannot perform an UPSERT in Cassandra without specifying the complete PRIMARY KEY.  Even UPSERTs with a partial PRIMARY KEY will not work.  And (as you have discovered) UPSERTing by an indexed value does not work, either.</p>

<blockquote>
  <p>How do I solve the problem when all events of a ticket should be moved to another ticket?</p>
</blockquote>

<p>Unfortunately, the only way to accomplish this, is to query the keys of each row in <code>events</code> (with a particular <code>ticket_id</code>) and UPSERT <code>ticket_id</code> by those keys.  The nice thing, is that you don't have to first <code>DELETE</code> them, because <code>ticket_id</code> is not part of the PRIMARY KEY.</p>

<blockquote>
  <p>How do I solve the problem when all events of a ticket should be moved to another ticket?</p>
</blockquote>

<p>I think your best plan here would be to forego a secondary index all together, and create a query table to work alongside your <code>events</code> table:</p>

<pre><code>CREATE TABLE eventsbyticketid (
  client_id bigint,
  bucket int,
  timestamp timeuuid,
  ...
  ticket_id bigint,
  PRIMARY KEY ((ticket_id), timestamp)
) WITH CLUSTERING ORDER BY (timestamp DESC);
</code></pre>

<p>This would allow you to query by <code>ticket_id</code> quickly (to obtain your <code>client_id</code>, <code>bucket</code>, and <code>timestamp</code>.  This would give you the information you need to UPSERT the new <code>ticket_id</code> on your <code>events</code> table.</p>

<p>You could also then perform a <code>DELETE</code> by <code>ticket_id</code> (on the <code>eventsbyticketid</code> table).  Cassandra does allow a <code>DELETE</code> operation with a partial PRIMARY KEY, as long as you have the full partition key (<code>ticket_id</code>).  So removing old <code>ticket_id</code>s from the query table would be easy.  And to ensure write atomicity, you could batch the UPSERTs together:</p>

<pre><code>BEGIN BATCH
  UPDATE events SET ticket_id = 321 WHERE client_id=2112 AND bucket='2015-04-22 14:53' AND timestamp=4a7e2730-e929-11e4-88c8-21b264d4c94d;
  UPDATE eventsbyticketid SET client_id=2112, bucket='2015-04-22 14:53' WHERE ticket_id=321 AND timestamp=4a7e2730-e929-11e4-88c8-21b264d4c94d
APPLY BATCH;
</code></pre>

<p>Which is actually the same as performing:</p>

<pre><code>BEGIN BATCH
  INSERT INTO events (client_id,bucket,timestamp,ticketid) VALUES(2112,'2015-04-22 14:53',4a7e2730-e929-11e4-88c8-21b264d4c94d,321);
  INSERT INTO eventsbyticketid (client_id,bucket,timestamp,ticketid) VALUES(2112,'2015-04-22 14:53',4a7e2730-e929-11e4-88c8-21b264d4c94d,321);
APPLY BATCH;
</code></pre>

<p>Side note: <code>timestamp</code> is actually a (reserved word) data type in Cassandra.  This makes it a pretty lousy name for a <code>timeuuid</code> column.</p>
",['table']
30205795,30209147,2015-05-13 04:37:56,"I am getting an InvalidTypeException whenever I am using the row.getToken(""fieldname"")?","<p>for the following piece of code I am getting an InvalidTypeException whenever I am using the <code>row.getToken(""fieldname"")</code>.</p>

<pre><code>Record RowToRecord(Row rw) {

    ColumnDefinitions cd = rw.getColumnDefinitions();
    Record rec = new Record();
    int i;
    for(i = 0; i &lt; cd.size(); i++) {
        rec.fields.add(cd.getName(i));
        System.out.println(cd.getName(i));
        //System.out.println((rw.getToken(cd.getName(i))).getValue());
        Token tk = rw.getToken(cd.getName(i));    //// InvalidTypeException on this line.
        //System.out.println(tk.getValue()+"" ""+tk.getType().toString());
        rec.values.add(tk.getValue());
        rec.types.add(tk.getType().toString());
        //Token tk = new Token();

    }
    return rec;
}
</code></pre>
",<cassandra><datastax-java-driver>,"<p><code>getToken</code> is meant to be called on a column that contains a Cassandra token. In 99% of cases, that will be the result of a call to the <code>token()</code> CQL function, for example the first column in this query:</p>

<pre><code>select token(id), col1 from my_table where id = ...
</code></pre>

<p>Your code is calling it for all columns, which will fail as soon as you have a column that doesn't match the CQL type for tokens.</p>

<p>That CQL type depends on the partitioner used in your cluster:</p>

<ul>
<li>murmur3 partitioner (the default): <code>token(...)</code> will return a BIGINT</li>
<li>random partitioner: VARINT</li>
<li>ordered partitioner: BLOB</li>
</ul>

<p>In theory you can call <code>getToken</code> on any column with this type (although in practice it probably only makes sense for columns that are the result of a <code>token()</code> call, as explained above).</p>
",['partitioner']
30220802,35818168,2015-05-13 16:46:21,Cassandra table synchronization,"<p>I just read the DataStax post ""<a href=""http://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling"" rel=""noreferrer"">Basic Rules of Cassandra Data Modeling</a>"" and, to sum up, we should modeling our database schema by our queries and not by our relations/objects. So, many tables can have the same duplicated data, for example <code>users_by_email</code> and <code>users_by_username</code> which both have the same data.</p>

<p>How can I handle the object update ?<br>
For example the user edit his email, do I <code>UPDATE</code> both tables manually or only <code>INSERT</code> the object with all columns and don't care about previous data (which are still in my database, but with a wrong column value => email).</p>

<p>In case of <code>UPDATE</code>, how can I handle data synchronization ?<br>
Currently, I'm doing it manually but is there a tool to help me ? Because, possibly, I can have 5 or 6 tables with different partition/clustering keys.<br>
I heard that Hadoop can do it, or Apache Spark.</p>
",<cassandra><datastax><nosql>,"<p>In Cassadnra, given an existing record, an update or insert using the same primary key will result in the old record marked for deletion (with a tombstone) and the new record becomes ""live"". There're few subtleties in the difference between Insert and Update, like counters and null values, but those are probably not relevant for the question.</p>

<p>Up to Cassandra 3.0, the responsibility of maintaining several views of the same data in sync is in hands of the client application. And yes, it means to insert/update the new data in all the different tables that require it.</p>

<p>Cassandra 3.0 introduced <a href=""http://www.datastax.com/dev/blog/new-in-cassandra-3-0-materialized-views"" rel=""nofollow"">""Materialized Views""</a>, which lets you maintain a ""master"" table of the data and several views on it, all managed by Cassandra. It requires careful data modelling so that the primary key of the 'master' table  contains the required entities to create the different views and related queries needed. </p>

<p>One additional note: If you find that your data is highly relational and requires several/many views to make it query-able, maybe Cassandra is not a good fit for the problem space and probably you should consider a RDBMS instead.</p>

<p>To extend on the example provided, probably user information is something we would like to keep in a relational DB, while high volume actions of those users could be registered in Cassandra. (purchases, clicks, heart rate samples, ...)</p>
",['table']
30262550,30263542,2015-05-15 14:50:00,How to model data in Cassandra for last 100 events for a customer,"<p>We have multiple customers with each customer running multiple sensors. Each sensor logs data frequently (event 20s). How do I create a data model in Cassandra to answer this query? </p>

<p>We have thought of the data model like this for other queries: </p>

<pre><code>Create Table Data{
  CustomerId,
  SensorId,
  Date,
  DataTime
  SensorData1,
  SensorData2,
  Primary key ((CustomerId, SensorId, Date), DataTime) 
}
</code></pre>
",<cassandra><data-modeling><cql>,"<p>To satisfy a query of the last 100 events for a customer, you'll need to make two adjustments to the model above:</p>

<ol>
<li><p>Adjust your PRIMARY KEY definition to partition only on your <code>CustomerId</code> and your date bucket (<code>Date</code>).  Then, you'll want to cluster on <code>DataTime</code>.  To ensure uniqueness of sensor, you'll probably also want to add <code>SensorId</code> on the end.</p></li>
<li><p>Add a <code>CLUSTERING ORDER</code> with a sort direction of <code>DESC</code> on <code>datatime</code>.  This will cluster your data on-disk by <code>datatime</code>, sorted with the most-recent times first.</p></li>
</ol>

<p>Basically, I created your table like this:</p>

<pre><code>CREATE TABLE sensordata2 (
    customerid uuid,
    datebucket text,
    datatime timeuuid,
    sensorid text,
    sensordata1 text,
    sensordata2 text,
    PRIMARY KEY ((customerid, datebucket), datatime, sensorid)
) WITH CLUSTERING ORDER BY (datatime DESC, sensorid ASC);
</code></pre>

<p>After inserting some test rows, I can now query the last 10 sensor readings <code>WHERE customerid 3221b1d7-13b4-40d4-b41c-8d885c63494f</code> like this:</p>

<pre><code>aploetz@cqlsh:stackoverflow2&gt; SELECT customerid, datebucket, sensorid, dateof(datatime), datatime, sensordata1, sensordata2
FROM sensordata2 WHERE customerid=3221b1d7-13b4-40d4-b41c-8d885c63494f 
AND datebucket='20150515' LIMIT 10;

 customerid                           | datebucket | sensorid | dateof(datatime)         | datatime                             | sensordata1 | sensordata2
--------------------------------------+------------+----------+--------------------------+--------------------------------------+-------------+-------------
 3221b1d7-13b4-40d4-b41c-8d885c63494f |   20150515 |       A1 | 2015-05-15 10:34:34-0500 | e3a15c20-fb17-11e4-93da-21b264d4c94d |          47 |          24
 3221b1d7-13b4-40d4-b41c-8d885c63494f |   20150515 |       A1 | 2015-05-15 10:34:34-0500 | e39ffc90-fb17-11e4-93da-21b264d4c94d |          46 |          23
 3221b1d7-13b4-40d4-b41c-8d885c63494f |   20150515 |       A1 | 2015-05-15 10:34:34-0500 | e39e4ee0-fb17-11e4-93da-21b264d4c94d |          45 |          22
 3221b1d7-13b4-40d4-b41c-8d885c63494f |   20150515 |       B1 | 2015-05-15 10:34:22-0500 | dc64a340-fb17-11e4-93da-21b264d4c94d |          47 |          24
 3221b1d7-13b4-40d4-b41c-8d885c63494f |   20150515 |       B1 | 2015-05-15 10:34:22-0500 | dc60aba0-fb17-11e4-93da-21b264d4c94d |          46 |          23
 3221b1d7-13b4-40d4-b41c-8d885c63494f |   20150515 |       B1 | 2015-05-15 10:34:22-0500 | dc5d0220-fb17-11e4-93da-21b264d4c94d |          45 |          22
 3221b1d7-13b4-40d4-b41c-8d885c63494f |   20150515 |       A1 | 2015-05-15 10:32:16-0500 | 90e27fa0-fb17-11e4-93da-21b264d4c94d |          47 |          24
 3221b1d7-13b4-40d4-b41c-8d885c63494f |   20150515 |       A1 | 2015-05-15 10:32:16-0500 | 90e0aae0-fb17-11e4-93da-21b264d4c94d |          46 |          23
 3221b1d7-13b4-40d4-b41c-8d885c63494f |   20150515 |       A1 | 2015-05-15 10:32:16-0500 | 90de8800-fb17-11e4-93da-21b264d4c94d |          45 |          22
 3221b1d7-13b4-40d4-b41c-8d885c63494f |   20150515 |       A1 | 2015-05-15 10:25:24-0500 | 9b5d1ae0-fb16-11e4-93da-21b264d4c94d |          47 |          24

(10 rows)
</code></pre>
",['table']
30327530,30343993,2015-05-19 13:46:12,Altering a column family in cassandra in a multiple node topology,"<p>I'm having the following issue when trying to alter cassandra: 
I'm altering the table straight forward:</p>

<pre><code>ALTER TABLE posts ADD is_black BOOLEAN;
</code></pre>

<p>on a single-node environment, both under EC2 server and on localhost everything work perfect - select, delete and so on.</p>

<p>When I'm altering on a cluster with 3 nodes - stuff are getting massy.
When I perform </p>

<pre><code>select().all().from(tableName).where..
</code></pre>

<p>I'm getting the following exception:</p>

<pre><code>java.lang.IllegalArgumentException: is_black is not a column defined in this metadata

    at com.datastax.driver.core.ColumnDefinitions.getAllIdx(ColumnDefinitions.java:273)

    at com.datastax.driver.core.ColumnDefinitions.getFirstIdx(ColumnDefinitions.java:279)

    at com.datastax.driver.core.ArrayBackedRow.getIndexOf(ArrayBackedRow.java:69)

    at com.datastax.driver.core.AbstractGettableData.getString(AbstractGettableData.java:137)
</code></pre>

<p>Apparently I'm not the only one who's having this behaviour:
<a href=""https://datastax-oss.atlassian.net/browse/JAVA-560"" rel=""nofollow"">reference</a> </p>

<p>p.s -  drop creating the keyspace is not a possibility for me since I cannot delete the data contained in the table. </p>
",<cassandra><cassandra-2.0><datastax><datastax-java-driver><cassandra-cli>,"<p>The bug was resolved :-)</p>

<p>I issue was that DataStax maintains in memory cache that contains the configuration of each node, this cache wasn't update when I alter the table since I used cqlsh instead of their SDK.</p>

<p>After restarting all the node, the in memory cache was dropped and the bug was resolved.</p>
",['table']
30331248,30335801,2015-05-19 16:30:49,"Strange Cassandra ReadTimeoutExceptions, depending on which client is querying","<p>I have a cluster of three Cassandra nodes with more or less default configuration. On top of that, I have a web layer consisting of two nodes for load balancing, both web nodes querying Cassandra all the time. After some time, with the data stored in Cassandra becoming non-trivial, one and only one of the web nodes started getting <code>ReadTimeoutException</code> on a specific query. The web nodes are identical in every way.</p>

<p>The query is very simple (<code>?</code> is placeholder for date, usually a few minutes before the current moment):</p>

<pre><code>SELECT * FROM table WHERE time &gt; ? LIMIT 1 ALLOW FILTERING;
</code></pre>

<p>The table is created with this query:</p>

<pre><code>CREATE TABLE table (
    user_id varchar,
    article_id varchar,
    time timestamp,
    PRIMARY KEY (user_id, time));
CREATE INDEX articles_idx ON table(article_id);
</code></pre>

<p>When it times-out, the client waits for a bit more than 10s, which, not surprisingly, is the timeout configured in <code>cassandra.yaml</code> for most connects and reads.</p>

<p>There are a couple of things that are baffling me:</p>

<ul>
<li>the query only timeouts when one of the web nodes execute it - one of the nodes always fail, one of the nodes always succeed. </li>
<li>the query returns instantaneously when I run it from <code>cqlsh</code> (although it seems it only hits one node when I run it from there)</li>
<li>there are other queries issued which take 2-3 minutes (a lot longer than the 10s timeout) that do not timeout at all</li>
</ul>

<p>I cannot trace the query in Java because it times out. Tracing the query in <code>cqlsh</code> didn't provide much insight. I'd rather not change the Cassandra timeouts as this is production system and I'd like to exhaust non-invasive options first. The Cassandra nodes all have plenty of heap, their heap is far from full, and GC times seem normal.</p>

<p>Any ideas/directions will be much appreciated, I'm totally out of ideas. Cassandra version is 2.0.2, using <code>com.datastax.cassandra:cassandra-driver-core:2.0.2</code> Java client. </p>
",<java><cassandra><cassandra-2.0>,"<p>A few things I noticed:</p>

<ol>
<li><p>While you are using <code>time</code> as a clustering key, it doesn't really help you because your query is not restricting by your partition key (<code>user_id</code>).  Cassandra only orders by clustering keys <em>within a partition</em>.  So right now your query is pulling back the first row which satisfies your WHERE clause, ordered by the hashed token value of <code>user_id</code>.  If you really do have tens of millions of rows, then I would expect this query to pull back data from the same <code>user_id</code> (or same select few) every time.</p></li>
<li><p><strong><em>""although it seems it only hits one node when I run it from there""</em></strong>  Actually, your queries <em>should</em> only hit one node when you run them.  Introducing network traffic into a query makes it really slow.  I think the default consistency in cqlsh is ONE.  This is where Carlo's idea comes into play.</p></li>
<li><p>What is the cardinality of <code>article_id</code>?  Remember, secondary indexes work the best on ""middle-of-the-road"" cardinality.  High (unique) and low (boolean) are both bad.</p></li>
<li><p>The <a href=""http://www.datastax.com/dev/blog/allow-filtering-explained-2"" rel=""nofollow"">ALLOW FILTERING</a> clause should not be used in (production) application-side code.  Like ever.  If you have 50 million rows in this table, then ALLOW FILTERING is first pulling all of them back, and then trimming down the result set based on your WHERE clause.</p></li>
</ol>

<p>Suggestions:</p>

<ol>
<li><p>Carlo might be on to something with the suggestion of trying a different (lower) consistency level.  Try setting a consistency level of <code>ONE</code> in your application and see if that helps.</p></li>
<li><p>Either perform an ALLOW FILTERING query, <em>or</em> a secondary index query.  They both suck, but definitely do not do both together.  I would not use either.  But if I had to pick, I would expect a secondary index query to suck less than an ALLOW FILTERING query.</p></li>
<li><p>To solve this adequately at the scale in which you are describing, I would duplicate the data into a query table.  As it looks like you are concerned with organizing time-sensitive data, and in getting the most-recent data.  A query table like this should do it:</p>

<p><code>CREATE TABLE tablebydaybucket (
    user_id varchar,
    article_id varchar,
    time timestamp,
    day_bucket varchar,
    PRIMARY KEY (day_bucket , time))
WITH CLUSTERING ORDER BY (time DESC);</code></p></li>
</ol>

<p>Populate this table with your data, and then this query will work:</p>

<pre><code>SELECT * FROM tablebydaybucket 
WHERE day_bucket='20150519' AND time &gt; '2015-05-19 15:38:49-0500' LIMIT 1;
</code></pre>

<p>This will partition your data by <code>day_bucket</code>, and cluster your data by <code>time</code>.  This way, you won't need ALLOW FILTERING or a secondary index.  Also your query is guaranteed to hit only one node, and Cassandra will not have to pull all of your rows back and apply your WHERE clause after-the-fact.  And clustering on <code>time</code> in DESCending order, helps your most-recent rows come back quicker.</p>
",['table']
30348264,30353307,2015-05-20 11:24:51,Cassandra query flexibility,"<p>I'm pretty new to the field of big data and currently stucking by a fundamental decision.</p>

<p>For a research project i need to store millions of log entries per minute to my Cassandra based data center, which works pretty fine. (single data center, 4 nodes)</p>

<pre><code>Log Entry
------------------------------------------------------------------
| Timestamp              | IP1         | IP2           ... 
------------------------------------------------------------------
| 2015-01-01 01:05:01    | 10.10.10.1  | 192.10.10.1   ...
------------------------------------------------------------------
</code></pre>

<p>Each log entry has a specific timestamp. The log entries should be queried by different time ranges in first instance. As recommended i start to ""model my query"" in a big row approach. </p>

<pre><code>Basic C* Schema
------------------------------------------------------------------
| row key              | column key a         | column key b     ... 
------------------------------------------------------------------
|  2015-01-01 01:05    | 2015-01-01 01:05:01  | 2015-01-01 01:05:23
------------------------------------------------------------------
</code></pre>

<p><em>Additional detail: 
column keys are composition of timestamp+uuid, to be unique and to    avoid overwritings;
log entries of a specific time are stored nearby on a node by its identical partition key;</em></p>

<p>Thus log entries are stored in shorttime intervals per row. For example every log entry for <code>2015-01-01 01:05</code> with the precision of a minute. Queries are not really peformed as a range query with an <code>&lt;</code> operator, rather entries are selected as blocks of a specified minute.</p>

<p>Range based queries succeed in a decent response time which is fine for me.</p>

<p><strong>Question:</strong>
In the next step we want to gain additional informations by queries, which are mainly focused on the <code>IP</code> field. For example: select all the entries which have <code>IP1=xx.xx.xx.xx</code> and <code>IP2=yy.yy.yy.yy</code>.</p>

<p>So obviously the current model is pretty not usable for additional IP focused CQL queries. So the problem is not to find a possible solution, rather the various choices of possible technologies which could be a possible solution:</p>

<ol>
<li>Try to solve the problem with standalone C* solutions. (Build a second model and administer the same data in a different shape)</li>
<li>Choose additional technologies like Spark...</li>
<li>Switch to HDFS/Hadoop - Cassandra/Hadoop solution...</li>
<li>and so on</li>
</ol>

<p>With my lack of knowledge in this field, it is pretty hard to find the best way which i should take. Especially with the feeling that the usage of a cluster computing framework would be an excessive solution.</p>
",<hadoop><cassandra><apache-spark><bigdata><cql>,"<p>As I understood your question, your table schema looks like this:</p>

<pre><code>create table logs (
  minute timestamp,
  id timeuuid,
  ips list&lt;string&gt;,
  message text,
  primary key (minute,id)
);
</code></pre>

<p>With this simple schema, you:</p>

<ul>
<li>can fetch all logs for a specific minute.</li>
<li>can fetch short inter-minute ranges of log events.</li>
<li>want to query dataset by IP.</li>
</ul>

<p>From my point of view, there are multiple ways of implementing this idea:</p>

<ul>
<li>create secondary index on IP addresses. But in C* you will lose the ability to query by timestamp: C* cannot merge primary and secondary indexes (like mysql/pgsql).</li>
<li>denormalize data. Write your log events to two tables at once, first being optimized for timestamp queries (minute+ts as PK), second being for IP-based queries (IP+ts as PK).</li>
<li>use spark for <em>analytical</em> queries. But spark will need to perform (full?) table scan (in a nifty distributed map-reduce way, but nevertheless it's a table scan) each time to extract all the data you've requested, so all your queries will require a lot of time to finish. This way can cause problems if you plan to have a lot of low-latency queries.</li>
<li>use external index like ElasticSearch for quering, and C* for storing the data.</li>
</ul>

<p>For my opinion, the C* way of doing such things is to have a set of separate tables for different queries. It will give you an ability to perform blazing-fast queries (but with increased storage cost).</p>
",['table']
30388311,30398280,2015-05-22 04:19:38,Cassandra: Selecting a Range of TimeUUIDs using the DataStax Java Driver,"<p>The use case that we are working to solve with Cassandra is this: We need to retrieve a list of entity UUIDs that have been updated within a certain time range within the last 90 days. Imagine that we're building a document tracking system, so our relevant entity is a Document, whose key is a UUID.</p>

<p>The query we need to support in this use case is: <em>Find all Document UUIDs that have changed between StartDateTime and EndDateTime.</em></p>

<p><strong>Question 1: What's the best Cassandra table design to support this query?</strong></p>

<p>I think the answer is as follows:</p>

<pre><code>CREATE TABLE document_change_events (
    event_uuid TIMEUUID,
    document_uuid uuid,
    PRIMARY KEY ((event_uuid), document_uuid)
) WITH default_time_to_live='7776000';
</code></pre>

<p>And given that we can't do range queries on partition keys, we'd need to use the <code>token()</code> method. As such the query would then be:</p>

<pre><code>SELECT document_uuid 
 WHERE token(event_uuid) &gt; token(minTimeuuid(?)) 
   AND token(event_uuid) &lt; token(maxTimeuuid(?))
</code></pre>

<p>For example:</p>

<pre><code>SELECT document_uuid 
 WHERE token(event_uuid) &gt; token(minTimeuuid('2015-05-10 00:00+0000')) 
   AND token(event_uuid) &lt; token(maxTimeuuid('2015-05-20 00:00+0000'))
</code></pre>

<p><strong>Question 2: I can't seem to get the following Java code using DataStax's driver to reliability return the correct results.</strong></p>

<p>If I run the following code 10 times pausing 30 seconds between, I will then have 10 rows in this table:</p>

<pre><code>private void addEvent() {

    String cql = ""INSERT INTO document_change_events (event_uuid, document_uuid) VALUES(?,?)"";

    PreparedStatement preparedStatement = cassandraSession.prepare(cql);
    BoundStatement boundStatement = new BoundStatement(preparedStatement);
    boundStatement.setConsistencyLevel(ConsistencyLevel.ANY);

    boundStatement.setUUID(""event_uuid"", UUIDs.timeBased());
    boundStatement.setUUID(""document_uuid"", UUIDs.random());

    cassandraSession.execute(boundStatement);

}
</code></pre>

<p>Here are the results:</p>

<pre><code>cqlsh:&gt; select event_uuid, dateOf(event_uuid), document_uuid from document_change_events;

 event_uuid                           | dateOf(event_uuid)       | document_uuid
--------------------------------------+--------------------------+--------------------------------------
 414decc0-0014-11e5-93a9-51f9a7931084 | 2015-05-21 18:51:09-0500 | 92b6fb6a-9ded-47b0-a91c-68c63f45d338
 9abb4be0-0014-11e5-93a9-51f9a7931084 | 2015-05-21 18:53:39-0500 | 548b320a-10f6-409f-a921-d4a1170a576e
 6512b960-0014-11e5-93a9-51f9a7931084 | 2015-05-21 18:52:09-0500 | 970e5e77-1e07-40ea-870a-84637c9fc280
 53307a20-0014-11e5-93a9-51f9a7931084 | 2015-05-21 18:51:39-0500 | 11b4a49c-b73d-4c8d-9f88-078a6f303167
 ac9e0050-0014-11e5-93a9-51f9a7931084 | 2015-05-21 18:54:10-0500 | b29e7915-7c17-4900-b784-8ac24e9e72e2
 88d7fb30-0014-11e5-93a9-51f9a7931084 | 2015-05-21 18:53:09-0500 | c8188b73-1b97-4b32-a897-7facdeecea35
 0ba5cf70-0014-11e5-93a9-51f9a7931084 | 2015-05-21 18:49:39-0500 | a079b30f-be80-4a99-ae0e-a784d82f0432
 76f56dd0-0014-11e5-93a9-51f9a7931084 | 2015-05-21 18:52:39-0500 | 3b593ca6-220c-4a8b-8c16-27dc1fb5adde
 1d88f910-0014-11e5-93a9-51f9a7931084 | 2015-05-21 18:50:09-0500 | ec155e0b-39a5-4d2f-98f0-0cd7a5a07ec8
 2f6b3850-0014-11e5-93a9-51f9a7931084 | 2015-05-21 18:50:39-0500 | db42271b-04f2-45d1-9ae7-0c8f9371a4db

(10 rows)
</code></pre>

<p>But if I then run this code:</p>

<pre><code>private static void retrieveEvents(Instant startInstant, Instant endInstant) {

    String cql = ""SELECT document_uuid FROM document_change_events "" + 
                 ""WHERE token(event_uuid) &gt; token(?) AND token(event_uuid) &lt; token(?)"";

    PreparedStatement preparedStatement = cassandraSession.prepare(cql);
    BoundStatement boundStatement = new BoundStatement(preparedStatement);
    boundStatement.setConsistencyLevel(ConsistencyLevel.LOCAL_QUORUM);

    boundStatement.bind(UUIDs.startOf(Date.from(startInstant).getTime()),
                        UUIDs.endOf(Date.from(endInstant).getTime()));

    ResultSet resultSet = cassandraSession.execute(boundStatement);

    if (resultSet == null) {
      System.out.println(""None found."");
      return;
    }

    while (!resultSet.isExhausted()) {
      System.out.println(resultSet.one().getUUID(""document_uuid""));
    }

}
</code></pre>

<p>It only retrieves three results:</p>

<pre><code>3b593ca6-220c-4a8b-8c16-27dc1fb5adde
ec155e0b-39a5-4d2f-98f0-0cd7a5a07ec8
db42271b-04f2-45d1-9ae7-0c8f9371a4db
</code></pre>

<p>Why didn't it retrieve all 10 results? And what do I need to change to achieve the correct results to support this use case?</p>

<p>For reference, I've tested this against dsc-2.1.1, dse-4.6 and using the DataStax Java Driver v2.1.6.</p>
",<java><cassandra><datastax-java-driver>,"<p><em>First of all, please only ask one question at a time.  Both of your questions here could easily stand on their own.  I know these are related, but it just makes the readers come down with a case of <strong>tl;dr</strong>.</em></p>

<p>I'll answer your <strong>2nd question</strong> first, because the answer ties into a fundamental understanding that is central to getting the data model correct.  When I INSERT your rows and run the following query, this is what I get:</p>

<pre><code>aploetz@cqlsh:stackoverflow2&gt; SELECT document_uuid FROM document_change_events 
WHERE token(event_uuid) &gt; token(minTimeuuid('2015-05-10 00:00-0500')) 
  AND token(event_uuid) &lt; token(maxTimeuuid('2015-05-22 00:00-0500'));

 document_uuid
--------------------------------------
 a079b30f-be80-4a99-ae0e-a784d82f0432
 3b593ca6-220c-4a8b-8c16-27dc1fb5adde
 ec155e0b-39a5-4d2f-98f0-0cd7a5a07ec8
 db42271b-04f2-45d1-9ae7-0c8f9371a4db

(4 rows)
</code></pre>

<p>Which is similar to what you are seeing.  Why didn't that return all 10?  Well, the answer becomes apparent when I include <code>token(event_uuid)</code> in my SELECT:</p>

<pre><code>aploetz@cqlsh:stackoverflow2&gt; SELECT token(event_uuid),document_uuid FROM document_change_events WHERE token(event_uuid) &gt; token(minTimeuuid('2015-05-10 00:00-0500')) AND token(event_uuid) &lt; token(maxTimeuuid('2015-05-22 00:00-0500'));

 token(event_uuid)    | document_uuid
----------------------+--------------------------------------
 -2112897298583224342 | a079b30f-be80-4a99-ae0e-a784d82f0432
  2990331690803078123 | 3b593ca6-220c-4a8b-8c16-27dc1fb5adde
  5049638908563824288 | ec155e0b-39a5-4d2f-98f0-0cd7a5a07ec8
  5577339174953240576 | db42271b-04f2-45d1-9ae7-0c8f9371a4db

(4 rows)
</code></pre>

<p>Cassandra stores partition keys (<code>event_uuid</code> in your case) in order by their hashed token value.  You can see this when using the <code>token</code> function.  Cassandra generates partition tokens with a process called <a href=""http://docs.datastax.com/en/cassandra/2.1/cassandra/architecture/architectureDataDistributeHashing_c.html"" rel=""noreferrer"">consistent hashing</a> to ensure even cluster distribution.  In other words, querying by token range doesn't make sense unless the actual (hashed) token values are meaningful to your application.</p>

<p>Getting back to your <strong>first question</strong>, this means you will have to find a different column to partition on.  My suggestion is to use a timeseries mechanism called a ""date bucket.""  Picking the date bucket can be tricky, as it depends on your requirements and query patterns...so that's really up to you to pick a useful one.</p>

<p>For the purposes of this example, I'll pick ""month.""  So I'll re-create your table partitioning on <code>month</code> and clustering by event_uuid:</p>

<pre><code>CREATE TABLE document_change_events2 (
    event_uuid TIMEUUID,
    document_uuid uuid,
    month text,
    PRIMARY KEY ((month),event_uuid, document_uuid)
) WITH default_time_to_live='7776000';
</code></pre>

<p>Now I can query by a date range, when also filtering by <code>month</code>:</p>

<pre><code>aploetz@cqlsh:stackoverflow2&gt; SELECT document_uuid FROM document_change_events2 
WHERE month='201505'
  AND event_uuid &gt; minTimeuuid('2015-05-10 00:00-0500')
  AND event_uuid &lt; maxTimeuuid('2015-05-22 00:00-0500');

 document_uuid
--------------------------------------
 a079b30f-be80-4a99-ae0e-a784d82f0432
 ec155e0b-39a5-4d2f-98f0-0cd7a5a07ec8
 db42271b-04f2-45d1-9ae7-0c8f9371a4db
 92b6fb6a-9ded-47b0-a91c-68c63f45d338
 11b4a49c-b73d-4c8d-9f88-078a6f303167
 970e5e77-1e07-40ea-870a-84637c9fc280
 3b593ca6-220c-4a8b-8c16-27dc1fb5adde
 c8188b73-1b97-4b32-a897-7facdeecea35
 548b320a-10f6-409f-a921-d4a1170a576e
 b29e7915-7c17-4900-b784-8ac24e9e72e2

(10 rows)
</code></pre>

<p>Again, <code>month</code> may not work for your application.  So put some thought behind coming up with an appropriate column to partition on, and then you should be able to solve this.</p>
",['table']
30418185,30418537,2015-05-23 22:27:24,Order By any field in Cassandra,"<p>I am researching cassandra as a possible solution for my up coming project. The more I research the more I keep hearing that it is a bad idea to sort on fields that is not setup for sorting when the table was created. </p>

<p>Is it possible to sort on any field? If there is a performance impact for sorting on fields not in the cluster what is that performance impact? I need to sort around or about 2 million records in the table.</p>
",<sorting><cassandra><cql>,"<blockquote>
  <p>I keep hearing that it is a bad idea to sort on fields that is not setup for sorting when the table was created.</p>
</blockquote>

<p>It's not so much that it's a bad idea.  It's just really not possible to make Cassandra sort your data by an arbitrary column.  Cassandra requires a query-based modeling approach, and that goes for sort order as well.  You have to decide <em>ahead of time</em> the kinds of queries you want Cassandra to support, and the order in which those queries return their data.</p>

<blockquote>
  <p>Is it possible to sort on any field?</p>
</blockquote>

<p>Here's the thing with how Cassandra sorts result sets: it doesn't.  Cassandra queries correspond to partition locations, and the data is read off of the disk and returned to you.  If the data is read in the same order that it was sorted in on-disk, the result set will be sorted.  On the other hand if you try a multi-key query or an index-based query where it has to jump around to different partitions, chances are that it will not be returned in any meaningful order.</p>

<p>But if you plan ahead, you can actually influence the on-disk sort order of your data, and then leverage that order in your queries.  This can be done with a modeling mechanism called a ""clustering column.""  Cassandra will allow you to specify multiple clustering columns, but they are only valid within a single partition.</p>

<p>So what does that mean?  Take <a href=""http://docs.datastax.com/en/cql/3.1/cql/ddl/ddl_compound_keys_c.html"" rel=""noreferrer"">this example from the DataStax documentation</a>.</p>

<pre><code>CREATE TABLE playlists (
  id uuid,
  artist text,
  album text,
  title text,
  song_order int,
  song_id uuid,
  PRIMARY KEY ((id),song_order))
WITH CLUSTERING ORDER BY (song_order ASC);
</code></pre>

<p>With this table definition, I can query a particular <code>playlist</code> by <code>id</code> (the partition key).  Within each <code>id</code>, the data will be returned ordered by <code>song_order</code>:</p>

<pre><code>SELECT id, song_order, album, artist, title 
FROM playlists WHERE id = 62c36092-82a1-3a00-93d1-46196ee77204
ORDER BY song_order DESC;

id                                   | song_order | album                 | artist         | title
------------------------------------------------------------------------------------------------------------------
62c36092-82a1-3a00-93d1-46196ee77204 | 4          | No One Rides For Free |      Fu Manchu |             Ojo Rojo    
62c36092-82a1-3a00-93d1-46196ee77204 | 3          |             Roll Away | Back Door Slam |  Outside Woman Blues
62c36092-82a1-3a00-93d1-46196ee77204 | 2          |          We Must Obey |      Fu Manchu |     Moving in Stereo
62c36092-82a1-3a00-93d1-46196ee77204 | 1          |          Tres Hombres |         ZZ Top |            La Grange
</code></pre>

<p>In this example, if I only need to specify an <code>ORDER BY</code> if I want to switch the sort direction.  As the rows are stored in <code>ASC</code>ending order, I need to specify <code>DESC</code> to see them in <code>DESC</code>ending order.  If I was fine with getting the rows back in <code>ASC</code>ending order, I don't need to specify <code>ORDER BY</code> at all.</p>

<p>But what if I want to order by artist?  Or album?  Or both?  Since one artist can have many albums (for this example), we'll modify the PRIMARY KEY definition like this:</p>

<pre><code>PRIMARY KEY ((id),artist,album,song_order)
</code></pre>

<p>Running the same query above (minus the <code>ORDER BY</code>) produces this output:</p>

<pre><code>SELECT id, song_order, album, artist, title 
FROM playlists WHERE id = 62c36092-82a1-3a00-93d1-46196ee77204;

id                                   | song_order | album                 | artist         | title
------------------------------------------------------------------------------------------------------------------
62c36092-82a1-3a00-93d1-46196ee77204 | 3          |             Roll Away | Back Door Slam |  Outside Woman Blues
62c36092-82a1-3a00-93d1-46196ee77204 | 4          | No One Rides For Free |      Fu Manchu |             Ojo Rojo    
62c36092-82a1-3a00-93d1-46196ee77204 | 2          |          We Must Obey |      Fu Manchu |     Moving in Stereo
62c36092-82a1-3a00-93d1-46196ee77204 | 1          |          Tres Hombres |         ZZ Top |            La Grange
</code></pre>

<p>Notice that the rows are now ordered by <code>artist</code>, and then <code>album</code>.  If we had two songs from the same album, then <code>song_order</code> would be next.</p>

<p>So now you might ask ""what if I just want to sort by <code>album</code>, and not <code>artist</code>?""  You can sort just by <code>album</code>, but not with this table.  You cannot skip clustering keys in your ORDER BY clause.  In order to sort only by <code>album</code> (and not <code>artist</code>) you'll need to design a different query table.  Sometimes Cassandra data modeling will have you duplicating your data a few times, to be able to serve different queries...<em>and that's ok</em>.</p>

<p>For more detail on how to build data models while leveraging clustering order, check out these two articles on <a href=""http://www.planetcassandra.org"" rel=""noreferrer"">PlanetCassandra</a>:</p>

<ul>
<li><a href=""http://planetcassandra.org/getting-started-with-time-series-data-modeling/"" rel=""noreferrer"">Getting Started With Time Series Data Modeling</a> - Patrick McFadin</li>
<li><a href=""http://planetcassandra.org/blog/we-shall-have-order/"" rel=""noreferrer"">We Shall Have Order!</a> - <em>Disclaimer - I am the author</em></li>
</ul>
",['table']
30444754,30448774,2015-05-25 19:20:49,"How do I model multiple ""many to many"" relationships in Cassandra?","<p>I've been reading up on Cassandra, I've done some tutorials and played around with CQL but now that it is time for me to design a schema I'm having some difficulty.</p>

<p>I'm trying to create a schema that will handle the following use case. I need to keep track of the workers that attend meetings and the topics they discuss in those meetings. So a meeting can have multiple workers attend it, multiple topics are discussed at each meeting and each worker can create multiple topics. These are the data fields:</p>

<p><strong>Worker</strong>: Worker ID, Worker Name</p>

<p><strong>Meeting</strong>: Meeting ID, Meeting Name, Meeting Time</p>

<p><strong>Topic</strong>: Topic ID, Topic Name, Creator</p>

<p>I need queries to see:</p>

<ol>
<li>who is attending a meeting?</li>
<li>what meetings a worker has attended in the past?</li>
<li>what topics a worker has created?</li>
<li>what meetings have discussed a particular topic?</li>
</ol>

<p>So what should the schema look like to handle this? I feel like this shouldn't be that hard but I can't make it make sense when I start making tables.</p>
",<cassandra><schema><nosql>,"<p>It's important to remember that Cassandra data modeling is a query-driven exercise.  As you have four queries to complete above, you may end-up creating four tables: one for each query needed.</p>

<p>I want you to be able to learn, so I won't do it all for you.  But here is how I would solve for queries #1 and #2.  For #1, I would create a table like this:</p>

<pre><code>CREATE TABLE meetingAttendance (
  meetingID uuid,
  meetingName text,
  meetingTime timestamp,
  workerID uuid,
  workerName text,
  PRIMARY KEY ((meetingID),workerName));
</code></pre>

<p>I'll go with <code>meetingID</code> as a partition key, and I'll cluster by <code>workerName</code> so that they come back in order.</p>

<p>For query #2, I'll create a query table like this:</p>

<pre><code>CREATE TABLE meetingsByWorker (
  workerID uuid,
  workerName text,
  meetingID uuid,
  meetingName text,
  meetingTime timestamp,
  topicID uuid,
  topicName text,
  PRIMARY KEY ((workerID),meetingTime))
WITH CLUSTERING ORDER BY (meetingtime DESC);
</code></pre>

<p>As we are querying meetings that a particular worker has attended, I'll partition on <code>workerID</code>.  As meetings are time-based, it makes sense to sort them by <code>meetingTime</code>.  By default they would sort is <code>ASC</code>ending order, but historical data usually makes sense to look at in <code>DESC</code>ending order, so I'll define a specific CLUSTERING ORDER and sort direction (<code>DESC</code>).</p>

<p>After INSERTing some rows into both tables, I can query attendance for a particular meeting like this:</p>

<pre><code>aploetz@cqlsh:stackoverflow2&gt; SELECT * FROM meetingattendance 
    WHERE meetingid=031e457b-2660-448b-a1d5-68c6cce3a820;

 meetingid                            | workername    | meetingname        | meetingtime              | workerid
--------------------------------------+---------------+--------------------+--------------------------+--------------------------------------
 031e457b-2660-448b-a1d5-68c6cce3a820 |         David | Project Prometheus | 2093-12-25 08:08:00-0600 | b83cbec4-95e5-4457-b037-c28c51d00418
 031e457b-2660-448b-a1d5-68c6cce3a820 | Holloway, Dr. | Project Prometheus | 2093-12-25 08:08:00-0600 | d28b4ee8-b1b9-401a-88d4-bc6b9727d712
 031e457b-2660-448b-a1d5-68c6cce3a820 |  Janek, Capt. | Project Prometheus | 2093-12-25 08:08:00-0600 | ebccf3ba-c1d2-4503-b717-897c7e89d968
 031e457b-2660-448b-a1d5-68c6cce3a820 |     Shaw, Dr. | Project Prometheus | 2093-12-25 08:08:00-0600 | c0e3e560-2332-4a46-9fdf-68bdb31abcb2
 031e457b-2660-448b-a1d5-68c6cce3a820 |       Vickers | Project Prometheus | 2093-12-25 08:08:00-0600 | 77cb9f64-3cb8-43f9-ab0c-b907b01c4404

(5 rows)
aploetz@cqlsh:stackoverflow2&gt; SELECT * FROM meetingattendance
    WHERE meetingid=c7cea773-4c99-445f-928d-5b8a511c843b;

 meetingid                            | workername | meetingname      | meetingtime              | workerid
--------------------------------------+------------+------------------+--------------------------+--------------------------------------
 c7cea773-4c99-445f-928d-5b8a511c843b |      David | Wake Mr. Weyland | 2093-12-29 13:01:00-0600 | b83cbec4-95e5-4457-b037-c28c51d00418
 c7cea773-4c99-445f-928d-5b8a511c843b |  Ford, Dr. | Wake Mr. Weyland | 2093-12-29 13:01:00-0600 | 939657c2-e0cb-4a61-87d8-2a1739161d2a
 c7cea773-4c99-445f-928d-5b8a511c843b |    Vickers | Wake Mr. Weyland | 2093-12-29 13:01:00-0600 | 77cb9f64-3cb8-43f9-ab0c-b907b01c4404
 c7cea773-4c99-445f-928d-5b8a511c843b |    Weyland | Wake Mr. Weyland | 2093-12-29 13:01:00-0600 | 306955b8-c7ee-4350-8aa4-4c5d64487d74

(4 rows)
</code></pre>

<p>Now if I want to see which meetings a particular worker has attended, I can also query for that, by <code>workerID</code>:</p>

<pre><code>aploetz@cqlsh:stackoverflow2&gt; SELECT workername, meetingtime, meetingid, meetingname
    FROM meetingsbyworker WHERE workerid=77cb9f64-3cb8-43f9-ab0c-b907b01c4404;

 workername | meetingtime              | meetingid                            | meetingname
------------+--------------------------+--------------------------------------+--------------------
    Vickers | 2093-12-29 13:01:00-0600 | c7cea773-4c99-445f-928d-5b8a511c843b |   Wake Mr. Weyland
    Vickers | 2093-12-26 18:22:00-0600 | 3ea1282b-a465-4626-bd76-c65dd17b9f26 |   Head Examination
    Vickers | 2093-12-25 08:08:00-0600 | 031e457b-2660-448b-a1d5-68c6cce3a820 | Project Prometheus

(3 rows)
aploetz@cqlsh:stackoverflow2&gt; SELECT workername, meetingtime, meetingid, meetingname
    FROM meetingsbyworker WHERE workerid=939657c2-e0cb-4a61-87d8-2a1739161d2a;

 workername | meetingtime              | meetingid                            | meetingname
------------+--------------------------+--------------------------------------+------------------
  Ford, Dr. | 2093-12-29 13:01:00-0600 | c7cea773-4c99-445f-928d-5b8a511c843b | Wake Mr. Weyland
  Ford, Dr. | 2093-12-26 18:22:00-0600 | 3ea1282b-a465-4626-bd76-c65dd17b9f26 | Head Examination

(2 rows)
</code></pre>

<p>Note that the data has been denormalized, and some column values appear redundantly.  If you decide that you still want entity tables for things like worker, that's ok too.  But again, ask yourself how often and how, exactly you plan on querying those tables.  The last two should be easy for you to solve by taking a similar approach.</p>
",['table']
30449194,30453712,2015-05-26 03:55:45,Failover not working with Cassandra when using DataStax C# Driver,"<p>I have a two node setup in Azure and I am trying to get failover working when connecting with the C# driver. My nodes seem to be communicating fine when working with cqlsh and within OpsCenter.</p>

<pre><code>var contact = ""publicipforfirstnode"";
_cluster = Cassandra.Cluster.Builder().AddContactPoint(contact).Build();
_session = _cluster.Connect(""demo"");
</code></pre>

<p>I initially connect with the public IP of the first node. This works fine. However in the configuration I use the internal network IPs assigned by my virtual network such as 10.1.0.4, 10.1.0.5, etc. I set them as the listen_address and broadcast_rpc_address for each node. Even though I use the internal IP in the configuration I can connect with the public IP just fine. I have a special firewall rule that allows me to connect from a certain machine on the public IP. However to avoid firewall rules for inner-node communication, I put the nodes on the same virtual network and no extra work is required.</p>

<p>This seems great until my first node goes down. <strong>It then tries the second node using the internal IP.</strong> </p>

<blockquote>
  <p>I get an error: All Hosts tried for query (Public IP of First
  Node), (Internal IP of Second Node)</p>
</blockquote>

<p>But since I am connecting from a machine not in the virtual network it can't reach this internal ip. My application won't be in the internal network so this seems like an issue. </p>

<p>Not using internal ips forces me to setup authentication and/or special firewall rules I'd rather not have to do. Is there any way to force the c# driver to use public ips and allow the nodes to communicate on internal ips? Using internal ips seems to be the recommended best practice unless you have multiple regions.</p>
",<c#><azure><cassandra><datastax-enterprise>,"<p>The IP configured as <code>broadcast_rpc_address</code> in the cassandra.yaml file is used by the drivers to connect to them.</p>

<p>In your case, if you want to connect with the driver using the public ip addresses, you should set the <code>broadcast_rpc_address</code> as the public IP address.</p>

<p>You can enable tracing in the driver to see what is happening under the hood:</p>

<pre><code>// Specify the minimum trace level you want to see
Cassandra.Diagnostics.CassandraTraceSwitch.Level = TraceLevel.Info;
// Add a standard .NET trace listener
Trace.Listeners.Add(new ConsoleTraceListener());
</code></pre>

<p><a href=""http://docs.datastax.com/en/cassandra/2.1/cassandra/configuration/configCassandra_yaml_r.html"" rel=""nofollow"">From the docs</a>:</p>

<ul>
<li>listen_address: The IP address or hostname that Cassandra binds to for connecting to other Cassandra nodes.</li>
<li>broadcast_rpc_address: RPC address to <strong>broadcast</strong> to drivers and other Cassandra nodes. This cannot be set to 0.0.0.0. If blank, it is set to the value of the rpc_address or rpc_interface. If rpc_address or rpc_interfaceis set to 0.0.0.0, this property must be set.</li>
</ul>
",['rpc_address']
30506511,30527317,2015-05-28 12:23:46,Get the last inserted data from Cassandra,"<p>We have a primary key in our table as <code>UUID</code>. To get the last record we do order by <code>desc uuid</code> field. However, because this field is not integer we do not get the last record always. Is there a way to get the last record? Please note that we do not want to make cluster columns. </p>

<p>Here is my table</p>

<pre><code>ArtistId (partition key)  | SongId (primary key, uuid) |  Song
--------------------------------------------
1                         | 9a9fa429-c349-4137         |  abc
1                         | 9a9fa429-b349-6137         |  cde
1                         | 9a9ga429-a349-6132         |  fgh
2                         | 249ga429-a909-6542         |  ijk
2                         | 249kg429-a239-3652         |  lmn
1                         | i55oa429-a909-3462         |  opq
1                         | e4235k59-4ik9-6542         |  rst
</code></pre>
",<cassandra><cql><cassandra-2.0><datastax><datastax-enterprise>,"<p>TL&amp;DR: <a href=""http://docs.datastax.com/en/cql/3.0/cql/cql_reference/timeuuid_functions_r.html"" rel=""nofollow"">TimeUUID</a> cql3 type to the rescue!</p>

<p>If you have a db schema like this:
</p>

<pre><code>create table music (
  artist_id int,
  song_id timeuuid,
  song text,
  primary key (artist_id, song_id)
) with clustering order by (song_id desc);
</code></pre>

<p>Note these changes to your original schema:</p>

<ul>
<li><code>uuid</code> changed to <code>timeuuid</code></li>
<li>clustering key is sorted in reverse</li>
</ul>

<p>The table contains some data in it:</p>

<pre><code> artist_id | dateOf(song_id)          | song
-----------+--------------------------+------
         1 | 2015-05-29 13:25:15+0300 |  baz
         1 | 2015-05-29 13:25:11+0300 |  bar
         1 | 2015-05-29 13:25:06+0300 |  foo
         2 | 2015-05-29 13:25:19+0300 |  qux
</code></pre>

<p>To get the latest song_id for a fixed artist_id, you can run query like:</p>

<pre><code>select artist_id,dateOf(song_id),song from music where artist_id=1 limit 1;
</code></pre>
",['table']
30514237,30515201,2015-05-28 18:19:33,What node does Cassandra store data on?,"<p>Is there a command or any way at all to know what data is stored on what nodes of Cassandra?</p>

<p>Im pretty new to Cassandra and haven't had much luck googling this question.</p>

<p>Thanks!</p>
",<cassandra><distributed-database>,"<p>You can get Cassandra to tell you which node(s) a particular key is on with <a href=""http://docs.datastax.com/en/cassandra/2.1/cassandra/tools/toolsGetEndPoints.html"" rel=""noreferrer"">nodetool getendpoints</a>.</p>

<pre><code>$ nodetool getendpoints mykeyspace tbl '8546200'
192.168.73.188
192.168.73.190
</code></pre>

<p>I don't know if that's what you're looking for or not.  AFAIK there isn't a way to flat-out query the responsible nodes for all rows in a table or keyspace.  But as Blake pointed out, your application doesn't really need to worry about that.</p>

<p>If you really wanted to find out, you could query your table using the <code>token</code> function on your partition key.  Here's an example using Blake's schema:</p>

<pre><code>SELECT token(partition_key),partition_key FROM tbl;
</code></pre>

<p>That would list the hashed tokens with your partition keys.  Then you could run a <code>nodetool ring</code> to list out the token ranges for each node, and see which nodes are responsible for that range.  Note that if you are using vNodes your output will be pretty big (256 lines for each, by default).</p>
",['table']
30625827,30627337,2015-06-03 16:33:11,"Cassandra queries performance, ranges","<p>I'm quite new with Cassandra, and I was wondering if there would be any impact in performance if a query is asked with ""date = '2015-01-01'"" or ""date >= '2015-01-01' AND date &lt;= '2015-01-01'"".</p>

<p>The only reason I want to use the ranges like that is because I need to make multiple queries and I want to have them prepared (as in prepared statements). This way the prepared statements number is cut by half.</p>

<p>The keys used are ((key1, key2), date) and (key1, date, key2) in the two tables I want to use this. The query for the first table is similar to:</p>

<pre><code>SELECT * FROM table1
WHERE key1 = val1
    AND key2 = val2
    AND date &gt;= date1 AND date &lt;= date2
</code></pre>
",<cassandra><cassandra-2.0><cqlsh>,"<p>For a <code>PRIMARY KEY (key1, date, key2)</code> that type of query just isn't possible.  If you do, you'll see an error like this:</p>

<blockquote>
  <p>InvalidRequest: code=2200 [Invalid query] message=""PRIMARY KEY column
  ""key2"" cannot be restricted (preceding column ""date"" is either not
  restricted or by a non-EQ relation)""</p>
</blockquote>

<p>Cassandra won't allow you to filter by a PRIMARY KEY component if the preceding column(s) are filtered by anything other than the equals operator.</p>

<p>On the other hand, your queries for <code>PRIMARY KEY ((key1, key2), date)</code> will work and perform well.  The reason, is that Cassandra uses the clustering key(s) (<code>date</code> in this case) to specify the on-disk sort order of data within a partition.  As you are specifying partition keys (<code>key1</code> and <code>key2</code>) your result set will be sorted by <code>date</code>, allowing Cassandra to satisfy your query by performing a continuous read from the disk.</p>

<p>Just to test that out, I'll even run two queries on a table with a similar key, and turn <a href=""http://docs.datastax.com/en/cql/3.1/cql/cql_reference/tracing_r.html"" rel=""nofollow"">tracing</a> on:</p>

<pre><code>SELECT * FROM log_date2 WHERe userid=1001 
AND time &gt; 32671010-f588-11e4-ade7-21b264d4c94d 
AND time &lt; a3e1f750-f588-11e4-ade7-21b264d4c94d;
</code></pre>

<p>Returns 1 row and completes in 4068 microseconds.</p>

<pre><code>SELECT * FROM log_date2 WHERe userid=1001 
AND time=74ad4f70-f588-11e4-ade7-21b264d4c94d;
</code></pre>

<p>Returns 1 row and completes in 4001 microseconds.</p>
",['table']
30625989,30639109,2015-06-03 16:42:40,Modeling account for rest communication cassandra,"<p>I need to model account (first name, last name, email as username etc.) in cassandra along with currently active token. </p>

<p>My initial idea was to create <code>account_by_email</code> which will have skinny row partitioned by email with static columns and have clustering by <code>access_token</code> (and maybe TTL) and than you can always find access token based on current email.</p>

<p>But we have requirement that clients will send after login only <code>access_token</code> and based on it current user must be pulled from DB. </p>

<p>I can create one more table where email will be partitioned by <code>access_token</code> but that seams to me as overhead and lot of partitions. Then I could get email from <code>access_token</code> and get user by email always.</p>

<p>Any better ideas and approaches, it seams that this is common use case but I cannot find any modeling details when cassandra is used as storage?</p>
",<cassandra><data-modeling>,"<blockquote>
  <p>I can create one more table where email will be partitioned by access_token but that seams to me as overhead and lot of partitions.</p>
</blockquote>

<p>What's wrong with a large number of partitions it a table? It's definitely a correct Cassandra-way of doing things:</p>

<pre><code>create table users (
  email text primary key,
  first_name text,
  last_name text,
  current_token text
);

create table tokens (
  auth_token text primary key,
  valid_until timestamp,
  email text
);
</code></pre>

<p>So you have a separate tables for users, and the <code>tokens</code> table which has token as a partition key. With this model you can:</p>

<ul>
<li>set a new token for a user by updating users.current_token with TTL and inserting a new token row to <code>tokens</code> table.</li>
<li>get user email by token, even if the token was expired.</li>
<li>get current user's active token.</li>
<li>have full token history for a user (but no way to run effective queries for that type of information, you can use Spark/SparkSQL/Hive for that).</li>
<li>automatically expire tokens by setting TTL for a single <code>current_token</code> column.</li>
</ul>
",['table']
30627835,30628055,2015-06-03 18:24:45,What is the difference between broadcast_address and broadcast_rpc_address in cassandra.yaml?,"<p><strong>GOAL:</strong> I am trying to understand the best way to configure my Cassandra cluster so that several different drivers across several different networking scenarios can communicate with it properly. </p>

<p><strong>PROBLEM/QUESTION:</strong> It is not entirely clear to me, after reading the documentation what the difference is between these two settings: broadcast_address and broadcast_rpc_address as it pertains to the way that a driver connects and interacts with the cluster. Which one or which combination of these settings should I use with my node's accessible network endpoint (DNS record attainable by the client's/drivers)?</p>

<p>Here is the documentation for broadcast_address from <a href=""http://docs.datastax.com/en/cassandra/2.1/cassandra/configuration/configCassandra_yaml_r.html?scroll=reference_ds_qfg_n1r_1k__broadcast_address"" rel=""noreferrer"" title=""cassandra.yaml documentation"">datastax</a>:
(Default: listen_address)note The IP address a node tells other nodes in the cluster to contact it by. It allows public and private address to be different. For example, use the broadcast_address parameter in topologies where not all nodes have access to other nodes by their private IP addresses.
If your Cassandra cluster is deployed across multiple Amazon EC2 regions and you use the EC2MultiRegionSnitch, set the broadcast_address to public IP address of the node and the listen_address to the private IP.</p>

<p>Here is the documentation for broadcast_rpc_address from <a href=""http://docs.datastax.com/en/cassandra/2.1/cassandra/configuration/configCassandra_yaml_r.html?scroll=reference_ds_qfg_n1r_1k__broadcast_rpc_address"" rel=""noreferrer"" title=""cassandra.yaml documentation"">datastax</a>:
(Default: unset)note RPC address to broadcast to drivers and other Cassandra nodes. This cannot be set to 0.0.0.0. If blank, it is set to the value of the rpc_address or rpc_interface. If rpc_address or rpc_interfaceis set to 0.0.0.0, this property must be set.</p>

<p>EDIT: This question pertains to Cassandra version 2.1, and may not be relevant in the future.</p>
",<cassandra><bigdata>,"<p>One of the users of #cassandra on freenode was kind enough to provide an answer to this question:</p>

<p>The rpc family of settings pertain to drivers that use the Thrift protocol to communicate with cassandra. For those drivers that use the native transport, the broadcast_address will be reported and used.</p>

<p>My test case confirms this.</p>
",['broadcast_address']
30648515,30649578,2015-06-04 15:51:55,Partially indexing Cassandra table with SOLR,"<p>One of the tables inside our Cassandra (DSE 4.7) Cluster contains south of 15 billion records. With the number of servers we have - it would be impossible to index them all with Solr.</p>

<p>So, is it possible to somehow index the data partially/sample and/or start indexing and then ""pause"" indexing let's say after 500mm records?</p>

<p>I assume the other option would be to just dump 500mm records and reload them into another ""temp"" table and index that...?</p>

<p>The point is, I would like to start indexing and have the ability to search and as we grow and add more servers - have the ability to index more and pause again.</p>

<p>Is that even possible?</p>

<p>Thanks!</p>
",<solr><cassandra><datastax-enterprise>,"<p>There is no way to index just a few rows. I agree that a parallel table (probably with TTL) is likely your best bet. </p>

<p>Here are some (pretty effective) tactics to minimize the size of your DSE Search index. You can probably shrink it by ~50% if you're not using things like <a href=""https://wiki.apache.org/solr/HighlightingParameters"" rel=""nofollow"">Highlighting</a> (term...) or <a href=""https://wiki.apache.org/solr/SolrRelevancyFAQ#index-time_boosts"" rel=""nofollow"">Boosts</a> (omitnorms):</p>

<p>• set termVectors=""false""</p>

<p>• set termPositions=""false"" </p>

<p>• set termOffsets=""false"" </p>

<p>• set omitNorms=""true"" </p>

<p>• Only index fields you intend to search</p>
",['table']
30665595,30666727,2015-06-05 11:38:52,Real time complex queries on Cassandra,"<p>We're looking for a tool (preferably open source) which helps us to perform complex queries (advanced filtering and joins, no need full SQL) in real time.</p>

<p>Assume that all the data needed fits in memory, and we want to avoid, if possible, the overhead of map reduce tools.</p>

<p>To be more specific, we need to load n partitions of a single table, and join them by clustering column.</p>

<pre><code>Variables Table:
Variable ID: Partition key
Person ID: Clustering key
Variable Value

Desired output columns:
Person ID, Variable 1 Value, Variable 2 Vale, ..., Variable N Value 
</code></pre>

<p>We can achieve it by an in-memory load-filter-join process, but we were wondering if there's any tool out there with this use case covered out of the box and with a fair performance.</p>

<p>We've tested Spark, but the partitioning of Spark C* connector is based on the primary key, so each Variable ID would be loaded in a different Spark node, and the join process would be really slow (all the data would travel all over the Spark cluster).</p>

<p>Any tips? known tools?</p>
",<cassandra>,"<p>I believe that you have a number of options to perform this task:</p>

<ul>
<li><strong>Rethink your database schema</strong>, denormalize it. <code>var_id:person_id:value</code> rows are not the best table schema if you want to query by person_id (and it smells really bad as an <a href=""https://softwareengineering.stackexchange.com/questions/93124/eav-is-it-really-bad-in-all-scenarios"">entity-attribute-value db antipattern</a>):</li>
</ul>

<blockquote>
  <p>EAV gives a flexibility to the developer to define the schema as needed and this is good in some circumstances. On the other hand it performs very poorly in the case of an ill-defined query and can support other bad practices. In other words, EAV gives you enough rope to hang yourself and in this industry, things should be designed to the lowest level of complexity because the guy replacing you on the project will likely be an idiot.</p>
</blockquote>

<p>You can use schema with multiple columns (cassandra can handle a lot of them):</p>

<pre><code>create table person_data (
  person_id int primary key,
  var1 text,
  var2 text,
  var3 text,
  var4 text,
  ....
);
</code></pre>

<p>If you don't have a predefined set of variables, you can use cql3 collections like map for storing the data in a more flexible way.</p>

<ul>
<li><p><strong>Create a secondary index on person_id</strong> (even it's a clustering key already). You can query for all data for a specific user without using joins, but with some issues:</p>

<ul>
<li>As your query will hit multiple partitions, it will require not a single disk seek, but a series of them, so your query latency may be higher than you're expecting.</li>
<li>secondary indexes are not free: C* must perform more work under the hood if you insert a row to a table with indexed columns.</li>
</ul></li>
<li><p><strong>Use external index</strong> like ElasticSearch/Solr if you plan to have a lot of complex queries which do not fit well into cql3.</p></li>
</ul>
",['table']
30724797,30731527,2015-06-09 06:46:26,"Commenting Cassandra's keyspace, table, column","<p>In Oracle there is possibility to <a href=""http://docs.oracle.com/cd/B19306_01/server.102/b14200/statements_4009.htm"" rel=""nofollow"">add a comment</a> about a table, view, materialized view, or column into the data dictionary, e.g.</p>

<pre><code>COMMENT ON COLUMN employees.job_id 
   IS 'abbreviated job title';
</code></pre>

<p>I found this particularly usefull as a tester when trying to understand ideas behind the names which are not necessarily self-explanable and in large databases (over 200 tables).</p>

<p>Is there such feature in Cassandra?</p>
",<cassandra><comments>,"<p>You can use 'with comment' option </p>

<pre><code>cqlsh:d2&gt; 
cqlsh:d2&gt; create table employee (id int primary key, name text) with comment = 'Employee id and name';
cqlsh:d2&gt; desc table employee;

CREATE TABLE d2.employee (
    id int PRIMARY KEY,
    name text
) WITH bloom_filter_fp_chance = 0.01
    AND caching = '{""keys"":""ALL"", ""rows_per_partition"":""NONE""}'
    AND comment = 'Employee id and name'
    AND compaction = {'min_threshold': '4', 'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32'}
    AND compression = {'sstable_compression': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99.0PERCENTILE';
</code></pre>

<p><a href=""https://docs.datastax.com/en/cql/3.0/cql/cql_reference/create_table_r.html#reference_ds_v3f_vfk_xj__setting-a-table-property"" rel=""nofollow"">Cassandra documentation</a></p>
",['table']
30743626,30743651,2015-06-09 22:03:46,"Cassandra node almost out of space, but nodetool cleanup is increasing disk use?","<p>One of our nodes was at 95% disk use and we added another node to the cluster to hopefully rebalance but the disk space didn't drop on the node. I tried doing nodetool cleanup assuming that excess keys were on the node, but the disk space is increasing! Will cleanup actually reduce the size?</p>
",<cassandra>,"<p>Yes it will, but you have to be careful because a compaction is calculated and it generates temporary files and tmp link files that will increase disk space until the cleaned up compacted table is calculated.  </p>

<p>So I would go into your data directory and figure out what your keyspace sizes are using  </p>

<pre><code>du -h -s *  
</code></pre>

<p>Then individually clean up the smaller keyspaces (you can specify a keyspace in the nodetool cleanup command with nodetool cleanup ) until you have some overhead. To get an idea of how much space is being freed, tail the log and cat/grep for cleaned compactions:</p>

<pre><code>tail &lt;system.log location&gt; | grep 'eaned'
</code></pre>

<p>I'd recommend you don't try to cleanup a keyspace that is more that half the size of your remaining disk space. Hopefully that is possible. </p>

<p>If you don't have enough space you'll have to shut down the node, attach a bigger disk, copy the data files over to the bigger disk, repoint the yaml to the new data directories, then restart up. This is useful for things like SSDs that are expensive and small, but the main spinning disks are cheaper and bigger.</p>
",['table']
30743927,30758514,2015-06-09 22:29:16,Cassandra: Insert with older timestamp,"<p>(Cassandra 2.0.9, using CQL)</p>

<p>I've accidentally updated a row in a table which was managing its own timestamp (100 * a specific sequence number). Now, because my timestamp is the current time, none of the updates are working. I understand why this is, but I'm trying to recover from it. I'm fortunate that I can delete these rows.</p>

<p>I've set <strong>gc_grace_seconds</strong> to 0 and run <strong>delete from <em>table</em> where key=<em>primarykey</em></strong> to remove the rows. After, I've used <strong>nodetool flush</strong> and <strong>nodetool compact</strong> <em>on every node</em> to cause the deletion to go through and get the resulting tombstones compacted and erased.  I've then bumped <strong>gc_grace_seconds</strong> back up to 10 days and tried to <strong>insert into</strong> a row with the same key but <em>using timestamp 1</em> .</p>

<p>This doesn't work. Just wondering if anyone has done a similar mistake and worked around it?</p>
",<cassandra><cql3>,"<p>I thought I would give this exercise a try.</p>

<pre><code>aploetz@cqlsh:presentation&gt; SELECT * FROm bladerunners WHERE id='B26354';
 id     | data                | name         | ts                       | type
--------+---------------------+--------------+--------------------------+--------------
 B26354 | Filed and monitored | Rick Deckard | 2015-02-16 12:00:03-0600 | Blade Runner

(1 rows)
</code></pre>

<p>Here is a look at how the data is stored, using the <code>cassandra-cli</code>:</p>

<pre><code>[default@presentation] get bladerunners[B26354];
=&gt; (name=, value=, timestamp=1427744637894310)
=&gt; (name=data, value=46696c656420616e64206d6f6e69746f7265642e, timestamp=1427744637894310)
=&gt; (name=name, value=5269636b204465636b617264, timestamp=1427744637894310)
=&gt; (name=ts, value=0000014b938c09a2, timestamp=1427744637894310)
=&gt; (name=type, value=426c6164652052756e6e6572, timestamp=1427744637894310)
Returned 5 results.
Elapsed time: 7.67 msec(s).
</code></pre>

<p>I will now delete the <code>data</code> column for this row, generating a tombstone:</p>

<pre><code>DELETE data FROM bladerunners WHERE id='B26354';
</code></pre>

<p>When I SELECT with <code>tracing on</code> I can see that the column shows ""null"" and I have a tombstone out there.</p>

<pre><code>aploetz@cqlsh:presentation&gt; SELECT * FROM bladerunners WHERe id='B26354';

 id     | data | name         | ts                       | type
--------+------+--------------+--------------------------+--------------
 B26354 | null | Rick Deckard | 2015-02-16 12:00:03-0600 | Blade Runner

...

Read 1 live and 1 tombstoned cells [SharedPool-Worker-2] | 2015-06-10 08:42:25.858000 | 192.168.23.129 |           2173
</code></pre>

<p>So I will set the <code>bladerunners</code> table's <code>gc_grace_seconds</code> to zero:</p>

<pre><code>ALTER TABLE bladerunners WITH gc_grace_seconds=0;
</code></pre>

<p>From the (Linux) command line, I will flush and compact my <code>presentation</code> keyspace:</p>

<pre><code>aploetz@dockingBay94:/local/dsc-cassandra-2.1.4$ bin/nodetool flush
aploetz@dockingBay94:/local/dsc-cassandra-2.1.4$ bin/nodetool compact presentation
</code></pre>

<p>When I SELECT with <code>tracing on</code>, I can see that the <code>data</code> column is still ""null,"" but now the tombstone is gone.</p>

<p>I will now re-INSERT the <code>data</code> column with a timestamp of 1:</p>

<pre><code>INSERT INTO bladerunners (id, data) VALUES ('B26354','Filed and monitored') USING TIMESTAMP 1;
</code></pre>

<p>When querying with the <code>cassandra-cli</code>, this is now what I see:</p>

<pre><code>[default@presentation] get bladerunners[B26354];
=&gt; (name=, value=, timestamp=1427744637894310)
=&gt; (name=data, value=46696c656420616e64206d6f6e69746f726564, timestamp=1)
=&gt; (name=name, value=5269636b204465636b617264, timestamp=1427744637894310)
=&gt; (name=ts, value=0000014b938c09a2, timestamp=1427744637894310)
=&gt; (name=type, value=426c6164652052756e6e6572, timestamp=1427744637894310)
Returned 5 results.
Elapsed time: 4.7 msec(s).
</code></pre>

<p>Note that the <code>data</code> column now has a timestamp of 1.</p>

<p>Try running your query with <code>tracing on</code> and see if your tombstones are really gone.  Also, check your table via the <code>cassandra-cli</code> to see how the timestamps are coming through.  Let me know if you need clarification on any of these steps.</p>

<p><strong>NOTE:</strong> I was just showing the flush/compact as part of the example or exercise.  I feel compelled to mention that DataStax recommends that users <strong><em>avoid</em></strong> manually running <code>nodetool compact</code> if at all possible.</p>
",['table']
30763210,30774695,2015-06-10 17:23:30,New Datastax driver for Tableau is not working,"<p>trying to run Tableau on top of DSE 4.7. It fails. I can't do something in worksheet or preview the data. Get this error:</p>

<p>""Missing EOF at 'tablename_i_try_to_query' ""</p>

<p>What is the right way to fix it?</p>
",<cassandra><odbc><tableau-api><datastax>,"<p>So I've resolved it.
When you setup datasource it Tableau, you have to specify cluster, database and table (a.k.a. column family).
I specified CF/table '<strong>tablename_i_try_to_query</strong>' and dragged it to the pane on the right. 
Then I specified database. It didn't work. Tableau generated query without specifying database.
Then I removed table from pane and put it back again. Tableau started to generate correct query with <strong>database.tablename_i_try_to_query</strong></p>
",['table']
30779774,30781880,2015-06-11 11:41:04,How to delete a record in Cassandra?,"<p>I have a table like this:</p>

<pre><code>CREATE TABLE mytable (
    user_id int,
    device_id ascii,
    record_time timestamp,
    timestamp timeuuid,
    info_1 text,
    info_2 int, 
    PRIMARY KEY (user_id, device_id, record_time, timestamp)
);
</code></pre>

<p>When I ask Cassandra to delete a record (an entry in the columnfamily) like this:</p>

<pre><code>DELETE from my_table where user_id = X and device_id = Y and record_time = Z and timestamp = XX;
</code></pre>

<p>it returns without an error, but when I query again the record is still there. Now if I try to delete a whole row like this:</p>

<pre><code>DELETE from my_table where user_id = X
</code></pre>

<p>It works and removes the whole row, and querying again immediately doesn't return any more data from that row.</p>

<p>What I am doing wrong? How you can remove a record in Cassandra?</p>

<p>Thanks</p>
",<cassandra><cassandra-2.0><cql3>,"<p>Ok, here is my theory as to what is going on.  You have to be careful with timestamps, because they will <em>store</em> data down to the millisecond.  But, they will only <em>display</em> data to the second.  Take this sample table for example:</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; SELECT id, datetime  FROM data;

 id     | datetime
--------+--------------------------
 B25881 | 2015-02-16 12:00:03-0600
 B26354 | 2015-02-16 12:00:03-0600

(2 rows)
</code></pre>

<p>The <code>datetime</code>s (of type timestamp) are equal, right?  Nope:</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; SELECT id, blobAsBigint(timestampAsBlob(datetime)),
                                  datetime FROM data;

 id     | blobAsBigint(timestampAsBlob(datetime)) | datetime
--------+-----------------------------------------+--------------------------
 B25881 |                           1424109603000 | 2015-02-16 12:00:03-0600
 B26354 |                           1424109603234 | 2015-02-16 12:00:03-0600

(2 rows)
</code></pre>

<p>As you are finding out, this becomes problematic when you use timestamps as part of your PRIMARY KEY.  It is possible that your timestamp is storing more precision than it is showing you.  And thus, you will need to provide that hidden precision if you will be successful in deleting that single row.</p>

<p>Anyway, you have a couple of options here.  One, find a way to ensure that you are not entering more precision than necessary into your <code>record_time</code>.  Or, you could define <code>record_time</code> as a timeuuid.</p>

<p>Again, it's a theory.  I could be totally wrong, but I have seen people do this a few times.  Usually it happens when they insert timestamp data using <code>dateof(now())</code> like this:</p>

<pre><code>INSERT INTO table (key, time, data) VALUES (1,dateof(now()),'blah blah');
</code></pre>
","['table', 'precision']"
30782581,35349566,2015-06-11 13:44:00,Reading rows using AllRowsReader but starting from a specific row,"<p>I have a batch job that reads through approximately 33 million rows in Cassandra, using the <code>AllRowsReader</code> as described <a href=""https://github.com/Netflix/astyanax/wiki/AllRowsReader-All-rows-query"" rel=""nofollow"">in the Astyanax wiki</a>:</p>

<pre><code>new AllRowsReader.Builder&lt;&gt;(getKeyspace(), columnFamily)
            .withPageSize(100)
            .withIncludeEmptyRows(false)
            .withConcurrencyLevel(1)
            .forEachRow(
                row -&gt; {
                    try {
                        return processRow(row);
                    } catch (Exception e) {
                        LOG.error(""Error while processing row!"", e);
                        return false;
                    }
                }
            )
            .build()
            .call();
</code></pre>

<p>If some sort of error stops the batch job, I would like to be able to pick up and continue reading from the row where it stopped, so that I don't have to start reading from the first row again. Is there any fast and simple way to do this? </p>

<p>Or isn't the <code>AllRowsReader</code> the right fit for this kind of task?</p>
",<java><cassandra><astyanax>,"<p>Since nobody has answered let me try this one. Cassandra uses partitioners to determine in which node it should place the row. 
There are mainly two type of partitioners:
1) Ordered
2) Unordered</p>

<p><a href=""https://docs.datastax.com/en/cassandra/2.2/cassandra/architecture/archPartitionerAbout.html"" rel=""nofollow"">https://docs.datastax.com/en/cassandra/2.2/cassandra/architecture/archPartitionerAbout.html</a></p>

<p>In case of Ordered Partitioner, rows are placed according to the lexicographic order.But in case of Unordered Partitioner you dont have any way to know about the order.</p>

<p>Ordered Partitioner are regarded as anti-pattern in cassandra because it makes cluster distribution pretty difficult.
<a href=""https://docs.datastax.com/en/cassandra/2.2/cassandra/planning/planPlanningAntiPatterns.html"" rel=""nofollow"">https://docs.datastax.com/en/cassandra/2.2/cassandra/planning/planPlanningAntiPatterns.html</a></p>

<p>I am assuming you should be using unordered partitioner in your code. So currently there is no way to tell cassandra which is using unordered partitioner that start from this particular row.</p>

<p>I hope this answers your question</p>
",['partitioner']
30837760,30841975,2015-06-15 05:13:42,Cassandra data model to store embedded documents,"<p>In mongodb we can able to store embedded documents into a collection.Then, How do we store embedded documents into cassandra??? For this sample JSON representation???</p>

<pre><code>UserProfile = {
name: ""user profile"",
Dave Jones: {
   email: {name: ""email"", value: ""dave@email.com"", timestamp: 125555555},
   userName: {name: ""userName"", value: ""Dave"", timestamp: 125555555}
},
Paul Simon: {
   email: {name: ""email"", value: ""paul@email.com"", timestamp: 125555555},
   phone: {name: ""phone"", value: ""4155551212"", timestamp: 125555555},
   userName: {name: ""userName"", value: ""Paul"", timestamp: 125555555}
}
}
</code></pre>
",<mongodb><database-design><cassandra>,"<p>If your document nesting level is not too deep, you can use <a href=""http://www.datastax.com/dev/blog/cql-in-2-1"" rel=""nofollow"">User Defined Types</a> from C* 2.1.</p>

<p>I personally suggest to rethink your schema into more flat form like:</p>

<p><code>
create table profiles (
  name text,
  name2 text,
  email text,
  username text,
  ts timestamp,
  primary key (name,name2) // compound primary key!
)
</code></p>
",['table']
30861162,30861462,2015-06-16 07:19:58,Select first N rows of Cassandra table,"<p>As stated in this <a href=""http://www.slideshare.net/jericevans/cql20"">doc</a> to select a range of rows i have to write this:</p>

<pre><code>select first 100 col1..colN from table;
</code></pre>

<p>but when I launch this on <strong><em>cql shell</em></strong> I get this error:</p>

<pre><code>&lt;ErrorMessage code=2000 [Syntax error in CQL query] message=""line 1:13 no viable alternative at input '100' (select [first] 100...)""&gt;
</code></pre>

<p>What's wrong?</p>
",<cassandra><cql>,"<p>According to the <a href=""https://cassandra.apache.org/doc/cql/CQL.html#Filteringrows"">Docs</a>, key word first is to limit the number of Columnns, not rows </p>

<p>to limit the number of rows , you must just keyword limit.</p>

<pre><code>select col1..colN from table  limit 100;
</code></pre>

<p>the default limit is 10000 </p>
",['table']
30861966,30904642,2015-06-16 08:04:12,cassandra search a row by secondary index returns null,"<p>I have created a TABLE and index As follows</p>

<pre>
CREATE TABLE refresh_token (
    user_id bigint,
    refresh_token text,
    access_token text,
    device_desc text,
    device_type text,
    expire_time timestamp,
    org_id bigint,
    PRIMARY KEY (user_id, refresh_token)
) WITH CLUSTERING ORDER BY (refresh_token ASC)
CREATE INDEX i_access_token ON demodb.refresh_token (access_token);
</pre>

<p>After i insert or delete data about millions times.I'm found when i user the follow query can not return any data. Actually,there has this row in the data.</p>

<p>when i query by  PRIMARY KEY</p>

<pre>select * from refresh_token where user_id=405198 and refresh_token='E82B57D9D64BECDBD6B5602A72816BD19016323504F803116F66A32598E04298';
</pre>

<p>it returns data:</p>

<pre>
 select * from refresh_token where user_id=405198 and refresh_token='E82B57D9D64BECDBD6B5602A72816BD19016323504F803116F66A32598E04298';

 user_id | refresh_token                                                    | access_token                                                     | device_desc | device_type | expire_time              | org_id
---------+------------------------------------------------------------------+------------------------------------------------------------------+-------------+-------------+--------------------------+--------------
  405198 | E82B57D9D64BECDBD6B5602A72816BD19016323504F803116F66A32598E04298 | E82B57D9D64BECDB16D4F3F9F81AC0EF7AF2C4B460CB0F33C9CEFA5846BA7BE1 |        null |        null | 2016-06-07 14:09:52+0800 | 481036337156
</pre>

<p>but when i query by secondary index,it return null.</p>

<pre> select * from refresh_token where access_token ='E82B57D9D64BECDB16D4F3F9F81AC0EF7AF2C4B460CB0F33C9CEFA5846BA7BE1';

 user_id | refresh_token | access_token | device_desc | device_type | expire_time | org_id
---------+---------------+--------------+-------------+-------------+-------------+--------</pre>

<p>thanks</p>
",<cassandra><secondary-indexes>,"<p>Secondary indexes are suggested only for fields with low cardinality.  Your access_token field looks like it has very high cardinality (and may even be unique for all million rows).  This is a known anti pattern in Cassandra.</p>

<p>High cardinality fields are good for things like partition keys because they will hash to a known location. But secondary indexes are not hashed and are found via local data structures on each node.  These local data structures become cumbersome and inefficient when there are a lot of different values being indexed.  I suspect you are hitting an internal timeout before the node with the matching access_token is finding the needle in the haystack.</p>

<p>If you need to find data by access_token, I'd suggest creating a second table where access_token is the partition key and use it to look up the corresponding user_id and refresh_token.  That way you will be using access_token as a hash and will get reliable and quick look ups.</p>
",['table']
31017382,31024039,2015-06-24 04:00:28,"Cassandra modelling, I have a billion of some kind of digital code to store, should I use wide row (CQL with cluster key)?","<p>I am currently doing Cassandra modelling, I have billions of some kind of digital code <code>hnm_code</code> to store, like this:</p>

<pre><code>create table hnm (
    create_batch_id int, // A creation batch can generate up to 1 million code.
    hnm_code text,       // Cardinality: billions
    product_name text,
    primary key (hnm_code)
);
</code></pre>

<p>The cardinality of <code>create_batch_id</code> is relatively small as compared to  of <code>hnm_code</code>. However, what I want is that I should be able to use a value of a single <code>hnm_code</code> column to inquire that record (the <code>create_batch_id</code> is unknown at the time of query).
should I use wide row (CQL with cluster key), like this?:</p>

<pre><code>create table hnm_with_cluster_key (
    create_batch_id int,
    hnm_code text,
    product_name text,
    primary key (create_batch_id, hnm_code)
);
</code></pre>

<p><em>Thanks! It would be nice if you could advise me on how can I achieve good performance on massive this query, and evenly distribution of hnm_code?</em></p>
",<cassandra><modeling>,"<blockquote>
  <p>what I want is that I should be able to use a value of a single hnm_code column to inquire that record</p>
</blockquote>

<p>In Cassandra, you should design your models to match your query patterns.  So this case says it all.  The first solution with a partition key on <code>hnm_code</code> will fulfill this.</p>

<blockquote>
  <p>the create_batch_id is unknown at the time of query</p>
</blockquote>

<p>If you were to use the second solution with <code>PRIMARY KEY (create_batch_id, hnm_code)</code>, you <em>would</em> need to know (and provide) <code>create_batch_id</code> at query time.</p>

<blockquote>
  <p>It would be nice if you could advise me on how can I achieve good performance on massive this query, and evenly distribution of hnm_code?</p>
</blockquote>

<p>Cassandra rows are distributed by the hashed value of the partition key.  So the higher the cardinality of that key, the more even distribution you will have in your cluster.  Also, Cassandra is designed to perform well with lookup by partition key, so your queries should be quite fast.</p>

<blockquote>
  <p>In addition, with the 2nd table definition, my query looks like this: <code>select * from hnm_with_cluster_key where hnm_code='1234' allow filtering;</code></p>
</blockquote>

<p>With a CQL rowcount in the billions, using the <code>ALLOW FILTERING</code> directive will <strong>not</strong> perform well.  I strongly recommend <em>against</em> that.</p>

<blockquote>
  <p>Now I suppose maybe I just need these 2 tables both, One for select a single hnm_code row by a single condition <code>hnm_code = $hnm_code</code>, one for select a creation batch of hnm_codes by <code>create_batch_id = $batch_id</code>, but I resent this duplication, considering that billions of rows is doubled.</p>
</blockquote>

<p>And therein lies the crux of your problem.  Cassandra simply does not support the type of query flexibility to allow this.  It is often not feasible to support multiple queries from a single table design.  If you need to support querying by <code>create_batch_id</code>, then you will need <em>both</em> of your tables.  Each model simply will not support well-performing queries for the other.</p>

<p>Yes, data duplication/redundancy may violate everything we were taught in school about normalization.  But Cassandra is just not designed to work with fully normalized models.  I wrote an article last year for Planet Cassandra that discusses some of these trade-offs: <a href=""http://planetcassandra.org/blog/escaping-from-disco-era-data-modeling/"" rel=""nofollow"">Escaping Disco-Era Data Modeling</a>.  </p>

<p>Essentially while massive data duplication is not something that anyone really wants to do, it can be a necessary trade-off in designing high-performing Cassandra models.</p>
",['table']
31066477,31074945,2015-06-26 06:35:18,Cassandra CQL where clause with multiple collection values?,"<p>My data model:-</p>

<pre><code>tid                                  | codes        | raw          | type
-------------------------------------+--------------+--------------+------
a64fdd60-1bc4-11e5-9b30-3dca08b6a366 | {12, 34, 53} | {sdafb=safd} |  cmd

CREATE TABLE MyTable (
tid       TIMEUUID,
type      TEXT,
codes     SET&lt;INT&gt;,
raw       TEXT,
PRIMARY KEY (tid)
);
CREATE INDEX ON myTable (codes);
</code></pre>

<p>How to query the table to return rows based on multiple set values.</p>

<p>This works:-</p>

<pre><code>select * from logData where codes contains 34;
</code></pre>

<p>But i want to get row based on multiple set values and none of this works:-</p>

<pre><code>select * from logData where codes contains 34, 12; or 
select * from logData where codes contains 34 and 12; or
select * from logData where codes contains {34, 12};
</code></pre>

<p>Kindly assit.</p>
",<cassandra><cql><cql3><cassandra-2.1>,"<p>If I create your table structure and insert a similar row to yours above, I can check for multiple values in the <code>codes</code> collection like this:</p>

<pre><code>aploetz@cqlsh:stackoverflow2&gt; SELECT * FROM mytable 
    WHERE codes CONTAINS 34 
      AND codes CONTAINS 12
      ALLOW FILTERING;

 tid                                  | codes        | raw          | type
--------------------------------------+--------------+--------------+------
 2569f270-1c06-11e5-92f0-21b264d4c94d | {12, 34, 53} | {sdafb=safd} |  cmd

(1 rows)
</code></pre>

<p>Now as others have mentioned, let me <em>also</em> tell you <strong><em>why this is a terrible idea</em></strong>...</p>

<p>With a secondary index on the collection (and with the cardinality appearing to be fairly high) every node will have to be checked for each query.  The idea with Cassandra, is to query by partition key as often as possible, that way you only have to hit one node per query.  Apple's Richard Low wrote a great article called <a href=""http://www.wentnet.com/blog/?p=77"" rel=""noreferrer"">The sweet spot for Cassandra secondary indexes</a>.  It should make you re-think the way you use secondary indexes.</p>

<p>Secondly, the only way I could get Cassandra to accept this query, was to use <a href=""http://www.datastax.com/dev/blog/allow-filtering-explained-2"" rel=""noreferrer"">ALLOW FILTERING</a>.  What this means, is that the only way Cassandra can apply all of your fitlering criteria (WHERE clause) is to pull back every row and individually filter-out the rows that do not meet your criteria.  Horribly inefficient.  To be clear, the ALLOW FILTERING directive is something that you should <em>never</em> use.</p>

<p>In any case, if <code>codes</code> are something that you will need to query by, then you should design an additional query table with <code>codes</code> as a part of the PRIMARY KEY.</p>
",['table']
31114565,31117533,2015-06-29 11:31:17,Cassandra UDT: error: unpack requires a string argument of length 4,"<p>Using [cqlsh 5.0.1 | Cassandra 2.1.5.469 | DSE 4.7.0 | CQL spec 3.2.0 | Native protocol v3]
and the C# 2.5 driver, <strong>I'm trying to represent user settings as a UDT</strong>.</p>

<p>The following is defined in a *.cql script:</p>

<pre><code>CREATE TYPE GeneralSettings (
    last_changed        timestamp,
    last_tokaned        timestamp,
    tokan               text,
    emails              list&lt;text&gt;,
);

CREATE TABLE users (
    id                  timeuuid,
    name                text,
    general_settings    frozen&lt;GeneralSettings&gt;,

    PRIMARY KEY (id)
);
</code></pre>

<p>The following is defined in my C# code:</p>

<pre><code>public class UserGeneralSettings
{
    public DateTimeOffset? LastChanged { get; set; }
    public DateTimeOffset? LastTokened { get; set; }
    public string Token { get; set; }
    public IEnumerable&lt;string&gt; Emails { get; set; }
}
</code></pre>

<hr>

<pre><code>session.UserDefinedTypes.Define(
            UdtMap.For&lt;UserGeneralSettings&gt;().Map(s =&gt; s.LastChanged, ""last_changed"")
                                             .Map(s =&gt; s.LastTokened, ""last_tokaned"")
                                             .Map(s =&gt; s.Token, ""tokan"")
                                             .Map(s =&gt; s.Emails, ""emails"")
        );
</code></pre>

<hr>

<pre><code>For&lt;User&gt;()
        .TableName(""users"")
        .PartitionKey(e =&gt; e.Id)
        .Column(e =&gt; e.Id, cm =&gt; cm.WithName(""id""))
        .Column(e =&gt; e.Name, cm =&gt; cm.WithName(""name""))
        .Column(e =&gt; e.GeneralSettings, cm =&gt; cm.WithName(""general_settings""));
</code></pre>

<p>I'm trying to insert the following data as a new user record, using the <a href=""http://docs.datastax.com/en/developer/csharp-driver/2.5/csharp-driver/reference/mapperComponent.html"" rel=""nofollow"">IMapper</a> interface:</p>

<pre><code>newUser.GeneralSettings = new UserGeneralSettings
        {
            Emails = new [] { ""test@provider.com"" },
            LastChanged = DateTimeOffset.UtcNow,
            LastTokened = DateTimeOffset.UtcNow,
            Token = ""abcd-efg-hijk-lmnop-qrst-uvw-xyz"",
        };
</code></pre>

<p>But even though the insertion goes smoothly, it looks like it is corrupting my users table because I can't select anything from that table using cqlsh.
The error that I'm getting is:</p>

<p><em>Traceback (most recent call last):</em></p>

<p><em>File ""/usr/share/dse/resources/cassandra/bin/cqlsh"", line 1056, in perform_simple_statement rows = self.session.execute(statement, trace=self.tracing_enabled)</em></p>

<p><em>File ""/usr/share/dse/resources/cassandra/bin/../lib/cassandra-driver-internal-only-2.5.1.zip/cassandra-driver-2.5.1/cassandra/cluster.py"", line 1405, in execute result = future.result(timeout)</em></p>

<p><em>File ""/usr/share/dse/resources/cassandra/bin/../lib/cassandra-driver-internal-only-2.5.1.zip/cassandra-driver-2.5.1/cassandra/cluster.py"", line 2976, in result raise self._final_exception</em></p>

<p><strong><em>error: unpack requires a string argument of length 4</em></strong></p>

<p>Does anyone know what causes this error? 
Is this a bug?</p>
",<c#><cassandra><cassandra-2.0>,"<p>This sounds a lot like <a href=""https://issues.apache.org/jira/browse/CASSANDRA-7656"" rel=""nofollow"">CASSANDRA-7656</a>.  Although, that was supposed to have been fixed in one of the later 2.1 release candidates.  Are any of the values you are inserting null?</p>

<p>My theory as to what is happening here, is that Cassandra matches a <code>timestamp</code> with a C# <code>DateTimeOffset</code>.  And that is <strong><em>not</em></strong> the same as a <code>DateTimeOffset?</code> (nullable).</p>

<p>Change your <code>UserGeneralSettings</code> class to use <code>DateTimeOffset</code> for your timesstamps and not a nullable type (<code>DateTimeOffset?</code>).  Then truncate your table and try your insert again.</p>
",['table']
31124834,31395074,2015-06-29 20:20:26,How do you create a table in Cassandra using phantom for Scala?,"<p>I am trying to run the example on <a href=""https://github.com/websudos/phantom/blob/develop/phantom-example/src/main/scala/com/websudos/phantom/example/basics/SimpleRecipes.scala"" rel=""nofollow"">https://github.com/websudos/phantom/blob/develop/phantom-example/src/main/scala/com/websudos/phantom/example/basics/SimpleRecipes.scala</a>
,So I created a Recipe and tried to insert it using <code>insertNewRecord(myRecipe)</code> and got the following exception: <code>....InvalidQueryException: unconfigured columnfamily my_custom_table</code>.
I checked using cqlsh and the keyspace was created but the table was not.</p>

<p>So my question is, how do I create the table using phantom?
This is never mentioned in any of the example code and I also could not figure it out by going over the phantom source code.</p>
",<scala><cassandra><phantom-dsl>,"<p>You need to create the table using schema autogeneration. Simply execute:</p>

<pre><code>import scala.concurrent.ExecutionContext.Implicits.global
import scala.concurrent.duration._
import com.websudos.phantom.dsl._


// To execute this, you need an implicit keySpace and a session.
Await.ready(SimpleRecipes.create.ifNotExists().future(), 3.seconds)
</code></pre>
",['table']
31125886,31128209,2015-06-29 21:28:22,How to get blob size in Cassandra,"<p>Is there a way to get the size of the blob value in cassandra with CQL?  Better yet, is there a way to get an average size of the blob column, especially with a condition?</p>

<p>Thanks!</p>
",<cassandra><cassandra-2.0>,"<p>I don't know of a way to do that in CQL. I assume you are interested in the original uncompressed size of the blob rather than the compressed size within Cassandra. I'd suggest adding an integer field to the table and store the size of the blob in it when you originally save the blob.  </p>

<p>If you use that integer field as a clustering column, then you could do a range query on it to get the rows that have blobs of a certain size range.  To get the average size of the blob's in a range, you could use CQL to retrieve the size column, then use java/python/etc. to calculate the average of the returned values.</p>
",['table']
31184376,31186364,2015-07-02 12:13:15,How to filter Cassandra result based on WRITETIME,"<p>I would like to get values, whose WRITETIME value is newer than a certain time.  I tried this query, but it fails:</p>

<pre><code>SELECT zoom,idx FROM tiles
WHERE zoom=5 AND writetime(tile) &gt; maxTimeuuid('2015-01-01 00:05+0000')
ALLOW FILTERING;
</code></pre>

<p>I get this error:</p>

<pre><code>SyntaxException: &lt;ErrorMessage code=2000 [Syntax error in CQL query] 
    message=""line 1:68 no viable alternative at input '(' (...and idx &gt; 0 
    and [writetime](...)""&gt;
</code></pre>

<p>For this table:</p>

<pre><code>CREATE TABLE tiles (
    zoom int,
    idx int,
    tile blob,
    PRIMARY KEY (zoom, idx)
) WITH COMPACT STORAGE
</code></pre>
",<cassandra><cql>,"<p><code>WRITETIME</code> is a function used for displaying the time a specific column was written.  It is not a part of the PRIMARY KEY, nor is it indexed, so it cannot be used in your WHERE clause.  To be able to query by the time a particular <em>row</em> (not column) was written, you should add that to your table as an additional column <strong>and</strong> as your first clustering key:</p>

<pre><code>CREATE TABLE tilesByLastWritten (
    zoom int,
    idx int,
    tile blob,
    lastwritten timeuuid,
    PRIMARY KEY (zoom, lastwritten, idx)
) WITH CLUSTERING ORDER BY (lastwritten DESC, idx ASC);
</code></pre>

<p>Now this query will work:</p>

<pre><code>aploetz@cqlsh:stackoverflow2&gt; SELECT * FROM tilesByLastWritten 
    WHERE zoom=5 AND lastwritten &gt; mintimeuuid('2015-07-02 08:30:00-0500');

 zoom | lastwritten                          | idx | tile
------+--------------------------------------+-----+------
    5 | 3a439c60-20bf-11e5-b9cb-21b264d4c94d |   1 | null

(1 rows)
</code></pre>

<p>Notes:</p>

<ul>
<li>Don't use the <code>ALLOW FILTERING</code> directive.  Basically this tells Cassandra that it's ok to pull all of your table's rows from all of your nodes, and then apply your filters.</li>
<li>Don't use <code>COMPACT STORAGE</code> on table creation.  This was specifically designed for people to convert new CQL3 tables to the legacy Thrift engine storage format.  If you're not doing <em>specifically that</em>, then you shouldn't use it.</li>
<li>I specified the CLUSTERING ORDER in my example to sort the <code>tiles</code> table by <code>lastwritten</code> in DESCending order.  Usually, timeseries-based applications care about getting the most-recent data, so this usually makes sense.  If this is not the case for you, then the (default) ASCending order should be fine.</li>
<li>In my example I included <code>idx</code> as the last clustering key, mainly for uniqueness.  If you find yourself having to build queries for that column, you may need a different query table (with a re-arranged primary key) to support that.</li>
</ul>

<p>For more help in this area, give Patrick McFadin's <a href=""https://academy.datastax.com/demos/getting-started-time-series-data-modeling"" rel=""noreferrer"">Getting Started With Timeseries Data Modeling</a> a read.</p>
",['table']
31192181,31353235,2015-07-02 18:37:27,write times in cassandra using spark-cassandra connector,"<p>I have this use case where I would need to constantly listen to a kafka topic and  write to 2000 column families(15 columns each.. time series data) based on a column value from a Spark streaming App. I have a local Cassandra installation set up. Creating these column families takes around 1.5 hrs on a CentOS VM using 3 cores and and 12 gigs of ram. In my spark streaming app I'm doing some preprocessing for storing these stream events to Cassandra. I'm running into issues with the amount of time it takes for my streaming app to complete this.<br>
  I was trying to save 300 events to multiple column families(roughly 200-250) based on key  for this my app takes around 10 minutes to save them. This seems to be strange as printing these events to screen grouped by key takes less than a minute, but only when I am saving them to Cassandra it takes time. 
  I have had no issues saving records in the order of 3 million to Cassandra . It took less than 3 minutes(but this was to a single column family in Cassandra). </p>

<p>My requirement is to be as real-time as possible and this seems to be nowhere close. Production environment would have roughly 400 events every 3 seconds.</p>

<p>Is there any tuning that i need to do With the YAML file in Cassandra or any changes to cassandra-connector itself</p>

<pre><code>INFO  05:25:14 system_traces.events                      0,0
WARN  05:25:14 Read 2124 live and 4248 tombstoned cells in system.schema_columnfamilies (see tombstone_warn_threshold). 2147483639 columns was requested, slices=[-]
WARN  05:25:14 Read 33972 live and 70068 tombstoned cells in system.schema_columns (see tombstone_warn_threshold). 2147483575 columns was requested, slices=[-]
WARN  05:25:15 Read 2124 live and 4248 tombstoned cells in system.schema_columnfamilies (see tombstone_warn_threshold). 2147483639 columns was requested, slices=[-]
WARN  05:25:15 Read 2124 live and 4248 tombstoned cells in system.schema_columnfamilies (see tombstone_warn_threshold). 2147483639 columns was requested, slices=[-]
WARN  05:25:15 Read 33972 live and 70068 tombstoned cells in system.schema_columns (see tombstone_warn_threshold). 2147483575 columns was requested, slices=[-]
WARN  05:25:15 Read 33972 live and 70068 tombstoned cells in system.schema_columns (see tombstone_warn_threshold). 2147483575 columns was requested, slices=[-]
INFO  05:25:16 ParNew GC in 340ms.  CMS Old Gen: 1308020680 -&gt; 1454559048; Par Eden Space: 251658240 -&gt; 0; 
WARN  05:25:16 Read 2124 live and 4248 tombstoned cells in system.schema_columnfamilies (see tombstone_warn_threshold). 2147483639 columns was requested, slices=[-]
WARN  05:25:16 Read 33972 live and 70068 tombstoned cells in system.schema_columns (see tombstone_warn_threshold). 2147483575 columns was requested, slices=[-]
WARN  05:25:17 Read 2124 live and 4248 tombstoned cells in system.schema_columnfamilies (see tombstone_warn_threshold). 2147483639 columns was requested, slices=[-]
WARN  05:25:17 Read 2124 live and 4248 tombstoned cells in system.schema_columnfamilies (see tombstone_warn_threshold). 2147483639 columns was requested, slices=[-]
WARN  05:25:17 Read 33972 live and 70068 tombstoned cells in system.schema_columns (see tombstone_warn_threshold). 2147483575 columns was requested, slices=[-]
WARN  05:25:17 Read 33972 live and 70068 tombstoned cells in system.schema_columns (see tombstone_warn_threshold). 2147483575 columns was requested, slices=[-]
INFO  05:25:17 ParNew GC in 370ms.  CMS Old Gen: 1498825040 -&gt; 1669094840; Par Eden Space: 251658240 -&gt; 0; 
WARN  05:25:18 Read 2124 live and 4248 tombstoned cells in system.schema_columnfamilies (see tombstone_warn_threshold). 2147483639 columns was requested, slices=[-]
WARN  05:25:18 Read 33972 live and 70068 tombstoned cells in system.schema_columns (see tombstone_warn_threshold). 2147483575 columns was requested, slices=[-]
WARN  05:25:18 Read 2124 live and 4248 tombstoned cells in system.schema_columnfamilies (see tombstone_warn_threshold). 2147483639 columns was requested, slices=[-]
WARN  05:25:18 Read 2124 live and 4248 tombstoned cells in system.schema_columnfamilies (see tombstone_warn_threshold). 2147483639 columns was requested, slices=[-]
WARN  05:25:19 Read 33972 live and 70068 tombstoned cells in system.schema_columns (see tombstone_warn_threshold). 2147483575 columns was requested, slices=[-]
WARN  05:25:19 Read 33972 live and 70068 tombstoned cells in system.schema_columns (see tombstone_warn_threshold). 2147483575 columns was requested, slices=[-]
INFO  05:25:19 ParNew GC in 382ms.  CMS Old Gen: 1714792864 -&gt; 1875460032; Par Eden Space: 251658240 -&gt; 0; 
W
</code></pre>
",<cassandra><apache-spark><spark-streaming><spark-cassandra-connector>,"<p>I suspect you're hitting edge cases in cassandra related to the large number of CFs/columns defined in the schema. Typically when you see tombstone warnings, it's because you've messed up the data model. However, these are in system tables, so obviously you've done something to the tables that the authors didnt expect (lots and lots of tables, and probably drop/recreating them a lot).</p>

<p>Those warnings were added because scanning past tombstones looking for live columns causes memory pressure, which causes GC, which causes pauses, which causes slowness.</p>

<p>Can you squish the data into significantly fewer column families? You may also want to try clearing out the tombstones (drop gcgs for that table to zero, run major compaction on system if it's allowed?, raise it back to default).</p>
",['table']
31209216,31215322,2015-07-03 14:31:28,Spark-cassandra connector: select list of keys,"<p>Cassandra 2.1, Spark 1.1, spark-cassandra-connector 1.1</p>

<p>I have a very very tall Column Family of key, value pairs. And I also have an RDD of keys that I'd like to select from that CF</p>

<p>What I'd like to do is something like </p>

<pre><code>import com.datastax.spark.connector._                                    

val ids = ...

val pairs = id.map{
 id =&gt; sc.cassandraTable(""cf"", ""tallTable"")
        .select(""the_key"", ""the_val"")
        .where(""the_key = ?"", id)
 }
</code></pre>

<p>However, referring to the the Spark Context in the map causes a NPE. I could make an RDD out of the full tallTable and then join on ids, however that is a very slow operation and I'd like to avoid it. </p>

<p>Is there a way to read a set of keys from Cassandra like this?</p>
",<scala><cassandra><apache-spark><spark-cassandra-connector>,"<p>The spark-cassandra connector offers an optimized method to realize a join of an RDD of keys with a Cassandra table:</p>

<pre><code>// Given a collection of ids
val ids = Seq(id,...)
// Make an RDD out of it
val idRdd = sc.parallelize(ids)
// join the ids with the cassandra table to obtain the data specific to those ids
val data = idRDD.joinWithCassandraTable(""cf"", ""tallTable"")
</code></pre>

<p>This functionality is available from spark-cassandra connector v1.2 onwards so I'd recommend you to upgrade.</p>
",['table']
31210871,31246150,2015-07-03 16:15:48,"Cassandra - Data Modeling Time Series - Avoiding ""Hot Spots""?","<p>I'm working on a Cassandra data model to store records uploaded by users.</p>

<p>The potential problem is, some users may upload 50-100k rows in a 5 minute period, which can result in a ""hot spot"" for the partiton key (user_id). (Datastax recommendation is to rethink data model if more than 10k rows per partition).</p>

<p>How can I avoid having too many records on a partition key in a short amount of time?</p>

<p>I've tried using the <a href=""https://academy.datastax.com/demos/getting-started-time-series-data-modeling"" rel=""nofollow"">Time Series suggestions from Datastax</a>, but even if I had year, month, day, hour columns, a hot spot may still occur.</p>

<pre><code>CREATE TABLE uploads (
    user_id text
   ,rec_id timeuuid
   ,rec_key text
   ,rec_value text
   ,PRIMARY KEY (user_id, rec_id)
);   
</code></pre>

<p>The use cases are:</p>

<ul>
<li>Get all upload records by user_id</li>
<li>Search for upload records by date range
range</li>
</ul>
",<cassandra><data-modeling><datastax><nosql>,"<p>A few possible ideas:</p>

<ol>
<li><p>Use a compound partition key instead of just user_id.  The second part of the partition key could be a random number from 1 to n.  For example if n were 5, then your uploads would be spread out over five partitions per user instead of just one.  The downside is when you do reads, you have to repeat them n times to read all the partitions.</p></li>
<li><p>Have a separate table to handle incoming uploads using the rec_id as the partition key.  This would spread the load of uploads equally across all the available nodes.  Then to get that data into the table with user_id as the partition key, periodically run a spark job to extract new uploads and add them to the user_id based table at a rate the the single partitions can handle.</p></li>
<li><p>Modify your front end to throttle the rate at which an individual user can upload records.  If only a few users are uploading at a high enough rate to cause a problem, it may be easier to limit them rather than modify your whole architecture.</p></li>
</ol>
",['table']
31217129,31217380,2015-07-04 05:08:44,Sorting in Cassandra on primary key of type timeuuid,"<p>I am new to cassandra. I want to get the sorted resultset on the basis of primary key i.e. timeuuid. My table stucture is.</p>

<pre><code>CREATE TABLE user_session
(
  session_id timeuuid,
  ip inet,
  device_type int,
  is_active int,
  last_access_time timestamp,
  logout_reason text,
  logout_type int,
  start_time timestamp,  
  uid int,
  PRIMARY KEY(session_id)
);
</code></pre>

<p>Can anyone help me out.</p>
",<sorting><cassandra>,"<p>You cannot use order by in your query on primary column. It is only supported on clustering column. You would have to change this table so you can perform such a query. It might look something like this:</p>

<pre><code>CREATE TABLE user_session
(
user_id int,
session_id timeuuid,
ip inet,
device_type int,
is_active int,
last_access_time timestamp,
logout_reason text,
logout_type int,
start_time timestamp,  
uid int,
PRIMARY KEY(user_id, session_id)
);
</code></pre>

<p>Then, your query would look like:</p>

<pre><code>select * from user_session where user_id=5 order by session_id ASC;
</code></pre>

<p>Basicaly, you need some primary key which will be used for searching data where only <code>EQ</code> and <code>IN</code> relations are allowed, so you can't have <code>user_id &gt; 5</code> or something similar, and then you can order your results by on clustering column which is in your case <code>session_id</code>. </p>

<p>Zoran</p>
",['table']
31300682,31301133,2015-07-08 18:23:23,CQLSH: Converting unix timestamp to datetime,"<p>I am performing a cql query on a column that stores the values as unix timestmap, but want the results to output as datetime. Is there a way to do this?</p>

<p>i.e. something like the following:</p>

<pre><code>select convertToDateTime(column) from table;
</code></pre>
",<cassandra><cql><cqlsh>,"<p>I'm trying to remember if there's an easier, more direct route.  But if you have a table with a UNIX timestamp and want to show it in a datetime format, you can combine the <code>dateOf</code> and <code>min</code>/<code>maxTimeuuid</code> functions together, like this:</p>

<pre><code>aploetz@cqlsh:stackoverflow2&gt; SELECT datetime,unixtime,dateof(mintimeuuid(unixtime)) FROM unixtime;

 datetimetext   | unixtime      | dateof(mintimeuuid(unixtime))
----------------+---------------+-------------------------------
     2015-07-08 | 1436380283051 |      2015-07-08 13:31:23-0500

(1 rows)
aploetz@cqlsh:stackoverflow2&gt; SELECT datetime,unixtime,dateof(maxtimeuuid(unixtime)) FROM unixtime;

 datetimetext   | unixtime      | dateof(maxtimeuuid(unixtime))
----------------+---------------+-------------------------------
     2015-07-08 | 1436380283051 |      2015-07-08 13:31:23-0500

(1 rows)
</code></pre>

<p>Note that timeuuid stores greater precision than either a UNIX timestamp or a datetime, so you'll need to first convert it to a TimeUUID using either the <code>min</code> or <code>maxtimeuuid</code> function.  Then you'll be able to use <code>dateof</code> to convert it to a datetime timestamp.</p>
","['table', 'precision']"
31305815,31306043,2015-07-08 23:52:10,CQL 3 Unable to Create Column Family With Randomized Name,"<p>When shelled into my Cassandra instance via cqlsh, I am able to create a CF using</p>

<pre><code>CREATE COLUMNFAMILY IF NOT EXISTS 
  sandbox.foo
  ( created TIMESTAMP, 
    updated TIMESTAMP,  
    PRIMARY KEY (created) )  ;
</code></pre>

<p>but when I run </p>

<pre><code>CREATE COLUMNFAMILY IF NOT EXISTS 
  sandbox.6f4922f45568161a8
  ( created TIMESTAMP, 
    updated TIMESTAMP,  
    PRIMARY KEY (created) ) ;
</code></pre>

<p>The command fails w/ error <code>SyntaxException: &lt;ErrorMessage code=2000 [Syntax error in CQL query] message=""line 3:2 no viable alternative at input '(' (... IF NOT EXISTS   sandbox.6f4922f45568161a8  [(]...)""&gt;</code>.</p>

<p>Any idea where I'm going wrong?</p>
",<cassandra><cql><cql3><cqlsh>,"<p>Yes.  The <a href=""http://docs.datastax.com/en/cql/3.1/cql/cql_reference/create_table_r.html"" rel=""nofollow""><code>CREATE TABLE</code> documentation</a> is pretty clear on this one:</p>

<blockquote>
  <p>Valid table names are strings of alphanumeric characters and underscores, <strong>which begin with a letter</strong>.</p>
</blockquote>

<p>Try placing a (random?) letter at the beginning of your table name, and then it should work.</p>
",['table']
31311062,31349661,2015-07-09 07:40:31,Cassandra Reading Benchmark with Spark,"<p>I'm doing a benchmark on Cassandra's Reading performance. In the test-setup step I created a cluster with 1 / 2 / 4 ec2-instances and data nodes. I wrote 1 table with 100 million of entries (~3 GB csv-file). Then I launch a Spark application which reads the data into a RDD using the spark-cassandra-connector. </p>

<p>However, I thought the behavior should be the following: The more instances Cassandra (same instance amount on Spark) uses, the faster the reads! With the writes everything seems to be correct (~2-times faster if cluster 2-times larger).</p>

<p>But: In my benchmark the read is always faster with a 1-instance-cluster then with a 2- or 4-instance-cluster!!!</p>

<p>My Benchmark Results:</p>

<p>Cluster-size 4: Write: 1750 seconds / Read: 360 seconds</p>

<p>Cluster-size 2: Write: 3446 seconds / Read: 420 seconds</p>

<p>Cluster-size 1: Write: 7595 seconds / Read: <strong>284 seconds</strong></p>

<p><br/></p>

<p><strong>ADDITIONAL TRY - WITH THE CASSANDRA-STRESS TOOL</strong></p>

<p>I launched the ""cassandra-stress"" tool on the Cassandra cluster (size 1 / 2 / 3 / 4 nodes), with following results:</p>

<pre><code>Clustersize    Threads     Ops/sek  Time
1              4           10146    30,1
               8           15612    30,1
              16           20037    30,2
              24           24483    30,2
             121           43403    30,5
             913           50933    31,7
2              4            8588    30,1
               8           15849    30,1
              16           24221    30,2
              24           29031    30,2
             121           59151    30,5
             913           73342    31,8
3              4            7984    30,1
               8           15263    30,1
              16           25649    30,2
              24           31110    30,2
             121           58739    30,6
             913           75867    31,8
4              4            7463    30,1
               8           14515    30,1
              16           25783    30,3
              24           31128    31,1
             121           62663    30,9
             913           80656    32,4
</code></pre>

<p><br/>Results: With 4 or 8 threads the single-node cluster is as fast or faster then the larger clusters!!!</p>

<p><br/><br/>Results as diagram:
<img src=""https://i.stack.imgur.com/3BGNw.png"" alt=""enter image description here"">
<br/>The data-sets are the cluster sizes (1/2/3/4), x-axis the threads, and y-axis the ops/sec. </p>

<p><strong>--> Question here: Are these results the cluster-wide results or is this a test for a local node (and so the result of only one instance of the ring)???</strong></p>

<p>Can someone give an explanation? Thank you!</p>
",<amazon-ec2><cassandra><apache-spark><benchmarking><spark-cassandra-connector>,"<p>I ran a similar test with a spark worker running on each Cassandra node.</p>

<p>Using a Cassandra table with 15 million rows (about 1.75 GB of data), I ran a spark job to create an RDD from the table with each row as a string, and then printed a count of the number of rows.</p>

<p>Here are the times I got:</p>

<pre><code>1 C* node, 1 spark worker - 1 min. 42 seconds
2 C* nodes, 2 spark workers - 55 seconds
4 C* nodes, 4 spark workers - 35 seconds
</code></pre>

<p>So it seems to scale pretty well with the number of nodes when the spark workers are co-located with the C* nodes.</p>

<p>By not co-locating your workers with Cassandra, you are forcing all the table data to go across the network.  That will be slow and perhaps in your environment is a bottleneck.  If you co-locate them, then you benefit from data locality since spark will create the RDD partitions from the tokens that are local to each machine.</p>

<p>You may also have some other bottleneck. I'm not familiar with EC2 and what it offers.  Hopefully it has local disk storage rather than network storage since C* doesn't like network storage.</p>
",['table']
31324024,31341675,2015-07-09 17:08:17,Spark Cassandra Aggregation java.lang.OutOfMemoryError: Java heap space,"<p>I've been trying to learn how to use Apache Spark, and I'm having issues trying to sum all the values in a column from Cassandra (using the datastax spark-cassandra-connector). Everything I try just results in <strong>java.lang.OutOfMemoryError: Java heap space</strong>. </p>

<p>Here's the code I'm submitting to the spark master:</p>

<pre><code>object Benchmark {
  def main( args: Array[ String ] ) {
    val conf    = new SparkConf()
                  .setAppName( ""app"" )
                  .set( ""spark.cassandra.connection.host"", ""ec2-blah.compute-1.amazonaws.com"" )
                  .set( ""spark.cassandra.auth.username"", ""myusername"" )
                  .set( ""spark.cassandra.auth.password"", ""mypassword"" )
                  .set( ""spark.executor.memory"", ""4g"" )
    val sc      = new SparkContext( conf )
    val tbl     = sc.cassandraTable( ""mykeyspace"", ""mytable"" )
    val res     = tbl.map(_.getFloat(""sclrdata"")).sum()

    println( ""sum = "" + res )
  }
}
</code></pre>

<p>Right now I only have a single spark worker node in my cluster, and it is definitely possible that given the size of the table, not all of it can fit in memory at once. However I didn't think this would be an issue since spark is supposed to lazily evaluate the commands, and summing all the values in a column shouldn't need to have the entire table reside in memory at once. </p>

<p>I'm very much a newbie to this topic, so any clarification as to why this wouldn't work or help as to how to do it correctly would be very appreciated.</p>

<p>Thanks</p>
",<scala><cassandra><apache-spark><datastax><spark-cassandra-connector>,"<p>Perhaps spark is building the entire table as a single in memory partition so that it can do the mapping operations on it.</p>

<p>I thought spark was supposed to spill to disk rather than throw OutOfMemoryExceptions, but maybe it isn't able to spill if there is just a single partition.  I saw a similar problem <a href=""https://stackoverflow.com/questions/31141998/why-apache-spark-is-performing-the-filters-on-client"">here</a>, and he solved it by specifying a split size like this:</p>

<pre><code>conf = new SparkConf();
        conf.setAppName(""Test"");
        conf.setMaster(""local[4]"");
        conf.set(""spark.cassandra.connection.host"", ""192.168.1.15"").
        set(""spark.executor.memory"", ""2g"").
        set(""spark.cassandra.input.split.size_in_mb"", ""67108864"");
</code></pre>

<p>So try setting spark.cassandra.input.split.size_in_mb in your conf.</p>

<p>I imagine this would allow spark to sum up chunks of the table and then evict those chunks from memory when it needs space for new chunks.</p>

<p>Another thing you could look into is specifying a storage level for the table RDD that would allow it to spill to disk.  I think you could do this by adding "".persist(StorageLevel.MEMORY_AND_DISK)"".  The default appears to be MEMORY_ONLY.  See more information on storage levels <a href=""http://spark.apache.org/docs/latest/programming-guide.html"" rel=""nofollow noreferrer"">here</a>, in the RDD persistence section.</p>
",['table']
31415250,31417467,2015-07-14 19:07:33,"Datastax java driver, convert scala collections to java error","<p>I am trying to store a Scala Map (that I'm trying to convert to a java.util.Map) into cassandra 2.1.8.</p>

<p>The data structure looks like this:</p>

<pre><code>Map[String -&gt; Set[Tuple[String, String, String]]]
</code></pre>

<p>I created the table as follows:</p>

<pre><code>CREATE TABLE mailing (emailaddr text PRIMARY KEY, totalmails bigint, emails map&lt;text, frozen&lt;set&lt;tuple&lt;text, text, text&gt;&gt;&gt;&gt;);
</code></pre>

<p>I first try to convert the Set's to java Set's:</p>

<pre><code>def emailsToCassandra(addr: emailAddress, mail: MailContent, number: Int) = {
println(""Inserting emails into cassandra"")

mail.emails.foreach(result =&gt;

  setAsJavaSet(result._2)
)
</code></pre>

<p>I then build the query and attempt to convert the Map to a java Map:</p>

<pre><code>val query = QueryBuilder.insertInto(""emails"", ""mailing"")
                        .value(""emailAddr"", addr.toString())
                        .value(""totalmails"", number)
                        .value(""emails"", mapAsJavaMap(mail.emails))
session.executeAsync(query)
</code></pre>

<p>I get back:</p>

<pre><code>java.lang.IllegalArgumentException: Value 1 of type class scala.collection.convert.Wrappers$MapWrapper does not correspond to any CQL3 type
</code></pre>

<p>I also tried to do this:</p>

<pre><code>val lol = mail.emails.asInstanceOf[java.util.Map[String, java.util.Set[Tuple3[String, String, String]]]]
</code></pre>

<p>Which didn't work</p>

<p>Thank you in advance</p>
",<scala><cassandra><datastax-java-driver>,"<p>There are a few things you are going to need to overcome here:</p>

<ol>
<li>Converting the map into the java.util.Map type. (You've already covered this by using mapAsJavaMap)</li>
<li>Converting the Set[Tuple3] into a java.util.Set type.</li>
<li>Converting Tuple3 into a <a href=""http://docs.datastax.com/en/developer/java-driver/2.1/java-driver/reference/tupleTypes.html"" rel=""noreferrer"">TupleValue</a>.</li>
</ol>

<p>Unfortunately the error returned by the driver (<code>java.lang.IllegalArgumentException: Value 1 of type class scala.collection.convert.Wrappers$MapWrapper does not correspond to any CQL3 type</code>) is misleading, as the Map type is converting correctly in your code, but the Tuple3 type is ultimately what it is having problems with.  I opened up <a href=""https://datastax-oss.atlassian.net/browse/JAVA-833"" rel=""noreferrer"">JAVA-833</a> to track this.</p>

<p>I'm making assumptions about what MailContent is, but here is some code that should make things work.  The main logic that does the heavy lifting is <code>emailsToCql</code> which maps <code>Tuple3[String, String, String]</code> into <code>TupleValue</code>, Set to java.util.Set and Map to java.util.Map.</p>

<pre class=""lang-scala prettyprint-override""><code>import com.datastax.driver.core.{DataType, TupleType, Cluster}
import com.datastax.driver.core.querybuilder.QueryBuilder

import scala.collection.JavaConverters._

object Scratch extends App {

  val cluster = Cluster.builder().addContactPoint(""127.0.0.1"").build()
  val session = cluster.connect()

  session.execute(""create keyspace if not exists emails WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };"")
  session.execute(""create table if not exists emails.mailing (emailaddr text PRIMARY KEY, totalmails bigint, emails map&lt;text, frozen&lt;set&lt;tuple&lt;text, text, text&gt;&gt;&gt;&gt;);"")

  val emailType = TupleType.of(DataType.text(), DataType.text(), DataType.text())

  case class MailContent(addr: String, emails: Map[String, Set[Tuple3[String, String, String]]]) {
    lazy val emailsToCql = emails.mapValues {
      _.map(v =&gt; emailType.newValue(v._1, v._2, v._3)).asJava
    }.asJava
  }

  val mailContent = MailContent(""test@email.com"", Map(
    ""dest@email.com"" -&gt; Set((""field1"", ""field2"", ""field3"")),
    ""dest2@email.com"" -&gt; Set((""2field1"", ""2field2"", ""2field3""))))

  val query = QueryBuilder.insertInto(""emails"", ""mailing"")
                .value(""emailAddr"", mailContent.addr)
                .value(""totalmails"", mailContent.emails.size)
                .value(""emails"", mailContent.emailsToCql)

  session.execute(query)

  cluster.close()
}
</code></pre>

<p>This yields a record that looks like the following in cqlsh:</p>

<pre><code> emailaddr      | emails                                                                                                       | totalmails
----------------+--------------------------------------------------------------------------------------------------------------+------------
 test@email.com | {'dest2@email.com': {('2field1', '2field2', '2field3')}, 'dest@email.com': {('field1', 'field2', 'field3')}} |          2
</code></pre>
",['table']
31422393,31427931,2015-07-15 05:50:19,How to create dynamic schema using CQL,"<p>I need to insert a new column to a row to handle semi structured data using CQL. Is it possible ? If it is possible, please advise. </p>
",<cassandra><cql><datastax>,"<p>Dynamically changing table structures is not advised in Cassandra as it involves all the nodes in the cluster to acknowledge the ALTER statement (among other issues you might have).</p>

<p>Your best options are : </p>

<ul>
<li>to use collections with basic types: a map could do the trick in your case</li>
<li>to use a combo of collections and UDTs</li>
<li>to use a text field and store your data in JSON for example (suitable if data isn't updated afterwards)</li>
<li>to use a blob and store whatever format you want in it (given you never update it as well)</li>
</ul>

<p>You could also change your model and handle the columns you want to add as rows instead.</p>

<p>If you want further advices, share your model and use case more precisely.</p>
",['table']
31425949,31426284,2015-07-15 09:02:46,Cassandra Prepared Statement - Binding Parameters Twice,"<p>I have a cql query I want to preform. The cql string looks like this: </p>

<pre><code>SELECT * FROM :columnFamilyName WHERE &lt;some_column_name&gt; = :name AND &lt;some_id&gt; = :id;
</code></pre>

<p>My application has two layers of abstraction above the datastax driver. In one layer I want to bind the first two parameters and in another layer I'd like to bind the last parameter.
The problem is, if I bind the first two parameters, I get a BoundStatement to which I cannot bind another parameter. Am I missing something? Can it be done?</p>

<p>We're using datastax driver version 2.0.3.</p>

<p>Thanks,
Anatoly.</p>
",<cassandra><cql><datastax><datastax-java-driver>,"<p>You should be able to bind any number of parameters to your BoundStatement using <em>boundStatement.setXXXX(index,value)</em> as follows : </p>

<pre><code>BoundStatement statement = new BoundStatement(query);
statement.setString(0, ""value"");
statement.setInt(1, 1);
statement.setDate(2, new Date());
ResultSet results = session.execute(statement);
</code></pre>

<p>The problem though is that you're trying to use a dynamic column family whose value changes with the value you want to bind.
As far as I know, this is not allowed so you should instead prepare one statement per table and then use the right bound statement.</p>
",['table']
31485141,31485445,2015-07-17 21:41:59,Cassandra data model for querying column with non-unique values,"<p>I am trying to come up with a data model that works for my situation. I have the following columns: runid, stat1, stat2, stat3.</p>

<p>I will be querying based on runid (ex. select * from table where runid=123) but runid will have repeated values so I cant just make it a primary key. 
Also, when querying I do not know anything about stat1-3 so I cannot make those columns a part of the primary key either. </p>

<p>(Background: My Cassandra  instance is populated by another program and my program will just pull information specific to a particular runid from it and display it on a screen)</p>

<p>This seems like a common enough scenario but I'm new to Cassandra. I know runid needs to be a part of the primary key since I have to run queries based on it but it has repeated values. </p>

<p>Any suggestions?</p>
",<cassandra><datamodel>,"<p>You are correct in thinking this is a common enough scenario and fortunately you don't have to know the full primary key to make queries!   Just the partition key (the first part of the primary key is required in your select criteria.</p>

<p>For example, if you create your table like this:</p>

<pre><code>CREATE TABLE test.runs (
    stat1 text,
    stat2 text,
    stat3 text,
    runid int,
    PRIMARY KEY (runid, stat1)
 );
</code></pre>

<p>You should be able to retrieve data by just specifying the partition key (runid):</p>

<pre><code>cassandra@cqlsh:test&gt; insert into runs (runid, stat1, stat2, stat3) values (0, '1', 'hi', 'hi'); 
cassandra@cqlsh:test&gt; insert into runs (runid, stat1, stat2, stat3) values (0, '2', 'lo', 'lo');
cassandra@cqlsh:test&gt; insert into runs (runid, stat1, stat2, stat3) values (0, '3', 'yo', 'yo');
cassandra@cqlsh:test&gt; insert into runs (runid, stat1, stat2, stat3) values (1, '1', '22', '33');
cassandra@cqlsh:test&gt; select * from runs where runid = 0;

 runid | stat1 | stat2 | stat3
-------+-------+-------+-------
     0 |     1 |    hi |    hi
     0 |     2 |    lo |    lo
     0 |     3 |    yo |    yo
</code></pre>

<p>That being said, it would probably be good to come up with a better secondary value for your primary key to allow multiple 'stat1' columns to have the same value within a runid, maybe a random uuid to create some unique value?</p>
",['table']
31502442,31502671,2015-07-19 14:47:31,Check CQL version with Cassandra and cqlsh?,"<p>How do you check which cql version is currently being used in cqlsh?</p>

<p>In sql, you do this:</p>

<pre><code>Select @@version
</code></pre>
",<cassandra><cql><cqlsh>,"<p>There are a couple of ways to go about this.</p>

<p>From within cqlsh, you can simply <code>show version</code>.</p>

<pre><code>aploetz@cqlsh&gt; show version
[cqlsh 5.0.1 | Cassandra 2.1.8 | CQL spec 3.2.0 | Native protocol v3]
</code></pre>

<p>However, that only works from within cqlsh.  Fortunately, you can also query <code>system.local</code> for that information as well.</p>

<pre><code>aploetz@cqlsh&gt; SELECT cql_version FROM system.local;

 cql_version
-------------
       3.2.0

(1 rows)
</code></pre>

<p>The <code>system.local</code> table has other useful bits of information about your current node, cluster, and Cassandra tool versions as well.  For more info, check it out with either <code>SELECT * FROM system.local;</code> (only has 1 row) or <code>desc table system.local</code>.</p>
",['table']
31545192,31587942,2015-07-21 16:57:55,Storing latitude longitude in cassandra table,"<p>How to store latitude and longitude into the Cassandra tables and how to query up the data within 5 kms radius </p>
",<cassandra><cql>,"<p>Your question is very broad and vague, so I will give a very broad answer.</p>

<p>You could store each latitude and longitude as a row in a Cassandra table, like this:</p>

<pre><code>CREATE TABLE locations (location text PRIMARY KEY, latitude float, longitude float);
</code></pre>

<p>Then to find all the locations within a 5 km radius of a specified latitude and longitude (let's call that location X), you'd need to check each row in the table using a client application you would create.</p>

<p>In pure Cassandra, you would SELECT * from the table to get all the rows (using paging if there are a lot of rows), and in your client application, for each row check if the distance between X and the row is less than 5 km, and output rows that match.</p>

<p>Or you could pair Cassandra with Apache spark and do the same calculation in parallel.</p>

<p>But there are a lot of different approaches you could take, so that's just one way.</p>
",['table']
31552504,31553005,2015-07-22 01:42:52,Why use a compound clustered key in Cassandra tables?,"<p>Why might one want to use a clustered index in a cassandra table?</p>

<p>For example; in a table like this:</p>

<pre><code>CREATE TABLE blah (
  key text,
  a text,
  b timestamp,
  c double,
  PRIMARY KEY ((key), a, b, c)
)
</code></pre>

<p>The clustered part is the <code>a, b, c</code> part of the <code>PRIMARY KEY</code>.</p>

<p>What are the benefits? What considerations are there?</p>
",<cassandra><data-modeling><bigtable><compound-key>,"<p>Clustering keys do three main things.</p>

<p>1) They affect the available query pattern of your table.</p>

<p>2) They determine the on-disk sort order of your table.</p>

<p>3) They determine the uniqueness of your primary key.</p>

<p>Let's say that I run an ordering system and want to store product data on my website.  Additionally I have several distribution centers, as well as customer contracted pricing.  So when a certain customer is on my site, they can only access products that are:</p>

<ul>
<li><p>Available in a distribution center (DC) in their geographic area.</p></li>
<li><p>Defined in their contract (so they may not necessarily have access to all products in a DC).</p></li>
</ul>

<p>To keep track of those products, I'll create a table that looks like this:</p>

<pre><code>CREATE TABLE customerDCProducts (
  customerid text,
  dcid text,
  productid text,
  productname text,
  productPrice int,
  PRIMARY KEY (customerid, dcid, productid));
</code></pre>

<p>For this example, if I want to see product 123, in DC 1138, for customer B-26354, I can use this query:</p>

<pre><code>SELECT * FROM customerDCProducts
WHERE customerid='B-26354' AND dcid='1138' AND productid='123';
</code></pre>

<p>Maybe I want to see products available in DC 1138 for customer B-26354:</p>

<pre><code>SELECT * FROM customerDCProducts 
WHERE customerid='B-26354' AND dcid='1138';
</code></pre>

<p>And maybe I just want to see all products in all DCs for customer B-26354:</p>

<pre><code>SELECT * FROM customerDCProducts 
WHERE customerid='B-26354';
</code></pre>

<p>As you can see, the clustering keys of <code>dcid</code> and <code>productid</code> allow me to run high-performing queries on my partition key (<code>customerid</code>) that are as focused as I may need.</p>

<p>The drawback?  If I want to query all products for a single DC, regardless of customer, I cannot.  I'll need to build a different query table to support that.  Even if I want to query just one product, I can't unless I also provide a <code>customerid</code> and  <code>dcid</code>.</p>

<p>What if I want my data ordered a certain way?  For this example, I'll take a cue from Patrick McFadin's article on <a href=""http://academy.datastax.com/demos/getting-started-time-series-data-modeling"" rel=""nofollow"">Getting Started With Time Series Data Modeling</a>, and build a table to keep track of the latest temperatures for weather stations.</p>

<pre><code>CREATE TABLE latestTemperatures (
  weatherstationid text,
  eventtime timestamp,
  temperature text,
  PRIMARY KEY (weatherstationid,eventtime),
) WITH CLUSTERING ORDER BY (eventtime DESC);
</code></pre>

<p>By clustering on <code>eventtime</code>, and specifying a <code>DESC</code>ending ORDER BY, I can query the recorded temperatures for a particular station like this:</p>

<pre><code>SELECT * FROM latestTemperatures 
WHERE weatherstationid='1234ABCD';
</code></pre>

<p>When those values are returned, they will be in <code>DESC</code>ending order by <code>eventtime</code>.</p>

<p>Of course, the one question that everyone (with a RDBMS background...so yes, <em>everyone</em>) wants to know, is how to query all results ordered by <code>eventtime</code>?  And again, you cannot.  Of course, you can query for all rows by omitting the WHERE clause, but that won't return your data sorted in any meaningful order.  It's important to remember that Cassandra can only enforce clustering order <em>within a partition key</em>.  If you don't specify one, your data will not be ordered (at least, not in the way that you want it to be).</p>

<p>Let me know if you have any additional questions, and I'll be happy to explain.</p>
",['table']
31576180,31602053,2015-07-23 00:23:25,Cassandra 2.1 system schema missing,"<p>I have a six node cluster running cassandra 2.1.6.  Yesterday I tried to drop a column family and received the message ""<em>Column family ID mismatch</em>"". <br/><br/> I tried running <strong>nodetool repair</strong> but after repair was complete I got the same message. I then tried selecting from the column family but got the message ""<em>Column family not found</em>"".  <br/> <br/>I ran the following query to get a list of all column families in my schema <br/>
<strong>select columnfamily_name from system.schema_columnfamilies where keyspace_name = 'xxx';</strong> <br/>
At this point I received the message 
""<em>Keyspace 'system' not found.</em>"" <br/><br/> I tried the command <strong>describe keyspaces</strong> and sure enough <em>system</em> was not in the list of keyspaces. <br/><br/>
I then tried <strong>nodetool resetlocalshema</strong> on one of the nodes missing the <em>system</em> keyspace and when that failed to resolve the problem I tried <strong>nodetool rebuild</strong>  but got the same messages after rebuild was complete. <br/> <br/> I tried stopping the nodes missing the <em>system</em> keyspace and restarted them, once the restart was completed the <em>system</em> keyspace was back and I was able to execute the above query successfully. However, the table I had tried to drop previously was not listed so I tried to recreate it and once again received the message <em>Column family ID mismatch</em>. <br/><br/>
Finally, I shutdown the cluster and restarted it... and everything works as expected.<br/><br/>
My questions are: <br/> How/why did the system keyspace disappear? <br/> What happened to the data being inserted into my column families while the system keyspace was missing from two of the six nodes? (my application didn't seem to have any problems) <br/> Is there a way I can detect problems like this automatically or do I have to manually check up on my keyspaces each day? <br/> Is there a way to fix the missing <em>system</em> keyspace and/or the <em>Column family ID mismatch</em> without restarting the entire cluster?</p>

<blockquote>
  <p><strong>EDIT</strong> <br/>
  As per Jim Meyers suggestion I queried the <em>cf_id</em> on each node of the cluster and confirmed that all nodes return the same value. <br/></p>
  
  <blockquote>
    <p><strong>select cf_id from system.schema_columnfamilies where columnfamily_name = 'customer' allow filtering;</strong>  <br/></p>
    
    <p>cf_id <br/>
    -------------------------------------- <br/>
    cbb51b40-2b75-11e5-a578-798867d9971f <br/> <br/>
    I then ran <strong>ls</strong> on my data directory and can see that there are multiple entries for a few of my tables <br/>
    customer-72bc62d0ff7611e4a5b53386c3f1c9f9 <br/>
    customer-cbb51b402b7511e5a578798867d9971f <br/><br/>
    My application dynamically creates tables at run time (always using <em>IF NOT EXISTS</em>), seems likely that the application issued the same create table command on separate nodes at the same time resulting in the schema mismatch.
    Since I've restarted the cluster everything seems to be working fine.<br/><br/>
    Is it safe to delete the extra file? <br/>
    i.e. <em>customer-72bc62d0ff7611e4a5b53386c3f1c9f9</em></p>
  </blockquote>
</blockquote>
",<cassandra>,"<p>1 The cause of this problem is a CREATE TABLE statement collision. Do not generate tables dynamically from multiple clients, even with IF NOT EXISTS. First thing you need to do is fix your code so that this does not happen. Just create your tables manually from cqlsh allowing time for the schema to settle. Always wait for schema agreement <a href=""https://datastax.github.io/java-driver/2.0.10/features/metadata/"" rel=""nofollow"">when modifying schema</a>.</p>

<p>2 Here's the fix:</p>

<p>1) Change your code to not automatically re-create tables (even with IF NOT EXISTS).</p>

<p>2) Run a rolling restart to ensure schema matches across nodes. Run nodetool describecluster around your cluster. Check that there is only one schema version. </p>

<p>ON EACH NODE:</p>

<p>3) Check your filesystem and see if you have two directories for the table in question in the data directory.</p>

<p>If THERE ARE TWO OR MORE DIRECTORIES:</p>

<p>4)Identify from schema_column_families which cf ID is the ""new"" one (currently in use). </p>

<p>cqlsh -e ""select * from system.schema_column_families""|grep </p>

<p>5) Move the data from the ""old"" one to the ""new"" one and remove the old directory. </p>

<p>6) If there are multiple ""old"" ones repeat 5 for every ""old"" directory.</p>

<p>7) run nodetool refresh</p>

<p>IF THERE IS ONLY ONE DIRECTORY:</p>

<p>No further action is needed.</p>

<h3>Futures</h3>

<p>Schema collisions will continue to be an issue until - <a href=""https://issues.apache.org/jira/browse/CASSANDRA-9424"" rel=""nofollow"">CASSANDRA-9424</a></p>

<p>Here's an example of it occurring on Jira and closed as <code>not a problem</code> <a href=""https://issues.apache.org/jira/browse/CASSANDRA-8387"" rel=""nofollow"">CASSANDRA-8387</a></p>
",['table']
31612199,31701968,2015-07-24 13:45:22,"Cassandra - Write doesn't fail, but values aren't inserted","<p>I have a cluster of 3 Cassandra 2.0 nodes. My application I wrote a test which tries to write and read some data into/from Cassandra. In general this works fine.</p>

<p>The curiosity is that after I restarted my computer, this test will fail, because after writting I read the same value I´ve write before and there I get null instead of the value, but the was no exception while writing. 
If I manually truncate the used column family, the test will pass. After that I can execute this test how often I want, it passes again and again. Furthermore it doesn´t matter if there are values in the Cassandra or not. The result is alwalys the same.</p>

<p>If I look at the CLI and the CQL-shell there are two different views:</p>

<p><a href=""https://i.stack.imgur.com/A8wsO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/A8wsO.png"" alt=""cassandra-cli""></a></p>

<p><a href=""https://i.stack.imgur.com/fMe7O.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fMe7O.png"" alt=""enter image description here""></a></p>

<p>Does anyone have an ideas what is going wrong? The timestamp in the CLI is updated after re-execution, so it seems to be a read-problem?</p>

<p>A part of my code:
For inserts I tried </p>

<pre><code>Insert.Options insert =   QueryBuilder.insertInto(KEYSPACE_NAME,TABLENAME)
                .value(ID, id)
                .value(JAHR, zonedDateTime.getYear())
                .value(MONAT, zonedDateTime.getMonthValue())
                .value(ZEITPUNKT, date)
                .value(WERT, entry.getValue())
                .using(timestamp(System.nanoTime() / 1000));
</code></pre>

<p>and</p>

<pre><code>Insert insert = QueryBuilder.insertInto(KEYSPACE_NAME,TABLENAME)
                .value(ID, id)
                .value(JAHR, zonedDateTime.getYear())
                .value(MONAT, zonedDateTime.getMonthValue())
                .value(ZEITPUNKT, date)
                .value(WERT, entry.getValue());
</code></pre>

<p>My select looks like</p>

<pre><code>Select.Where select = QueryBuilder.select(WERT)
            .from(KEYSPACE_NAME,TABLENAME)
            .where(eq(ID, id))
            .and(eq(JAHR, zonedDateTime.getYear()))
            .and(eq(MONAT, zonedDateTime.getMonthValue()))
            .and(eq(ZEITPUNKT, Date.from(instant)));
</code></pre>

<p>Consistencylevel is QUORUM (for both) and replicationfactor 3</p>
",<cassandra><cql><cassandra-2.0><datastax-java-driver><cassandra-cli>,"<p>I'd say this seems to be a problem with timestamps since a truncate solves the problem. In Cassandra last write wins and this could be a problem caused by the use of System.nanoTime() since</p>
<blockquote>
<p>This method can only be used to measure elapsed time and is not related to any other notion of system or wall-clock time.</p>
<p>...</p>
<p>The values returned by this method become meaningful only when the difference between two such values, obtained within the same instance of a Java virtual machine, is computed.</p>
</blockquote>
<p><a href=""http://docs.oracle.com/javase/7/docs/api/java/lang/System.html#nanoTime()"" rel=""nofollow noreferrer"">http://docs.oracle.com/javase/7/docs/api/java/lang/System.html#nanoTime()</a></p>
<p>This means that the write that occured before the restart could have been performed &quot;in the future&quot; compared to the write after the restart. This would not fail the query, but the written value would simply not be visible due to the fact that there is a &quot;newer&quot; value available.</p>
<p>Do you have a requirement to use sub-millisecond precision for the insert timestamps? If possible I would recommend using System.currentTimeMillis() instead of nanoTime().</p>
<p><a href=""http://docs.oracle.com/javase/7/docs/api/java/lang/System.html#currentTimeMillis()"" rel=""nofollow noreferrer"">http://docs.oracle.com/javase/7/docs/api/java/lang/System.html#currentTimeMillis()</a></p>
<p>If you have a requirement to use sub-millisecond precision it would be possible to use System.currentTimeMillis() with some kind of atomic counter that ranged between 0-999 and then use that as a timestamp. This would however break if multiple clients insert the same row at the same time.</p>
",['precision']
31623232,31626258,2015-07-25 05:18:32,How to perform a query with multiple greater/less than condition in CQL/cassandra?,"<p>I am writing a map app.
Once I got the bounds of map I have to query the DB for points.
The structure:</p>

<pre>
CREATE TABLE map.items (
    id timeuuid,
    type int,
    lat double,
    lng double,
    rank int
)</pre>

<p>and I would like to query like this:</p>

<pre>select * from map.items 
where type=1 
and (lat &gt; 30 and lat &lt; 35)
and (lng &gt; 100 and lng &lt; 110)
order by rank desc
limit 10;</pre>

<p>Newbie to cassandra, please help or provide references.
Thank you!</p>
",<cassandra><cql><cql3>,"<p>CQL can only apply a range query on one clustering key, so you won't be able to do that directly in CQL.</p>

<p>A couple possible approaches:</p>

<ol>
<li><p>Pair Cassandra with Apache Spark.  Read the table into an RDD and apply a filter operation to keep only rows that match the ranges you are looking for.  Then do a sort operation by rank.  Then use collect() to gather and output the top ten results.</p></li>
<li><p>Or in pure Cassandra, partition your data by latitude.  For example one partition might contain data points for all latitudes >= 30 and &lt; 31, and so on.  Your client would then do multiple queries of each latitude partition needed (i.e. 30, 31, 32, 33, and 34) and use a range query on longitude (using longitude as a clustering column).  As results are returned, keep the highest ten ranked rows and discard the others.</p></li>
</ol>
",['table']
31669991,31675056,2015-07-28 07:29:39,How does cassandra find the node that contains the data?,"<p>I've read quite a few articles and a lot of question/answers on SO about Cassandra but I still can't figure out how Cassandra decides which node(s) to go to when it's reading the data.</p>

<p>First, some assumptions about an imaginary cluster:</p>

<ol>
<li>Replication Strategy = simple</li>
<li>Using Random Partitioner</li>
<li>Cluster of 10 nodes</li>
<li>Replication Factor of 5</li>
</ol>

<p>Here's my understanding of how writes work based on various Datastax articles and other blog posts I've read:</p>

<ul>
<li>Client sends the data to a random node</li>
<li>The ""random"" node is decided based on the MD5 hash of the primary key.</li>
<li><p>Data is written to the commit_log and memtable and then propagated 4 times (with RF = 5).</p></li>
<li><p>The 4 next nodes in the ring are then selected and data is persisted in them.</p></li>
</ul>

<p>So far, so good.</p>

<p>Now the question is, when the client sends a read request (say with CL = 3) to the cluster, how does Cassandra know which nodes (5 out of 10 as the worst case scenario) it needs to contact to get this data? Surely it's not going to all 10 nodes as that would be inefficient.</p>

<p>Am I correct in assuming that Cassandra will again, do an MD5 hash of the primary key (of the request) and choose the node according to that and then walks the ring?</p>

<p>Also, how does the network topology case work? if I have multiple data centers, how does Cassandra know which nodes in each DC/Rack contain the data? From what I understand, only the first node is obvious (since the hash of the primary key has resulted in that node explicitly).</p>

<p>Sorry if the question is not very clear and please add a comment if you need more details about my question.</p>

<p>Many thanks,</p>
",<cassandra><cassandra-2.0>,"<blockquote>
  <p>Client sends the data to a random node</p>
</blockquote>

<p>It might seem that way, but there is actually a non-random way that your driver picks a node to talk to.  This node is called a ""coordinator node"" and is typically chosen based-on having the least (closest) ""network distance.""  Client requests can really be sent to any node, and at first they will be sent to the nodes which your driver knows about.  But once it connects and understands the topology of your cluster, it may change to a ""closer"" coordinator.</p>

<p>The nodes in your cluster exchange topology information with each other using the <a href=""http://docs.datastax.com/en/cassandra/2.1/cassandra/architecture/architectureGossipAbout_c.html"">Gossip Protocol</a>.  The gossiper runs every second, and ensures that all nodes are kept current with data from whichever <a href=""http://docs.datastax.com/en/cassandra/2.1/cassandra/architecture/architectureSnitchesAbout_c.html"">Snitch</a> you have configured.  The snitch keeps track of which data centers and racks each node belongs to.</p>

<p>In this way, the coordinator node also has data about which nodes are responsible for each token range.  You can see this information by running a <code>nodetool ring</code> from the command line.  Although if you are using vnodes, that will be trickier to ascertain, as data on all 256 (default) virtual nodes will quickly flash by on the screen.</p>

<p>So let's say that I have a table that I'm using to keep track of ship crew members by their first name, and let's assume that I want to look-up Malcolm Reynolds.  Running this query:</p>

<pre><code>SELECT token(firstname),firstname, id, lastname 
FROM usersbyfirstname  WHERE firstname='Mal';
</code></pre>

<p>...returns this row:</p>

<pre><code> token(firstname)     | firstname | id | lastname
----------------------+-----------+----+-----------
  4016264465811926804 |       Mal |  2 |  Reynolds
</code></pre>

<p>By running a <code>nodetool ring</code> I can see which node is responsible for this token:</p>

<pre><code>192.168.1.22  rack1       Up     Normal  348.31 KB   3976595151390728557                         
192.168.1.22  rack1       Up     Normal  348.31 KB   4142666302960897745                         
</code></pre>

<p>Or even easier, I can use <code>nodetool getendpoints</code> to see this data:</p>

<pre><code>$ nodetool getendpoints stackoverflow usersbyfirstname Mal
Picked up JAVA_TOOL_OPTIONS: -javaagent:/usr/share/java/jayatanaag.jar 
192.168.1.22
</code></pre>

<p>For more information, check out some of the items linked above, or try running <code>nodetool gossipinfo</code>.</p>
",['table']
31672860,31692346,2015-07-28 09:51:46,Setting number of Spark tasks on a Cassandra table scan,"<p>I have a simple Spark job reading 500m rows from a 5 node Cassandra cluster that always runs 6 tasks, which is causing write issues due to the size of each task. I have tried adjusting the input_split_size, which seems to have no effect. At the moment I am forced to repartition the table scan, which is not ideal as it's expensive.</p>

<p>Having read a few posts I tried to increase the num-executors in my launch script (below), although this had no effect.</p>

<p>If there is no way to set the number of tasks on a Cassandra table scan, that's fine I'll make do.. but I have this constant niggling feeling that I'm missing something here.</p>

<p>Spark workers live on the C* nodes which are 8-core, 64gb servers with 2TB SSDs in each.</p>

<pre><code>...
val conf = new SparkConf(true).set(""spark.cassandra.connection.host"",
cassandraHost).setAppName(""rowMigration"")
  conf.set(""spark.shuffle.memoryFraction"", ""0.4"")
  conf.set(""spark.serializer"", ""org.apache.spark.serializer.KryoSerializer"")
  conf.set(""spark.executor.memory"", ""15G"")
  conf.set(""spark.cassandra.input.split.size_in_mb"", ""32"") //default 64mb
  conf.set(""spark.cassandra.output.batch.size.bytes"", ""1000"") //default
  conf.set(""spark.cassandra.output.concurrent.writes"", ""5"") //default

val sc = new SparkContext(conf)

val rawEvents = sc.cassandraTable(cassandraKeyspace, eventTable)
  .select(""accountid"", ""userid"", ""eventname"", ""eventid"", ""eventproperties"")
  .filter(row=&gt;row.getString(""accountid"").equals(""someAccount""))
  .repartition(100)

val object = rawEvents
  .map(ele =&gt; (ele.getString(""userid""),
    UUID.randomUUID(),
    UUID.randomUUID(),
    ele.getUUID(""eventid""),
    ele.getString(""eventname""),
    ""event type"",
    UUIDs.unixTimestamp(ele.getUUID(""eventid"")),
    ele.getMap[String, String](""eventproperties""),
    Map[String, String](),
    Map[String, String](),
    Map[String, String]()))
  .map(row=&gt;MyObject(row))

Object.saveToCassandra(targetCassandraKeyspace,eventTable)
</code></pre>

<p>launch script:</p>

<pre><code>#!/bin/bash
export SHADED_JAR=""Migrate.jar""
export SPARKHOME=""${SPARKHOME:-/opt/spark}""
export SPARK_CLASSPATH=""$SHADED_JAR:$SPARK_CLASSPATH""
export CLASS=com.migration.migrate
""${SPARKHOME}/bin/spark-submit"" \
        --class ""${CLASS}"" \
        --jars $SHADED_JAR,$SHADED_JAR \
        --master spark://cas-1-5:7077  \
        --num-executors 15 \
        --executor-memory 20g \
        --executor-cores 4 ""$SHADED_JAR"" \
        --worker-cores 20 \
        -Dcassandra.connection.host=10.1.20.201 \
        -Dzookeeper.host=10.1.20.211:2181 \
</code></pre>

<p><strong>EDIT - Following Piotr's answer:</strong></p>

<p>I have set the ReadConf.splitCount on sc.cassandraTable as follows, however this does not change the number of tasks generated, meaning I still need to repartition the table scan. I'm starting to think that I'm thinking about this wrong and that the repartition is a necessity. Currently the job is taking about 1.5 hours, and repartitioning the table scan into 1000 tasks of roughly 10MB each has reduced the write time to minutes.</p>

<pre><code>val cassReadConfig = new ReadConf {
      ReadConf.apply(splitCount = Option(1000)
        )
    }

    val sc = new SparkContext(conf)

    val rawEvents = sc.cassandraTable(cassandraKeyspace, eventTable)
    .withReadConf(readConf = cassReadConfig)
</code></pre>
",<scala><cassandra><apache-spark><spark-cassandra-connector>,"<p>Since spark connector 1.3, split sizes are estimated based on the system.size_estimates Cassandra table available since Cassandra 2.1.5. This table is refreshed periodically by Cassandra and soon after loading/removing new data or joining new nodes, its contents may be incorrect. Check if the estimates there reflect your data amount. It is a relatively new feature, so it is also quite possible there are some bugs there.</p>

<p>If the estimates are wrong, or you're running older Cassandra, we left an ability to override the automatic split size tuning. sc.cassandraTable takes ReadConf parameter in which you can set splitCount, which would force a fixed number of splits.</p>

<p>As for split_size_in_mb parameter, indeed there was a bug for some time in the project source, but it has been fixed before being released to any version published to maven. So unless you're compiling the connector from (old) source, you shouldn't hit it. </p>
",['table']
31675268,31676725,2015-07-28 11:41:02,"InvalidRequest: code=2200 [Invalid query] message=""Invalid operator >= for PRIMARY KEY part ""","<p>I have a following dataset in cassandra : </p>

<p><strong>Table Structure</strong></p>

<pre><code>CREATE TABLE userlog (
 term text,
 ts timestamp,
 year int,
 month int,
 day int,
 hour int,
 weekofyear int,
 dayofyear int,
 count counter,
 PRIMARY KEY (term, ts, year,month,day,hour,weekofyear,dayofyear)
);
</code></pre>

<p>.</p>

<pre><code>term             | ts                       | year | month | day | hour | weekofyear | dayofyear | count
------------------+--------------------------+------+-------+-----+------+------------+-----------+-------
www.datastax.com | 2028-07-13 17:06:28+0530 | 2015 |     7 |  28 |   16 |         31 |       209 |     2
www.datastax.com | 2015-07-28 16:17:36+0530 | 2015 |     7 |  28 |   16 |         31 |       209 |     6
www.datastax.com | 2015-07-28 16:17:36+0530 | 2015 |     7 |  28 |   16 |         31 |       209 |     2
www.datastax.com | 2015-07-28 16:17:36+0530 | 2015 |     7 |  28 |   16 |         31 |       209 |     2
www.datastax.com | 2015-07-28 16:21:15+0530 | 2015 |     7 |  28 |   16 |         31 |       209 |     2
www.datastax.com | 2015-07-28 16:21:33+0530 | 2015 |     7 |  28 |   16 |         31 |       209 |     2
www.datastax.com | 2015-07-28 16:21:50+0530 | 2015 |     7 |  28 |   16 |         31 |       209 |     2
www.datastax.com | 2015-07-28 16:21:52+0530 | 2015 |     7 |  28 |   16 |         31 |       209 |     2
www.datastax.com | 2015-07-28 16:21:53+0530 | 2015 |     7 |  28 |   16 |         31 |       209 |     2
             www | 2015-07-28 16:46:00+0530 | 2015 |     7 |  28 |   16 |         31 |       209 |     2
            www. | 2015-07-28 16:47:00+0530 | 2015 |     7 |  28 |   16 |         31 |       209 |     2
            www. | 2015-07-28 16:48:00+0530 | 2015 |     7 |  28 |   16 |         31 |       209 |     2
            www. | 2015-07-28 16:50:00+0530 | 2015 |     7 |  28 |   16 |         31 |       209 |     2
            www. | 2015-07-28 16:55:00+0530 | 2015 |     7 |  28 |   16 |         31 |       209 |     2
</code></pre>

<p>When I run this query :</p>

<pre><code>SELECT * FROM userlog  WHERE ts &gt;= '2015-07-28 16:46' AND  ts &lt;= '2015-07-28 16:55' ALLOW FILTERING;
</code></pre>

<p>I get correct result :</p>

<pre><code>term | ts                       | year | month | day | hour | weekofyear | dayofyear | count
------+--------------------------+------+-------+-----+------+------------+-----------+-------
www  | 2015-07-28 16:46:00+0530 | 2015 |     7 |  28 |   16 |         31 |       209 |     2
www. | 2015-07-28 16:47:00+0530 | 2015 |     7 |  28 |   16 |         31 |       209 |     2
www. | 2015-07-28 16:48:00+0530 | 2015 |     7 |  28 |   16 |         31 |       209 |     2
www. | 2015-07-28 16:50:00+0530 | 2015 |     7 |  28 |   16 |         31 |       209 |     2
www. | 2015-07-28 16:55:00+0530 | 2015 |     7 |  28 |   16 |         31 |       209 |     2
</code></pre>

<p>but when I try to delete the rows with the same conditions </p>

<pre><code>DELETE FROM userlog WHERE ts &gt;= '2015-07-28 16:46' AND  ts &lt;= '2015-07-28 16:55';
</code></pre>

<p>It throws following error :</p>

<pre><code>InvalidRequest: code=2200 [Invalid query] message=""Invalid operator &gt;= for PRIMARY KEY part ts""
</code></pre>

<p>Am i missing something ? How to delete data in the specified time-range ? Also, is there any way to get the data in the specified time range (other than what I am doing?)</p>
",<cassandra><cql><cqlsh>,"<p>The DELETE command does not support range queries or the ALLOW FILTERING clause.  From the documentation it only supports the = and IN operators:</p>

<p>You can delete an individual row by fully specifying both the partition and clustering columns.</p>

<p>You can delete a whole partition by just specifying the partition key.</p>

<p>And you can use the IN operator to do a few of these at once.</p>

<p>If you want to selectively delete rows from within a partition, you could first query for them using SELECT and then in your application issue a delete for each row returned from the SELECT.</p>

<p>Usually you wouldn't want to use ALLOW FILTERING on your SELECT statements since that is very inefficient, so normally you would specify the partition key when doing a range query SELECT.  You would structure your schema so that the information you need for an operation is in known partitions so that you wouldn't need to do a full table scan to find things.</p>
",['table']
31705984,31734167,2015-07-29 16:20:01,What is the limitation with one-node Cassandra cluster?,"<p>I am experimenting with Cassandra and Opscenter.  In the Opscenterd's log file, I found this line</p>

<blockquote>
  <p>2015-07-29 16:10:16+0000 [] ERROR: Problem while calling CreateClusterConfController (SingleNodeProvisioningError): Due to a limitation with one-node clusters, OpsCenter will not be able to communicate with the Datastax Agent unless list
  en_address/broadcast_address in cassandra.yaml are set to 172.17.42.1. Please ensure these match before continuing.</p>
</blockquote>

<p>Because I deployed Cassandra and Opscenter in different Docker containers, I must set <code>listen_address</code> to the container's internal IP (because Cassandra sitting in a container knows nothing about its host) and <code>broadcast_address</code> to the corresponding host's bridge IP.  This is the normal setup if you deploy Cassandra on machines behind separate gateways (like AWS EC2 where each instance has a private and a public IP).</p>

<p><strong>Question 1</strong>: What exactly is the limitation with one-node cluster?</p>

<p><strong>Question 2</strong>: How should I workaround the problem in this case?</p>

<p>Thanks</p>
",<java><docker><cassandra><opscenter><nosql>,"<blockquote>
  <p>Question 1: What exactly is the limitation with one-node cluster?</p>
</blockquote>

<p>OpsCenter (via underlying python driver) is reading cluster information from Cassandra’s <code>system</code> tables (namely, <code>system.peers</code> and <code>system.local</code>), with most of the information coming from <code>system.peers</code>, including broadcast interfaces for each of the nodes.</p>

<p>However, that table does not contain information about the node itself, only about its peers. When there are no peers, there is no way to get broadcast address from Cassandra itself, and that’s what OpsCenter uses to tie actual Cassandra instances to the internal representation. In this case OpsCenter uses whatever address you specified as a seed (<code>172.17.42.1</code> here), and when agents report with a different IP (they’re getting Cassandra’s broadcast address via JMX), OpsCenter would discard those messages.</p>

<blockquote>
  <p>Question 2: How should I workaround the problem in this case?</p>
</blockquote>

<p>Try setting <code>local_address</code> in <code>address.yaml</code> to <code>172.17.42.1</code>, this should do the trick.</p>
",['table']
31723110,31723919,2015-07-30 11:54:26,keyspace and tables not replicating across data centre in cassandra,"<p>I am trying to create cassandra cluster. For that I have a single node data centres
One data center is named DC1 and the other is DC2. Hence there are 2 single node data center. I followed the steps given here
<a href=""http://docs.datastax.com/en/cassandra/2.0/cassandra/initialize/initializeMultipleDS.html"" rel=""nofollow"">http://docs.datastax.com/en/cassandra/2.0/cassandra/initialize/initializeMultipleDS.html</a></p>

<p>Since I have single node data center, so my seed will be a single machine. I can do ndoetool -h  status to both the machines. I created a keyspace like this</p>

<pre><code>CREATE KEYSPACE sams WITH replication = {'class': 'NetworkTopologyStrategy', 'DC1': '1', 'DC2': '1'}  AND durable_writes = true;
</code></pre>

<p>and a table like </p>

<pre><code>CREATE TABLE apikey (   appname text,   appkey text,   PRIMARY KEY ((appname), appkey) );
</code></pre>

<p>I create keyspace and table in one data centre. This should get replicated on the other machine but it does not replicate. There are no keyspaces shown on the other data center neither any tables.</p>

<p>What am I missing here?</p>

<p>Adding nodetool output</p>

<pre><code>nodetool -h cassandra1 status
Datacenter: DC1
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address          Load       Tokens  Owns (effective)  Host ID                               Rack
UN  100.117.100.107  178.28 KB  256     100.0%            0c5da294-2a86-472d-98ec-857ed5140417  RAC1


 nodetool -h cassandra2 status
Datacenter: DC2
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address         Load       Tokens  Owns    Host ID                               Rack
UN  100.117.150.55  162.94 KB  256     ?       9f3e49f6-debc-4a9c-ba93-bc65b3851a48  RAC1
</code></pre>
",<cassandra>,"<p>It looks like you have accidentally set up two independent clusters rather than a single cluster that spans two data centers.</p>

<p>Make sure in cassandra.yaml that both nodes have the same cluster_name and that both have the same list of ip's for ""- seeds:"".  Since you only have two nodes, I'd list both ip's as the seeds rather than just one.</p>

<p>If the nodes have joined the same cluster, then it should show both of them when you run nodetool status.</p>
",['cluster_name']
31733395,31734440,2015-07-30 20:26:00,How to prevent Cassandra commit logs filling up disk space,"<p>I'm running a two node Datastax AMI cluster on AWS. Yesterday, Cassandra started refusing connections from everything. The system logs showed nothing. After a <em>lot</em> of tinkering, I discovered that the commit logs had filled up all the disk space on the allotted mount and this seemed to be causing the connection refusal (deleted some of the commit logs, restarted and was able to connect).</p>

<p>I'm on DataStax AMI 2.5.1 and Cassandra 2.1.7</p>

<p>If I decide to wipe and restart everything from scratch, how do I ensure that this does not happen again?</p>
",<cassandra><datastax><datastax-java-driver><cassandra-2.1>,"<p>You could try lowering the <code>commitlog_total_space_in_mb</code> setting in your <code>cassandra.yaml</code>.  The default is 8192MB for 64-bit systems (it should be commented-out in your <code>.yaml</code> file... you'll have to un-comment it when setting it).  It's usually a good idea to plan for that when sizing your disk(s).</p>

<p>You can verify this by running a <code>du</code> on your commitlog directory:</p>

<pre><code>$ du -d 1 -h ./commitlog
8.1G    ./commitlog
</code></pre>

<p>Although, a smaller commit log space will cause more frequent flushes (increased disk I/O), so you'll want to keep any eye on that.</p>

<p><strong>Edit 20190318</strong></p>

<p>Just had a related thought (on my 4-year-old answer).  I saw that it received some attention recently, and wanted to make sure that the right information is out there.</p>

<p>It's important to note that sometimes the commit log can grow in an ""out of control"" fashion.  Essentially, this can happen because the write load on the node exceeds Cassandra's ability to keep up with flushing the memtables (and thus, removing old commitlog files).  If you find a node with dozens of commitlog files, and the number seems to keep growing, this might be your issue.</p>

<p>Essentially, your <code>memtable_cleanup_threshold</code> may be too low.  Although this property is deprecated, you can still control how it is calculated by lowering the number of <code>memtable_flush_writers</code>.</p>

<pre><code>memtable_cleanup_threshold = 1 / (memtable_flush_writers + 1)
</code></pre>

<p>The documentation has been updated as of 3.x, but used to say this:</p>

<pre><code># memtable_flush_writers defaults to the smaller of (number of disks,
# number of cores), with a minimum of 2 and a maximum of 8.
# 
# If your data directories are backed by SSD, you should increase this
# to the number of cores.
#memtable_flush_writers: 8
</code></pre>

<p>...which (I feel) led to many folks setting this value <em>WAY</em> too high.</p>

<p>Assuming a value of 8, the <code>memtable_cleanup_threshold</code> is <code>.111</code>.  When the footprint of all memtables exceeds this ratio of total memory available, flushing occurs.  Too many flush (blocking) writers can prevent this from happening expediently.  With a single <code>/data</code> dir, I recommend setting this value to <strong>2</strong>.</p>
",['memtable_flush_writers']
31743833,31749245,2015-07-31 10:21:51,Cassandra Data Modeling Design,"<p>I am fairly new to Cassandra and have been reading a lot over the last month.
<br>I am working on a small use case.
<br>Query: Top X Players based on AmountPlayed in a TimeRange.</p>

<p>So at any given time range i would want to aggreagate players TotalAmountPlayed and derive Top X Players.</p>

<p>I followed the approach of creating UDF (using C*-2.2.0 version) for aggregation of AmountPlayed by a Player.</p>

<p>Below is my Time Series data model i designed for this use case.</p>

<pre><code>CREATE COLUMNFAMILY PlayerRating
(
PlayerNumber int, ===&gt; Unique account number
GameID text, ===&gt; unique machine ID per slot
AmountPlayed double, ===&gt; AmountPlayed by the player
EventTime timestamp, ===&gt; Event generated TimeStamp
PRIMARY KEY ((PlayerNumber, GameID),EventTime)) WITH CLUSTERING ORDER BY(EventTime desc);
</code></pre>

<p>Please let me know if my Data Model design is proper for My Query.</p>

<p>Thank You !!</p>
",<cassandra>,"<p>I think it might be easier to put all the players for each game into a single partition.</p>

<p>That way you could aggregate all the players with one query instead of making a separate query for each player.  Then you could aggregate the playing time for each player into a map (see an example of how to define the UDF's for that <a href=""http://christopher-batey.blogspot.com/2015/05/cassandra-aggregates-min-max-avg-group.html"" rel=""nofollow"">here</a>).</p>

<p>So your table would look something like this:</p>

<pre><code>CREATE TABLE playing_time_by_game (game_id text, event_time int, player_id text, amount_played int, PRIMARY KEY (game_id, event_time));
</code></pre>

<p>Then create the UDF to total by player_id:</p>

<pre><code>CREATE FUNCTION state_group_and_total( state map&lt;text, int&gt;, type text, amount int )
     CALLED ON NULL INPUT
     RETURNS map&lt;text, int&gt;
     LANGUAGE java AS '
     Integer count = (Integer) state.get(type);  if (count == null) count = amount; else count = count + amount; state.put(type, count); return state; ' ;
</code></pre>

<p>Then create the aggregate function:</p>

<pre><code>CREATE OR REPLACE AGGREGATE group_and_total(text, int) 
     SFUNC state_group_and_total 
     STYPE map&lt;text, int&gt; 
     INITCOND {};
</code></pre>

<p>Then insert some data:</p>

<pre><code>SELECT * from playing_time_by_game ;

 game_id | event_time | amount_played | player_id
---------+------------+---------------+-----------
   game1 |          0 |             8 |   player1
   game1 |          1 |            12 |   player2
   game1 |          5 |             1 |   player2
   game1 |          8 |            50 |   player1
   game2 |          0 |           200 |   player1
</code></pre>

<p>Now you can aggregate by player_id:</p>

<pre><code>SELECT group_and_total(player_id, amount_played) from playing_time_by_game;

 t2.group_and_total(player_id, amount_played)
----------------------------------------------
              {'player1': 258, 'player2': 13}
</code></pre>

<p>And you can restrict the query to the game partition and time range:</p>

<pre><code>SELECT group_and_total(player_id, amount_played) from playing_time_by_game where game_id='game1' and event_time &gt;=0 and event_time &lt;=7;

 t2.group_and_total(player_id, amount_played)
----------------------------------------------
                {'player1': 8, 'player2': 13}
</code></pre>

<p>You could probably also define a FINALFUNC to sort and keep just the top ten items in the map.  See <a href=""http://docs.datastax.com/en/cql/3.3/cql/cql_using/useCreateUDA.html"" rel=""nofollow"">this</a>.</p>
",['table']
31773477,31793250,2015-08-02 15:34:02,Spark joinWithCassandraTable() on map multiple partition key ERROR,"<p>I'm trying to filter on a small part of a huge Cassandra table by using: </p>

<pre><code>val snapshotsFiltered = sc.parallelize(startDate to endDate).map(TableKey(_2)).joinWithCassandraTable(""listener"",""snapshots_test_b"")
</code></pre>

<p>I want to map the rows in the cassandra table on 'created' column that is part of the partition key.</p>

<p>My table key (the partition key of the table) defined as:</p>

<pre><code>case class TableKey(imei: String, created: Long, when: Long)
</code></pre>

<p>The result is an error:</p>

<blockquote>
  <p>[error] /home/ubuntu/scala/test/test.scala:61: not enough arguments for method apply: (imei: String, created: Long)test.TableKey in object TableKey.
  [error] Unspecified value parameter created.
  [error]         val snapshotsFiltered = sc.parallelize(startDate to endDate).map(TableKey(_2)).joinWithCassandraTable(""listener"",""snapshots_test_b"")
  [error]                                                                                  ^
  [error] one error found
  [error] (compile:compile) Compilation failed</p>
</blockquote>

<p>It worked with only one object in the partition key as in the <a href=""https://github.com/datastax/spark-cassandra-connector/blob/master/doc/2_loading.md"" rel=""nofollow"">Documentation</a>.</p>

<p>Why there is a problem with multiple partition key?- answered.</p>

<p>EDIT: I tried to use the joinWithCassandraTable in the right form:</p>

<pre><code>val snapshotsFiltered = sc.parallelize(startDate to endDate).map(TableKey(""*"",_,startDate)).joinWithCassandraTable(""listener"",""snapshots_test_c"")
</code></pre>

<p>When I trying to run it on Spark there is no errors but it stuck on ""[stage 0:> (0+2)/2]"" forever...</p>

<p>What goes wrong?</p>
",<scala><cassandra><apache-spark><datastax-enterprise>,"<p>The error is telling you that the class <code>TableKey</code> requires 3 components to initialize yet there was only a single argument passed. This is a Scala compilation error and isn't related to C* or Spark.</p>

<pre><code> val snapshotsFiltered = sc.parallelize(startDate to endDate)
   .map(TableKey(_2))  /// Table Key does not have a single element constructor so this will fail
   .joinWithCassandraTable(""listener"",""snapshots_test_b"")
</code></pre>

<hr>

<p>In general though, C* uses the entire <code>partition key</code> do determine where a particular row lives. Because of this you can only efficiently pull out data if you know the entire <code>partition key</code> so passing only a portion of it has no value.</p>

<p>The joinWithCassandraTable requires full <code>partition key</code> values so it can effeciently do it's work. if you only have a portion of the <code>parition key</code> you will be required to perform a full table scan and use Spark to filter. </p>

<p>If you only want to filter based on a <code>clustering column</code> you can do so by pushing down a <code>where</code> clause to C* such as </p>

<pre><code>sc.cassandraTable(""ks"",""test"").where(""clustering_key &gt; someValue"")
</code></pre>
",['table']
31785110,31793736,2015-08-03 10:32:09,How to understand bloom_filter_fp_chance and read_repair_chance in Cassandra,"<p><strong>Bloom Filters</strong> </p>

<pre><code>When data is requested, the Bloom filter checks if the row exists before doing disk I/O. 
</code></pre>

<p><strong>Read Repair</strong></p>

<pre><code>Read Repair perform a digest query on all replicas for that key
</code></pre>

<p>My confusion is how to set this value between 0 to 1,. What happens when the value varies?</p>

<p>Thanks in advance,.</p>
",<cassandra>,"<p>The bloom_filter_fp_chance and read_repair_chance control two different things.  Usually you would leave them set to their default values, which should work well for most typical use cases.</p>

<p>bloom_filter_fp_chance controls the precision of the bloom filter data for SSTables stored on disk.  The bloom filter is kept in memory and when you do a read, Cassandra will check the bloom filters to see which SSTables <em>might</em> have data for the key you are reading.  A bloom filter will often give false positives and when you actually read the SSTable, it turns out that the key does not exist in the SSTable and reading it was a waste of time.  The better the precision used for the bloom filter, the fewer false positives it will give (but the more memory it will need).</p>

<p>From the documentation:</p>

<pre><code>0 Enables the unmodified, effectively the largest possible, Bloom filter
1.0 Disables the Bloom Filter
The recommended setting is 0.1. A higher value yields diminishing returns.
</code></pre>

<p>So a higher number gives a higher chance of a false positive (fp) when reading the bloom filter.</p>

<p>read_repair_chance controls the probability that a read of a key will be checked against the other replicas for that key.  This is useful if your system has frequent downtime of the nodes resulting in data getting out of sync.  If you do a lot of reads, then the read repair will slowly bring the data back into sync as you do reads without having to run a full repair on the nodes.  Higher settings will cause more background read repairs and consume more resources, but would sync the data more quickly as you do reads.</p>

<p>See documentation on these settings <a href=""http://docs.datastax.com/en/cql/3.1/cql/cql_reference/tabProp.html"">here</a>.</p>
",['precision']
31790072,31791790,2015-08-03 14:34:36,Failover and Replication in 2-node Cassandra cluster,"<p>I run KairosDB on a 2-node Cassandra cluster, RF = 2, Write CL = 1, Read CL = 1. If 2 nodes are alive, client sends half of data to node 1 (e.g. metric from METRIC_1 to METRIC_5000) and the other half of data to node 2 (e.g. metric from METRIC_5001 to METRIC_10000). Ideally, each node always has a copy of all data. But if one node is dead, client sends all data to the alive node.</p>

<p>Client started sending data to the cluster. After 30 minutes, I turned node 2 off for 10 minutes. During this 10-minute period, client sent all data to node 1 properly. After that, I restarted node 2 and client continued sending data to 2 nodes properly. One hour later I stopped the client.</p>

<p>I wanted to check if the data which was sent to node 1 when node 2 was dead had been automatically replicated to node 2 or not. To do this, I turned node 1 off and queried the data within time when node 2 was dead from node 2 but it returned nothing. This made me think that the data had not been replicated from node 1 to node 2. I posted a question <a href=""https://stackoverflow.com/questions/31737836/doesnt-cassandra-perform-late-replication-when-a-node-down-and-up-again"">Doesn't Cassandra perform “late” replication when a node down and up again?</a>. It seems that the data was replicated automatically but it was so slow.</p>

<p>What I expect is data in both 2 servers are the same (for redundancy purpose). That means the data sent to the system when node 2 is dead must be replicated from node 1 to node 2 automatically after node 2 becomes available (because RF = 2).</p>

<p>I have several questions here:</p>

<p>1) Is the replication truly slow? Or did I configure something wrong?</p>

<p>2) If client sends half of data to each node as in this question I think it's possible to lose data (e.g. node 1 receives data from client, while node 1 is replicating data to node 2 it suddenly goes down). Am I right?</p>

<p>3) If I am right in 2), I am going to do like this: client sends all data to both 2 nodes. This can solve 2) and also takes advantages of replication if one node is dead and is available later. But I am wondering that, this would cause duplication of data because both 2 nodes receive the same data. Is there any problem here?</p>

<p>Thank you!</p>
",<cassandra><cassandra-2.0><kairosdb>,"<p>Can you check the value of hinted_handoff_enabled in cassandra.yaml config file?</p>

<p>For your question: Yes you may lose data in some cases, until the replication is fully achieved, Cassandra is not exactly doing late replication - there are three mechanisms.</p>

<ul>
<li>Hinted handoffs  <a href=""http://docs.datastax.com/en/cassandra/2.2/cassandra/operations/opsRepairNodesHintedHandoff.html"" rel=""nofollow"">http://docs.datastax.com/en/cassandra/2.2/cassandra/operations/opsRepairNodesHintedHandoff.html</a></li>
<li>Repairs - <a href=""http://docs.datastax.com/en/cassandra/2.0/cassandra/tools/toolsRepair.html"" rel=""nofollow"">http://docs.datastax.com/en/cassandra/2.0/cassandra/tools/toolsRepair.html</a></li>
<li>Read Repairs - those may not help much on your use case - <a href=""http://wiki.apache.org/cassandra/ReadRepair"" rel=""nofollow"">http://wiki.apache.org/cassandra/ReadRepair</a></li>
</ul>

<p>AFAIK, if you are running a version greater than 0.8, the hinted handoffs should duplicate the data after node restarts without the need for a repair, unless data is too old (this should not be the case for 10 minutes). I don't know why those handoffs where not sent to your replica node when it was restarted, it deserves some investigation.</p>

<p>Otherwise, when you restart the node, you can force Cassandra to make sure that data is consistent by running a repair (e.g. by running nodetool repair).</p>

<p>By your description I have the feeling you are getting confused between the coordinator node and the node that is getting the data (even if the two nodes hold the data, the distinction is important).</p>

<p>BTW, what is the client behaviour with metrics sharding between node 1 and node 2 you are describing? Neither KairosDB nor Cassandra work like that, is it your own client that is sending metrics to different KairosDB instances? </p>

<p>The Cassandra partition is not made on metric name but on row key (partition key exactly, but it's the same with kairosDB). So every 3-weeks data for each unique series will be associated a token based on hash code, this token will be use for sharding/replication on the cluster.
KairosDB is able to communicate with several nodes and would round robin between those as coordinator nodes.</p>

<p>I hope this helps.</p>
",['hinted_handoff_enabled']
31804797,31837678,2015-08-04 08:54:03,Retrieve all the columnfamily / tables of thrift and CQL from the keyspace in cassandra,"<p>I use hector to manipulate cassandra of version 2.1.8 and want to retrieve all the tables from certain keyspace in an application. I use ""KeyspaceDefinition.getCfDefs()"" to retrieve the columnfamily list in a keyspace.</p>

<p>However, I found that the getCfDefs() function can just retrieve the columnfamily created by the thrift api such as ""me.prettyprint.hector.api.Cluster.updateColumnFamily"" but not table created by CQL such as cqlsh client.</p>

<p>Then, how to retrieve all the tables from certain keyspace by using hector?</p>
",<cassandra><hector><nosql>,"<p>You can inspect all table metadata for a keyspace programmatically using the system metadata tables:</p>

<pre><code>select * from system.schema_columnfamilies where keyspace_name='ks';
select * from system.schema_columns where keyspace_name='ks';
</code></pre>

<p>You would need to aggregate these columns and tables yourself.</p>

<p>Alternatively, if you're not tied to this client, you can use cqlsh to ""describe"" the keyspace in question, or use one of the DataStax drivers with a metadata API (<a href=""http://docs.datastax.com/en/drivers/java/2.2/com/datastax/driver/core/Metadata.html"" rel=""nofollow"">java</a>, <a href=""http://datastax.github.io/python-driver/api/cassandra/metadata.html#schemas"" rel=""nofollow"">python</a>).</p>
",['table']
31815665,31817416,2015-08-04 17:29:48,Cassandra 2.1.8 remote access,"<p>I'm configuring Cassandra to accept remote accesses (cqlsh ).
Here's the following that I changed in the cassandra.yaml:</p>

<ul>
<li><p>start_native_transport: true</p></li>
<li><p>start_rpc: true</p></li>
<li><p>rpc_address: my-server-ip</p></li>
</ul>

<p>But when I'm launching Cassandra I get the following error: </p>

<blockquote>
  <p>""Failed to bind port 9042 on my-server-ip""</p>
</blockquote>

<p>If I set start_native_transport: false, I get no error, but I can't get a remote access Cassandra.</p>

<p>Does anyone knows the problem?</p>

<p>Thanks</p>
",<cassandra>,"<p>Check your listen_address in cassandra.yaml. It defaults to localhost, which will not allow for external access. Change it to the private IP, and you will be able to talk to it from the outside. The rpc_address is for Thrift requests.</p>
","['rpc_address', 'listen_address']"
31828952,31837422,2015-08-05 09:47:51,How to get top 5 records in cassandra 2.2,"<p>I need a help. I have a query which get top 5 records group by date (not date + time) and sum of amount.</p>

<p>I wrote the following but it returns all the records not just top 5 records</p>

<pre><code>CREATE OR REPLACE FUNCTION state_groupbyandsum( state map&lt;text, double&gt;, datetime text, amount text )
CALLED ON NULL INPUT
RETURNS map&lt;text, double&gt;
LANGUAGE java 
AS 'String date = datetime.substring(0,10); Double count = (Double) state.get(date);  if (count == null) count = Double.parseDouble(amount); else count = count +  Double.parseDouble(amount); state.put(date, count); return state;' ;


CREATE OR REPLACE AGGREGATE groupbyandsum(text, text) 
SFUNC state_groupbyandsum
STYPE map&lt;text, double&gt;
INITCOND {};

select groupbyandsum(datetime, amout) from warehouse;
</code></pre>

<p>Could you please help out to get just 5 records.</p>
",<java><cassandra><user-defined-functions><cql3>,"<p>Here's one way to do that.  Your group by state function could be like this:</p>

<pre><code>CREATE FUNCTION state_group_and_total( state map&lt;text, double&gt;, type text, amount double )
CALLED ON NULL INPUT
RETURNS map&lt;text, double&gt;
LANGUAGE java AS '
     Double count = (Double) state.get(type);
     if (count == null)
         count = amount;
     else
         count = count + amount;
     state.put(type, count);
     return state;
';
</code></pre>

<p>That will build up a map of all the amount rows selected by your query WHERE clause.  Now the tricky part is how to keep just the top N.  One way to do it is by using a FINALFUNC which gets executed after all the rows have been put in the map.  So here's a function to do that using a loop to  find the maximum value in the map and move it to a result map.  So to find the top N it would iterate over the map N times (there are more efficient algorithms than this, but it's just a quick and dirty example).</p>

<p>So here's an example to find the top two:</p>

<pre><code>CREATE FUNCTION topFinal (state map&lt;text, double&gt;)
CALLED ON NULL INPUT
RETURNS map&lt;text, double&gt;
LANGUAGE java AS '
    java.util.Map&lt;String, Double&gt; inMap = new java.util.HashMap&lt;String, Double&gt;(),
                                  outMap = new java.util.HashMap&lt;String, Double&gt;();

    inMap.putAll(state);

    int topN = 2;
    for (int i = 1; i &lt;= topN; i++) {
        double maxVal = -1;
        String moveKey = null;
        for (java.util.Map.Entry&lt;String, Double&gt; entry : inMap.entrySet()) {

            if (entry.getValue() &gt; maxVal) {
                maxVal = entry.getValue();
                moveKey = entry.getKey();
            }
        }
        if (moveKey != null) {
            outMap.put(moveKey, maxVal);
            inMap.remove(moveKey);
        }
    }

    return outMap;
';
</code></pre>

<p>Then lastly you need to define the AGGREGATE to call the two functions you defined:</p>

<pre><code>CREATE OR REPLACE AGGREGATE group_and_total(text, double) 
     SFUNC state_group_and_total 
     STYPE map&lt;text, double&gt; 
     FINALFUNC topFinal
     INITCOND {};
</code></pre>

<p>So let's see if that works.</p>

<pre><code>CREATE table test (partition int, clustering text, amount double, PRIMARY KEY (partition, clustering));
INSERT INTO test (partition , clustering, amount) VALUES ( 1, '2015', 99.1);
INSERT INTO test (partition , clustering, amount) VALUES ( 1, '2016', 18.12);
INSERT INTO test (partition , clustering, amount) VALUES ( 1, '2017', 44.889);
SELECT * from test;

 partition | clustering | amount
-----------+------------+--------
         1 |       2015 |   99.1
         1 |       2016 |  18.12
         1 |       2017 | 44.889
</code></pre>

<p>Now, drum roll...</p>

<pre><code>SELECT group_and_total(clustering, amount) from test where partition=1;

 agg.group_and_total(clustering, amount)
-------------------------------------------
            {'2015': 99.1, '2017': 44.889}
</code></pre>

<p>So you see it kept the top 2 rows based on the amount.</p>

<p>Note that the keys won't be in sorted order since it's a map, and I don't think we can control the key order in the map, so sorting in the FINALFUNC would be a waste of resources.  If you need the map sorted then you could do that in the client.</p>

<p>I think you could do more work in the state_group_and_total function to drop items from the map as you go along.  That might be better to keep the map from getting too big.</p>
",['table']
31927336,31927714,2015-08-10 19:14:03,How to do a sync drop and create keyspace in cassandra?,"<p>I don't want any data in any of the tables in keyspace. So I decided to drop the keyspace if exist and create it immediately. I am using the below code to achieve the same.</p>

<pre><code> CassandraConnector(conf).withSessionDo { session =&gt;
  session.execute(s""DROP KEYSPACE if EXISTS $keyspace"")
  session.execute(""""""CREATE KEYSPACE if NOT EXISTS %s
  WITH replication = {'class':'SimpleStrategy','replication_factor':'1'};"""""".format(keyspace)
</code></pre>

<p>)
  }</p>

<p>But it is failing to create a keyspace. From the logs I could only see a warning stating that</p>

<pre><code>Received a DROPPED notification for table test.table_tracker, but this keyspace is unknown in our metadata.
</code></pre>

<p>I have also tried using python cassandra driver. But the results are same. 
I believe there is some race condition and the drop keyspace happens async(correct me if I am wrong).</p>

<p>How do I synchronously drop and create a keyspace ?</p>
",<cassandra><spark-cassandra-connector>,"<p>If you simply want to drop all the data from a table(s), you should consider using <a href=""http://docs.datastax.com/en/cql/3.1/cql/cql_reference/truncate_r.html"">TRUNCATE</a> which drops all data from a table while retaining its schema, i.e.:</p>

<pre><code>TRUNCATE test.table_tracker;
</code></pre>

<p>Performing schema changes should be done sparingly, so you should avoid it when possible.  In addition there are a number of different issues in all versions of cassandra around dropping and recreating a keyspace quickly, so the truncate route is much more reliable.</p>

<p>One problem you may be running into is that maybe you are not waiting on schema agreement after executing each operation which can be problematic especially in the case when manipulating the same keyspace/table in subsequent operations.  The java-driver (and thus the spark connector) will only wait up to a configurable amount of time between schema changes.  See this guidance in the <a href=""http://datastax.github.io/java-driver/2.0.10/features/metadata/"">java-driver metadata doc</a> for guidance on how to wait for agreement, i.e.:</p>



<pre><code>if (cluster.getMetadata().checkSchemaAgreement()) {
    // schema is in agreement
} else {
    // schema is not in agreement
}
</code></pre>
",['table']
31943630,31978705,2015-08-11 13:49:30,Cassandra Primary Key Selection to Bound Partition Growth,"<p>We are currently testing Cassanda as a database for a big amount of meta-data on communication events. As most queries will be limited to a single customer, it makes sense to shard by customer ID. However, this would mean the partitions would keep growing infinitely over time. I'm struggling a bit to come up with a solution that seems clean enough.</p>

<p>The first idea is to use a composite key of customer ID and some time interval. Are there other options, that might be better and grow more organically?</p>

<p>As we want to have as few partition reads as possible, I was thinking to simply use the year to put an upper bound on data per customer per partition. However, this would distribute the data rather unevenly, if I'm not mistaking. Could this be solved by moving to months or even weeks/days?</p>

<p>I'm sure that this is a problem that often comes up and I'm interested in hearing the various solutions that people put into place.</p>

<p>EDIT: To be more clean on the type of query, they will calculate aggregates over big time slices, per customer. Ideally, we would only have this:</p>

<p>PRIMARY KEY ((customer_id), timestamp)</p>

<p>However, as I've mentioned, this would lead to unbound growth per partition over the years.</p>
",<cassandra><primary-key><partitioning><composite-key>,"<p>Well a partition can hold a ton of rows, but if your volume over the years will be a concern, you could borrow an idea from hash tables.  When more than one value hashes to a value, the extra values are stored as an overflow linked list.  </p>

<p>We can extend the same idea to a partition.  When a partition for a high volume customer ""fills up"", we add extra partitions to a list.</p>

<p>So you could define your table like this:</p>

<pre><code>CREATE TABLE events (
    cust_id int,
    bucket int, 
    ts int,
    overflow list&lt;int&gt; static,
    PRIMARY KEY ((cust_id, bucket), ts));
</code></pre>

<p>For most customers, you would just set bucket to zero and use a single partition.  But if the zero partition got too big, then add a 1 to the static list to indicate that you are now also storing data in bucket 1.  You can then add more partitions to the list as needed.</p>

<p>For example:</p>

<pre><code>INSERT INTO events (cust_id, bucket, ts) VALUES (123, 0, 1);
INSERT INTO events (cust_id, bucket, ts) VALUES (123, 0, 2);

SELECT * from events;

 cust_id | bucket | ts | overflow
---------+--------+----+----------
     123 |      0 |  1 |     null
     123 |      0 |  2 |     null
</code></pre>

<p>Now imagine you want to start using a second partition for this customer, just add it to the static list:</p>

<pre><code>UPDATE events SET overflow = overflow + [1] WHERE cust_id=123 and bucket=0;
INSERT INTO events (cust_id, bucket, ts) VALUES (123, 1, 3);
INSERT INTO events (cust_id, bucket, ts) VALUES (123, 1, 4);
</code></pre>

<p>So to check if a customer is using any overflow bucket partitions:</p>

<pre><code>SELECT overflow FROM events WHERE cust_id=123 and bucket=0 limit 1;

 overflow
----------
      [1]
</code></pre>

<p>Now you can do range queries over the partitions:</p>

<pre><code>SELECT * FROM events WHERE cust_id=123 and bucket IN(0,1) AND ts&gt;1 and ts&lt;4;

 cust_id | bucket | ts | overflow
---------+--------+----+----------
     123 |      0 |  2 |      [1]
     123 |      1 |  3 |     null
</code></pre>

<p>You could define ""bucket"" to have whatever meaning you wanted, like year or something.  Note that the overflow list is defined as static, so it is only stored once with each partition and not with each event row.</p>

<p>Probably the more conventional approach would be to partition by cust_id and year, but then you need to know the start and end years somehow in order to do queries.  With the overflow approach, the first bucket is the master and has a standard known value like 0 for reads.  But a drawback is you need to do a read to know which bucket to write to, but if each customer generates a large group of events during a communication session, then maybe the overhead of that wouldn't be too much. </p>
",['table']
31995903,31996665,2015-08-13 18:33:21,CQL3.2: DROP TABLE with certain prefix?,"<p>I have a Cassandra 2.1.8 database with a bunch of tables, all in the form of either ""prefix1_tablename"" or ""prefix2_tablename"".</p>

<p>I want to DROP every table that begins with prefix1_ and leave anything else alone.</p>

<p>I know I can grab table names using the query:</p>

<pre><code>SELECT columnfamily_name FROM system.schema_columnfamilies
WHERE keyspace_name='mykeyspace'
</code></pre>

<p>And I thought about filtering the results somehow to get only prefix1_ tables, putting them into a table with DROP TABLE prepended to each one, then executing all the statements in my new table. It was similar thinking to strategies I've seen for people solving the same problem with MySQL or Oracle.</p>

<p>With CQL3.2 though, I don't have access to User-Defined Functions (at least according to the docs I've read...) and I don't know how to do something like execute statements off of a table query result, as well as even how to filter out prefix1_ tables with no LIKE operator in Cassandra.</p>

<p>Is there a way to accomplish this?</p>
",<cassandra><cql><prefix>,"<p>I came up with a Bash shell script to solve my own issue. Once I realized that I could export the column families table to a CSV file, it made more sense to me to perform the filtering and text manipulation with grep and awk as opposed to finding a 'pure' cqlsh method.</p>

<p>The script I used:</p>

<pre><code>#!/bin/bash

# No need for a USE command by making delimiter a period
cqlsh -e ""COPY system.schema_columnfamilies (keyspace_name, columnfamily_name) 
TO 'alltables.csv' WITH DELIMITER = '.';""

cat alltables.csv | grep -e '^mykeyspace.prefix1_' \
    | awk '{print ""DROP TABLE "" $0 "";""}' &gt;&gt; remove_prefix1.cql

cqlsh -f 'remove_prefix1.cql'

rm alltables.csv remove_prefix1.cql
</code></pre>
",['table']
32012777,32013024,2015-08-14 14:43:25,ERROR Session: Error creating pool to /127.0.0.1:9042,"<p>I am trying to insert values in cassandra when I come across this error:</p>

<pre><code>15/08/14 10:21:54 INFO Cluster: New Cassandra host /a.b.c.d:9042 added
15/08/14 10:21:54 INFO Cluster: New Cassandra host /127.0.0.1:9042 added
INFO CassandraConnector: Connected to Cassandra cluster: Test Cluster
15/08/14 10:21:54 ERROR Session: Error creating pool to /127.0.0.1:9042
com.datastax.driver.core.TransportException: [/127.0.0.1:9042] Cannot connect
        at com.datastax.driver.core.Connection.&lt;init&gt;(Connection.java:109)
        at com.datastax.driver.core.PooledConnection.&lt;init&gt;(PooledConnection.java:32)
        at com.datastax.driver.core.Connection$Factory.open(Connection.java:586)
        at com.datastax.driver.core.SingleConnectionPool.&lt;init&gt;(SingleConnectionPool.java:76)
        at com.datastax.driver.core.HostConnectionPool.newInstance(HostConnectionPool.java:35)
        at com.datastax.driver.core.SessionManager.replacePool(SessionManager.java:271)
        at com.datastax.driver.core.SessionManager.access$400(SessionManager.java:40)
        at com.datastax.driver.core.SessionManager$3.call(SessionManager.java:308)
        at com.datastax.driver.core.SessionManager$3.call(SessionManager.java:300)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
Caused by: java.net.ConnectException: Connection refused: /127.0.0.1:9042
</code></pre>

<p>My replication factor is 1. There are 5 nodes in the Cass cluster (they're all up). rpc_address: 0.0.0.0, broadcast_rpc_address: 127.0.0.1</p>

<p>I would think that I should see 5 of those ""INFO Cluster: New Cassandra host.."" line from above for each of the 5 nodes. <strong>But instead I see 127.0.0.1, I am not sure why.</strong></p>

<p>I also noticed that in the cassandra.yaml file, all the 5 nodes are listed under seed. (which I know is not advised but I did not set up this  cluster)</p>

<pre><code>seed_provider:
class_name: org.apache.cassandra.locator.SimpleSeedProvider
parameters:
seeds: ""ip1, ip2, ip3, ip4, ip5""
</code></pre>

<p>Where ipx is the ipaddr for node x.</p>

<p>And under cassandra-topology.properties it just says the following and does not mention any of the 5 nodes.</p>

<pre><code># default for unknown nodes                                                                                                                     

default=DC1:r1
</code></pre>

<p>Can someone explain why I am seeing the ERROR Session: Error creating pool to /127.0.0.1:9042 error.</p>

<p>Kind of new to Cassandra.. thanks in advance!</p>
",<configuration><cassandra>,"<p>I think the problem is your rpc_broadcast_address is set to 127.0.0.1.  Is there a reason in particular you are doing this?</p>

<p>The java driver uses the system.peers table to look up the ip address to use to connect to hosts.  If rpc_broadcast_address is set this is what will be present in system.peers and the driver will try to use it.  If rpc_broadcast_address is not set, rpc_address will be used.  In either case, you'll want to set one of these addresses to an address that will be accessible by your client.  If you set rpc_address, you will want to remove broadcast_rpc_address.</p>
","['rpc_address', 'table']"
32037393,32039385,2015-08-16 16:26:07,Cassandra - secondary index and query performance,"<p>my schema for the table is : <br/><strong>A)</strong></p>

<pre><code>CREATE TABLE friend_list (
    userId uuid,
    friendId uuid,
    accepted boolean, 
    ts_accepted timestamp,
    PRIMARY KEY ((userId ,accepted), ts_accepted)
   ) with clustering order by (ts_accepted desc);
</code></pre>

<p>Here I am able to perform queries like:</p>

<pre><code>1.  SELECT * FROM friend_list WHERE userId=""---"" AND accepted=true;
2.  SELECT * FROM friend_list WHERE userId=""---"" AND accepted=false;
3.  SELECT * FROM friend_list WHERE userId=""---"" AND accepted IN (true,false);
</code></pre>

<p>But the 3rd query involves more read, so I tried to change the schema like this :</p>

<p><strong>B)</strong></p>

<pre><code> CREATE TABLE friend_list (
        userId uuid,
        friendId uuid,
        accepted boolean, 
        ts_accepted timestamp,
        PRIMARY KEY (userId , ts_accepted)
       ) with clustering order by (ts_accepted desc);
CREATE INDEX ON friend_list (accepted);
</code></pre>

<p>With this type B schema, the 1st and 2nd queries works, but I can simplify the third query as :</p>

<pre><code>3. SELECT * FROM friend_list WHERE userId=""---"";
</code></pre>

<p>I believe that the second schema gives much better performance for third query, as it won't do the condition check on every row.</p>

<p>Cassandra experts...Please suggest me which is the best schema on achieving this.A or B.</p>
",<sql><cassandra><query-performance><database><nosql>,"<p>First of all , are you aware that your second schema does not work at all like the first one ? In the first one the 'accepted' field was part of the key, but in the second not at all ! You don't have the same unique constraint, you should check that it is not a problem for your model.</p>

<p>Second if you only want to not have to include the 'acceptation' field for every request you have two possibilities : </p>

<p>1 - You can use 'acceptation' as a clustering column : </p>

<pre><code>PRIMARY KEY ((userId), accepted, ts_accepted)
</code></pre>

<p>This way your 3rd request can be : </p>

<pre><code>SELECT * FROM friend_list WHERE userId=""---"";
</code></pre>

<p>And you will get the same result more efficiently.</p>

<p>But this approach has a problem, it will create larger partitions, which is not the best for good performances.</p>

<p>2 - Create two separate tables</p>

<p>This approach is much more adequate for the Cassandra spirit. With Cassandra it is not unusual to duplicate the data if it can improve the efficiency of the requests.</p>

<p>So in your case you would keep your first schema for the first table and the first and second request, </p>

<p>and you would create another table with the same data but a schema slightly different , either with the secondary index if the 'accepted' does not need to be part of the primary key (as you did for your second schema), or a primary key like this :</p>

<pre><code>PRIMARY KEY ((userId), accepted, ts_accepted)
</code></pre>

<p>I would definitely prefer the secondary index for the second table if possible because the accepted column has a low cardinality (2) and thus very well fitted for secondary indexes.</p>

<p>EDIT : </p>

<p>Also you used a timestamp in your primary key. Be aware that it may be a problem if you can have the same user creating two rows in this table. Because the timestamp does not guaranty unicity : what happens if the two rows are created the same millisecond ?</p>

<p>You should probably use a TimeUUID. This type very commonly used in Cassandra guaranty the unicity by combining a Timestamp and UUID.</p>

<p>Furthermore a timestamp in a primary key can create temporary hotspots in a Cassandra node, definitely beter to avoid.</p>
",['table']
32047428,32047924,2015-08-17 09:53:13,Cassandra performance on clustered column sorting vs secondary index,"<p>my schema is : <br/><br/><strong>A)</strong></p>

<pre><code>CREATE TABLE friend_list (
    userId uuid,
    friendId uuid,
    accepted boolean, 
    ts_accepted timestamp,
    PRIMARY KEY ((userId) ,accepted, ts_accepted)
   ) with clustering order by (accepted desc, ts_accepted desc);
</code></pre>

<p><strong>B)</strong><br/></p>

<pre><code>CREATE TABLE friend_list (
        userId uuid,
        friendId uuid,
        accepted boolean, 
        ts_accepted timestamp,
        PRIMARY KEY (userId , ts_accepted)
       ) with clustering order by (ts_accepted desc);
CREATE INDEX ON friend_list (accepted);
</code></pre>

<p>Which will give the best performance for the query :</p>

<pre><code>SELECT * FROM friend_list WHERE userId=""---"" AND accepted=true;
</code></pre>

<p>With my understanding, Cassandra automatically sorts the clustered columns in ASC order and we specify DESC if we need to change the default sorting order for efficient queries.</p>

<p>With my schema A, I am making 'accepted' as a clustered key, but I need to sort it unnecessarily as I definitely have to sort 'ts_accepted' to DESC. 
Will this unwanted sorting of 'accepted' affects performance ?</p>

<p>If so, say I am making 'accepted' as secondary index in schema B. I know secondary index are not bad for low cardinal values(boolean). But still the query might have some performance issue.</p>

<p>Please let me know the efficient way on achieving this query.  </p>
",<cassandra><cassandra-2.0><query-performance><nosql>,"<p>I'd go for A.</p>

<p>If you can avoid a secondary index, avoid it (exception: you know it'll be a spark job that would benefit from it). If you still need a secondary index, redesign your model. If you still need it, feel horrible inside, and then maybe consider it.</p>

<p>The cost of clustering order by that you fear isn't appropriate. Cassandra stores clustering columns sorted anyway...ASC or DESC doesn't change things. You're using a tad more space, but for your query, you want to hit ""accepted"", so it's justified. I'm guessing ts_accepted is needed for some other reason? The only catch here is that if you need or have access to ts_accepted in your query, you need to provide an accepted equality filter. Performance wise, I don't see an issue.</p>

<p>As for B, indexes on extremely low cardinality columns (like bools) is bad. Consider how the data is stored - for each node, Cassandra maintains a table where the key is the value (true / false) and the values are the keys of all data for that node that matches the key. That has the potential for be a very wide column. Would you do that if you were modelling a separate table? No. And you shouldn't do that with an index either.</p>

<p>I don't know about the rest of the data, but if you're looking to get friends that have been accepted, why bother with a boolean? You may be able to use the ts_accepted column to infer the boolean. If they have a value, it's accepted, right?</p>

<p>One thing you should be aware of is that you can't update a column that's part of the pk. </p>

<p>Lastly, you ARE hitting the partition key (UserId) for your query. This is very good for your query. It means it'll hit exactly one partition. Depending on your use case (and size of entries), it might even be feasible to load the whole partition and filter client / app side. Of course, this depends on expected friend list sizes, and the data size vs network traffic vs app processing you need / are willing to do. For example, loading 100 entries and filtering for accepted app side, and loading 50 entries by filtering db side might have similar performance numbers.</p>
",['table']
32061657,32061899,2015-08-18 00:14:39,"Cassandra ""select as"" to be case sensitive","<p>Cassandra version: 2.2.0</p>

<p>I have Cassandra table like below:</p>

<pre><code> app_id                           | ts                       | os_name   
----------------------------------+--------------------------+-----------
 bb61ffae3b8f73e61572b61450d6d5d6 | 2015-08-04 22:00:00-0700 |   Android
 bb61ffae3b8f73e61572b61450d6d5d6 | 2015-08-05 22:00:00-0700 |   Android
</code></pre>

<p>and when I ""<strong>select app_id as appID</strong>"", the returned column name is all lower-case.</p>

<pre><code>cqlsh:myDB&gt; SELECT app_id as appID FROM daily_activity_report limit 10;
</code></pre>

<p>Then I got:</p>

<pre><code> appid
----------------------------------
 bb61ffae3b8f73e61572b61450d6d5d6
 7b87263e57a34deccb462b26786896b3
</code></pre>

<p>In my query I'm specifying <strong>upper-case</strong> ""SELECT distinct app_id as <strong>appID</strong>"", but returned is <strong>lower-case</strong> ""<strong>appid</strong>"".</p>

<p>Is there any config variable I'm missing to enable case sensitive column name? How to make it case sensitive?</p>

<p>Thanks a lot.</p>

<hr>

<p>update:
I also tried to add quote to the column name <strong>'appID'</strong> as below, but no luck.</p>

<pre><code>cqlsh:myDB&gt; SELECT app_id as 'appID' FROM daily_activity_report limit 10;
</code></pre>

<p>response:</p>

<pre><code>SyntaxException: &lt;ErrorMessage code=2000 [Syntax error in CQL query] 
message=""line 1:26 no viable alternative at input 'appID' (SELECT distinct app_id as ['appI]...)""&gt;
</code></pre>
",<database><cassandra><cassandra-2.0><case-sensitive><case-insensitive>,"<p>From the DataStax documentation, </p>

<blockquote>
  <p>Keyspace, column, and table names created using CQL 3 are case-insensitive unless enclosed in double quotation marks. If you enter names for these objects using any uppercase letters, Cassandra stores the names in lowercase. You can force the case by using double quotation marks.</p>
</blockquote>

<p><a href=""http://docs.datastax.com/en/cql/3.0/cql/cql_reference/ucase-lcase_r.html"" rel=""noreferrer"">http://docs.datastax.com/en/cql/3.0/cql/cql_reference/ucase-lcase_r.html</a></p>

<p>Using double quotes instead of single quotes should work for you.</p>
",['table']
32062552,32081643,2015-08-18 02:20:55,Querying in KairosDB/OpenTSDB,"<p>I have 3 million records with entries like:</p>

<pre><code>~/Abharthan/kairosdb$ head -10 export.txt
    {""name"": ""meterreadings"", ""timestamp"":""1359695700"",""tags"": {""Building_id"":""1"",""building_type"":""ElementarySchool"",""meter_type"":""temperature"",""unit"":""F""},""value"":""34.85""}
    {""name"": ""meterreadings"", ""timestamp"":""1359695700"",""tags"": {""Building_id"":""2"",""building_type"":""Park"",""meter_type"":""temperature"",""unit"":""F""},""value"":""0""}
    {""name"": ""meterreadings"", ""timestamp"":""1359695700"",""tags"": {""Building_id"":""3"",""building_type"":""Industrial"",""meter_type"":""temperature"",""unit"":""F""},""value"":""0.07""}
    {""name"": ""meterreadings"", ""timestamp"":""1359695700"",""tags"": {""Building_id"":""4"",""building_type"":""RecreationCenter"",""meter_type"":""temperature"",""unit"":""F""},""value"":""0""}
    {""name"": ""meterreadings"", ""timestamp"":""1359695700"",""tags"": {""Building_id"":""5"",""building_type"":""Park"",""meter_type"":""temperature"",""unit"":""F""},""value"":""2.2""}
    {""name"": ""meterreadings"", ""timestamp"":""1359695700"",""tags"": {""Building_id"":""6"",""building_type"":""CommunityCenter"",""meter_type"":""temperature"",""unit"":""F""},""value"":""31.41""}
    {""name"": ""meterreadings"", ""timestamp"":""1359695700"",""tags"": {""Building_id"":""7"",""building_type"":""Office"",""meter_type"":""temperature"",""unit"":""F""},""value"":""0""}
    {""name"": ""meterreadings"", ""timestamp"":""1359695700"",""tags"": {""Building_id"":""8"",""building_type"":""ElementarySchool"",""meter_type"":""temperature"",""unit"":""F""},""value"":""10.88""}
    {""name"": ""meterreadings"", ""timestamp"":""1359695700"",""tags"": {""Building_id"":""9"",""building_type"":""ElementarySchool"",""meter_type"":""temperature"",""unit"":""F""},""value"":""42.27""}
    {""name"": ""meterreadings"", ""timestamp"":""1359695700"",""tags"": {""Building_id"":""10"",""building_type"":""ElementarySchool"",""meter_type"":""temperature"",""unit"":""F""},""value"":""10.14""}
</code></pre>

<p>These are 1 year readings for meterreadings for each building with building_id collected every hour.</p>

<p>My starting data timestamp is: 1359695700 and ending timestamp is: 1422853200.
I want to query this DB to get the following:</p>

<pre><code>Query hourly average for one building(say building_id:100) for one year (output expected of 8760 points)
Query monthly sum for one building for one year (output expected of 12 points)
</code></pre>

<p>I have written following two queries to get results for above two queries:</p>

<p>Query1:</p>

<blockquote>
  <p>{ ""start_absolute"":1359695700, ""end_absolute"":1422853200,
  ""metrics"":[{""tags"":{""Building_id"":[""100""]},""name"":""meterreadings"",""aggregators"":[{""name"":""avg"",""align_sampling"":true,""sampling"":{""value"":""1"",""unit"":""hours""}}]}]}</p>
  
  <p>Response: 200
  {""queries"":[{""sample_size"":70168,""results"":[{""name"":""meterreadings"",""group_by"":[{""name"":""type"",""type"":""number""}],""tags"":{""Building_id"":[""100""],""building_type"":[""MiddleSchool""],""meter_type"":[""temperature""],""unit"":[""F""]},""values"":[[1359695700,42.45377343113282],[1360800000,36.42662912912908],[1364400000,41.12510250000007],[1368000000,54.915547499999946],[1371600000,65.07990000000015],[1375200000,55.8904375],[1378800000,47.33335249999986],[1382400000,38.952450000000034],[1386000000,41.99267000000001],[1389600000,41.28209500000009],[1393200000,40.31645895895911],[1396800000,40.758327499999915],[1400400000,54.05608750000002],[1404000000,63.410385],[1407600000,65.38089749999993],[1411200000,45.99822500000001],[1414800000,39.669450137465724],[1418400000,39.039874999999945],[1422000000,41.795917721519]]}]}]}</p>
</blockquote>

<p>Query 2:</p>

<blockquote>
  <p>{ ""start_absolute"":1359695700, ""end_absolute"":1422853200,
  ""metrics"":[{""tags"":{""Building_id"":[""100""]},""name"":""meterreadings"",""aggregators"":[{""name"":""sum"",""align_sampling"":true,""sampling"":{""value"":""1"",""unit"":""months""}}]}]}</p>
  
  <p>Response: 200
  {""queries"":[{""sample_size"":70168,""results"":[{""name"":""meterreadings"",""group_by"":[{""name"":""type"",""type"":""number""}],""tags"":{""Building_id"":[""100""],""building_type"":[""MiddleSchool""],""meter_type"":[""temperature""],""unit"":[""F""]},""values"":[[1359695700,3337957.570000005]]}]}]}</p>
</blockquote>

<p>I am not getting what I expected, am I missing something.</p>
",<cassandra><opentsdb><kairosdb>,"<p>The answer is simple,as I pointed out previously as a possible problem :-)  cf. <a href=""https://stackoverflow.com/questions/31926735/kairosdb-error-metric0name-abcd-tagxyz-value-may-not-be-empty"">Kairosdb error metric[0](name=abcd).tag[xyz].value may not be empty</a></p>

<p>KairosDB has millisecond precision - All the timestamps in KairosDB are Unix milliseconds.</p>

<p>But your timestamps are in Unix seconds, and that's your problem.</p>

<p>Therefore you need to multiply by 1000 all your timestamps in the data acquisition and in the queries.  </p>

<p>For instance query2 asks for all samples during less than 24h between January 16 1970 to January 17 1970, as you aggregate on one month you get only one result. </p>

<p>E.g. for data acquisition:</p>

<pre><code>{""name"": ""meterreadings"", ""timestamp"":""1359695700000"",""tags"": {""Building_id"":""1"",""building_type"":""ElementarySchool"",""meter_type"":""temperature"",""unit"":""F""},""value"":""34.85""}
    {""name"": ""meterreadings"", ""timestamp"":""1359695700000"",""tags"": {""Building_id"":""2"",""building_type"":""Park"",""meter_type"":""temperature"",""unit"":""F""},""value"":""0""}
</code></pre>

<p>...And query:</p>

<pre><code>{ ""start_absolute"":1359695700000, ""end_absolute"":1422853200000, ""metrics"":[{""tags"":{""Building_id"":[""100""]},""name"":""meterreadings"",""aggregators"":[{""name"":""sum"",""align_sampling"":true,""sampling"":{""value"":""1"",""unit"":""months""}}]}]}
</code></pre>
",['precision']
32068765,32073167,2015-08-18 09:37:22,How to change PARTITION KEY column in Cassandra?,"<p>Suppose we have such table: </p>

<pre><code>create table users (
    id text,
    roles set&lt;text&gt;,
    PRIMARY KEY ((id))
);
</code></pre>

<p>I want all the values of this table to be stored on the same Cassandra node (OK, not really the same, same 3, but have all the data mirrored, but you got the point), so to achieve that i want to change this table to be like this:</p>

<pre><code>create table users_v2 (
    partition int,
    id text,
    roles set&lt;text&gt;,
    PRIMARY KEY ((partition), id)
);
</code></pre>

<p>How can i do that without losing the data from the first table?
It seems to be impossible to ALTER TABLE in order to add such column. i'm OK with that.
What i try to do is to copy data from the first table and insert to the second table. 
When i do it as it is, the partition column іs missing, which is expected.
I can ALTER the first table and add a 'partition' column to the end, and then COPY in correct order, but i can't update all the rows in the first table to set the all some partition, and it seems to be no ""default"" value when column is added.</p>
",<cassandra><cql><cqlsh>,"<p>You simply cannot alter the primary key of a Cassandra table. You need to create another table with your new schema and perform a data migration. I would suggest that you use Spark for that since it is really easy to do a migration between two tables with only a few lines of code.</p>

<p><a href=""https://stackoverflow.com/questions/22139747/alter-composite-primary-key-in-cassandra-cql-3-0"">This</a> also answer to the alter primary key question.</p>
",['table']
32093347,32094057,2015-08-19 10:47:10,how to do the query in cassandra If i have two cluster key in column family,"<p>I have a column family and syntax like this:</p>

<pre><code>CREATE TABLE sr_number_callrecord ( 
  id int, 
  callerph text, 
  sr_number text, 
  callid text, 
  start_time text, 
  plan_id int, 
  PRIMARY KEY((sr_number), start_time, callerph) 
);
</code></pre>

<p>I want to do the query like :</p>

<pre><code>  a) select * from dummy where sr_number='+919xxxx8383' 
                   and start_time &gt;='2014-12-02 08:23:18' limit 10;

  b)  select * from dummy where sr_number='+919xxxxxx83' 
                          and start_time &gt;='2014-12-02 08:23:18' 
                          and callerph='+9120xxxxxxxx0' limit 10;
</code></pre>

<p>First query works fine but second query is giving error like </p>

<pre><code>Bad Request: PRIMARY KEY column ""callerph"" cannot be restricted 
(preceding column ""start_time"" is either not restricted or by a non-EQ 
relation)  
</code></pre>

<p>If I get the result in first query, In second query I am just adding one<br>
  more cluster key to get filter result and the row will be less</p>
",<cassandra><cql><cassandra-2.0><pycassa>,"<p>Just like you cannot skip PRIMARY KEY components, you may only use a non-equals operator on the <em>last</em> component that you query (which is why your 1st query works).</p>

<p>If you do need to serve both of the queries you have listed above, then you will need to have separate query tables for each.  To serve the second query, a query table (with the same columns) will work if you define it with a PRIMARY KEY like this:</p>

<pre><code>PRIMARY KEY((sr_number), callerph, start_time)
</code></pre>

<p>That way you are still specifying the parts of your PRIMARY KEY in order, and your non-equals condition is on the last PRIMARY KEY component.</p>
",['table']
32166506,32214754,2015-08-23 12:12:05,Cassandra Python Driver Reproducible Timeouts,"<p>We are trying to use Apache Cassandra for a large project and we have a Python script that runs <code>INSERT</code> queries into a Database Cluster. </p>

<p>While testing the script, on the developer laptops (MacOSX), it works perfectly and performs all INSERTs without a problem.</p>

<p>Every time it runs in production machines (Linux), it always has a:</p>

<p><code>cassandra.OperationTimedOut: errors={}, last_host=cassandra1.example.com
</code></p>

<p>We are using the <a href=""https://github.com/datastax/python-driver"" rel=""nofollow"">DataStax Python Driver</a> and use more than one hosts (cassandra1.example.com and cassandra2.example.com) when creating the cluster. </p>

<p>Both computers have the same kind and level of access in terms of networks (no firewall, etc.). The production servers have 4 ms ping with the database while the developer laptops average at 40-50.</p>

<p>Any ideas what seems to be the problem?</p>
",<python><cassandra><datastax>,"<p>The problem has been resolved and the solution is as follows:</p>

<p>We had a Python Class which was essentially used as an object. In the initializer, we created the cluster and connected to it and then passed the session/connection/cluster variables as properties using <code>self.cluster</code>, <code>self.session</code>, etc. </p>

<p>Later on, from a method in this Class, we called the <code>execute()</code> statement of <code>self.session</code>:</p>

<p><code>def executeQuery(self, id)
    self.session.execute(""INSERT INTO table (id) VALUES ("" + str(id) + "");"")
</code></p>

<p>We then replaced the object initializer with an empty function and put all Cassandra-related functions inside the <code>executeQuery()</code> method. The problem was resolved and no timeouts occurred. </p>
",['table']
32172394,32173129,2015-08-23 23:04:28,Cassandra - alternate way for clustering key with ORDER BY and UPDATE,"<p>My schema is :</p>

<pre><code>CREATE TABLE friends (
     userId timeuuid,
     friendId timeuuid,
     status varchar, 
     ts timeuuid,   
     PRIMARY KEY (userId,friendId)
);

CREATE TABLE friends_by_status (
    userId timeuuid,
    friendId timeuuid,
    status varchar, 
    ts timeuuid,   
    PRIMARY KEY ((userId,status), ts)
)with clustering order by (ts desc);
</code></pre>

<p>Here, whenever a friend-request is made, I'll insert record in both tables.
When I want to check one to one status of users, i'll use this query:</p>

<pre><code>SELECT status FROM friends WHERE userId=xxx AND friendId=xxx;
</code></pre>

<p>When I need to query all the records with pending status, i'll use :</p>

<pre><code>SELECT * FROM friends_by_status WHERE userId=xxx AND status='pending';
</code></pre>

<p>But, when there is a status change, I can update the 'status' and 'ts' in the <strong>'friends'</strong> table, but not in the <strong>'friends_by_status'</strong> table as both are part of PRIMARY KEY.</p>

<p>You could see that even if I denormalise it, I definitely need to update 'status' and 'ts' in 'friends_by_status' table to maintain consistency.</p>

<p>Only way I can maintain consistency is to delete the record and insert again.
<br/>But frequent delete is also not recommended in cassandra model. <a href=""http://www.slideshare.net/planetcassandra/8-axel-liljencrantz-23204252"" rel=""nofollow"">As said in Cassaandra Spottify summit.</a></p>

<p>I find this as the biggest limitation in Cassandra.</p>

<p>Is there any other way to sort this issue.</p>

<p>Any solution is appreciated. </p>
",<cassandra><cassandra-2.0><clustered-index><nosql>,"<p>I don't know how soon you need to deploy this, but in Cassandra 3.0 you could handle this with a materialized view.  Your friends table would be the base table, and the friends_by_status would be a view of the base table.  Cassandra would take care updating the view when you changed the base table.</p>

<p>For example:</p>

<pre><code>CREATE TABLE friends ( userid int, friendid int, status varchar, ts timeuuid, PRIMARY KEY (userId,friendId) );
CREATE MATERIALIZED VIEW friends_by_status AS
    SELECT userId from friends WHERE userID IS NOT NULL AND friendId IS NOT NULL AND status IS NOT NULL AND ts IS NOT NULL
    PRIMARY KEY ((userId,status), friendID);

INSERT INTO friends (userid, friendid, status, ts) VALUES (1, 500, 'pending', now());
INSERT INTO friends (userid, friendid, status, ts) VALUES (1, 501, 'accepted', now());
INSERT INTO friends (userid, friendid, status, ts) VALUES (1, 502, 'pending', now());
SELECT * FROM friends;                

 userid | friendid | status   | ts
--------+----------+----------+--------------------------------------
      1 |      500 |  pending | a02f7fe0-49f9-11e5-9e3c-ab179e6a6326
      1 |      501 | accepted | a6c80980-49f9-11e5-9e3c-ab179e6a6326
      1 |      502 |  pending | add10830-49f9-11e5-9e3c-ab179e6a6326
</code></pre>

<p>So now in the view you can select rows by the status:</p>

<pre><code>SELECT * FROM friends_by_status WHERE userid=1 AND status='pending';

 userid | status  | friendid
--------+---------+----------
      1 | pending |      500
      1 | pending |      502

(2 rows)
</code></pre>

<p>And then when you update the status in the base table, it automatically updates in the view:</p>

<pre><code>UPDATE friends SET status='pending' WHERE userid=1 AND friendid=501;
SELECT * FROM friends_by_status WHERE userid=1 AND status='pending';

 userid | status  | friendid
--------+---------+----------
      1 | pending |      500
      1 | pending |      501
      1 | pending |      502

(3 rows)
</code></pre>

<p>But note that in the view you couldn't have ts as part of the key, since you can only add one non-key field from the base table as part of the key in the view, which in your case would be adding 'status' to the key.</p>

<p>I think the first beta release for 3.0 is coming out tomorrow if you want to try this out.</p>
",['table']
32176695,32177708,2015-08-24 07:33:30,Cassandra account modeling with indexes,"<p>We are modeling account table in cassandra with social logins, we chose email as primary key and skinny row implementation. Our cassandra is on version <code>2.1.6</code>. Here is table definition:</p>

<pre><code>CREATE TABLE account_by_email (
    email_address text,
    account_password text,
    first_name text,
    last_name text,
    registered_at timestamp,
    roles set&lt;text&gt;,
    facebook_id text,
    twitter_id text,
    linkedin_id text,
    password_reset_token blob,
    password_reset_token_valid_until timestamp,
    profile_image_url text,
    PRIMARY KEY (email_address) ) WITH COMMENT='Accounts in system by email.';
</code></pre>

<p>This workks fine for email access since we can access fast each account when we know email address which is situation after login. </p>

<p>User has, in addition to email login option to login / signup with social accounts. When social account login is used flow is go to social network, receive social id (facebook, twitter, linkedin) and maybe email and query account table by social id to get full account or just email and continue on using email on each API request.</p>

<p>We currently added indexes on <code>facebook_id</code>, <code>twitter_id</code>, <code>linkedin_id</code> to support this since we are in MVP phase with one node and we chose fats implementation over performance.</p>

<p>What is proper way to model this? Here are couple of suggestions we are thinking of:</p>

<ul>
<li>leave index implementation since fetch by social id is happening only on login once (after that email is used)</li>
<li>have one table for each social id which will hold social id email pair</li>
<li>have one table for each social id which will hold full account (account can be edited so this will add complexity to update)</li>
<li>something else?</li>
</ul>

<p>And another question, is index implementation with high cardinality field (as social id) really that bad when you model access path which happens rarely?  </p>
",<cassandra><data-modeling><datastax-java-driver><cassandra-2.1>,"<p>My take on this would be the following : </p>

<p>Create an account table that holds all information about the user, and uses a uuid as partition key : </p>

<pre><code>CREATE TABLE account (
    userid uuid,
    first_name text,
    last_name text,
    registered_at timestamp,
    roles set&lt;text&gt;,
    password_reset_token blob,
    password_reset_token_valid_until timestamp,
    profile_image_url text,
    PRIMARY KEY (userid) );
</code></pre>

<p>Create a single table that links any of your login source to the user account : </p>

<pre><code>CREATE TABLE account_by_login_source (
        user_external_id text, // Can be an email address or a social network id       
        login_source text,   // Can be any of ""email"", ""facebook"", ""twitter"",... 
        userid uuid,
        account_password text,  // only useful for email login, since you handle auth
        PRIMARY KEY ((user_social_id, login_source)));
</code></pre>

<p>When you create your user, generate a uuid, insert a row in the account table  and a corresponding row in the account_login_source table.</p>

<p>This way, your users you can use multiple login sources and link them to a single account. You'll just have to run 2 very efficient queries in order to log the user in.</p>

<p>Using secondary indexes without specifying a partition key will definitely be a problem since requests will eventually timeout as your cluster grows.
If you run queries like the following : </p>

<pre><code>SELECT * FROM account_by_email where facebook_id = 'userid';
</code></pre>

<p>Cassandra will have to scan every node in the cluster, in order to get a single row.
From experience, I advise not to use this technique which leads to a lot of despair once in production...</p>
",['table']
32209385,32210527,2015-08-25 16:33:28,Cassandra spark connector joinWithCassandraTable on field with differents name,"<p>I'm looking to make a join on a RDD and a cassandra table which have not the same name for the same key
ex (simplified):</p>

<pre><code>case class User(id : String, name : String)
</code></pre>

<p>and</p>

<pre><code>case class Home( address : String, user_id : String)
</code></pre>

<p>If  would like to do : </p>

<pre><code>rdd[Home].joinWithCassandraTable(""testspark"",""user"").on(SomeColumns(""id""))
</code></pre>

<p>How can I precise the name of the field on which the join will be made.
And I don't want to map the rdd to have only the right id because I would like to join all values after the joinWithCassandraTable.</p>
",<scala><cassandra><apache-spark><datastax-enterprise><spark-cassandra-connector>,"<p>You can use the ""as"" syntax just like in a select to change the mapping of what the joined columns are.</p>

<p>An example</p>

<pre><code>sc.cassandraTable[Home](""ks"",""home"").joinWithCassandraTable(""ks"",""user"").on(SomeColumns(""id"" as ""user_id"")).collect
</code></pre>

<p>Will map the ""id"" column from the user table to the ""user_id"" field from the <code>Home</code> case class.</p>
",['table']
32214077,32215525,2015-08-25 21:06:09,Save and Execute returns previous row and current row in cassandra,"<p>I'm on <strong>cassandra 2.1</strong> and have what is probably a newbie question here.</p>

<p>I overwrite an existing row with a newer timestamp (Date column).</p>

<pre><code>QueryBuilder.insertInto(...)
session.execute(statement);
then do 

String sql = ""SELECT * FROM ""+keyspace+"".""+tablename;
ResultSet resultSet = session.execute(sql);
</code></pre>

<p>I get two rows returned. One is the statement previous to the insertInto statment and the other is the new value updated by the insertInto statment. </p>

<p>How do I get only one row? <code>cqlsh</code> shows only one row.</p>
",<java><cassandra><datastax>,"<p>You have probably defined your table to use your Date column as a clustering column.  This is used for collecting time series data, so inserting a row with a different timestamp creates a new row rather than updating an existing row.</p>

<p>If you want to update (overwrite) an existing row, all the fields you have listed for the PRIMARY KEY must have identical values.</p>

<p>Also if you are using the UPDATE statement, be aware that UPDATE and INSERT do the same thing in Cassandra, so using UPDATE is just like an INSERT if the key values don't exactly match an existing row.</p>
",['table']
32250271,32256801,2015-08-27 13:08:17,joinWithCassandraTable getting much slower on growing table size,"<p>I'm currently using this stack:</p>

<ul>
<li>Cassandra 2.2 (multinode)</li>
<li>Spark/Streaming 1.4.1</li>
<li>Spark-Cassandra-Connector 1.4.0-M3 </li>
</ul>

<p>I've got this DStream[Ids] with RDDs counting around 6000-7000 elements. <code>id</code> is the Partition Key.</p>

<pre><code>val ids: DStream[Ids] = ...
ids.joinWithCassandraTable(keyspace, tableName, joinColumns = SomeColumns(""id""))
</code></pre>

<p>As <code>tableName</code> gets bigger, let's say around 30k ""rows"", the query takes much longer, and I'm having trouble staying under the batch duration threshold. It performs similarly to using a massive <code>IN</code>-clause, which I've understood is not advisable.</p>

<p>Are there more effective ways to do this?</p>

<p>Answer:
Always remember to repartition your local RDDs with <code>repartitionByCassandraReplica</code> before doing joins with Cassandra to ensure that each partition is only working against the local Cassandra node. In my case, I also had to ramp up the partitions on the joining local RDD/DStream in order for the tasks to spread evenly across workers.</p>
",<scala><cassandra><apache-spark><spark-streaming><spark-cassandra-connector>,"<p>Is ""id"" a partition key in your table?  If not, I think it needs to be, otherwise you may be doing a table scan which would run progressively slower as the table gets bigger.</p>

<p>Also to get good performance with this method I believe you need to use the repartitionByCassandraReplica() operation on your ids RDD so that the joins are a local operation on each node.</p>

<p>See <a href=""https://github.com/datastax/spark-cassandra-connector/blob/master/doc/2_loading.md#repartitioning-rdds-based-on-a-cassandra-tables-replication"" rel=""nofollow"">this</a>.</p>
",['table']
32269232,32278131,2015-08-28 10:46:05,No rows inserted in table when import from CSV in Cassandra,"<p>I am trying to import a CSV file to a Cassandra table however I am facing a problem.
When inserted successfully, at least this is what Cassandra tells, I still can't see any record. Here is a little more details :</p>

<pre><code>qlsh:recommendation_engine&gt; COPY row_historical_game_outcome_data  FROM '/home/adelin/workspace/docs/re_raw_data2.csv' WITH DELIMITER='|';

2 rows imported in 0.216 seconds.
cqlsh:recommendation_engine&gt; select * from row_historical_game_outcome_data;

 customer_id | game_id | time | channel | currency_code | game_code | game_name | game_type | game_vendor | progressive_winnings | stake_amount | win_amount
-------------+---------+------+---------+---------------+-----------+-----------+-----------+-------------+----------------------+--------------+------------

(0 rows)
cqlsh:recommendation_engine&gt; 
</code></pre>

<p>And this is how my data looks like </p>

<pre><code>'SomeName'|673|'SomeName'|'SomeName'|'TYPE'|'M'|123123|0.20000000000000001|0.0|'GBP'|2015-07-01 00:01:42.19700|0.0|
'SomeName'|673|'SomeName'|'SomeName'|'TYPE'|'M'|456456|0.20000000000000001|0.0|'GBP'|2015-07-01 00:01:42.19700|0.0| 
</code></pre>

<p><strong>This is cassandra version apache-cassandra-2.2.0</strong> </p>

<p>EDITED: </p>

<pre><code>CREATE TABLE row_historical_game_outcome_data (
    customer_id int,
    game_id int,
    time timestamp,
    channel text,
    currency_code text,
    game_code text,
    game_name text,
    game_type text,
    game_vendor text,
    progressive_winnings double,
    stake_amount double,
    win_amount double,
    PRIMARY KEY ((customer_id, game_id, time))
) WITH bloom_filter_fp_chance = 0.01
    AND caching = '{""keys"":""ALL"", ""rows_per_partition"":""NONE""}'
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy'}
    AND compression = {'sstable_compression': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99.0PERCENTILE';
</code></pre>

<p>I've also tried the following as suggested by <strong>uri2x</strong>  </p>

<p>But still nothing : </p>

<pre><code>select * from row_historical_game_outcome_data;

 customer_id | game_id | time | channel | currency_code | game_code | game_name | game_type | game_vendor | progressive_winnings | stake_amount | win_amount
-------------+---------+------+---------+---------------+-----------+-----------+-----------+-------------+----------------------+--------------+------------

(0 rows)
cqlsh:recommendation_engine&gt; COPY row_historical_game_outcome_data (""game_vendor"",""game_id"",""game_code"",""game_name"",""game_type"",""channel"",""customer_id"",""stake_amount"",""win_amount"",""currency_code"",""time"",""progressive_winnings"")  FROM '/home/adelin/workspace/docs/re_raw_data2.csv' WITH DELIMITER='|';

2 rows imported in 0.192 seconds.
cqlsh:recommendation_engine&gt; select * from row_historical_game_outcome_data;

 customer_id | game_id | time | channel | currency_code | game_code | game_name | game_type | game_vendor | progressive_winnings | stake_amount | win_amount
-------------+---------+------+---------+---------------+-----------+-----------+-----------+-------------+----------------------+--------------+------------

(0 rows)
</code></pre>
",<csv><cassandra><cql><cassandra-2.0><cqlsh>,"<p>Ok, I had to change a couple of things about your data file to make this work:</p>

<pre><code>SomeName|673|SomeName|SomeName|TYPE|M|123123|0.20000000000000001|0.0|GBP|2015-07-01 00:01:42|0.0
SomeName|673|SomeName|SomeName|TYPE|M|456456|0.20000000000000001|0.0|GBP|2015-07-01 00:01:42|0.0
</code></pre>

<ul>
<li>Removed the trailing pipe.</li>
<li>Truncated the time down to seconds.</li>
<li>Removed all single quotes.</li>
</ul>

<p>Once I did that, then I executed:</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; COPY row_historical_game_outcome_data 
(game_vendor,game_id,game_code,game_name,game_type,channel,customer_id,stake_amount,
 win_amount,currency_code , time , progressive_winnings) 
FROM '/home/aploetz/cassandra_stack/re_raw_data3.csv' WITH DELIMITER='|';

Improper COPY command.
</code></pre>

<p>This one was a little tricky.  I finally figured out that <code>COPY</code> did not like the column name <code>time</code>.  I adjusted the table to use the name <code>game_time</code> instead, and re-ran the <code>COPY</code>:</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; DROP TABLE row_historical_game_outcome_data ;
aploetz@cqlsh:stackoverflow&gt; CREATE TABLE row_historical_game_outcome_data (
             ...     customer_id int,
             ...     game_id int,
             ...     game_time timestamp,
             ...     channel text,
             ...     currency_code text,
             ...     game_code text,
             ...     game_name text,
             ...     game_type text,
             ...     game_vendor text,
             ...     progressive_winnings double,
             ...     stake_amount double,
             ...     win_amount double,
             ...     PRIMARY KEY ((customer_id, game_id, game_time))
             ... );

aploetz@cqlsh:stackoverflow&gt; COPY row_historical_game_outcome_data
(game_vendor,game_id,game_code,game_name,game_type,channel,customer_id,stake_amount,
 win_amount,currency_code , game_time , progressive_winnings)
FROM '/home/aploetz/cassandra_stack/re_raw_data3.csv' WITH DELIMITER='|';

3 rows imported in 0.738 seconds.
aploetz@cqlsh:stackoverflow&gt; SELECT * FROM row_historical_game_outcome_data ;

 customer_id | game_id | game_time                | channel | currency_code | game_code | game_name | game_type | game_vendor | progressive_winnings | stake_amount | win_amount
-------------+---------+--------------------------+---------+---------------+-----------+-----------+-----------+-------------+----------------------+--------------+------------
      123123 |     673 | 2015-07-01 00:01:42-0500 |       M |           GBP |  SomeName |  SomeName |      TYPE |    SomeName |                    0 |          0.2 |          0
      456456 |     673 | 2015-07-01 00:01:42-0500 |       M |           GBP |  SomeName |  SomeName |      TYPE |    SomeName |                    0 |          0.2 |          0

(2 rows)
</code></pre>

<ul>
<li>I'm not sure why it says ""3 rows imported,"" so my guess is that it is counting the header row.</li>
<li>Your keys are all partition keys.  Not sure if you really understood that or not.  I only point it out, because I can't think of a reason to specify multiple partition keys <em>without</em> also specifying a clustering key(s).</li>
<li>I cannot find anything in the DataStax docs indicating that ""time"" is a reserved word.  It's probably a bug in cqlsh.  But seriously, you should probably specify your time-based column names as something other than ""time"" anyway.</li>
</ul>
",['table']
32301013,32302162,2015-08-30 20:49:47,Cassandra data modelling less then 1000 records to fit in one row,"<p>We have some entity uniquely identified by generated UUID. We need to support find by name query. Also we need to support sorting to be by name.</p>

<p>We know that there will be no more than 1000 of entities of that type which can perfectly fit in one row. Is it viable idea to hardcode primary key, use name as clustering key and id as clustering key there to satisfy uniqueness. Lets say we need school entity. Here is example:</p>

<pre><code>CREATE TABLE school (
  constant text,
  name text,
  id uuid,
  description text,
  location text,
  PRIMARY KEY ((constant), name, id)
);
</code></pre>

<p>Initial state would be give me all schools and then filtering by exact name will happen. Our reasoning behind this was to place all schools in single row for fast access, have name as clustering column for filtering and have id as clustering column to guaranty uniqueness. We can use <code>constant = school</code> as known hardcoded value to access this row.</p>

<p>What I like about this solution is that all values are in one row and we get fast reads. Also we can solve sorting easy by clustering column. What I do not like is hardcoded value for <code>constant</code> which seams odd. We could use <code>name</code> as PK but then we would have 1000 records spread across couple of partitions, probably find all without name would be slower and would not be sorted.</p>

<p><strong>Question 1</strong></p>

<p>Is this viable solution and are there any problems with it which we do not see? I did not see any example on Cassandra data modelling with hardcoded primary key probably for the reason so we are doubting this solution.</p>

<p><strong>Question 2</strong></p>

<p>Name is editable field, it will probably be changed rarely (someone can make typo or school can change name) but it can change. What is best way to achieve this? Delete insert inside batch (LTE can be applied to same row with conditional clause)?</p>
",<cassandra><data-modeling><key-value-store>,"<p>Yes this is a good approach for such a small dataset.  Just because Cassandra can partition large datasets across multiple nodes does not mean that you need to use that ability for every table.  By using a constant for the partition key, you are telling Cassandra that you want the data to be stored on one node where you can access it quickly and in sorted order.  Relational databases act on data in a single node all the time, so this is really not such an unusual thing to do.</p>

<p>For safety you will probably want to use a replication factor higher than one so that there are at least two copies of the single partition.  In that way you will not lose access to the data if the one node where it is stored went down.</p>

<p>This approach could cause problems if you expect to have a lot of clients (i.e. thousands of clients) frequently reading and writing to this table, since it could become a hot spot.  With only 1000 records you can probably keep all the rows cached in memory by setting the table to cache all keys and rows.</p>

<p>You probably won't find a lot of examples where this is done because people move to Cassandra for the support of large datasets where they want the scalability that comes from using multiple partitions.  So examples are geared towards that.</p>
",['table']
32307615,32307980,2015-08-31 09:07:23,Can I use num_tokens as load factor in Cassandra?,"<p>If you run cassandra on machines with different sizes of hard disk (e.g. one with one TB, another with 2TB), can I use num_tokens as load factor?
I want to reduce the risk of one node running out of disk space and balance the usage of disk on different machines. </p>

<p>I know, the more data one node collects, the more probable it might become a hotspot. Apart from that, which other considerations do I need to take care of? Which limits or practical restrictions exist for the number of nodes?
Can I change the number of nodes later if disk space changes without trouble?</p>

<p>I would appreciate some advice on that topic because I have not found much information about that in google or at the website of cassandra.</p>

<p>EDIT: numnodes replaced by num_tokens</p>
",<cassandra><cassandra-2.0>,"<p>Are you referring to <a href=""http://docs.datastax.com/en/cassandra/2.1/cassandra/configuration/configCassandra_yaml_r.html?scroll=reference_ds_qfg_n1r_1k__num_tokens"" rel=""nofollow"">num_tokens</a> settings? Yes, you can use a different number of tokens based on the hardware resources. Nodes with a larger number of tokens will see higher load and disk usage. Once set, the num_tokens setting cannot be changed at a later point without decommissioning the node.</p>
",['num_tokens']
32380788,32381418,2015-09-03 16:17:40,Cassandra filter based on secondary index,"<p>We have been using Cassandra for awhile now and we are trying to get a really optimized table going that will be able to quickly query and filter on about 100k rows. </p>

<p>Our model looks something like this:</p>

<pre><code>class FailedCDR(Model):  
    uuid = columns.UUID(partition_key=True, primary_key=True)
    num_attempts = columns.Integer(index=True)
    datetime = columns.Integer()
</code></pre>

<p>If I describe the table it clearly shows that <code>num_attempts</code> is index.</p>

<pre><code>CREATE TABLE cdrs.failed_cdrs (
    uuid uuid PRIMARY KEY,
    datetime int,
    num_attempts int
) WITH bloom_filter_fp_chance = 0.01
    AND caching = '{""keys"":""ALL"", ""rows_per_partition"":""NONE""}'
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy'}
    AND compression = {'sstable_compression': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99.0PERCENTILE';
CREATE INDEX index_failed_cdrs_num_attempts ON cdrs.failed_cdrs (num_attempts);
</code></pre>

<p>We want to be able to run a filter similar to this:</p>

<pre><code>failed = FailedCDR.filter(num_attempts__lte=9)
</code></pre>

<p>But this happens:</p>

<pre><code>QueryException: Where clauses require either a ""="" or ""IN"" comparison with either a primary key or indexed field
</code></pre>

<p>How can we accomplish a similar task?</p>
",<python><django><python-2.7><cassandra>,"<p>If you want to do a range query in CQL, you need the field to be a clustering column.</p>

<p>So you'll want the num_attempts field to be a clustering column.</p>

<p>Also if you want to do a single query, you need all the rows you want to query in the same partition (or a small number of partitions that you can access using an IN clause).  Since you only have 100K rows, that is small enough to fit in one partition.</p>

<p>So you could define your table like this:</p>

<pre><code>CREATE TABLE test.failed_cdrs (
    partition int,
    num_attempts int,
    uuid uuid,
    datetime int,
    PRIMARY KEY (partition, num_attempts, uuid));
</code></pre>

<p>You would insert your data with a constant for the partition key, such as 1.</p>

<pre><code>INSERT INTO failed_cdrs (uuid, datetime, num_attempts, partition)
    VALUES ( now(), 123, 5, 1);
</code></pre>

<p>Then you can do range queries like this:</p>

<pre><code>SELECT * from failed_cdrs where partition=1 and num_attempts &gt;=8;
</code></pre>

<p>The drawback to this method is that to change the value of num_attempts, you need to delete the old row and insert a new row since you are not allowed to update key fields.  You could do the delete and insert for that in a batch statement.</p>

<p>A better option that will become available in Cassandra 3.0 is to make a materialized view that has num_attempts as a clustering column, in which case Cassandra would take care of the delete and insert for you when you updated num_attempts in the base table.  The 3.0 release is currently in beta testing.</p>
",['table']
32408987,32414249,2015-09-05 02:40:49,Data modeling with Cassandra,"<p>I'm having trouble getting Cassandra's way of modeling data, mainly because this is my first time using a non-relational database. I'm not sure about how to make my model right now. Basically my model consists of Galaxies, Stars, Nebulas, Planets and Moons. A galaxy can have all of those, so it'd be an N:M relation. From what I understand, the idea would be to make a single table with all those components, or at least that's how I understood Cassandra's data modeling strategy and the idea of denormalizing. </p>

<p>It doesn't seem right to me though, and I'm not sure how to go about it. </p>
",<cassandra><data-modeling><database><nosql>,"<p>One way to look at this, would be in terms of the relationships of the entities you are trying to model.  For instance, galaxies have stars, stars have planets, planets have moons; and they all have certain characteristics (orbital period in days, radius in km).  You could model that similar to this:</p>

<pre><code>CREATE TABLE galaxyobjects (
  galaxy text,
  star text,
  planet text,
  moon text,
  spectralclass text,
  radiuskm double,
  orbitalperioddays double,
  PRIMARY KEY ((galaxy, star), planet, moon)
);
</code></pre>

<p>Of course, spectral class would really only apply to the <code>star</code> column, but in a denormalized model you would see it on every row.</p>

<p>After inserting some data, my table may look like this:</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; SELECT * FROM galaxyobjects;

 galaxy    | star       | planet      | moon   | orbitalperioddays | radiuskm | spectralclass
-----------+------------+-------------+--------+-------------------+----------+---------------
 Milky Way | Kepler-186 | Kepler-186f |    n/a |          129.9459 |     7072 |            M1
 Milky Way |        Sun |       Earth |   Moon |              27.3 |   3474.8 |            G2
 Milky Way |        Sun |       Earth |    n/a |           365.256 |     6371 |            G2
 Milky Way |        Sun |     Jupiter | Europa |             3.551 |   1560.8 |            G2
 Milky Way |        Sun |     Jupiter |     Io |              1.77 |   1821.6 |            G2
 Milky Way |        Sun |     Jupiter |    n/a |           4332.59 |    71492 |            G2

(6 rows)
</code></pre>

<p>Now, if I wanted to query for Jupiter and its moons:</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; SELECT * FROM galaxyobjects 
    WHERE galaxy='Milky Way' AND star='Sun' and planet='Jupiter';

 galaxy    | star | planet  | moon   | orbitalperioddays | radiuskm | spectralclass
-----------+------+---------+--------+-------------------+----------+---------------
 Milky Way |  Sun | Jupiter | Europa |             3.551 |   1560.8 |            G2
 Milky Way |  Sun | Jupiter |     Io |              1.77 |   1821.6 |            G2
 Milky Way |  Sun | Jupiter |    n/a |           4332.59 |    71492 |            G2

(3 rows)
</code></pre>

<p>Notes:</p>

<ul>
<li>I designated <code>galaxy</code> and <code>star</code> as partition keys.  As Cassandra has a max of 2 billion columns per partition, and a galaxy can definitely have billions of objects orbiting inside it, I thought that additionally partitioning by <code>star</code> would be necessary.  Note that in this model, you must designate both a <code>galaxy</code> and a <code>star</code> in each query.</li>
<li>I suppose you could extend the partition key to include <code>planet</code>, but then you wouldn't be able to query a <code>star</code> for its planets.</li>
<li>In this model, <code>planet</code> and <code>moon</code> are clustering keys, so they are not required to be specified on each query.  However you cannot skip them, so you would not be able specify (in your WHERE clause) a <code>moon</code> without also specifying a <code>planet</code>.</li>
<li>My data modeling senses don't really like using <code>n/a</code> as a clustering key when querying a planet on its own (without a moon).  So perhaps there's a better way to model that.  Maybe a collection of moons would be more appropriate?</li>
</ul>
",['table']
32447699,32499545,2015-09-08 00:47:43,Cassandra data model for simple messaging app,"<p>I am trying to learn Cassandra and always find the best way is to start with creating a very simple and small application. Hence I am creating a basic messaging application which will use Cassandra as the back-end. I would like to do the following:</p>

<ul>
<li>User will create an account with a username, email, and password. The
email and the password can be changed at anytime.</li>
<li>The user can add another user as their contact. The user would add a
contact by searching their username or email. The contacts don't need
to be mutual meaning if I add a user they are my contact, I don't
need to wait for them to accept/approve anything like in Facebook.</li>
<li>A message is sent from one user to another user. The sender needs to
be able to see the messages they sent (ordered by time) and the
messages which were sent to them (ordered by time). When a user opens
the app I need to check the database for any new messages for that
user. I also need to mark if the message has been read.</li>
</ul>

<p>As I come from the world of relational databases my relational database would look something like this:</p>

<pre><code>UsersTable
    username (text)
    email (text)
    password (text)
    time_created (timestamp)
    last_loggedIn (timestamp)
------------------------------------------------ 
ContactsTable
    user_i_added (text)
    user_added_me (text)
------------------------------------------------     
MessagesTable
    from_user (text)
    to_user (text)
    msg_body (text)
    metadata (text)
    has_been_read (boolean)
    message_sent_time (timestamp)
</code></pre>

<p>Reading through a couple of Cassandra textbooks I have a thought of how to model the database. My main concern is to model the database in a very efficient manner. Hence I am trying to avoid things such as secondary indexes etc. This is my model so far:</p>

<pre><code>CREATE TABLE users_by_username (
    username text PRIMARY KEY,
    email text,
    password text
    timeCreated timestamp
    last_loggedin timestamp
)

CREATE TABLE users_by_email (
    email text PRIMARY KEY,
    username text,
    password text
    timeCreated timestamp
    last_loggedin timestamp
)
</code></pre>

<p>To spread data evenly and to read a minimal amount of partitions (hopefully just one) I can lookup a user based on their username or email quickly. The downside of this is obviously I am doubling my data, but the cost of storage is quite cheap so I find it to be a good trade off instead of using secondary indexes. Last logged in will also need to be written in twice but Cassandra is efficent at writes so I believe this is a good tradeoff as well.</p>

<p>For the contacts I can't think of any other way to model this so I modelled it very similar to how I would in a relational database. This is quite a denormalized design I beleive which should be good for performance according to the books I have read?</p>

<pre><code>CREATE TABLE ""user_follows"" (
  follower_username text,
  followed_username text,
  timeCreated timestamp, 
  PRIMARY KEY (""follower_username"", ""followed_username"")
);

CREATE TABLE ""user_followedBy"" (

  followed_username text,
  follower_username text,
  timeCreated timestamp,
  PRIMARY KEY (""followed_username"", ""follower_username"")
);
</code></pre>

<p>I am stuck on how to create this next part. For messaging I was thinking of this table as it created wide rows which enables ordering of the messages.
I need messaging to answer two questions. It first needs to be able to show the user all the messages they have and also be able to show the user
the messages which are new and are unread. This is a basic model, but am unsure how to make it more efficent?</p>

<pre><code>CREATE TABLE messages (
    message_id uuid,
    from_user text,
    to_user text,
    body text,
    hasRead boolean,
    timeCreated timeuuid,
    PRIMARY KEY ((to_user), timeCreated )
) WITH CLUSTERING ORDER BY (timeCreated ASC);
</code></pre>

<p>I was also looking at using things such as STATIC columns to 'glue' together the user and messages, as well as SETS to store contact relationships, but from my narrow understanding so far the way I presented is more efficient. I ask if there are any ideas to improve this model's efficiency, if there are better practices do the things I am trying to do, or if there are any hidden problems I can face with this design? </p>

<p>In conclusion, I am trying to model around the queries. If I were using relation databases these would be essentially the queries I am looking to answer:</p>

<pre><code>To Login:
SELECT * FROM USERS WHERE (USERNAME = [MY_USERNAME] OR EMAIL = [MY_EMAIL]) AND PASSWORD = [MY_PASSWORD];
------------------------------------------------------------------------------------------------------------------------
Update user info:
UPDATE USERS (password) SET password = [NEW_PASSWORD] where username = [MY_USERNAME];
UPDATE USERS (email) SET password = [NEW_PASSWORD ] where username = [MY_USERNAME];
------------------------------------------------------------------------------------------------------------------------ 
To Add contact (If by username):
INSERT INTO followings(following,follower)  VALUES([USERNAME_I_WANT_TO_FOLLOW],[MY_USERNAME]);
------------------------------------------------------------------------------------------------------------------------
To Add contact (If by email):
SELECT username FROM users where email = [CONTACTS_EMAIL];
    Then application layer sends over another query with the username:
INSERT INTO followings(following,follower)  VALUES([USERNAME_I_WANT_TO_FOLLOW],[MY_USERNAME]);
------------------------------------------------------------------------------------------------------------------------
To View contacts:
SELECT following FROM USERS WHERE follower = [MY_USERNAME];
------------------------------------------------------------------------------------------------------------------------
To Send Message:,
INSERT INTO MESSAGES (MSG_ID, FROM, TO, MSG, IS_MSG_NEW) VALUES (uuid, [FROM_USERNAME], [TO_USERNAME], 'MY MSG', true);
------------------------------------------------------------------------------------------------------------------------
To View All Messages (Some pagination type of technique where shows me the 10 recent messages, yet shows which ones are unread):
SELECT * FROM MESSAGES WHERE TO = [MY_USERNAME] LIMIT 10;
------------------------------------------------------------------------------------------------------------------------
Once Message is read:
UPDATE MESSAGES SET IS_MSG_NEW = false WHERE TO = [MY_USERNAME] AND MSG_ID = [MSG_ID];
</code></pre>

<p>Cheers</p>
",<database><cassandra><data-modeling><cqlsh>,"<p>Yes it's always a struggle to adapt to the limitations of Cassandra when coming from a relational database background.  Since we don't yet have the luxury of doing joins in Cassandra, you often want to cram as much as you can into a single table.  In your case that would be the users_by_username table.</p>

<p>There are a few features of Cassandra that should allow you to do that.</p>

<p>Since you are new to Cassandra, you could probably use Cassandra 3.0, which is currently in beta release.  In 3.0 there is a nice feature called materialized views.  This would allow you to have users_by_username as a base table, and create the users_by_email as a materialized view.  Then Cassandra will update the view automatically whenever you update the base table.</p>

<p>Another feature that will help you is user defined types (in C* 2.1 and later).  Instead of creating separate tables for followers and messages, you can create the structure of those as UDT's, and then in the user table keep lists of those types.</p>

<p>So a simplified view of your schema could be like this (I'm not showing some of the fields like timestamps to keep this simple, but those are easy to add).</p>

<p>First create your UDT's:</p>

<pre><code>CREATE TYPE user_follows (
    followed_username text,
    street text,
);

CREATE TYPE msg (
    from_user text,
    body text
);
</code></pre>

<p>Next we create your base table:</p>

<pre><code>CREATE TABLE users_by_username (
    username text PRIMARY KEY,
    email text,
    password text,
    follows list&lt;frozen&lt;user_follows&gt;&gt;,
    followed_by list&lt;frozen&lt;user_follows&gt;&gt;,
    new_messages list&lt;frozen&lt;msg&gt;&gt;,
    old_messages list&lt;frozen&lt;msg&gt;&gt;
);
</code></pre>

<p>Now we create a materialized view partitioned by email:</p>

<pre><code>CREATE MATERIALIZED VIEW users_by_email AS
    SELECT username, password, follows, new_messages, old_messages FROM users_by_username
    WHERE email IS NOT NULL AND password IS NOT NULL AND follows IS NOT NULL AND new_messages IS NOT NULL
    PRIMARY KEY (email, username);
</code></pre>

<p>Now let's take it for a spin and see what it can do.  Let's create a user:</p>

<pre><code>INSERT INTO users_by_username (username , email , password )
    VALUES ( 'someuser', 'someemail@abc.com', 'somepassword');
</code></pre>

<p>Let the user follow another user:</p>

<pre><code>UPDATE users_by_username SET follows = [{followed_username: 'followme2', street: 'mystreet2'}] + follows
    WHERE username = 'someuser';
</code></pre>

<p>Let's send the user a message:</p>

<pre><code>UPDATE users_by_username SET new_messages = [{from_user: 'auser', body: 'hi someuser!'}] + new_messages
    WHERE username = 'someuser';
</code></pre>

<p>Now let's see what's in the table:</p>

<pre><code>SELECT * FROM users_by_username ;

 username | email             | followed_by | follows                                                 | new_messages                                 | old_messages | password
----------+-------------------+-------------+---------------------------------------------------------+----------------------------------------------+--------------+--------------
 someuser | someemail@abc.com |        null | [{followed_username: 'followme2', street: 'mystreet2'}] | [{from_user: 'auser', body: 'hi someuser!'}] |         null | somepassword
</code></pre>

<p>Now let's check that our materialized view is working:</p>

<pre><code>SELECT new_messages, old_messages FROM users_by_email WHERE email='someemail@abc.com'; 

 new_messages                                 | old_messages
----------------------------------------------+--------------
 [{from_user: 'auser', body: 'hi someuser!'}] |         null
</code></pre>

<p>Now let's read the email and put it in the old messages:</p>

<pre><code>BEGIN BATCH
    DELETE new_messages[0] FROM users_by_username WHERE username='someuser'
    UPDATE users_by_username SET old_messages = [{from_user: 'auser', body: 'hi someuser!'}] + old_messages where username = 'someuser'
APPLY BATCH;

 SELECT new_messages, old_messages FROM users_by_email WHERE email='someemail@abc.com';

 new_messages | old_messages
--------------+----------------------------------------------
         null | [{from_user: 'auser', body: 'hi someuser!'}]
</code></pre>

<p>So hopefully that gives you some ideas you can use.  Have a look at the documentation on collections (i.e. lists, maps, and sets), since those can really help you to keep more information in one table and are sort of like tables within a table.</p>
",['table']
32448987,32464409,2015-09-08 03:50:03,How to retrieve a very big cassandra table and delete some unuse data from it?,"<p>I hava created a cassandra table with 20 million records. Now I want to delete the expired data  decided by one none primary key column. But it doesn't support the operation on the column. So I try to retrieve the table and get the data line by line to delete the data.Unfortunately,it is too huge to retrieve. Otherwise,I couldn't delete the whole table, how could I achieve my goal?</p>
",<cassandra>,"<p>Your question is actually, how to get the data from the table in bulks (also called <em>pagination</em>).</p>

<p>You can do that by selecting different slices from your primary key: For example, if your primary key is some sort of ID, select a range of IDs each time, process the results and do whatever you want to do with them, then get the next range, and so on.</p>

<p>Another way, which depends on the driver you're working with, will be to use fetch_size. You can see a Python example <a href=""https://datastax.github.io/python-driver/query_paging.html"" rel=""nofollow"">here</a> and a Java example <a href=""http://datastax.github.io/java-driver/2.0.10/features/paging/"" rel=""nofollow"">here</a>.</p>
",['table']
32457456,33393281,2015-09-08 12:08:30,Remove core from Datastax Solr,"<p>I have a <code>Cassandra</code> table for which I have enabled <code>Solr</code> indexing, using command</p>

<p><code>dsetool create_core &lt;keyspace&gt;.&lt;table&gt; [&lt;option&gt; ...]</code></p>

<p>Question is how to delete this core?</p>

<p>I have tried <code>unload</code> via <code>HTTP Api</code> which returned <code>UNLOAD unsupported!</code></p>
",<solr><cassandra><core>,"<p>Update,</p>

<p>unload_core is now avaliable as of DSE 4.8 -- <a href=""http://docs.datastax.com/en/datastax_enterprise/4.8/datastax_enterprise/RNdse.html?scroll=concept_dgf_wph_ts__48Chgs"">DSP-1533</a></p>

<p>Verbatim from the <a href=""http://docs.datastax.com/en/datastax_enterprise/4.8/datastax_enterprise/srch/srchUnloadCore.html"">DataStax docs</a></p>

<blockquote>
  <p>Unloading a Solr core To disable full text search on a core, unload
  the core without removing its backing table.</p>
  
  <p>To simplify Solr code unloading, use dsetool unload_core. The syntax
  of the command is:</p>
  
  <p>$ dsetool unload_core . [ ...] where 
  is one or more of the following options:
  Option    Settings    Default Description of default setting
  deleteDataDir=    true or false   false   Retains the underlying Cassandra
  data. deleteResources=    true or false   false   Retains the core
  configuration and schema resources. distributed=  true or
  false true    Deletes resources and data across the cluster. The
  distributed option governs the removal of data and resources. The Solr
  secondary indexes on the backing table are removed through Cassandra
  schema propagation even if distributed=false. Note: If one or more
  nodes fail to unload the core in distributed operations, an error
  message indicates the failing node or nodes. Issue the unload again.</p>
</blockquote>
",['table']
32470254,32479789,2015-09-09 03:01:36,NoSQL database for an address book with billions of records,"<p>Which database is a suitable choice to store an address book with billions of rows (name, email address, phone number, etc. )?
The application will be very read intensive (>99%) and need high consistency available with servers distributed worldwide.
The query will be on either email address or phone number.
I am currently considering HBase, Cassandra or MongoDB.</p>
",<mongodb><cassandra><hbase><database><nosql>,"<p>Cassandra might be a good choice for that.  It has support for multiple data centers so for worldwide support you can set up a few DC's around the world to reduce latency by having clients access the nearest data center.</p>

<p>For fast lookups based on email address and phone number you'd probably store the data denormalized in two tables, with one table using email as the primary key and another table using phone number as the primary key.</p>

<p>You should be able to get the read performance you want by adding more nodes, since read performance would scale with the number of nodes you had   in each data center.</p>

<p>Now if you want to do ad hoc queries of this data based on fields other than the primary key, then Cassandra would not be a good choice.</p>
",['table']
32510462,32512182,2015-09-10 19:49:50,How to check whether a given key is in a map structure in CQL,"<p>I am trying to update a value stroed in map of given key using cql. Can anyone tell me how to do it? The following is my table:</p>

<pre><code>create table game(game_id uuid, game_name text, participant_id_name map&lt;uuid, text&gt;, PRIMARY KEY (game_id));
create index on game(participant_id_name);
</code></pre>

<p>Now I have a given participant's uuid and want to update his/her name, but I dont know the game_id. I wonder how can I check if the participant belongs to participant_id_name column and then update the name.</p>
",<cassandra><cql>,"<p>I do not believe your data model is optimal given what you are trying to do, so I am proposing the following:</p>

<pre><code>create table game(game_id uuid, game_name text, participant_id int, participant_id_name text, PRIMARY KEY (game_id), participant_id);
</code></pre>

<p>With this table, you have a partition key of game_id and a clustering column of participant_id, so the partition will include everything for the game_id ordered by the participant_id column.  I believe this makes sense for what you are trying to do.  Instead of using a database generated unique id for the participant_id, I suggest having the application provide an integer when inserting the name of the person so you know both pieces of data.</p>

<p>(Please note I do not have complete information, and am making a best effort with the information provided.)</p>
",['table']
32510572,32524877,2015-09-10 19:56:56,aggregate sum on Counter type of Cassandra,"<p>I'm using Cassandra 2.2.0, and I've read <a href=""http://docs.datastax.com/en/cql/3.3/cql/cql_using/useCounters.html"" rel=""nofollow"">using Counter type</a> but it doesn't provide much detail.</p>

<p>Is there a way to aggregate on Counter type column for Cassandra, similar to the following?</p>

<pre><code>SELECT sum(my_counter_column) FROM my_table ;
</code></pre>

<p>The above query results in this error:    </p>

<pre><code>InvalidRequest: code=2200 [Invalid query] message=""Invalid call to
function sum, none of its type signatures match (known type
signatures: system.sum : (tinyint) -&gt; tinyint, system.sum : (smallint)
-&gt; smallint, system.sum : (int) -&gt; int, system.sum : (bigint) -&gt; bigint, 
system.sum : (float) -&gt; float, system.sum : (double) -&gt;
double, system.sum : (decimal) -&gt; decimal, system.sum : (varint) -&gt;
varint)""
</code></pre>

<p>I know I can fetch all data and then do aggregation in the client, but I'm just wondering if it can be done within Cassandra. Thanks a lot.</p>
",<database><cassandra><cql>,"<p>Looks like an oversight when the system <code>sum()</code> function was implemented.  You should probably enter a jira ticket for it <a href=""https://issues.apache.org/jira/browse/cassandra/?selectedTab=com.atlassian.jira.jira-projects-plugin:summary-panel"" rel=""nofollow"">here</a> for 2.2.x and 3.x so that it can be fixed.</p>

<p>In the meantime you can work around the problem by defining your own user defined aggregate function (in Cassandra 2.2 and later) like this:</p>

<pre><code>CREATE FUNCTION agg_counter ( state bigint, val counter )
  CALLED ON NULL INPUT
  RETURNS bigint
  LANGUAGE java
  AS '
      if (val != null)
          state = state + val;
      return state;
  ';

CREATE AGGREGATE sum_counter ( counter )
  SFUNC agg_counter
  STYPE bigint
  INITCOND 0;
</code></pre>

<p>Then you would use it like this:</p>

<pre><code>SELECT sum_counter(countercol) FROM table WHERE partition=1;
</code></pre>

<p>I tried that in 3.0.0-beta2 and it works.  It should also work in 2.2.</p>

<p>Just remember to set <code>enable_user_defined_functions: true</code> in cassandra.yaml before you try to create the function.</p>
",['table']
32516928,32523307,2015-09-11 06:29:42,There's no better way to Count Keys In Cassandra？,"<p>I have a log table in cassandra, and now I want to search the rows count of the table.</p>

<p>First, I use the <code>select count(*) from log</code>,but it's very, very slow.
Then I want to use the <code>counter</code> type, and then the problem is coming. My table is a TTL table, all rows keep an hour, use the <code>counter</code> type become very difficult.</p>
",<cassandra>,"<p>Cassandra isn't efficient for doing table scan operations.  It is good at ingesting high volumes of data and then accessing small slices of that data rather than the whole table.</p>

<p>So if you want to count keys without using a counter, you need to break the table into chunks of data that are small enough to be processed quickly.  For example if you want to use count(*), you should only use it on a single partition, and keep the partition size below about 100,000 rows.</p>

<p>In your case you might want to partition your data by hour (or something small like 5 minute intervals if you insert a lot of log lines per second).</p>

<p>Be careful with using a TTL of an hour if you are inserting a lot of data continuously since it could cause a lot of tombstones.  To avoid building up tombstones you should delete each hour partition after the hour has passed.</p>
",['table']
32562500,32565104,2015-09-14 10:25:32,Cassandra non counter family,"<p>I attempted to create a table with counter as one of the column type in cassandra but getting the following error:</p>

<blockquote>
  <p>ConfigurationException: ErrorMessage code=2300 [Query invalid because
  of configuration issue] message=""Cannot add a counter column
  (transaction_count) in a non counter column family""</p>
</blockquote>

<p>My table schema is as follows:</p>

<pre><code>CREATE TABLE MARKET_DATA_TRANSACTION_COUNT (
TRADE_DATE TIMESTAMP,      
SECURITY_EXCHANGE TEXT,
PRODUCT_CODE TEXT,
SYMBOL TEXT,
SPREAD_TYPE TEXT,     
USER_DEFINED TEXT,
PRODUCT_GUID TEXT,
CHANNEL_ID INT,  
SECURITY_TYPE TEXT,
INSTRUMENT_GUID TEXT,
SECURITY_ID INT,   
TRANSACTION_COUNT COUNTER,
PRIMARY KEY (TRADE_DATE));
</code></pre>
",<cassandra><cql3>,"<p>That's a limitation of the current counter implementation.  You can't mix counters and regular columns in the same table.  So you need a separate table for counters.</p>

<p>They are thinking of removing this limitation in Cassandra 3.x.  See this <a href=""https://issues.apache.org/jira/browse/CASSANDRA-9810"" rel=""nofollow"">Jira ticket</a>.</p>
",['table']
32567571,32568318,2015-09-14 14:38:05,"Failed to run custom aggregation OperationTimedOut: errors={}, last_host=127.0.0.1","<p>I've a running apache-cassandra-2.2.1 with <code>enable_user_defined_functions</code> set to <code>true</code> in <code>cassandra.yml</code>. I've defined a custom aggregation based on <a href=""http://christopher-batey.blogspot.fr/2015/05/cassandra-aggregates-min-max-avg-group.html"" rel=""nofollow"">this article</a> as follows:</p>

<pre><code>CREATE FUNCTION sumFunc(current double, candidate double) CALLED ON NULL INPUT RETURNS double LANGUAGE java AS 'if(current == null) return candidate; return current + candidate;'
CREATE AGGREGATE sum(double) SFUNC sumFunc STYPE double INITCOND null;
</code></pre>

<p>When I call this from CQLSH console I see a timeout:</p>

<pre><code>cqlsh:test&gt; SELECT word, sum(frequency) FROM words;
OperationTimedOut: errors={}, last_host=127.0.0.1
</code></pre>

<p>I can successful run any other query, I can also run the query (but I don't get full result set) from scala:</p>

<pre><code>CassandraConnector(conf).withSessionDo { session =&gt;
  val result: ResultSet = session.execute(""SELECT word, SUM(frequency) FROM test.words;"")
  while(result.isExhausted == false) {
    println(result.one)
  }
}
</code></pre>
",<scala><stored-procedures><cassandra><cql><spark-cassandra-connector>,"<p>First off your query may not do what you expect since it won't group by each word in the table.  You would get a sum of all the frequencies in the table.  To get the sum of the frequencies for a word you'd need to do this:</p>

<pre><code>SELECT word, sum(frequency) FROM words WHERE word='someword';
</code></pre>

<p>Second, I have seen time out errors when trying to aggregate large partitions over about 300,000 rows (see <a href=""https://stackoverflow.com/questions/32128069/how-to-set-a-timeout-and-throttling-rate-for-a-large-user-defined-aggregate-quer"">this</a>).  So it's possible your table of words is too large to be aggregated before the timeout error kicks in.  I wish Cassandra wouldn't time out on queries that are making progress, but it seems to have some hardcoded timeouts that will abort tasks regardless of if they are progressing or are actually stuck.</p>

<p>Since your query doesn't have a WHERE clause, you are trying to aggregate a whole table rather than just a single partition.  That would be a lot more likely to cause timeout errors since the aggregation will take place on data spread on multiple nodes instead of a single node, so you should try to restrict aggregation to a single partition.</p>

<p>I would think for INITCOND you'd want to use 0 rather than null.</p>

<p>The name of your aggregate might conflict with the built in system sum function, so you might want to pick a different name.  But offhand it looks like you could use the built in sum function instead of defining one (Cassandra 2.2 has undocumented built in functions for sum(), avg(), min(), and max()).</p>
",['table']
32585575,32636793,2015-09-15 12:01:48,Cassandra java query performance count(*) or all().size(),"<p>I want to know, which is faster using apache cassandra in combination with java. I have the following options to get my result:</p>

<pre><code>Statement s = QueryBuilder.select().from(""table"").where(QueryBuilder.eq(""source"",source);
ResultSet resultSet = session.execute(s);
if (resultSet.all().size() == 0) {
  //Do Something
}
</code></pre>

<p>The second option to achieve my count is:</p>

<pre><code>ResultSet rs = session.execute(""SELECT COUNT(*) as coun FROM table WHERE source = '""+source+""'"");
Row r = rs.one();
if (r.getLong(""count"") == 0) {
  //Do Something
}
</code></pre>

<p>In every query, the maximum count is 1. Now my question is, which would be faster in general.</p>
",<java><cassandra><cassandra-2.0>,"<p>I tested several queries on multiple tables, the version with count(*) is much faster than using resultSet.all().size() == 0. I used CQLSH to try which is faster with the following queries, which should be equal to the java one's:</p>

<pre><code>SELECT COUNT(*) as coun FROM table WHERE source = '...';
</code></pre>

<p>And the slower one:</p>

<pre><code>SELECT * FROM table WHERE source = '...';
</code></pre>
",['table']
32605319,32608743,2015-09-16 09:55:43,Select rows where a column is null in Cassandra,"<p>I have following table in Cassandra (CQL spec 3.3.0 ):-</p>

<pre><code>Users: &lt;a,b,c,d&gt;, Primary key a Int, B: Map&lt;Int,Date&gt;
</code></pre>

<p>Where Users table has column names as <code>a,b,c,d</code>. Now I want to select all the columns for the rows where column b is <code>null</code>. I did following query:-</p>

<pre><code>select from Users where b = null;
</code></pre>

<p>but this fails with error = <code>""message=""Invalid null value in condition""</code></p>

<ol>
<li>The answer here:- <a href=""https://stackoverflow.com/questions/20981075/how-can-i-search-for-records-that-have-a-null-empty-field-using-cql"">null doesn't exists</a> clarifies the semantics
of null but still its not clear how do I select the columns which
are not present in a row?</li>
<li>Here <a href=""https://stackoverflow.com/questions/11800309/cql-unable-to-null-check-in-where-clause?rq=1"">Nulls in Cassandra</a> an answer claims null to be supported by latest cassandra, but looks like it doesn't work for maps.</li>
</ol>

<p>Is there any way to solve it? Or should I select all the columns from all the rows and manually check null programmatically, which of course would be very inefficient for a large database.</p>
",<cassandra>,"<p>Cassandra doesn't support querying based on null, even for secondary indexes.</p>

<p>You could add another field to your table called something like ""uninitialized"" as a boolean that you set to true when you first insert the row, and you'd create a secondary index on the ""uninitialized"" field.</p>

<p>When you set the map to something, you would set ""uninitialized"" to false.  Then to find the rows where the map is null, you'd query for rows where ""uninitialized"" is true.</p>
",['table']
32644054,32653958,2015-09-18 04:47:19,Cassandra static column - How many copies?,"<p>I have been reading about static column and have a question which will impact my design. I will have a column which will store quite a long string. This column will be part of a table which will a few hundred million rows. My question then is there only 1 copy of this static column in storage or does it still store multiple copies. I assume there is only one copy as in Java, but I cannot find a definitive answer anywhere. The one column I want to declare static will be quite big so I do not want to duplicate this data inside of storage. </p>
",<cassandra>,"<p>A static column will be stored on disk once per <code>SSTABLE</code>. However, do not assume it will be written to the disk only one time before you answer the following:</p>

<ol>
<li>How many tables will have the static column?</li>
<li>What is the replication factor?</li>
</ol>

<p>An example, you are following the one table per query data design methodology and have two queries that will request the static column. You should store that in two separate tables which means it is written to disk twice. If that is replicated 3 times amongst the nodes then it will be written 2*3 times total.</p>

<p>If you would like to learn to estimate disk space usage, check out <a href=""https://academy.datastax.com/courses/ds220-data-modeling/physical-partition-size"" rel=""nofollow"">Datastax DS220 unit Physical Partition Size</a>.</p>
",['table']
32675181,32675645,2015-09-20 02:35:31,Cassandra - >500mb CSV file produces ~50mb size table?,"<p>I am new to Cassandra and trying to figure out how sizing works. I created a keyspace and a table. I then generated a script to create 1 million rows in java into a csv file and insert it into my database. The CSV file was ~545 mb in size. I then loaded it into the database and ran <strong>nodetool cfstats</strong> command and received this output. It says the total space used is 50555052 bytes ( ~50 mb). How can this be? With overhead of indexes, columns, etc how can my total data be smaller than just the raw CSV data (not just smaller, but so much smaller)?  Maybe I am not reading something here correctly, but does this seem right? I am using Cassandra 2.2.1 on a single machine.</p>

<pre><code>Table: users
        SSTable count: 1
        Space used (live): 50555052
        Space used (total): 50555052
        Space used by snapshots (total): 0
        Off heap memory used (total): 1481050
        SSTable Compression Ratio: 0.03029072054256705
        Number of keys (estimate): 984133
        Memtable cell count: 240336
        Memtable data size: 18385704
        Memtable off heap memory used: 0
        Memtable switch count: 19
        Local read count: 0
        Local read latency: NaN ms
        Local write count: 1000000
        Local write latency: 0.044 ms
        Pending flushes: 0
        Bloom filter false positives: 0
        Bloom filter false ratio: 0.00000
        Bloom filter space used: 1192632
        Bloom filter off heap memory used: 1192624
        Index summary off heap memory used: 203778
        Compression metadata off heap memory used: 84648
        Compacted partition minimum bytes: 643
        Compacted partition maximum bytes: 770
        Compacted partition mean bytes: 770
        Average live cells per slice (last five minutes): 0.0
        Maximum live cells per slice (last five minutes): 0
        Average tombstones per slice (last five minutes): 0.0
        Maximum tombstones per slice (last five minutes): 0
</code></pre>

<p>My Java code to generate the CSV file looks like this:</p>

<pre><code>try{

            FileWriter writer = new FileWriter(sFileName);
            for(int i=0;i&lt;1000000;i++){


            writer.append(""Username "" + i);
            writer.append(',');
            writer.append(new Timestamp(date.getTime()).toString());
            writer.append(',');
            writer.append(""myfakeemailaccnt@email.com"");
            writer.append(',');
            writer.append(new Timestamp(date.getTime()).toString());
            writer.append(',');
            writer.append(""eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOnRydWV9.TJVA95OrM7E2cBab30RMHrHDcEfxjoYZgeFONFh7HgQ"");
            writer.append(',');
            writer.append(""eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOnRydWV9.TJVA95OrM7E2cBab30RMHrHDcEfxjoYZgeFONFh7HgQ"");
            writer.append(',');
            writer.append(""eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOnRydWV9.TJVA95OrM7E2cBab30RMHrHDcEfxjoYZgeFONFh7HgQ"");
            writer.append(',');
            writer.append(""tr"");
            writer.append('\n');

            }   
            writer.flush();
            writer.close();

        }
        catch(IOException e)
        {
             e.printStackTrace();
        } 
</code></pre>
",<cassandra>,"<p>So I thought of the biggest 3 pieces of data:</p>

<pre><code>eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOnRydWV9.TJVA95OrM7E2cBab30RMHrHDcEfxjoYZgeFONFh7HgQ
</code></pre>

<p>and thought they are the same maybe Cassandra is compressing them, even though it said it is only a ratio of 3%. So I changed my Java code to produce different data.</p>

<pre><code>public class Main {

    private static final String ALPHA_NUMERIC_STRING = ""ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789"";

    public static void main(String[] args) {

        generateCassandraCSVData(""users.csv"");

    }

    public static String randomAlphaNumeric(int count) {
        StringBuilder builder = new StringBuilder();
        while (count-- != 0) {
        int character = (int)(Math.random()*ALPHA_NUMERIC_STRING.length());
        builder.append(ALPHA_NUMERIC_STRING.charAt(character));
        }
        return builder.toString();
        }


    public static void generateCassandraCSVData(String sFileName){

    java.util.Date date= new java.util.Date();


        try{

            FileWriter writer = new FileWriter(sFileName);
            for(int i=0;i&lt;1000000;i++){



            writer.append(""Username "" + i);
            writer.append(',');
            writer.append(new Timestamp(date.getTime()).toString());
            writer.append(',');
            writer.append(""myfakeemailaccnt@email.com"");
            writer.append(',');
            writer.append(new Timestamp(date.getTime()).toString());
            writer.append(',');
            writer.append("""" + randomAlphaNumeric(150) + """");
            writer.append(',');
            writer.append("""" + randomAlphaNumeric(150) + """");
            writer.append(',');
            writer.append("""" + randomAlphaNumeric(150) + """");
            writer.append(',');
            writer.append(""tr"");
            writer.append('\n');


            //generate whatever data you want
            }   
            writer.flush();
            writer.close();

        }
        catch(IOException e)
        {
             e.printStackTrace();
        } 

    }

}
</code></pre>

<p>So now the data for those 3 large columns is all random strings, not the same anymore. This is what was produced now:</p>

<pre><code>Table: users
        SSTable count: 4
        Space used (live): 554671040
        Space used (total): 554671040
        Space used by snapshots (total): 0
        Off heap memory used (total): 1886175
        SSTable Compression Ratio: 0.6615549506522498
        Number of keys (estimate): 1019477
        Memtable cell count: 270024
        Memtable data size: 20758095
        Memtable off heap memory used: 0
        Memtable switch count: 25
        Local read count: 0
        Local read latency: NaN ms
        Local write count: 1323546
        Local write latency: 0.048 ms
        Pending flushes: 0
        Bloom filter false positives: 0
        Bloom filter false ratio: 0.00000
        Bloom filter space used: 1533512
        Bloom filter off heap memory used: 1533480
        Index summary off heap memory used: 257175
        Compression metadata off heap memory used: 95520
        Compacted partition minimum bytes: 311
        Compacted partition maximum bytes: 770
        Compacted partition mean bytes: 686
        Average live cells per slice (last five minutes): 0.0
        Maximum live cells per slice (last five minutes): 0
        Average tombstones per slice (last five minutes): 0.0
        Maximum tombstones per slice (last five minutes): 0
</code></pre>

<p>So now the CSV file is again ~550mb and my table now is ~550mb too. It seems then if non key column data is the same (low cardinally) Cassandra it compressed this data very efficiently somehow? If this is the case, this is a very important concept (one I have never read before) to know when modeling your database as you can then save a lot of storage space if you keep this mind. </p>
",['table']
32689864,32695103,2015-09-21 07:44:54,"If Cassandra facilitates the use of non-normalized data, how do users edit it without creating inconsistencies?","<p>If Apache Cassandra's architecture encourages the use of non-normalized column families designed specifically for anticipated queries, how do users edit data that is replicated across many columns without creating inconsistencies? </p>

<p>e.g., example 3 here: <a href=""http://www.ebaytechblog.com/2012/07/16/cassandra-data-modeling-best-practices-part-1/"" rel=""nofollow"">http://www.ebaytechblog.com/2012/07/16/cassandra-data-modeling-best-practices-part-1/</a></p>

<p>If Jay was no longer interested in iphones, deleting this piece of information would require that columns in 2 separated column families be deleted. Do users just need to code add/edit/delete functions that appropriately update all the relevant tables, or does Cassandra somehow know how records are related and handle this for users?</p>
",<cassandra><denormalization>,"<p>In the Cassandra 2.x world, the way to keep your denormalized query tables consistent is to use atomic batches.</p>

<p>In an example taken from the CQL documentation, assume that I have two tables for user data.  One is the ""users"" table and the other is ""users_by_ssn.""  To keep these two tables in sync (should a user change their ""state"" of residence) I would need to apply an upsert like this:</p>

<pre><code>BEGIN BATCH;
  UPDATE users
    SET state = 'TX'
    WHERE user_uuid = 8a172618-b121-4136-bb10-f665cfc469eb;
  UPDATE users_by_ssn 
    SET state = 'TX'
    WHERE ssn = '888-99-3987';
APPLY BATCH;
</code></pre>
",['table']
32705543,32707335,2015-09-21 22:32:48,Data modelling a user table with multiple addresses in Cassandra,"<p>I am just starting out in Cassandra. I am testing following data model. I was wondering how can this be modeled in Cassandra?</p>

<pre><code>Users
{ 
    ""userId"" : ""73180"" , 
    ""firstName"" : ""John"" , 
    ""lastName"" : ""Doe""
    ""addresses"" : 
    { 
        ""type"" : ""homeAddress""
        ""street"" : ""a pretty street"" , 
        ""city"" : ""Some city"" , 
        ""state"" : ""CT"",
        ""country"" : ""US""
        ""zipcode"" : 55555
    } , 
    { 
        ""type"" : ""businessAddress""
        ""street"" : ""an office street"" , 
        ""city"" : ""Some city"" , 
        ""state"" : ""CT"",
        ""country"" : ""US""
        ""zipcode"" : 55555
    } , 
}
</code></pre>

<p>Specifically, how can I model multiple addresses for each user?</p>
",<cassandra><data-modeling><cassandra-2.0>,"<p>While modeling addresses as a user defined type (UDT) is a good approach, you should also consider what your query patterns will look like.  Will you be querying users only by <code>userid</code>?  Or will you want to query them by name?</p>

<p>To demonstrate this, I'll create an address UDT and design a <code>usersByLastName</code> query table:</p>

<pre><code>CREATE TYPE address (
  street TEXT,
  city TEXT,
  state TEXT,
  postal TEXT,
  country TEXT);

CREATE TABLE usersByLastName (
  userid BIGINT,
  firstName TEXT,
  lastName TEXT,
  addresses MAP &lt;TEXT, FROZEN &lt;address&gt;&gt;,
  PRIMARY KEY (lastName,firstName,userid));
</code></pre>

<p>Note that <code>addresses</code> is created as a MAP of <code>address</code>, so that you have some flexibility in type (home, office, shipping, billing...etc) and number of addresses that you can add per user.</p>

<p>Next, I'll INSERT some data and query by <code>lastName</code>, which yields:</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; SELECT * FROM usersbylastname WHERE lastname='Doe';

 lastname | firstname | userid | addresses
----------+-----------+--------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
      Doe |      Jane |  73184 | {'business': {street: 'A Office St.', city: 'Somecity', state: 'CT', postal: '55555', country: 'US'}, 'home': {street: 'B Pretty St.', city: 'Somecity', state: 'CT', postal: '55555', country: 'US'}, 'shipping': {street: '1187 Huntervasser', city: 'Los Angeles', state: 'CA', postal: '90036', country: 'US'}}
      Doe |      John |  73180 | {'business': {street: 'A Office St.', city: 'Somecity', state: 'CT', postal: '55555', country: 'US'}, 'home': {street: 'A Pretty St.', city: 'Somecity', state: 'CT', postal: '55555', country: 'US'}}

(2 rows)
</code></pre>

<p>This is one way which you could model this.  Again, it is important to consider your query requirements.</p>

<p>Edit: I'm at the Cassandra Summit keynote right now, and Jonathan Ellis just demo'd a user table with a map of addresses, pretty much exactly how I did.</p>
",['table']
32715401,32754775,2015-09-22 11:13:45,Cassandra: Migrate keyspace data from Multinode cluster to SingleNode Cluster,"<p>I have a keyspace in a multi-node cluster in QA environment. I want to copy that keyspace to my local single-node cluster. Is there any direct way to do this? I can't afford to write some code like SSTableLoader implementation at this point of time. Please suggest the quickest way.</p>
",<cassandra>,"<p><strong>Make sure you have plenty of free disk space on your new node and that you've properly set replication factor and consistency levels in your tests/build for your new, single node ""cluster""</strong> </p>

<p>First, restore the exact schema from the old cluster to your new node. After that the data can be loaded in two ways:</p>

<p>1.) Execute the ""sstableloader"" utility on every node in your old cluster and point it at your new node. sstableloader is token aware, but in your case it will end up shipping all data to your new, single node cluster.</p>

<pre><code>sstableloader -d NewNode /Path/To/OldCluster/SStables 
</code></pre>

<p>2.) Snapshot the keyspace and copy the raw sstable files from the snapshot folders of each table in your old cluster to your new node.  Once they're all there, copy the files to their corresponding table directory and run ""nodetool refresh.""</p>

<pre><code># Rinse and repeat for all tables    
nodetool snapshot -t MySnapshot
cd /Data/keyspace/table-UUID/snapshots/MySnapshot/
rsync -avP ./*.db User@NewNode:/NewData/Keyspace/table-UUID
...
# when finished, exec the following for all tables in your new node
nodetool refresh keyspace table
</code></pre>

<p>Option #1 is probably best because it will stream the data and compact naturally on the new node. It's also less manual work. Option #2 is good, quick, and dirty if you don't have a direct line from one cluster to the other. You probably won't notice much difference since it's probably a relatively small keyspace for QA.</p>
",['table']
32729181,32732661,2015-09-23 00:30:27,Installing Cassandra on Vagrant Centos using Puppet missing dsc22,"<p>I'm new to puppet. I know that cassandra is missing from yum so I figured a puppet recipe would download and install it, but it seems like <code>locp/cassandra</code> is just trying to install it from yum. The recipe is supposed to work, but I don't see anything on <a href=""https://github.com/locp/cassandra"" rel=""nofollow"">https://github.com/locp/cassandra</a> as to why it's not working for me or any thing I need to set up before it should work.</p>

<p>I used librarian-puppet to install the modules in puppet/modules.</p>

<p><strong>Error</strong></p>

<pre><code>==&gt; default: Notice: /Stage[main]/Cassandra/File[/var/lib/cassandra/data]: Dependency Package[dsc22] has failures: true
</code></pre>

<p><strong>Vagrantfile</strong></p>

<pre><code># -*- mode: ruby -*-
# vi: set ft=ruby :

Vagrant.configure(2) do |config|
  config.vm.box = ""puphpet/centos65-x64""

  config.vm.provision ""puppet"" do |p|
    p.module_path = ""puppet/modules""
    p.manifests_path = ""puppet/manifests""
    p.manifest_file = ""site.pp""
  end
end
</code></pre>

<p><strong>puppet/manifests/site.pp</strong></p>

<pre><code>class { 'cassandra':
  cluster_name    =&gt; 'foobar',
  listen_address  =&gt; ""${::ipaddress}"",
}
</code></pre>

<p><strong>puppet/Puppetfile</strong></p>

<pre><code>forge 'https://forgeapi.puppetlabs.com'

mod ""locp/cassandra""
</code></pre>
",<cassandra><centos><vagrant><puppet>,"<p>Thats probably because the repo is not configured (see <a href=""http://docs.datastax.com/en/cassandra/2.2/cassandra/install/installRHEL.html"" rel=""nofollow"">here</a>)</p>

<p>Add the following to your <code>site.pp</code> and make sure to add a <code>require</code> on it in your cassandra class</p>

<pre><code>class repo {
  yumrepo { ""datastax"":
    descr          =&gt; ""DataStax Repo for Apache Cassandra"",
    baseurl        =&gt; ""http://rpm.datastax.com/community"",
    gpgcheck       =&gt; ""0"",
    enabled        =&gt; ""1"";
  } 
}

class { 'cassandra':
  cluster_name    =&gt; 'foobar',
  listen_address  =&gt; ""${::ipaddress}"",
  require         =&gt; Yumrepo[""datastax""],
}

include repo
include cassandra
</code></pre>
","['cluster_name', 'listen_address']"
32729912,32735777,2015-09-23 02:13:15,How to quickly migrate from one table into another one with different table structure in the same/different cassandra?,"<p>I had one table with more than 10,000,000 records in Cassandra, but for some reason, I want to build another Cassandra table with the same fields and several additional fields, and I will migrate the previous data into it. And now the two tables are in the same Cassandra cluster. </p>

<p>I want to ask how to finish this task in a shortest time?</p>

<p>And If my new table in the different Cassandra, How to do it?</p>

<p>Any advice will be appreciated!  </p>
",<cassandra><cassandra-2.0>,"<p>If you just need to add blank fields to a table, then the best thing to do is use the <code>alter table</code> command to add the fields to the existing table.  Then no copying of the data would be needed and the new fields would show up as <code>null</code> in the existing rows until you set them to something.</p>

<p>If you want to change the structure of the data in the new table, or write it to a different cluster, then you'd probably need to write an application to read each row of the old table, transform the data as needed, and then write each row to the new location.</p>

<p>You could also do this by exporting the data to a csv file, write a program to restructure the csv file as needed, then import the csv file into the new location.</p>

<p>Another possible method would be to use Apache Spark.  You'd read the existing table into an RDD, transform and filter the data into a new RDD, then save the transformed RDD to the new table.  That would only work within the same cluster and would be fairly complex to set up.</p>
",['table']
32821373,32824380,2015-09-28 10:59:57,Cassandra order by and filter through secondary indexes,"<p>My cassandra table structure is as follows:</p>

<pre><code>CREATE TABLE sujata
          ... (ID int, roll_number int, age int, PRIMARY KEY (ID,roll_number));
</code></pre>

<p>I had inserted some of the records where ID acts as a partition for a number of rows. I am performing the following query:</p>

<pre><code>SELECT count(*) FROM sujata WHERE ID=1 ORDER BY roll_number ASC and age=24 Allow Filtering;
</code></pre>

<p>I am getting the following error:</p>

<pre><code>missing EOF at 'and' (...1 ORDER BY roll_number ASC [and] age...)""&gt;
</code></pre>

<p>I don't know if it possible to filter out the results after performing order by. Please suggest where am i missing. Thank you.</p>
",<cassandra><cql>,"<ol>
<li><p>Don't use ALLOW FILTERING.  It doesn't perform or
scale, because it allows you to query Cassandra in ways that it is
not designed to support.</p></li>
<li><p>CQL is interpreting your statement as <code>ORDER BY
roll_number ASC and age=24</code> as you trying to ORDER BY two things. 
The AND belongs in your WHERE clause, and needs to be specified
<em>before</em> ORDER BY.</p></li>
<li><p>Cassandra uses your clustering keys to write your data's
on-disk sort order.  ORDER BY only allows you to flip the sort
direction (ascending vs. descending) of your clustering order.  So
if you have already specified the correct sort order in your table
definition, then you don't need to specify ORDER BY.</p></li>
<li><p>To query your table by both <code>ID</code> and <code>age</code>, you need to
design your PRIMARY KEY with those two columns as the first two. 
You can only query on columns defined in your PRIMARY KEY (secondary
indexes not withstanding), and then only in the same order (you
can't skip keys).  To accomplish this, I created a query table
(<code>sujataByIDAndAge</code>) that looks like this:</p></li>
</ol>

<p>.</p>

<pre><code>CREATE TABLE sujataByIDAndAge (
    ID int, 
    roll_number int, 
    age int, 
PRIMARY KEY (ID,age,roll_number));
</code></pre>

<p>Now after inserting a few rows:</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; INSERT INTO sujatabyidandage  (id, roll_number, age) 
                             VALUES (2, 20, 26);
aploetz@cqlsh:stackoverflow&gt; INSERT INTO sujatabyidandage  (id, roll_number, age) 
                             VALUES (1, 100, 24);
aploetz@cqlsh:stackoverflow&gt; INSERT INTO sujatabyidandage  (id, roll_number, age) 
                             VALUES (1, 110, 24);
aploetz@cqlsh:stackoverflow&gt; INSERT INTO sujatabyidandage  (id, roll_number, age) 
                             VALUES (1, 190, 24);
</code></pre>

<p>Now I can run your query, and I don't need to specify ORDER BY or ALLOW FILTERING:</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; SELECT COUNT(*) FROM sujatabyidandage WHERE ID=1 AND age=24;

 count
-------
     3

(1 rows)
</code></pre>

<p>Also important to mention, is that if result set order is important to you then you must query by the primary keys.  Cassandra only enforces order of results within a partition key (ID in your case).</p>
",['table']
32884757,32891906,2015-10-01 10:03:32,Excessive partitioning (too many tasks) on Apache Spark/Cassandra cluster,"<p>I'm running a simplistic application on Spark/Cassandra cluster. Since moving to a new environment (Spark 1.5 instead of 1.2 and minor Cassandra version upgrade too) substantial performance downgrade was observed (from 4 s. to 1-5 m. for same task and same amounts of data).</p>

<p>After initial investigation it seems, that for exactly same code from spark-driver's perspective, there are many more tasks generated (20+k, where it used to be up to 5) and logs on executor's end also reflect the same situation: </p>

<p>many sequential executions of the same query on different partitions:</p>

<pre><code>...
CassandraTableScanRDD: Fetched 0 rows from x.y for partition 20324 in 0.138 s.
CassandraTableScanRDD: Fetched 0 rows from x.y for partition 20327 in 0.058 s.
CassandraTableScanRDD: Fetched 0 rows from x.y for partition 20329 in 0.053 s.
...
</code></pre>

<p>where it used to be a single one:</p>

<pre><code>CassandraTableScanRDD: Fetched 905 rows from x.y for partition 0 in 2.992 s.
</code></pre>

<p>Since application code is the same, I wonder what could possibly have caused such a difference in partitioning behavior and what can be done to remediate that?</p>

<p>NB! Setup of both environments if different, configuration is not shared/inherited.</p>

<p>Thanks.</p>
",<performance><cassandra><apache-spark>,"<p>The new version of the Spark Cassandra Connector uses a System table inside of more modern Cassandra to estimate split size. This table is updated every (5 minutes currently) although the number of splits you are seeing is extremely large. The value read out of this table is divided by your split size. </p>

<p>If you are using C* less than 2.1.5 this table does not exist and the partitioning will need to be done manually.</p>

<p><a href=""https://github.com/datastax/spark-cassandra-connector/blob/master/doc/FAQ.md#what-does-inputsplitsize_in_mb-use-to-determine-size"" rel=""noreferrer"">https://github.com/datastax/spark-cassandra-connector/blob/master/doc/FAQ.md#what-does-inputsplitsize_in_mb-use-to-determine-size</a></p>

<p>You can manually pass in the number of splits via the ReadConf if you are continuing to see issues. </p>
",['table']
32895197,32895457,2015-10-01 19:17:10,Checking data if its present in a node,"<p>I have a 6 node cluster and I kept the replication factor as 3 and 3 for the two data centers. I inserted a single row as test and later more rows. As the replication factor is 6 I want to check if the data is written into all nodes. How can I individually check if the data is present in that node. Worst option i got is shutting down the remaining 5 nodes and checking select statement from the 1 node and repeating same on all nodes. Is there any better way to check this? THanks for your time.</p>
",<cassandra><datastax>,"<p>You can use <code>nodetool getendpoints</code> for this.  Here is a sample table to keep track of Blade Runners.</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; SELECT * FROm bladerunners;

 id     | type         | datetime                 | data
--------+--------------+--------------------------+---------------------------------------------
 B25881 | Blade Runner | 2015-02-16 18:00:03+0000 | Holden- Fine as long as nobody unplugs him.
 B26354 | Blade Runner | 2015-02-16 18:00:03+0000 |               Deckard- Filed and monitored.

(2 rows)
</code></pre>

<p>Now if I exit back out to my command prompt, I can use <code>nodetool getendpoints</code>, followed by my keyspace, table, and a partition key value.  The list of nodes containing data for that key should be displayed underneath:</p>

<pre><code>aploetz@dockingBay94:~$ nodetool getendpoints stackoverflow bladerunners 'B26354'
127.0.0.1
</code></pre>
",['table']
32912109,32914199,2015-10-02 16:54:03,Cassandra Data Model for Sensor Data - Value | Timestamp,"<p>I'm new to Cassandra and I'm trying to define a data model that fits my requirements.</p>

<p>I have a sensor that collects one value every millisecond and I have to store those data in Cassandra. The queries that I want to perform are:</p>

<p>1) Give me all the sensor values from - to these timestamp values</p>

<p>2) Tell me when this range of values was recorded</p>

<p>I'm not sure if there exist a common schema that can satisfy both queries because I want to perform range queries on both values. For the first query I should use something like: </p>

<pre><code>CREATE TABLE foo (
value text,
timestamp timestamp,
PRIMARY KEY (value, timestamp));
</code></pre>

<p>but then for the second query I need the opposite since I can't do range queries on the partition key without using a token that restricts the timestamp:</p>

<pre><code>CREATE TABLE foo (
value text,
timestamp timestamp,
PRIMARY KEY (timestamp, value));
</code></pre>

<p>So do I need two tables for this? Or there exist another way?
Thanks</p>

<p>PS: I need to be as fast as possible while reading</p>
",<cassandra><data-modeling>,"<blockquote>
  <p>I have a sensor that collects one value every millisecond and I have to store those data in Cassandra.</p>
</blockquote>

<p>The main problem I see here, is that you're going to run into Cassandra's limit of 2 billion col values per partition fairly quickly.  DataStax's Patrick McFadin has a good example for weather station data (<a href=""https://academy.datastax.com/demos/getting-started-time-series-data-modeling"" rel=""nofollow"">Getting Started with Time Series Data Modeling</a>) that seems to fit here.  If I apply it to your model, it looks something like this:</p>

<pre><code>CREATE TABLE fooByTime (
    sensor_id text,
    day text,
    timestamp timestamp,
    value text,
PRIMARY KEY ((sensor_id,day),timestamp)
);
</code></pre>

<p>This will partition on both sensor_id and day, while sorting rows within the partition by timestamp.  So you could query like:</p>

<pre><code>&gt; SELECT * FROM fooByTime WHERE sensor_id='5' AND day='20151002' 
  AND timestamp &gt; '2015-10-02 00:00:00' AND timestamp &lt; '2015-10-02 19:00:00';

 sensor_id | day      | timestamp                | value
-----------+----------+--------------------------+-------
         5 | 20151002 | 2015-10-02 13:39:22-0500 |    24
         5 | 20151002 | 2015-10-02 13:49:22-0500 |    23
</code></pre>

<p>And yes, the way to model in Cassandra, is to have one table for each query pattern.  So your second table where you want to range query on value might look something like this:</p>

<pre><code>CREATE TABLE fooByValues (
    sensor_id text,
    day text,
    timestamp timestamp,
    value text,
PRIMARY KEY ((sensor_id,day),value)
);
</code></pre>

<p>And that would support queries like:</p>

<pre><code>&gt; SELECT * FROm foobyvalues WHERE sensor_id='5' 
  AND day='20151002' AND value &gt; '20' AND value &lt; '25';

 sensor_id | day      | value | timestamp
-----------+----------+-------+--------------------------
         5 | 20151002 |    22 | 2015-10-02 14:49:22-0500
         5 | 20151002 |    23 | 2015-10-02 13:49:22-0500
         5 | 20151002 |    24 | 2015-10-02 13:39:22-0500
</code></pre>
",['table']
32938333,32948347,2015-10-04 20:57:16,ranking where the score is a function of time,"<p>I would like to migrate my db which is currently on mysql to C*. At the moment I have a table that I have trouble imagining how to ""migrate"" it.</p>

<p><strong>Entity</strong></p>

<ul>
<li>Id</li>
<li>score(s)</li>
<li>hotscore</li>
</ul>

<p>Where hotscore is f(s,d) = log10 + (s.t/45000). S is score and t is timestamp since epoch.</p>

<p>Essentially what I would be looking into querying is the top 20 of that entity. With mysql and a cron job I'm updating the hotscore every minute. For that reason hot score cannot be suited for a partition key. I'm trying to see if I can make this happen before moving to c*. As far as I know a primary key like <code>(id, hotscore)</code> wouldn't be good because it means C* has to scan every entry.</p>
",<cassandra><cassandra-2.0>,"<p>You'll soon be able to handle this use case with materialized views when Cassandra 3.0 is released.</p>

<p>See an example of ordering rows in a materialized view <a href=""http://www.datastax.com/dev/blog/new-in-cassandra-3-0-materialized-views"" rel=""nofollow noreferrer"">here</a> and <a href=""https://stackoverflow.com/questions/32014367/cassandra-list-10-most-recently-modified-records/32017197#32017197"">here</a>.</p>

<p>The way it works is in your base table you don't use the score as a clustering column, but you do use it as a clustering column in the materialized view.  Then when you update the base table, the ordering in the view is automatically updated.</p>
",['table']
32944030,34257027,2015-10-05 08:17:43,How to work with and query dynamic column families in Phantom for Cassandra?,"<p>I have recently started working with heavy and massive data which also needs to go through regular transaction.</p>

<p>Choosing Cassandra, my data model uses dynamic columns. I understand that with CQL one can alter tables and insert or query columns to get required data.</p>

<p>However, I was using Phantom client with Scala for Cassandra and reading through the documentation I could not find a way to write to or query from dynamic column families.</p>

<p>Given that we use case classes, how can one work with dynamic columns with Cassandra in Scala?</p>
",<scala><cassandra><phantom-dsl>,"<p>I would suggest that you not dynamically alter table schemas as part of your data model. Cassandra is a row oriented database with partitioning and clustering of rows within partitions. So whatever you are trying to represent by adding or removing columns would be better handled by setting values in a fixed set of columns.</p>

<p>Although Cassandra allows table definitions to be altered to add and remove columns, this would normally be done only when adding a new feature to an application, so you would have an operator manually alter the schema, and then use modified application code to make use of the new schema.</p>

<p>I consider it dangerous for a client application to modify the schema by creating or altering tables since you run the risk of having multiple clients make changes at the same time.</p>
",['table']
32979220,32984576,2015-10-06 20:33:04,Cassandra vs MongoDB - Storing JSON data with previously unknown keys?,"<p>I'm trying to integrate a NoSQL database to store JSON data, rather than a SQL database to store JSON data (A column that stores a JSON object).</p>

<p>For MongoDB, I can insert a JSON file just by doing: </p>

<pre><code>document = &lt;JSON OBJECT&gt;
collection.insert(document)
</code></pre>

<p>However, for Cassandra, according to this webpage: <a href=""http://www.datastax.com/dev/blog/whats-new-in-cassandra-2-2-json-support"" rel=""noreferrer"">http://www.datastax.com/dev/blog/whats-new-in-cassandra-2-2-json-support</a></p>

<p>It cannot be schema less, meaning that I would need to create a table beforehand:</p>

<pre><code>CREATE TABLE users (
    id text PRIMARY KEY,
    age int,
    state text
);
</code></pre>

<p>And then insert the data:</p>

<pre><code>INSERT INTO users JSON '{""id"": ""user123"", ""age"": 42, ""state"": ""TX""}';
</code></pre>

<p>The issue is that I want to try and use Cassandra, I've just completed DataStax's tutorial, but it seems that I would need to know the keys of the JSON data beforehand, which is not possible.</p>

<p>Or should I alter the table when there is a new data column if there is an unknown key? That doesn't sound like a very good design decision.</p>

<p>Can anyone point me to the right direction? Thanks</p>
",<json><mongodb><cassandra>,"<p>This JSON support is very misleading - it's JSON in Cql support, not in storage.</p>

<blockquote>
  <p>Or should I alter the table when there is a new data column if there
  is an unknown key? That doesn't sound like a very good design
  decision.</p>
</blockquote>

<p>Indeed this isn't good decision - your fields in JSON can have different types across entities - one column name couldn't serve it all. Also, adding new field requires schema propagation across your cluster, so the very first insert (which would contain of alter table + insert data) would be very slow.</p>

<p>Cassandra doesn't give you any built in mechanism, but what you can do, is to put whole JSON in one field and expose needed properties in additional separate columns. For example:</p>

<pre><code>CREATE TABLE users (
    id text PRIMARY KEY,
    json text, //in json age and state
    age int //explicit duplicated property - if you need e.g. index
);
</code></pre>

<p>BTW. AFAIK Cassandra used to support your case long time ago, but now it's more 'strongly typed'.</p>
",['table']
32998185,33265112,2015-10-07 16:58:37,cassandra spark connector read performance,"<p>I have some Spark experience but just starting out with Cassandra. I am trying to do a very simple read and getting really bad performance -- can't tell why. Here is the code I am using:</p>
<pre><code>sc.cassandraTable(&quot;nt_live_october&quot;,&quot;nt&quot;)
  .where(&quot;group_id='254358'&quot;)
  .where(&quot;epoch &gt;=1443916800 and epoch&lt;=1444348800&quot;)
  .first
</code></pre>
<p><a href=""https://i.stack.imgur.com/GGke8.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GGke8.jpg"" alt=""enter image description here"" /></a></p>
<p>all 3 params are part of the key on the table:</p>
<blockquote>
<p>PRIMARY KEY (<strong>group_id, epoch,</strong> group_name, auto_generated_uuid_field)
) WITH CLUSTERING ORDER BY (epoch ASC, group_name ASC, auto_generated_uuid_field ASC)</p>
</blockquote>
<p>And the output I see from my driver is like this:</p>
<blockquote>
<p>15/10/07 15:05:02 INFO CassandraConnector: Connected to Cassandra
cluster: shakassandra 15/10/07 <strong>15:07</strong>:02 ERROR Session: Error
creating pool to attila./198.xxx:9042
com.datastax.driver.core.ConnectionException:
[attila./198.xxx:9042] Unexpected error
during transport initialization
(com.datastax.driver.core.OperationTimedOutException: [attila
/198.xxx:9042] Operation timed out)</p>
<p>15/10/07 15:07:02 INFO SparkContext: Starting job: take at
CassandraRDD.scala:121</p>
<p>15/10/07 15:07:03 INFO BlockManagerInfo:
Added broadcast_5_piece0 in memory on
<strong>osd09</strong>:39903 (size: 4.8 KB, free: 265.4 MB)</p>
<p>15/10/07 <strong>15:08:23</strong> INFO TaskSetManager: Finished task 0.0 in stage 6.0
(TID 8) in 80153 ms on <strong>osd09</strong> (1/1)</p>
<p>15/10/07 15:08:23 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 8)
in 80153 ms on osd09 (1/1)</p>
<p>15/10/07 15:08:23
INFO DAGScheduler: ResultStage 6 (take at CassandraRDD.scala:121)
finished in 80.958 s 15/10/07 15:08:23 INFO TaskSchedulerImpl: Removed
TaskSet 6.0, whose tasks have all completed, from pool</p>
<p>15/10/07 15:08:23 INFO DAGScheduler: Job 5 finished: take at
CassandraRDD.scala:121, took <strong>81.043413</strong> s</p>
</blockquote>
<p>I expect this query to be really fast yet it's taking over a minute. A few things jump out at me</p>
<ol>
<li>It takes almost two minutes to get the session error -- I pass the IPs of 3 nodes to Spark Cassandra connector -- is there a way to tell it to skip failed connections faster?</li>
<li>The task gets sent to a Spark worker which is not a Cassandra node -- this seems pretty strange to me -- is there a way to get information as to why the scheduler chose to send the task to a remote node?</li>
<li>Even if the task was sent to a remote node, the Input Size(Max) on that worker shows up as 334.0 B / 1 but the executor time is 1.3 min (see picture). This seems really slow -- I would expect time to be spent on deserialization, not compute...
<a href=""https://i.stack.imgur.com/JiwVQ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JiwVQ.jpg"" alt=""enter image description here"" /></a></li>
</ol>
<p>Any tips on how to debug this, where to look for potential problems much appreciated. Using Spark 1.4.1 with connector 1.4.0-M3, cassandra ReleaseVersion: 2.1.9, all defaults on tuneable connector params</p>
",<apache-spark><cassandra><spark-cassandra-connector>,"<p>I think the problem lays into distribution of data between partitions. Your table has one cluster (partitioning) key - groupId, epoch is a clustering column only. Data distributes on cluster nodes only by groupId, so you have a huge partition with groupId='254358' on one node on the cluster. 
When you run your query Cassandra reaches very fast partition with groupId='254358' and then filter all rows to find records with epoch between 1443916800 and 1444348800. If there are a lot of rows the query will be really slow. Actually this query is not distributed it will always run on one node.</p>

<p>Better practice extract date or even hour and add it as partitioning key, in your case something like</p>

<pre><code>PRIMARY KEY ((group_id, date), epoch, group_name, auto_generated_uuid_field) 
WITH CLUSTERING ORDER BY (epoch ASC, group_name ASC, auto_generated_uuid_field ASC)
</code></pre>

<p>To verify my hypothesis you can run your current query in cqlsh with turning on tracing read <a href=""http://docs.datastax.com/en/cql/3.0/cql/cql_reference/tracing_r.html"" rel=""nofollow"">here</a> how to do it. So the problem has nothing in connect with Spark.</p>

<p>About error and time to get it, everything is fine because you receive error after timeout happened. </p>

<p>Also I remember recommendations of spark-cassandra-connector to place Spark slaves joint to Cassandra nodes exactly to distribute queries by partitioning key. </p>
",['table']
33027148,33030269,2015-10-08 22:43:49,"""Invalid operator IN for PRIMARY KEY"" when updating multiple clustering columns using IN clause","<pre><code>CREATE TABLE IF NOT EXISTS my_counters (
  partition_key text, 
  clustering_key text,
  count counter,
  PRIMARY KEY ((partition_key), clustering_key)
);
</code></pre>

<p>I now want to increment counters under two clustering keys.
According to Update spec, this should be possible:
<a href=""http://docs.datastax.com/en/cql/3.1/cql/cql_reference/update_r.html"" rel=""nofollow"">http://docs.datastax.com/en/cql/3.1/cql/cql_reference/update_r.html</a></p>

<p>But I am getting ""Invalid operator IN for PRIMARY KEY..."" error</p>

<pre><code>UPDATE my_counters SET count = count + 1 WHERE partition_key = ? AND clustering_key IN (?, ?)
</code></pre>

<p>Is this a specific limitation for counters? I know i can write a counter batch using one clustering key per query, but I would rather not.</p>
",<cassandra><cql><cql3>,"<p>From <a href=""http://docs.datastax.com/en/cql/3.3/cql/cql_reference/update_r.html"" rel=""nofollow"">http://docs.datastax.com/en/cql/3.3/cql/cql_reference/update_r.html</a>
The IN relation is supported only for the last column of the partition key.</p>

<p>Also an update cannot be done without specifying the complete PRIMARY KEY (partition key + clustering key) </p>

<pre><code>create table spending_by_country_state (country text,state text,amount int, primary key ((country,state)));

select * from spending_by_country_state;

 country | state     | amount
---------+-----------+--------
   India | Karnataka |  20000
   India |    Kerala |  10000

cqlsh:test&gt; update spending_by_country_state set amount = 10001 where country = 'India' and state in ('Karnataka','Kerala');
cqlsh:test&gt; select * from spending_by_country_state;

 country | state     | amount
---------+-----------+--------
   India | Karnataka |  10001
   India |    Kerala |  10001

(2 rows)
</code></pre>
",['table']
33154231,33156250,2015-10-15 16:52:30,CQL no viable alternative at input '(' error,"<p>I have a issue with my CQL and cassandra is giving me <code>no viable alternative at input '(' (...WHERE id = ? if [(]...)</code> error message. I think there is a problem with my statement. </p>

<pre><code>UPDATE &lt;TABLE&gt; USING TTL 300 
  SET &lt;attribute1&gt; = 13381990-735b-11e5-9bed-2ae6d3dfc201
  WHERE &lt;attribute2&gt; = dfa2efb0-7247-11e5-a9e5-0242ac110003 
    IF (&lt;attribute1&gt; = null OR &lt;attribute1&gt; = 13381990-735b-11e5-9bed-2ae6d3dfc201) AND &lt;attribute3&gt; = 0; 
</code></pre>

<p>Any idea were the problem is in the statement about?</p>
",<cassandra><cql><database><nosql>,"<p>It would help to have your complete table structure, so to test your statement I made a couple of educated guesses.</p>

<p>With this table:</p>

<pre><code>CREATE TABLE lwtTest (attribute1 timeuuid, attribute2 timeuuid PRIMARY KEY, attribute3 int);
</code></pre>

<p>This statement works, as long as I don't add the lightweight transaction on the end:</p>

<pre><code>UPDATE lwttest USING TTL 300 SET attribute1=13381990-735b-11e5-9bed-2ae6d3dfc201 
WHERE attribute2=dfa2efb0-7247-11e5-a9e5-0242ac110003;
</code></pre>

<p>Your lightweight transaction...</p>

<pre><code>IF (attribute1=null OR attribute1=13381990-735b-11e5-9bed-2ae6d3dfc201) AND attribute3 = 0;
</code></pre>

<p>...has a few issues.</p>

<ul>
<li>""null"" in Cassandra is not similar (at all) to its RDBMS counterpart.  Not every row needs to have a value for every column.  Those CQL rows without values for certain column values in a table will show ""null.""  But you cannot query by ""null"" since it isn't really there.</li>
<li>The OR keyword does not exist in CQL.</li>
<li>You cannot use extra parenthesis to separate conditions in your WHERE clause or your lightweight transaction.</li>
</ul>

<p>Bearing those points in mind, the following UPDATE and lightweight transaction runs without error:</p>

<pre><code>UPDATE lwttest USING TTL 300 SET attribute1=13381990-735b-11e5-9bed-2ae6d3dfc201 
WHERE attribute2=dfa2efb0-7247-11e5-a9e5-0242ac110003
IF attribute1=13381990-735b-11e5-9bed-2ae6d3dfc201 AND attribute3=0;

 [applied]
-----------
     False
</code></pre>
",['table']
33156461,33157571,2015-10-15 19:02:50,What does number in Cassandra database tables denote?,"<p>When we create a new table in cassandra database it automatically creates a folder for it in data location. While creating folder it also attach alphanumeric number with it after hyphen(-). What does that number denote? </p>
",<cassandra><cassandra-2.0><cassandra-cli>,"<p>It's just a unique identifier(UUID). This helps avoid issues with Drop-Recreate operations where a newly created table has the same name as an old table. This way they will have unique folders so if a drop message gets lost or whatnot the old data won't get restored into the new table. </p>
",['table']
33174885,33210893,2015-10-16 16:04:48,Paging in the right way to import data in R from Cassandra,"<p>I have a problem that is making me mad. I am trying to import data in R from Cassandra.</p>

<pre><code>    library(RJDBC)
    library(glmnet)


     cassandra_conn &lt;- dbConnect(JDBC(""org.apache.cassandra.cql.jdbc.CassandraDriver"",list.files(""/data/data/LIBS"",pattern=""jar$"",full.names=T),identifier.quote=""`""),""jdbc:cassandra://ciccio01:9160/banza"")

     name &lt;- ""sector""
     sect &lt;- order_to_campaign[[name]]

     cl_imp_ &lt;- dbGetQuery(cassandra_conn, paste0(""select * from adv where order_id = '"",sett[1], ""'""))

     for(i in 2:length(sect))
     {
        print(sect[i])
        cl_imp_ &lt;- rbind(cl_imp_ , dbGetQuery(cassandra_conn, paste0(""select * from adv where order_id = '"",sect[i], ""'"")))  
     }
</code></pre>

<p>Now, the first query works fine. But then, in the for loop, I always get a TimeOutException() and I can't understand why. I Googled everything, without being able to understand to what that exception is due. 
Finally, together with a colleague of mine, we thought that it might be due to paging problems in R. The way we solved this is writing the query in the shell, and then saving the data in another file and finally reading that file in R. </p>

<p>My question is: is it possible that this is the only way to do it? I think there must be a much easier way to solve this problem using very simple queries.</p>

<p>This is the schema of my table:</p>

<pre><code>    CREATE TABLE adv (
    user_id text,
    order_id text,
    advertiser_id text,
    PRIMARY KEY (user_id, order_id)
    ) WITH CLUSTERING ORDER BY (order_id ASC);

    CREATE INDEX adv_order_id_idx ON mytable (order_id);
</code></pre>

<p>Any help is very much appreciated.</p>

<p>Davide</p>
",<r><cassandra>,"<p>As I thought, you are using a clustering column. In your primary key, <em>user_id</em>  is your partition key and <em>order_id</em>  is your clustering column. And you created an index on your table to be able to query over your clustering column.</p>

<p>In most cases, <strong>indexes are not a good idea</strong> !</p>

<p>Basically, NoSql database is not well designed to manage indexes over large table. So try to avoid that. A good alternative is to create sort of an index table by hand. Each time you insert in <em>adv</em>, dont forget to insert in <em>adv_idx</em> as well (in two different queries). It would look like this :</p>

<pre><code>CREATE TABLE adv (
    user_id text,
    order_id text,
    advertiser_id text,
    PRIMARY KEY (user_id, order_id)
) WITH CLUSTERING ORDER BY (order_id ASC);

CREATE TABLE adv_idx (
    order_id text,
    user_id text,
    PRIMARY KEY (order_id, user_id)
)
</code></pre>

<p>To retrieve information relative to <em>order_id</em> you query <em>adv_idx</em> and then foreach returned <em>user_id</em> you query <em>adv</em>. Cassandra dont have performance issue any more. Nevertheless the number of queries from the client side is bigger and it is longer to process.</p>

<p>Another solution is to add some redundancy :</p>

<pre><code>CREATE TABLE adv (
    user_id text,
    order_id text,
    advertiser_id text,
    PRIMARY KEY (user_id, order_id)
) WITH CLUSTERING ORDER BY (order_id ASC);

CREATE TABLE adv_by_order (
    order_id text,
    user_id text,
    advertiser_id text,
    PRIMARY KEY (order_id, user_id)
)
</code></pre>

<p>Ok, your database is twice bigger now. But your performance is much better !</p>

<p>I tend to say that <strong>redundancy is fine !</strong></p>
",['table']
33212414,33389204,2015-10-19 10:45:12,Cassandra - What is the reasonable maximum number of tables?,"<p>I am new to Cassandra. As I understand the maximum number of tables that can be stored per keyspace is Integer.Max_Value. However, what are the implications from the performance perspective (speed, storage, etc) of such a big number of tables? Is there any recommendation regarding that?</p>
",<database><cassandra><key-value-store>,"<p>While there are legitimate use cases for having lots of tables in Cassandra, they are rare. Your use case might be one of them, but make sure that it is. Without knowning more about the problem you're trying to solve, it's obviously hard to give guidance. Many tables will require more resources, obviously. How much? That depends on the settings, and the usage.</p>

<p>For example, if you have a thousand tables and write to all of them at the same time there will be contention for RAM since there will be memtables for each of them, and there is a certain overhead for each memtable (how much depends on which version of Cassandra, your settings, etc.).</p>

<p>However, if you have a thousand tables but don't write to all of them at the same time, there will be less contention. There's still a per table overhead, but there will be more RAM to keep the active table's memtables around.</p>

<p>The same goes for disk IO. If you read and write to a lot of different tables at the same time the disk is going to do much more random IO.</p>

<p>Just having lots of tables isn't a big problem, even though there is a limit to how many you can have – you can have as many as you want provided you have enough RAM to keep the structures that keep track of them. Having lots of tables and reading and writing to them all at the same time will be a problem, though. It will require more resources than doing the same number of reads and writes to fewer tables.</p>
",['table']
33242811,33263929,2015-10-20 17:15:46,Migrate Datastax Enterprise Cassandra to Apache Cassandra or Datastax Community?,"<p>I have a large, but simple Cassandra database on a Datastax 4.6 cluster.  The license renewal is prohibitive for this very simple use case and I am trying to migrate to either a straight Apache or Datastax Comunity version.  First is it possible to do an inline update?  </p>

<p>I have altered all the keyspaces to remove the ""EverywhereStrategy"" replication strategy but I still get an error that the DSC version of cassandra I'm trying to get to join the cluster doesn't support it.  I'm using Like Cassandra versions (2.0.16) and most other things seem to be close.</p>

<p><code>java.lang.RuntimeException: org.apache.cassandra.exceptions.ConfigurationException: Unable to find replication strategy class 'org.apache.cassandra.locator.EverywhereStrategy'
</code></p>

<p>If it's not possible to do an inline upgrade what would be the best strategy to migrate a decent size (30 node, 150Tb) cluster?</p>
",<cassandra><migration><datastax>,"<p>So to make this work you have to extract any of the DSE features that you may have on any of your tables.  </p>

<p>This meant I had to change the replication strategy on the dse_system table from EverywhereStrategy to SimpleStrategy with RF=3 (or almost anything after conversion you can drop this keyspace) The error message was:</p>

<p><code>java.lang.RuntimeException: org.apache.cassandra.exceptions.ConfigurationException: Unable to find replication strategy class 'org.apache.cassandra.locator.EverywhereStrategy'</code></p>

<p>I Also had to drop the unused CFS keyspaces.  We never used the hadoop/CFS integration so we had nothing in these keyspaces anyway.  I didn't capture the error for this.</p>

<p>We did have a solr index on a table we were testing on this cluster about a year ago so I had to drop this columnfamily.  The error message was:</p>

<p><code>java.lang.RuntimeException: java.lang.ClassNotFoundException: com.datastax.bdp.search.solr.Cql3SolrSecondaryIndex</code></p>

<p>There may be other incompatibilities if you use other features of Datastax Enterprise that you would have to remove, but this was enough for me to get the migration working.</p>
",['table']
33262463,33275369,2015-10-21 14:47:06,DataStax C# Driver: how to create table with mapping of IEnumerable to set?,"<p>I want to use the Table.CreateIfNotExists() to create my schema dynamically, but I can't figure out how to make it create a ""set"" column type for an IEnumerable instead of ""list"".</p>

<p>Is this possible without using CQL statements to create the table?</p>

<pre><code>For&lt;ClassWithSet&gt;()
            .TableName(""withset"")
            .PartitionKey(u =&gt; u.Id)
            .Column(u =&gt; u.SomeStrings, cm =&gt; cm.WithName(""somestrings"").WithDbType&lt;IEnumerable&lt;string&gt;&gt;());

var table = new Table&lt;ClassWithSet&gt;(session);
table.CreateIfNotExists();
</code></pre>
",<c#><cassandra><datastax>,"<p>You should use <code>SortedSet&lt;T&gt;</code> as db type:</p>

<pre><code>For&lt;ClassWithSet&gt;()
            .TableName(""withset"")
            .PartitionKey(u =&gt; u.Id)
            .Column(u =&gt; u.SomeStrings, 
                cm =&gt; cm.WithName(""somestrings"").WithDbType&lt;SortedSet&lt;string&gt;&gt;());

var table = new Table&lt;ClassWithSet&gt;(session);
table.CreateIfNotExists();
</code></pre>
",['table']
33269706,33273306,2015-10-21 21:36:22,table definition statement for cassandra for range queries?,"<p>Here is the table data</p>

<pre><code>video_id uuid
user_id timeuuid
added_year int
added_date timestamp
title text
description text
</code></pre>

<p>I want to construct table based on the following query</p>

<p><code>select * from video_by_year where added_year&lt;2013;</code></p>

<p>create table videos_by_year (</p>

<pre><code>video_id uuid
user_id timeuuid
added_year int
added_date timestamp
title text
description text
PRIMARY KEY ((added_year) added_year)
</code></pre>

<p>) ;</p>

<p><strong>NOTE</strong>: I have used <code>added_year</code> as both primary key and clustering key which is not correct I suppose.</p>
",<cassandra><data-modeling><cassandra-2.0>,"<p>So one of the issues with data modeling in cassandra is that the first component - the partition key - must use ""="". The reason for this is pretty clear if you realize what cassandra's doing - it uses that value, hashes it (md5 or murmur3), and uses that to determine which servers in the cluster own that partition. </p>

<p>For that reason, you can't use an inequality - it would require scanning every row in the cluster. </p>

<p>If you need to get videos added before 2013, consider a system where you use some portion of the date as partition key, and then SELECT from each of those date 'buckets', which you can do asynchronously and in parallel. For example:</p>

<pre><code>create table videos_by_year (
 video_id uuid
 user_id timeuuid
 added_date_bucket text
 added_date timestamp
 title text
 description text
 PRIMARY KEY ((added_date_bucket), added_date, video_id)
) ;
</code></pre>

<p>I used text for added_date_bucket so you could use 'YYYY', or 'YYYY-MM' or similar. Note that depending on how quickly you add videos to the system, you may even want 'YYYY-MM-DD' or 'YYYY-MM-DD-HH:ii:ss', because you'll hit a practical limit of a few million videos per bucket. </p>

<p>You could get clever and have the video_id be a timeuuid, then you get added_date and video_id in a single column. </p>
",['table']
33281173,33283621,2015-10-22 12:34:25,retrieve rows by date: cassandra,"<p>In the data there is one column <em>timestamp</em> with pattern:</p>

<p><code>YEAR-MM-DD hour:minute:second</code></p>

<p>I want to retrieve rows where this timestamp column date e.g. is > 2015-07-01</p>

<p>I dont want to compare time, just date.
I tried:</p>

<p><code>SELECT * FROM table_name where timestamp &gt;= '2015-10-01'  limit 10;</code></p>

<p>But does not give desired output.</p>
",<cassandra><cql>,"<p>One problem here, is that Cassandra requires an EQ condition (<code>IN</code> also works, but I don't recommend that) on the 1st partition key before allowing any other types of conditions (&lt;, >, etc).  Other solutions like using ALLOW FILTERING or an index may ""work,"" but they will not perform at scale, and also will not return the rows in any kind of order.</p>

<p>Cassandra can only enforce a sort order <em>within</em> a partition key.  For example, this means that you can select where rows with a certain date/times value in their first <a href=""http://docs.datastax.com/en/cql/3.1/share/glossary/gloss_clustering_column.html"" rel=""nofollow"">clustering column</a>, given a certain, specific value for the <a href=""http://docs.datastax.com/en/cql/3.1/share/glossary/gloss_partition_key.html"" rel=""nofollow"">partition key</a>.</p>

<p>Example:</p>

<pre><code>CREATE TABLE stackoverflow.timestamptest (
    userid text,
    activetime timestamp,
    value text,
    PRIMARY KEY (userid, activetime)
) WITH CLUSTERING ORDER BY (activetime ASC)

aploetz@cqlsh:stackoverflow&gt; SELECT * FROm timestamptest 
                             WHERE userid='a' AND activetime &gt; '2015-09-25';                
 userid | activetime               | value
--------+--------------------------+---------
      a | 2015-09-25 06:33:33-0500 |  value1
      a | 2015-09-25 06:34:33-0500 |  value2
      a | 2015-10-22 09:26:00-0500 |  value3

(3 rows)
</code></pre>

<p>Basically, you need to further partition your table in a way that makes sense for your application and query requirements.</p>

<p>This tends to be a confusing issue for many Cassandra dev, so I have included a link to an article that I wrote for DataStax earlier this year: <a href=""http://www.planetcassandra.org/blog/we-shall-have-order/"" rel=""nofollow"">We Shall Have Order!</a></p>
",['table']
33329494,33423348,2015-10-25 12:08:45,Spark JoinWithCassandraTable on TimeStamp partition key STUCK,"<p>I'm trying to filter on a small part of a huge C* table by using:</p>

<pre><code>    val snapshotsFiltered = sc.parallelize(startDate to endDate).map(TableKey(_)).joinWithCassandraTable(""listener"",""snapshots_tspark"")

    println(""Done Join"")
    //*******
    //get only the snapshots and create rdd temp table
    val jsons = snapshotsFiltered.map(_._2.getString(""snapshot""))
    val jsonSchemaRDD = sqlContext.jsonRDD(jsons)
    jsonSchemaRDD.registerTempTable(""snapshots_json"")
</code></pre>

<p>With: </p>

<pre><code>    case class TableKey(created: Long) //(created, imei, when)--&gt; created = partititon key | imei, when = clustering key
</code></pre>

<p>And the cassandra table schema is:</p>

<pre><code>CREATE TABLE listener.snapshots_tspark (
created timestamp,
imei text,
when timestamp,
snapshot text,
PRIMARY KEY (created, imei, when) ) WITH CLUSTERING ORDER BY (imei ASC, when ASC)
AND bloom_filter_fp_chance = 0.01
AND caching = '{""keys"":""ALL"", ""rows_per_partition"":""NONE""}'
AND comment = ''
AND compaction = {'min_threshold': '4', 'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32'}
AND compression = {'sstable_compression': 'org.apache.cassandra.io.compress.LZ4Compressor'}
AND dclocal_read_repair_chance = 0.1
AND default_time_to_live = 0
AND gc_grace_seconds = 864000
AND max_index_interval = 2048
AND memtable_flush_period_in_ms = 0
AND min_index_interval = 128
AND read_repair_chance = 0.0
AND speculative_retry = '99.0PERCENTILE';
</code></pre>

<p>The problem is that the process freezes after the println done with no errors on spark master ui.</p>

<pre><code>[Stage 0:&gt;                                                                                                                                (0 + 2) / 2]
</code></pre>

<p>Won`t the Join work with timestamp as the partition key? Why it freezes?</p>
",<mysql><scala><cassandra><apache-spark><datastax-enterprise>,"<p>By using:</p>

<pre><code>sc.parallelize(startDate to endDate)
</code></pre>

<p>With the startData and endDate as Longs generated from Dates by the format:</p>

<pre><code>(""yyyy-MM-dd HH:mm:ss"")
</code></pre>

<p>I made spark to build a huge array (100,000+ objects) to join with C* table and it didn't stuck at all- C* worked hard to make the join happen and return the data. </p>

<p>Finally, I changed my range to:</p>

<pre><code>case class TableKey(created_dh: String)
val data = Array(""2015-10-29 12:00:00"", ""2015-10-29 13:00:00"", ""2015-10-29 14:00:00"", ""2015-10-29 15:00:00"")
val snapshotsFiltered = sc.parallelize(data, 2).map(TableKey(_)).joinWithCassandraTable(""listener"",""snapshots_tnew"")
</code></pre>

<p>And it is ok now.</p>
",['table']
33348633,33427270,2015-10-26 14:46:24,Filter on the C* side - push down filter/where range queries to C* from Spark,"<p>I working on spark 1.2.1 with datastax/spark-cassandra-connector and C* table filled with 1B+ rows (datastax-enterprise dse 4.7.0). I need to perform a range filter/where query on time stamp parameter.</p>

<p>What is the best way to do it without loading the whole 1B+ rows table to sparks memory (it could take hours to finish) and practically push the query back to C*?</p>

<p>Using rdd with JoinWithCassandraTable or using data frame with pushdown?
Is there something else?</p>
",<mysql><cassandra><apache-spark><datastax-enterprise><spark-cassandra-connector>,"<p>JoinWithCassandraTable turned to be the best solution in my case. I have learned a lot from this post: <a href=""http://www.datastax.com/dev/blog/zen-art-spark-maintenance"" rel=""nofollow noreferrer"">http://www.datastax.com/dev/blog/zen-art-spark-maintenance</a> and post an answer to the linked question: <a href=""https://stackoverflow.com/questions/33329494/spark-joinwithcassandratable-on-timestamp-partition-key-stuck?lq=1"">Spark JoinWithCassandraTable on TimeStamp partition key STUCK</a></p>

<p>It is all about building your C* table in the right way (extra important to choose good partition keys) for your future queries.</p>
",['table']
33377841,33388341,2015-10-27 20:27:22,Writing to two cassandra tables with time overlap,"<p>I am writing to two cassandra tables, the tables have different keyspaces. I am wondering about how the write actually happens.</p>

<p>I see this explanation at: <a href=""https://academy.datastax.com/demos/brief-introduction-apache-cassandra"" rel=""nofollow"">https://academy.datastax.com/demos/brief-introduction-apache-cassandra</a></p>

<blockquote>
  <p>Cassandra is well known for its impressive performance in both reading
  and writing data. Data is written to Cassandra in a way that provides
  both full data durability and high performance. Data written to a
  Cassandra node is first recorded in an on-disk commit log and then
  written to a memory-based structure called a memtable. When a
  memtable’s size exceeds a configurable threshold, the data is written
  to an immutable file on disk called an SSTable. Buffering writes in
  memory in this way allows writes always to be a fully sequential
  operation, with many megabytes of disk I/O happening at the same time,
  rather than one at a time over a long period. This architecture gives
  Cassandra its legendary write performance</p>
</blockquote>

<p>But this does not explain what happens if I write to two tables in overlapping time period.</p>

<p>Let's say I am writing to <code>Table 1</code> and <code>Table 2</code> at the same time. The entries that I want to write would still be stored in the same <code>memtable</code>, correct? They would essentially be mixed, right?</p>

<p>Let's say I am writing 100,000,000 entries for <code>Table 1</code> and 10 minutes later I started to write entries 100 for <code>Table 2</code>. The 100 for <code>Table 2</code> would still have to wait for entries for <code>Table 1</code> to be processed, since they are sharing the same <code>memtable</code> right?</p>

<p>Is my understanding about how <code>memtable</code> is shared correct? Is there a way for different keyspaces to have their own <code>memtable</code>. For example, if I really want to make sure that entries for <code>Table 2</code> get written without a delay, is that possible?</p>

<p>.</p>
",<cassandra>,"<p>Each table have its own memtable. Cassandra does not mix things. That is why it can easily and efficiently flush data on the disk when memtables total space is full.</p>

<p>This <a href=""http://docs.datastax.com/en/cassandra/2.0/cassandra/dml/dml_write_path_c.html#concept_ds_wt3_32w_zj__dml-flushing"" rel=""nofollow"">Datastax document</a> is a good summary of how writing in Cassandra is performed from commitlog to sstable and compaction.</p>
",['table']
33387454,33421339,2015-10-28 09:35:24,Is there a way to get random rows each time if the data does not change in Cassandra like MySQL RAND(),"<pre>
CREATE TABLE users (
 userId uuid,
 firstname varchar,
 mobileNo varchar,
 PRIMARY KEY (userId)
);
</pre>

<pre>
CREATE TABLE users_by_firstname (
 userId uuid,
 firstname varchar,
 mobileNo varchar,
 PRIMARY KEY (firstname,userId)
);
</pre>

<p>I have 100 rows in these tables. I want to get randomly selected 10 rows each time. </p>

<p>In MySQL</p>

<blockquote>
  <blockquote>
    <p>select * from users order by RAND() limit 10;</p>
  </blockquote>
</blockquote>

<p>In Cassandra</p>

<blockquote>
  <blockquote>
    <p>select * from users limit 10;<br>
    select * from users_by_firstname limit 10;</p>
  </blockquote>
</blockquote>

<p>But from 1st table I would get the static 10 rows sorted by the generated hash of the partition key (userId).</p>

<p>From the second one I would get the static 10 rows sorted by userId.
But it will not be random if the data does not change. </p>

<p>Is there any way to get random rows each time in Cassandra.</p>

<p>Thanks<br>
Chaity</p>
",<cassandra><cassandra-2.0><cql3><cqlsh>,"<p>It's not possible to archive this directly. There are possibilities to emulate this (this solution is not really random, but you should receive different values), but it's not really a perfect idea.</p>

<p>What you could do is, create a random value in the cassandra token range -2^63 - 2^64. With this random value you can perform such a query:</p>

<blockquote>
  <p>select * from users_by_firstname where token(userId) > #generated_value# limit 10;</p>
</blockquote>

<p>Using this method you can define a random 'starting point' from where you can receive 10 users. As I said, this method is not perfect and it certainly needs some thoughts on how to generate the random token. An edge case could be, that your random value is so far on one side of the ring, that you would receive less than 10 values.</p>

<p>Here is a short example:</p>

<p>Lets say you have a users table with the following users:</p>

<pre><code> token(uuid)          | name
----------------------+---------
 -2540966642987085542 |    Kate
 -1621523823236117896 | Pauline
 -1297921881139976049 |  Stefan
  -663977588974966463 |    Anna
  -155496620801056360 |    Hans
   958005880272148645 |     Max
  3561637668096805189 |    Doro
  5293579765126103566 |    Paul
  8061178154297884044 |   Frank
  8213365047359667313 |   Peter
</code></pre>

<p>Lets now say you generate the value 42 as a start-token, the select would be</p>

<blockquote>
  <p>select token(uuid), name from test where token(uuid) > 42 limit 10;</p>
</blockquote>

<p>In this example the result would be</p>

<pre><code> token(id)           | name
---------------------+-------
  958005880272148645 |   Max
 3561637668096805189 |  Doro
 5293579765126103566 |  Paul
 8061178154297884044 | Frank
 8213365047359667313 | Peter
</code></pre>

<p>This method might be a reasonable approach if you have a lot of data, and a balanced cluster. To make sure you don't run into these edge case you could limit the range to not come near the edges of the cassandra token range.</p>
",['table']
33401962,33404252,2015-10-28 21:32:16,How can I alter a user defined type and have it propagate through my tables?,"<p>I am trying to update a user defined type in Cassandra with a renamed field. This is my alter statement.</p>

<pre><code>ALTER TYPE test.test_type RENAME a TO b;
</code></pre>

<p>This works fine and doing <code>DESC TYPE test.test_type</code> shows me the new schema. But I have a table with a schema like</p>

<pre><code>col1 text PRIMARY KEY
col2 frozen&lt;test_type&gt;
</code></pre>

<p>and this does not seem to have been updated.</p>

<p>If I run <code>SELECT * FROM test_table</code> I get the old type fields</p>

<pre><code>c1      | c2
--------+------------
value1  | {a: 'a1'}
</code></pre>

<p>If I try to insert, I am forced to use the new type</p>

<pre><code>INSERT INTO test_table (c1, c2) VALUES ('value2', {a: 'a2'});
</code></pre>

<blockquote>
  <p>InvalidRequest: code=2200 [Invalid query] message=""Unknown field 'a' in value of user defined type test_type""</p>
</blockquote>

<p>But even when I insert with the new type, I still get the old fields.</p>

<pre><code>INSERT INTO test_table (c1, c2) VALUES ('value2', {b: 'a2'});
SELECT * FROM test_table;

c1      | c2
--------+------------
value1  | {a: 'a1'}
value2  | {a: 'a2'}
</code></pre>

<p>I've also tried running <code>ALTER TABLE</code> to re-type the column with the new type, adding a new column to the table with type <code>frozen&lt;test_type&gt;</code>, and even dropping the table and recreating it (which isn't even an option in production). No matter what, the type stays the same when I select from it.</p>

<p>How am I supposed to actually change the type?</p>
",<cassandra><alter-table><user-defined-types>,"<p>Found the issue is logged in this Jira. 
<a href=""https://issues.apache.org/jira/browse/CASSANDRA-9188"" rel=""nofollow"">https://issues.apache.org/jira/browse/CASSANDRA-9188</a></p>

<p>Restarting cqlsh will return the values that you expect. The underlying table and type has been properly modified, it's just that cqlsh has a bug where it will not return the update values until cqlsh is restarted. </p>
",['table']
33445964,33488236,2015-10-30 22:48:24,Set Cassandra Clustering Order on TableDef with Datastax's Spark Cassandra Connector,"<p><strong>Every time I try to create a new table in cassandra with a new <code>TableDef</code> I end up with a clustering order of ascending and I'm trying to get descending.</strong></p>

<p>I'm using Cassandra 2.1.10, Spark 1.5.1, and Datastax Spark Cassandra Connector 1.5.0-M2.</p>

<p>I'm creating a new <code>TableDef</code></p>

<pre><code>val table = TableDef(""so"", ""example"", 
  Seq(ColumnDef(""parkey"", PartitionKeyColumn, TextType)),
  Seq(ColumnDef(""ts"", ClusteringColumn(0), TimestampType)),
  Seq(ColumnDef(""name"", RegularColumn, TextType)))

rdd.saveAsCassandraTableEx(table, SomeColumns(""key"", ""time"", ""name""))
</code></pre>

<p>What I'm expecting to see in Cassandra is</p>

<pre><code>CREATE TABLE so.example (
    parkey text,
    ts timestamp,
    name text,
    PRIMARY KEY ((parkey), ts)
) WITH CLUSTERING ORDER BY (ts DESC);
</code></pre>

<p>What I end up with is </p>

<pre><code>CREATE TABLE so.example (
    parkey text,
    ts timestamp,
    name text,
    PRIMARY KEY ((parkey), ts)
) WITH CLUSTERING ORDER BY (ts ASC);
</code></pre>

<p>How can I force it to set the clustering order to descending?</p>
",<scala><cassandra><apache-spark><datastax><spark-cassandra-connector>,"<p>I was not able to find a direct way of doing this. Additionally there are a lot of other options you may want to specify. I ended up extending <code>ColumnDef</code> and <code>TableDef</code> and overriding the <code>cql</code> method in <code>TableDef</code>. An example of the solution I came up with is below. If someone has a better way or this becomes natively supported I'd be happy to change the answer.</p>

<pre><code>// Scala Enum
object ClusteringOrder {
  abstract sealed class Order(val ordinal: Int) extends Ordered[Order]
    with Serializable {
    def compare(that: Order) = that.ordinal compare this.ordinal

    def toInt: Int = this.ordinal
  }

  case object Ascending extends Order(0)
  case object Descending extends Order(1)

  def fromInt(i: Int): Order = values.find(_.ordinal == i).get

  val values = Set(Ascending, Descending)
}

// extend the ColumnDef case class to add enum support
class ColumnDefEx(columnName: String, columnRole: ColumnRole, columnType: ColumnType[_],
  indexed: Boolean = false, val clusteringOrder: ClusteringOrder.Order = ClusteringOrder.Ascending)
  extends ColumnDef(columnName, columnRole, columnType, indexed)

// Mimic the ColumnDef object
object ColumnDefEx {
  def apply(columnName: String, columnRole: ColumnRole, columnType: ColumnType[_],
    indexed: Boolean, clusteringOrder: ClusteringOrder.Order): ColumnDef = {
    new ColumnDefEx(columnName, columnRole, columnType, indexed, clusteringOrder)
  }

  def apply(columnName: String, columnRole: ColumnRole, columnType: ColumnType[_],
    clusteringOrder: ClusteringOrder.Order = ClusteringOrder.Ascending): ColumnDef = {
    new ColumnDefEx(columnName, columnRole, columnType, false, clusteringOrder)
  }

  // copied from ColumnDef object
  def apply(column: ColumnMetadata, columnRole: ColumnRole): ColumnDef = {
    val columnType = ColumnType.fromDriverType(column.getType)
    new ColumnDefEx(column.getName, columnRole, columnType, column.getIndex != null)
  }
}

// extend the TableDef case class to override the cql method
class TableDefEx(keyspaceName: String, tableName: String, partitionKey: Seq[ColumnDef],
  clusteringColumns: Seq[ColumnDef], regularColumns: Seq[ColumnDef], options: String)
  extends TableDef(keyspaceName, tableName, partitionKey, clusteringColumns, regularColumns) {

  override def cql = {
    val stmt = super.cql
    val ordered = if (clusteringColumns.size &gt; 0)
      s""$stmt\r\nWITH CLUSTERING ORDER BY (${clusteringColumnOrder(clusteringColumns)})""
    else stmt
    appendOptions(ordered, options)
  }

  private[this] def clusteringColumnOrder(clusteringColumns: Seq[ColumnDef]): String =
    clusteringColumns.map { col =&gt;
      col match {
        case c: ColumnDefEx =&gt; if (c.clusteringOrder == ClusteringOrder.Descending)
          s""${c.columnName} DESC"" else s""${c.columnName} ASC""
        case c: ColumnDef =&gt; s""${c.columnName} ASC""
      }
    }.toList.mkString("", "")

  private[this] def appendOptions(stmt: String, opts: String) =
    if (stmt.contains(""WITH"") &amp;&amp; opts.startsWith(""WITH"")) s""$stmt\r\nAND ${opts.substring(4)}""
    else if (!stmt.contains(""WITH"") &amp;&amp; opts.startsWith(""AND"")) s""WITH ${opts.substring(3)}""
    else s""$stmt\r\n$opts""
}

// Mimic the TableDef object but return new TableDefEx
object TableDefEx {
  def apply(keyspaceName: String, tableName: String, partitionKey: Seq[ColumnDef],
    clusteringColumns: Seq[ColumnDef], regularColumns: Seq[ColumnDef], options: String = """") =
    new TableDefEx(keyspaceName, tableName, partitionKey, clusteringColumns, regularColumns,
      options)

  def fromType[T: ColumnMapper](keyspaceName: String, tableName: String): TableDef =
    implicitly[ColumnMapper[T]].newTable(keyspaceName, tableName)
}
</code></pre>

<p>This allowed me to create new tables in this manner:</p>

<pre><code>val table = TableDefEx(""so"", ""example"", 
  Seq(ColumnDef(""parkey"", PartitionKeyColumn, TextType)),
  Seq(ColumnDefEx(""ts"", ClusteringColumn(0), TimestampType, ClusteringOrder.Descending)),
  Seq(ColumnDef(""name"", RegularColumn, TextType)))

rdd.saveAsCassandraTableEx(table, SomeColumns(""key"", ""time"", ""name""))
</code></pre>
",['table']
33460506,33639635,2015-11-01 08:42:34,Not able to run multi node cluster on AWS EC2,"<p>Trying to configure 3 nodes in Cassandra on AWS EC2 (have set ICMP, SSH, HTTP and HTTPS in security groups accordingly)</p>

<p>Cluster name is also set as same in all the three cassandra.yaml file i.e. 'Test Cluster'</p>

<p>Mentioned there corresponding ip as example for node1
listen_address: node1 
seeds:  ""node1,node2,node3""
rpc_address: 0.0.0.0
broadcast_rpc_address: 1.2.3.4</p>

<p>Machines are able to ping each other, using the IP obtained by ifconfig.</p>

<p>But when I run cassandra on the three mentioned machine I get</p>

<p>INFO  08:33:52 No gossip backlog; proceeding.
cassandra 2.2.3
Machine Ubuntu</p>

<p>Is something I am missing?</p>
",<amazon-web-services><amazon-ec2><cassandra>,"<p>although you've posted very little, it might be your snitch. Use GossipingPropertyFileSnitch or Ec2Snitch. Also, you shouldn't make every node a seed. In a 3-node cluster you need only 1 or maybe 2 for redundancy. The seeds: list in cassandra.yaml should be the same on each node. You shouldn't set both the rpc_address and the broadcast_rpc_address. Pick one or the other.</p>
",['rpc_address']
33541435,33546087,2015-11-05 09:56:26,Cassandra vs HBase for Hadoop jobs,"<p>What are advantages of Cassandra over HBase when it comes to MapReduce jobs?</p>

<p>I have a lot of small files that I would like to move from HDFS to a database and that files would be input for MapReduce jobs. I don't take all files, but for a certain user, so possibly the whole row, at least a column family. I could take files from certain period.</p>

<p>I know that HBase is <em>the Hadoop database</em>, so I expect that integrates good for what I need, but I also read that Cassandra has much better performance. But I would like to know what is the situation when you use it as input for <strong>MapReduce</strong> jobs. Is the performance still a lot better than in case of HBase?</p>

<p>I must emphasize that I'm not looking for comparison of HBase and Cassandra in general, but in concrete case of MapReduce jobs. Questions like <a href=""https://stackoverflow.com/questions/7237271/large-scale-data-processing-hbase-vs-cassandra"">this</a> do not talk concretely about performance in case of MapReduce jobs. Also, I'm looking for fresh information (the question I mentioned is from 2011, I believe there might have been some changes since then).</p>
",<hadoop><cassandra><hbase>,"<p>Both databases have a great read and write performance. Possibly HBase for bulk reading has a slightly better performances, than Cassandra. But I have two use cases when HBase will work significant faster than Cassandra, due to it design. </p>

<p>First when you need for map reduce only some portion of data based on the column names, e.g. a html pages and some parsed information from it. You put html in one column family, the parsed information in other. The different column families lie in different files in HDFS, so to read only one you will don't need to read other. This gives you significant benefits in performance because, in case when you will need read only parsed data, which a occupied several times less space on disck than html. In case of Cassandra you will need read whole table. </p>

<p>Second when you need access information ordered by row key or some part of table based on this order, e.g . read html page from some domain. In case of HBase you can have a row key as sum of domain and url. HBase have a good balancer for cases of unhashed row keys. But Cassandra have not or you should use some trick for balancing in this case, or will need to scan whole table.</p>

<p>Hope this use cases will give you some picture, when better to use HBase and when Cassandra.</p>
",['table']
33585373,33587749,2015-11-07 17:03:47,Cassandra - difference in efficiency between simple and compound key,"<p>I have a problem with understanding a one thing from this article - <a href=""http://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling"" rel=""nofollow"">http://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling</a></p>

<p>Exercise - We want get all users by groupname.</p>

<p>Solution:</p>

<pre><code>CREATE TABLE groups (
    groupname text,
    username text,
    email text,
    age int,
    PRIMARY KEY (groupname, username)
);

SELECT * FROM groups WHERE groupname = 'footballers';
</code></pre>

<p>But to find all users in group we can set: <code>PRIMARY KEY (groupname)</code> and it work's also.</p>

<p>Why is needed in this case a clustering key (username)? I know that when we set username as the clustering key we can use it in a <code>WHERE</code> clause. But to find users only by groupname is any difference between <code>PRIMARY KEY (groupname)</code> and <code>PRIMARY KEY (groupname, username)</code> in terms of query efficiency?</p>
",<cassandra><datastax><cql><cassandra-2.0><nosql>,"<p>Clustering keys provide multiple benefits: Query flexibility, result set ordering (within a partition key) and extended uniqueness.</p>

<blockquote>
  <p>But to find all users in group we can set: <code>PRIMARY KEY (groupname)</code></p>
</blockquote>

<p>Try that once.  Create a new table using only <code>groupname</code> as your PRIMARY KEY, and then try to insert multiple <code>username</code>s for each group.  You will find that there will only ever be one group, and that the <code>username</code> column will be overwritten for each new user within that group.</p>

<blockquote>
  <p>But to find users only by <code>groupname</code> is any difference between <code>PRIMARY KEY (groupname)</code> and <code>PRIMARY KEY (groupname, username)</code> in terms of query efficiency?</p>
</blockquote>

<p>If <code>PRIMARY KEY (groupname)</code> performs faster, the most-likely reason is because there can be only a single row returned.</p>

<p>In this case, defining  <code>username</code> as a clustering key provides:</p>

<ol>
<li><p>The ability to sort by <code>username</code> within a group.</p></li>
<li><p>The ability to query for a specific <code>username</code> within a group.</p></li>
<li><p>The ability to add multiple <code>username</code>s within a group.</p></li>
</ol>
",['table']
33606103,33608240,2015-11-09 09:34:24,Cassandra UDT and JAVA,"<p>I have some questions about UDT, not sure if it's bug or not.</p>

<p>here is my type definition and table definition</p>

<pre><code>CREATE TYPE test_udt_bigint (
    id    varchar,
    data Map&lt;int, bigint&gt;
);

CREATE TYPE test_udt (
    id    varchar,
    data Map&lt;int, int&gt;
);

CREATE TABLE test_tbl_bigint (
    row_id varchar PRIMARY KEY,
    udt_data map&lt;varchar, frozen&lt;test_udt_bigint&gt;&gt;
);

CREATE TABLE test_tbl_int (
    row_id varchar PRIMARY KEY,
    udt_data map&lt;varchar, frozen&lt;test_udt&gt;&gt;
);
</code></pre>

<p>After creating those objects, I used cqlsh to insert data, it got success, and I can use select command to retrieve data. But after inserting data via JAVA, it will cause lots of problems.</p>

<p>Here is the repository I used for inserting data:
<a href=""https://github.com/sophiah/cassandra_test/tree/master/cassandra-test-udt"" rel=""nofollow"">https://github.com/sophiah/cassandra_test/tree/master/cassandra-test-udt</a></p>

<p>After inserting data into test_tbl_udt, everything looks great and I can select via cqlsh as normal:</p>

<pre><code>cqlsh:testcassandra&gt; select * from test_tbl_int;

row_id | udt_data
--------+------------------------------------------------
test | {'key-01': {id: 'mapkey-01 ', data: {10: 20}}}
xxx |  {'key-01': {id: 'mapkey-01', data: {10: 20}}}
</code></pre>

<p>but, after inserting data into test_tbl_bigint, there are something =
wrong:</p>

<pre><code>cqlsh:testcassandra&gt; select * from test_tbl_int;
Traceback (most recent call last):
File ""bin/cqlsh"", line 1093, in perform_simple_statement
    rows = self.session.execute(statement, trace=self.tracing_enabled)
File ""/opt/apache-cassandra-2.1.11/bin/../lib/cassandra-driver-internal-only-2.7.2.zip/cassandra-driver-2.7.2/cassandra/cluster.py"", line 1602, in execute
    result = future.result()
File ""/opt/apache-cassandra-2.1.11/bin/../lib/cassandra-driver-internal-only-2.7.2.zip/cassandra-driver-2.7.2/cassandra/cluster.py"", line 3347, in result
    raise self._final_exception
error: unpack requires a string argument of length 4

cqlsh:testcassandra&gt; select * from test_tbl_bigint;
NoHostAvailable: ('Unable to complete the operation against any hosts', {&lt;Host: 127.0.0.1 datacenter1&gt;: ConnectionShutdown('Connection to 127.0.0.1 is defunct',)})
</code></pre>

<p>any suggestion?</p>

<p>thanks</p>
",<java><cassandra><cassandra-cli>,"<p>The short answer is that the table name is wrong in <code>da_test_tbl_bigint.java</code>, it tries to insert into <code>test_tbl_int</code>. It's not yet clear to me why the driver doesn't catch the error, I'll update my answer when I've figured it out.</p>
",['table']
33615276,33617102,2015-11-09 18:00:02,Cassandra: how to search key columns with DECIMAL?,"<p>I understand Cassandra is designed for String based Key/Value pair.
I have a need to have Cassandra table with Decimal keys. Is there anyway to search the keys with range of numeric values. Like keys between 3 and 6 (inclusive)??.</p>

<p><p>
<p></p>

<h2>Sample Key Column</h2>

<p>1</p>

<p>3.3</p>

<p>6.345</p>

<p>9</p>

<p>10</p>

<p>2.5</p>
",<select><search><cassandra><int><decimal>,"<p>Let's try this out.  Assume a simple table with a <code>decimal</code> key, and a <code>text</code> value.</p>

<pre><code>CREATE TABLE decimalRangePK (dec decimal, value text, PRIMARY KEY (dec));
</code></pre>

<p>In this case, <code>dec</code> is my partition key.  And it is my only key, as there is not a clustering key present.  After INSERTing some data, here is what I have:</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; SELECT * FROM decimalrangepk ;

 dec  | value
------+-------
  2.5 |   ghi
 6.35 |   abc
    9 |   def
  3.2 |   3.2
    1 |     1
  3.3 |   3.3
   10 |   ten

(7 rows)
</code></pre>

<p>So I assume that you are trying a range query on your partition key, like this:</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; SELECT * FROM decimalrangeck WHERE dec&gt;=3.3 AND dec&lt;=9;
InvalidRequest: code=2200 [Invalid query] message=""Cannot execute this query as it might involve data filtering and thus may have unpredictable performance. If you want to execute this query despite the performance unpredictability, use ALLOW FILTERING""
</code></pre>

<p>As you can see, this doesn't work.  Cassandra cannot execute range query on a partition key.  However, because clustering keys are used enforce on-disk sort order (within a partition key) you <em>can</em> execute a range query on a clustering key.</p>

<p>In this next example, I'll try this again.  But this time I will partition my data by date, like this:</p>

<pre><code>CREATE TABLE decimalRangeCK (dateBucket text, dec decimal, value text,
     PRIMARY KEY (dateBucket,dec));
</code></pre>

<p>After inserting some rows, I'll query the table and it will look slightly different:</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; SELECT * FROM decimalrangeck ;

 datebucket | dec  | value
------------+------+-------
   20151108 |    1 |     1
   20151108 |  3.2 |   3.2
   20151110 |  2.5 |   ghi
   20151110 |   10 |   ten
   20151109 |    1 |     1
   20151109 |  3.3 |   3.3
   20151109 | 6.35 |   abc
   20151109 |    9 |   def

(8 rows)
</code></pre>

<p>Now I can run a range query on <code>dec</code>, <strong><em>as long as I also provide a partition key</em></strong>:</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; SELECT * FROM decimalrangeck WHERE datebucket='20151109'
                             AND dec&gt;=3.3 AND dec&lt;=9;

 datebucket | dec  | value
------------+------+-------
   20151109 |  3.3 |   3.3
   20151109 | 6.35 |   abc
   20151109 |    9 |   def

(3 rows)
</code></pre>

<p>As you can see, picking a good partition key is very important.  High cardinality, unique partition keys are great for data distribution, but don't really give you a whole lot of query flexibility.</p>
",['table']
33719681,33850565,2015-11-15 12:30:55,PrestoDB v0.125 SELECT only returns subset of Cassandra records,"<p>SELECT statements in PrestoDB v0.125 with a Cassandra connector to a Datastax Cassandra cluster only return 200 rows, even where table contains many more rows than that. Aggregate queries like SELECT COUNT() over the same table also return a result of just 200. </p>

<p>(This behaviour is identical when querying with pyhive connector &amp; with base presto CLI).</p>

<p>Documentation isn't much help, but am guessing that the issue is pagination &amp; a need to set environment variables (which the documentation doesn't explain):
<a href=""https://prestodb.io/docs/current/installation/cli.html"" rel=""nofollow"">https://prestodb.io/docs/current/installation/cli.html</a></p>

<p>Does anyone know how I can remove this limit of 200 rows returned? What specific environment variable setting do I need?</p>
",<cassandra><presto>,"<p>For those who come after - the solution is in the cassandra.properties connector configuration for presto. The key setting is: </p>

<ul>
<li>cassandra.limit-for-partition-key-select</li>
</ul>

<p>This needs to be set higher than the total number of rows in the table you are querying, otherwise select queries will respond with only a fraction of the stored data (not having located all of the partition keys).</p>

<p>Complete copy of my config file (which may help!):</p>

<pre><code>connector.name=cassandra
# Comma separated list of contact points
cassandra.contact-points=host1,host2
# Port running the native Cassandra protocol
cassandra.native-protocol-port=9042
# Limit of rows to read for finding all partition keys.
cassandra.limit-for-partition-key-select=2000000000
# maximum number of schema cache refresh threads, i.e. maximum number of parallel requests
cassandra.max-schema-refresh-threads=10
# schema cache time to live
cassandra.schema-cache-ttl=1h
# schema refresh interval
cassandra.schema-refresh-interval=2m
# Consistency level used for Cassandra queries (ONE, TWO, QUORUM, ...)
cassandra.consistency-level=ONE
# fetch size used for Cassandra queries
cassandra.fetch-size=5000
# fetch size used for partition key select query
cassandra.fetch-size-for-partition-key-select=20000
</code></pre>
",['table']
33898986,33901691,2015-11-24 16:32:44,Cassandra 2.2.3 Java Requirements,"<p>Does anyone know if Cassandra 2.2.3 supports Oracle Java 8? Is there a place where we can go to see supported Java versions?</p>
",<cassandra>,"<p>I am not aware of a table or location that lists a cross-reference of all Cassandra versions and the Java versions that they support.  But the <a href=""http://docs.datastax.com/en/cassandra/2.2/cassandra/install/installTarball.html"" rel=""nofollow noreferrer"">Cassandra 2.2 installation documentation</a> has this information:</p>
<blockquote>
<p><strong>Prerequisites</strong></p>
<ul>
<li>Latest version of Oracle Java Platform, Standard Edition 8 (JDK) or OpenJDK 7.</li>
</ul>
</blockquote>
<p>That can change with the version of Cassandra that you are installing.  The <a href=""http://docs.datastax.com/en/cassandra/2.1/cassandra/install/installTarball_t.html"" rel=""nofollow noreferrer"">Cassandra 2.1 installation doc</a> also adds:</p>
<blockquote>
<p>Note: It is recommended to use the latest version of Oracle Java 8 on all nodes. (Oracle Java 7 is also supported.)</p>
</blockquote>
",['table']
33927026,33936417,2015-11-25 21:51:50,Modelling Cassandra Many-to-Many Relation for Sensor Data,"<p>I am building an application with Cassandra as the data store which captures data from a large number of sensors and allows different monitoring components to monitor those sensors.</p>

<p>For example, a server room might have a temperature sensor and 10 different server monitoring components might receive values from that one sensor. Likewise, a monitoring component will receive data from multiple sensors.</p>

<p>My (very simplified) conceptual schema looks something like:</p>

<p><a href=""https://i.stack.imgur.com/fvH2n.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fvH2n.png"" alt=""ERD""></a></p>

<p>I need to run the following queries:</p>

<ol>
<li>Historical values for an individual sensor</li>
<li>The latest value of every attribute on a monitoring component</li>
</ol>

<p>and it's the second one that I'm having a problem with.</p>

<p>When a Measurement arrives I only know the sensor ID, timestamp and value. How can I model a table that allows me to keep the current value for every attribute on a Monitor?</p>

<p>I tried the following table:</p>

<pre><code>CREATE TABLE monitor_subscriptions (
    sensor_id uuid,
    monitor_id uuid,
    attribute text, # e.g. 'Temperature'
    timestamp timestamp,
    value double,
    PRIMARY KEY (sensor_id, monitor_id, attribute)
);
</code></pre>

<p>What I'm attempting to do is update the timestamp/value of every monitor that is subscribed to that sensor but then obviously the following query doesn't work because I'm not specifying <code>monitor_id</code> or <code>attribute</code>:</p>

<pre><code>UPDATE monitor_subscriptions
SET timestamp = ?, value = ?
WHERE sensor_id = ?;
</code></pre>

<p>At the point I receive the new measurement though, I only know the <code>sensor_id</code>, <code>timestamp</code> and <code>value</code>.</p>
",<database-design><cassandra><cql>,"<p>I guess that you might revisit your monitor_subscriptions table to be:</p>

<ul>
<li>append-only, not updating the latest value, but inserting a new one every time</li>
<li>split into two different tables optimal for your specific queries.</li>
</ul>

<p>For example:</p>

<pre><code>create table sensor_data (
  sensor_id uuid,
  timestamp timestamp,
  value double,
  primary key (sensor_id, timestamp)
) with clustering order by (timestamp desc);
</code></pre>

<p>This table is used for storing raw sensor readings, you can effectively query it for latest data for specific sensor. If you plan to insert a lot of sensor readings (like every second), you may want to add current day to clustering key to deal with possible compaction issues later.</p>

<p>And the monitor table may be looking like this:</p>

<pre><code>create table monitor_subscriptions (
  monitor_id uuid,
  sensor_id uuid,
  attribute text,
  primary key (monitor_id, attribute, sensor_id)
)
</code></pre>

<p>This table can be used to query for all attributes for the monitor or all sensors for these attributes. So to query latest value for each attribute, you:</p>

<ol>
<li>Query monitor_subscriptions for attribute->sensor mapping (best case: 1 disk read)</li>
<li>Query sensor_data for each sensor (best case: N disk reads, where N =  number of sensors).</li>
</ol>
",['table']
33943960,34461701,2015-11-26 17:09:01,Apache Cassandra 3.0.0 Materialized View: can the view's partition key change due to changes to the underlying table?,"<p>Just thinking about this so please correct my understanding if any of this isn't right. </p>

<p>Environment: Apache Cassandra v3.0.0</p>

<p>Say you have a table and a materialized view created on it:</p>

<pre><code>create table source(
id text, field text, stamp timestamp, data text, 
primary key(id, field))

create materialized view myview as
select * from source
where data is not null and id is not null and field is not null
primary key (data, field, id)
</code></pre>

<p>My understanding is that <code>myview.data</code> would essentially be the partition key for the view here (and data in <code>source</code> is automatically replicated by the server into <code>myview</code>?). </p>

<p><strong>If that is true, what happens internally when a table update is performed on <code>source</code> table and the <code>source.data</code> column is updated?</strong></p>
",<cassandra><materialized-views><cqlsh><cassandra-3.0>,"<p>I posted this to Cassandra's user mailing list and got the following two useful replies that answered the question.</p>

<p>It should all just work as expected, as if by magic. That's the whole point of having MV, so that Cassandra does all the bookkeeping for you. Yes, the partition key can change, so an update to the base table can cause one (or more) MV rows to be deleted and one (or more) new MV rows to be created. It does not change the partition key per se, but it is as if it were changed and the row moved. <strong>This can in fact result in the row moving from one node to another if the column(s) used in the MV partition key change in the base table row.</strong></p>

<p>-- Jack Krupansky</p>

<p>In the case of an update to the source table where data is changed, a tombstone will be generated for the old value and an insert will be generated for the new value. This happens serially for the source partition, so if there are multiple updates to the same partition, a tombstone will be generated for each intermediate value.</p>

<p>This blog post has more details: <a href=""http://www.datastax.com/dev/blog/new-in-cassandra-3-0-materialized-views"" rel=""nofollow"">http://www.datastax.com/dev/blog/new-in-cassandra-3-0-materialized-views</a></p>

<p>-Carl Yeksigian</p>
",['table']
33981408,33983962,2015-11-29 09:47:48,selecting on two tables in Cassandra,"<p>I use <code>Cassandra</code> for a project, and it's my first project. 
, and I'm trying to do a simple request on two tables, but that doesn't work...</p>

<p>I want to do something like:</p>

<p><code>Select * from table1, table2 where table1.test = test and table2.test2 = 123;</code></p>

<p>Is it possible to request on two tables in <code>Cassandra</code>? And how can I do that? </p>

<p>Thanks</p>
",<cassandra><request><cql>,"<blockquote>
  <p>I'm trying to do a simple request on two tables</p>
</blockquote>

<p>What you're trying to do is known as a ""distributed join"" and Cassandra is specifically designed to prevent you from doing this.</p>

<p>The way to solve these types of problems, is with a process called <em>denormalization</em>.  Let's say you have simple two tables <code>carMake</code> and <code>carModel</code>:</p>

<pre><code> makeid | make
--------+--------
      1 |  Chevy
      2 |  Dodge
      3 |   Ford

 modelid | makeid | model
---------+--------+---------
      15 |      3 |   Focus
      11 |      3 | Mustang
      32 |      2 | Charger
      82 |      3 |  Fusion
</code></pre>

<p>Now, in a traditional RDBMS if I wanted to SELECT all car models with a make of ""Ford"" I would execute a JOIN query.  But with Cassandra, the idea is to solve this problem at the modeling stage, by building a table which supports the ability to query make and model of a car at the same time:</p>

<pre><code>CREATE TABLE carMakeModel (
    carid int,
    make text,
    model text,
    PRIMARY KEY (make,carid));

aploetz@cqlsh:stackoverflow&gt; SELECT * FROM carMakeModel WHERE make='Ford';

 make | carid | model
------+-------+---------
 Ford |     1 | Mustang
 Ford |     2 |   Focus
 Ford |     3 |  Fusion

(3 rows)
</code></pre>

<p>Some key points to note here:</p>

<ul>
<li><code>make</code> is duplicated as much as is necessary.  You'll notice that ""Ford"" is specified 3 times in the result set.  If you had data for 13 models of Fords, you would store the value of ""Ford"" 13 times.</li>
<li>PRIMARY KEYs in Cassandra are unique.  I have <code>carid</code> added as a part of the PRIMARY KEY to ensure uniqueness for each <code>model</code>, otherwise an INSERT for each <code>make</code> would overwrite itself.</li>
</ul>
",['table']
34055752,34057523,2015-12-03 00:31:39,SELECT contant value is Cassandra,"<p>I'm trying to do a simple select in Cassandra CQL3 containing a hardcoded value (constant) under a constant column name and I simply can't get it working</p>

<p>Here's the query</p>

<p><code>SELECT 1 as ""id""</code></p>

<p>Works fine in all kinds of DBMS I use but throws this error here:</p>

<p><code>Error: ResponseError: line 1:7 no viable alternative at input '1' (SELECT [1]...)</code></p>

<p>What's the correct syntax?</p>
",<cassandra><cql><cql3>,"<p>Unfortunately, CQL is not SQL, and queries like this do not work in cqlsh as they do in their relational counterparts.  The DataStax <a href=""http://docs.datastax.com/en/cql/3.1/cql/cql_reference/select_r.html"" rel=""noreferrer"">SELECT documentation</a> indicates that a selector must be one of:</p>

<ol>
<li>column name</li>
<li>WRITETIME (column name)</li>
<li>TTL (column name)</li>
<li>function</li>
</ol>

<p>Now while a <code>SELECT 1 as id</code> query may not work, there are other, slightly more useful things that do.  For instance, if I need to quickly generate a UUID, I can do so with the following query:</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; SELECT uuid() FROM system.local;

 system.uuid()
--------------------------------------
 a55c17f7-d19d-4531-85be-75551e3fd546

(1 rows)
</code></pre>

<p>This works the way it does for two reasons:</p>

<ol>
<li><p>The SELECT clause invokes the uuid() function.</p></li>
<li><p>The system.local table only ever contains a single row.  If you ran this SELECT against another table, it would return as many UUIDs as there were CQL rows.</p></li>
</ol>
",['table']
34071942,34089867,2015-12-03 17:02:47,What happens when lightweight transaction and normal insert were executed at the same time?,"<p>I've read the Cassandra article about <a href=""http://www.datastax.com/dev/blog/lightweight-transactions-in-cassandra-2-0"" rel=""nofollow"">lightweight-transaction</a>, and I think I've understood how the Paxos works.</p>

<p>I understood that with Paxos, CAS (compare-and-set) get linearized; the Paxos algorithm is used to determine which CAS is accepted. (CAS operation is used as a ""value"" of Paxos algorithm)</p>

<p>My question is, what happens if the INSERT lightweight-transaction and normal INSERT statement were executed for a same primary key?</p>

<p>In my understanding, normal INSERT operation does not utilize the facilities of Paxos, and thus if used along with LWT, something not desired would happen.</p>

<p>Am I correct? If so, what would happen? If not, how is the normal INSERT gets handled?</p>
",<cassandra>,"<p>Using both regular inserts and CAS operations for the same table is not recommended. Chances are that regular inserts will be executed out of order, which is what you're trying to prevent using paxos.</p>

<p>The same is even true for reads. Any reads on tables that are updated using CAS must use <code>SERIAL</code> consistency level to make sure that you always see the result of the last CAS operation. Using <code>SERIAL</code> will coordinate query execution using paxos together with any updates and thus prevents out of order execution.</p>
",['table']
34076987,34096990,2015-12-03 21:56:50,Data modeling easy table in Cassandra not working,"<p>I have to design a web page where a group leader can invite people to join his/her group.  My requirements are really simple.</p>

<ol>
<li><p>No sending duplicate emails out, if person was already contacted.</p></li>
<li><p>Show the group leader a list of invites sorted by invite date in ascending order.</p></li>
</ol>

<p>Seems easy.  I created this table.</p>

<pre><code>CREATE TABLE invites (
email_address text,
invite_date timeuuid,
PRIMARY KEY (email_address, invite_date)
) WITH CLUSTERING ORDER BY (invite_date ASC);
</code></pre>

<p>Problem 1: LWT no use with invite_date as a Cluster column.</p>

<p>I figured I'd use LWT to ensure email_address is unique, only to find out IF NOT EXISTS only seems to work on the <strong>whole</strong> PRIMARY KEY, so LWT in C* does not work for me.  </p>

<p>Problem 2: I cannot get an ordered list of invites back to save me life even with invite_date as a Cluster column.</p>

<p>If I take invite_date out, I cannot issue an 'order by' in CQL.  That said, having invite_date out of the PK let's me use LWT...</p>

<p>I can't even get a 2 column table to fulfill 2 easy requirements!  Any help on data modeling design for this problem is much appreciated.</p>

<p><em>New Dec. 4, 2015:</em></p>

<p><strong>Additional to business requirements, a technical requirement I have is:</strong>
I want to make sure I model this correctly in Cassandra, so that it allows me to use CQL's LIMIT and pagingState capabilities in the Java driver.  This means, I cannot just read all the rows in, sort on Java side and return the results.</p>
",<cassandra><cassandra-2.0><cql3>,"<p>Problem 1: 
I think that the easiest way to handle this might be to have two separate tables, one for the emails_in_group and one for invites_by_group.  This will allow each query to be fulfilled independantly.  The emails_in_group table would look something like this:</p>

<pre><code>CREATE TABLE emails_in_group ( 
 email_address text,  
 group_id text,
 PRIMARY KEY (email_address , group_id));  
</code></pre>

<p>Then this, combined with the table as defined in Problem 2 below could be updated using a conditional batch statement as shown here:
<a href=""http://docs.datastax.com/en/cql/3.1/cql/cql_using/use-batch-static.html"" rel=""nofollow"">http://docs.datastax.com/en/cql/3.1/cql/cql_using/use-batch-static.html</a></p>

<p>Problem 2:
So the basic problem here is that as you have your data currently modeled each email_address value will be in it's own partition and then within that partition the invite_date will be ordered.  @bydsky is right when he said that you need to add something like a group_id to your table and make it the partition key portion of your Primary Key.  If you do this and then add invite_date as a cluster column to the partition key then all records for that group_id will be stored in the same partition and the Order By will work.  Order By only works within the same partiton, not across partitions which is what you were asking it to do.</p>

<pre><code>CREATE TABLE invites_by_group ( 
 group_id text,  
 email_address text,  
 invite_date timestamp,  
 PRIMARY KEY (group_id, invite_date));   
</code></pre>
",['table']
34202013,34206886,2015-12-10 12:33:44,Use JDBC (eg Squirrel SQL) to query Cassandra with Spark SQL,"<p>I have a Cassandra cluster with a co-located Spark cluster, and I can run the usual Spark jobs by compiling them, copying them over, and using the ./spark-submit script. I wrote a small job that accepts SQL as a command-line argument, submits it to Spark as Spark SQL, Spark runs that SQL against Cassandra and writes the output to a csv file.</p>

<p>Now I feel like I'm going round in circles trying to figure out if it's possible to query Cassandra via Spark SQL directly in a JDBC connection (eg from Squirrel SQL). The Spark SQL documentation says </p>

<pre><code>Connect through JDBC or ODBC.

A server mode provides industry standard JDBC and ODBC connectivity for
business intelligence tools.
</code></pre>

<p>The Spark SQL Programming Guide says</p>

<pre><code>Spark SQL can also act as a distributed query engine using its JDBC/ODBC or
command-line interface. In this mode, end-users or applications can interact
with Spark SQL directly to run SQL queries, without the need to write any 
code.
</code></pre>

<p>So I can run the Thrift Server, and submit SQL to it. But what I can't figure out, is how do I get the Thrift Server to connect to Cassandra? Do I simply pop the Datastax Cassandra Connector on the Thrift Server classpath? How do I tell the Thrift Server the IP and Port of my Cassandra cluster? Has anyone done this already and can give me some pointers?</p>
",<apache-spark><cassandra><apache-spark-sql><spark-cassandra-connector>,"<p>Configure those properties in spark-default.conf file</p>

<pre><code>spark.cassandra.connection.host    192.168.1.17,192.168.1.19,192.168.1.21
# if you configured security in you cassandra cluster
spark.cassandra.auth.username   smb
spark.cassandra.auth.password   bigdata@123
</code></pre>

<p>Start your thrift server with spark-cassandra-connector dependencies and mysql-connector dependencies with some port that you will connect via JDBC or Squirrel.</p>

<pre><code>sbin/start-thriftserver.sh --hiveconf hive.server2.thrift.bind.host 192.168.1.17 --hiveconf hive.server2.thrift.port 10003 --jars &lt;shade-jar&gt;-0.0.1.jar --driver-class-path &lt;shade-jar&gt;-0.0.1.jar
</code></pre>

<p>For getting cassandra table run Spark-SQL queries like</p>

<pre><code>CREATE TEMPORARY TABLE mytable USING org.apache.spark.sql.cassandra OPTIONS (cluster 'BDI Cassandra', keyspace 'testks', table 'testtable');
</code></pre>
",['table']
34206482,34207186,2015-12-10 16:05:11,How to evaluate the performance of Cassandra?,"<p>I'am new in Cassandra. I have studied and performed some tests on Cassandra database and I got some questions:</p>

<ol>
<li><p>Given that Cassandra encourage denormalization and duplication of data, when data that are present in multiple column families are updated from just one of the column families how data consistency is guaranteed?</p></li>
<li><p>The number of columns in a table affects query performance?</p></li>
<li><p>It's true that the greater the number of records returned by a query, its performance is worse?</p></li>
<li><p>What kind of circumstances is useful to use mapreduce in Cassandra?</p></li>
</ol>
",<mapreduce><cassandra><cassandra-2.0><query-performance>,"<blockquote>
  <p>Given that Cassandra encourage denormalization and duplication of<br>
  data, when data that are present in multiple column families are<br>
  updated from just one of the column families how data consistency is
  guaranteed?</p>
</blockquote>

<p>This was the very reason <a href=""http://docs.datastax.com/en/cql/3.1/cql/cql_reference/batch_r.html"" rel=""nofollow"">BATCH</a> was introduced in Cassandra. Even with BATCH, you're still in a distributed system and need to think as such when modeling your data. Since you don't have a specific problem, we'll keep talking theoretically. </p>

<blockquote>
  <p>The number of columns in a table affects query performance?</p>
</blockquote>

<p>It's not so much the number of columns as it is the size of each individual partition. The larger the partition, the harder some of Cassandra's internal mechanisms (such as compaction) has to work. If you are not familiar with how data is stored on disk, I suggest taking a look at <a href=""https://academy.datastax.com/courses/ds101-introduction-cassandra"" rel=""nofollow"">THIS</a> tutorial. </p>

<blockquote>
  <p>It's true that the greater the number of records returned by a query,
  its performance is worse?</p>
</blockquote>

<p>It's physics. More data = more IO, bandwidth, objects for GC to collect ETC. Given Cassandra is built as a transactional datastore, it's not build for extremely large data returns/full table scans (very few truly distributed systems are). The tutorial linked above does a good job of explaining. </p>

<blockquote>
  <p>What kind of circumstances is useful to use mapreduce in Cassandra?</p>
</blockquote>

<p>If you're interested in running analytics on Cassandra, I suggest going the route of using Spark as there has been a lot of work to optimize the relationship of Spark and Cassandra both at the commercial and open source level. When you're comfortable with how Cassandra works, I suggest taking a look at <a href=""https://academy.datastax.com/courses/getting-started-apache-spark"" rel=""nofollow"">THIS</a> tutorial if you're interested in doing any sort of analytics on Cassandra. It talks to the commercial offering, but the concepts/tutorials will also apply to the open source as well.</p>
",['table']
34231718,34235617,2015-12-11 20:08:11,How to ensure data consistency in Cassandra on different tables?,"<p>I'm new in Cassandra and I've read that Cassandra encourages denormalization and duplication of data. This leaves me a little confused.
Let us imagine the following scenario:</p>

<p>I have a keyspace with four tables: A,B,C and D. </p>

<pre><code>CREATE TABLE A (
  tableID int,
  column1 int,
  column2 varchar,
  column3 varchar,
  column4 varchar,
  column5 varchar,
  PRIMARY KEY (column1, tableID)
);
</code></pre>

<p>Let us imagine that the other tables (B,C,D) have the same structure and the same data that table A, only with a different primary key, in order to respond to other queries.</p>

<p>If I upgrade a row in table A how I can ensure consistency of data in other tables that have the same data?</p>
",<cassandra><duplicates><cassandra-2.0><data-consistency>,"<p>Cassandra provides <code>BATCH</code> for this purpose. From the <a href=""https://docs.datastax.com/en/cql/3.1/cql/cql_reference/batch_r.html"" rel=""noreferrer"">documentation</a>:</p>
<blockquote>
<p>A BATCH statement combines multiple data modification language (DML) statements (INSERT, UPDATE, DELETE) into a single logical operation, and sets a client-supplied timestamp for all columns written by the statements in the batch. Batching multiple statements can save network exchanges between the client/server and server coordinator/replicas. However, because of the distributed nature of Cassandra, spread requests across nearby nodes as much as possible to optimize performance. Using batches to optimize performance is usually not successful, as described in Using and misusing batches section. For information about the fastest way to load data, see &quot;Cassandra: Batch loading without the Batch keyword.&quot;</p>
<p>Batches are atomic by default. In the context of a Cassandra batch operation, atomic means that if any of the batch succeeds, all of it will. To achieve atomicity, Cassandra first writes the serialized batch to the batchlog system table that consumes the serialized batch as blob data. When the rows in the batch have been successfully written and persisted (or hinted) the batchlog data is removed. There is a performance penalty for atomicity. If you do not want to incur this penalty, prevent Cassandra from writing to the batchlog system by using the UNLOGGED option: BEGIN UNLOGGED BATCH</p>
</blockquote>
<p>UNLOGGED BATCH is almost always undesirable and I believe is removed in future versions. Normal batches provide the functionality you desire.</p>
",['table']
34246581,34268901,2015-12-13 00:20:47,Query in Cassandra that will sort the whole table by a specific field,"<p>I have a table like this</p>

<pre><code>CREATE TABLE my_table(
category text,
name text,
PRIMARY KEY((category), name)
) WITH CLUSTERING ORDER BY (name ASC);
</code></pre>

<p>I want to write a query that will sort by name through the entire table, not just each partition.</p>

<p>Is that possible? What would be the ""Cassandra way"" of writing that query?</p>

<p>I've read other answers in the StackOverflow site and some examples created single partition with one id (bucket) which was the primary key but I don't want that because I want to have my data spread across the nodes by category</p>
",<cassandra><cql><cqlsh>,"<p>Always model cassandra tables through the access patterns (relational db / cassandra fill different needs).</p>

<ul>
<li><p>Up to Cassandra 2.X, one had to model new column families (tables) for each  access pattern. So if your access pattern needs a specific column to be sorted then model a table with that column in the partition/clustering key. So the code will have to insert into both the <em>master</em> table and into the projection table. <em>Note depending on your business logic this may be difficult to synchronise if there's concurrent update, especially if there's update to perform <strong>after</strong> a read on the projections.</em></p></li>
<li><p>With Cassandra 3.x, there is now materialized views, that will allow you to have a similar feature, but that will be handled internally by Cassandra. Not sure it may fit your problem as I didn't play too much with 3.X but that may be worth investigation.</p>

<p>More on materialized view on their <a href=""http://www.datastax.com/dev/blog/new-in-cassandra-3-0-materialized-views"" rel=""nofollow"">blog</a>.</p></li>
</ul>
",['table']
34294830,34295524,2015-12-15 16:46:40,How to keep 2 Cassandra tables within same partition,"<p>I tried reading up on datastax blogs and documentation but could not find any specific on this</p>

<p>Is there a way to keep 2 tables in Cassandra to belong to same partition?
For example:</p>

<pre><code>CREATE TYPE addr (
  street_address1 text,
  city text,
  state text,
  country text,
  zip_code text,
);

CREATE TABLE foo (
  account_id timeuuid,
  data text,
  site_id int,
  PRIMARY KEY (account_id)
};

CREATE TABLE bar (
  account_id timeuuid,
  address_id int,
  address frozen&lt;addr&gt;,
  PRIMARY KEY (account_id, address_id)
);
</code></pre>

<p>Here I need to ensure that both of these tables/CF will live on same partition that way for the same account_id both of these set of data can be fetched from the same node</p>

<p>Any pointers are highly appreciated.</p>

<p>Also, if someone has some experience in using UDT (User Defined Types), I would like to understand how the backward compatibility would work. If I modify ""addr"" UDT to have a couple of more attributes (say for example zip_code2 int, and name text), how does the older rows that does have these attribute work? Is it even compatible?</p>

<p>Thanks </p>
",<cassandra><datastax><datastax-enterprise><cql3><datastax-java-driver>,"<p>If two table share the same replication strategy and same partition key they will colocate their partitions. So as long as the two tables are in the same keyspace AND their partition keys match</p>

<p>PRIMARY KEY (<em>account_id</em>) ==   PRIMARY KEY (<em>account_id</em>, address_id)</p>

<p>Any given account_id will be on (and replicated to) the same machines. </p>
",['table']
34324321,34324906,2015-12-16 23:49:17,Using Spark in conjunction with Cassandra?,"<p>In our current infrastructure we use a Cassandra cluster as our backend database, and via Solr we use a web UI for our customers to perform read queries on our database as necessary.</p>

<p>I've been asked to look into Spark as something that we could implement in the future, but I'm having trouble understanding how it will improve what we currently do.</p>

<p>So my basic questions are:</p>

<p>1)  Is Spark something that would replace Solr for querying the database, like when a user is looking something up on our site?</p>

<p>2) Just a general idea, what type of infrastructure would be necessary to improve our current situation (5 Cassandra nodes, all of which also run Solr).
In other words, we would simple be looking at building another cluster of just Spark nodes?</p>

<p>3) Can Spark nodes run on the same physical machine as Cassandra?  I'm guessing it would be a bad idea due to memory constraints as my very basic understanding of Spark is that it does everything in memory.</p>

<p>4) Any good quick/basic resources I can use to start figuring out how Spark might benefit us?  I have access to Datastax Academy courses so I'm going through those, just wondering if there is anything else to help with my research.</p>

<p>Basically once I figure out what it is, and more importantly how/if it is something we can use to our advantage I'll start playing with some test instances, but I should probably familiarize myself with the basics first.</p>
",<apache-spark><cassandra><datastax-enterprise>,"<p>1) No, Spark is a batch processing system and Solr is live indexing solution. Latency on solr is going to be sub second, Spark jobs are meant to take minutes (or more). There should really be no situation where Spark can be a drop in replacement for Solr.</p>

<p>2) I generally recommend a second Datacenter running both C* and Spark on the same machines. This will have the data from the first Datacenter via replication.</p>

<p>3) Spark Does not do everything in memory. Depending on your use case it can be a great idea to run on the same machines as C*. This can allow for data locality in reading from C* and help out significantly on table scan times. I usually also recommend colocating Spark Executors and C* nodes. </p>

<p>4) DS Academy 320 course is probably the best resource out there atm. <a href=""https://academy.datastax.com/courses/getting-started-apache-spark"">https://academy.datastax.com/courses/getting-started-apache-spark</a></p>
",['table']
34455066,34460870,2015-12-24 15:45:57,How many systems a given rack can hold,"<p>I am getting trained on Apache casssandra, where terms like nodes and racks are quite predominantly used. I understand that a rack is to hold n number of systems or nodes. But was wondering whether there is any upper limit to it, like a rack can hold this many systems.</p>

<p>It was just for the sake of understanding, if any one knows the answer or any points to make on this, please share.</p>
",<cassandra>,"<p>There is no limit to the number of systems in a rack - racks are used to balance replicas within a datacenter, but are not bounded (though certainly, you're limited to a reasonable number of nodes in a datacenter, typically around 1000).</p>

<p>Rack awareness can be tricky, especially if you have fewer racks than your replication_factor, or if your racks are of uneven sizes. Adding a rack after the fact, in particular, can cause significant problems - many operators disable rack awareness to avoid edge cases.</p>
",['rack']
34470610,34475700,2015-12-26 10:47:09,Cassandra: secondary index inside a single partition (per partition indexing)?,"<p>This question is I hope not answered in the usual ""secondary index v. clustering key"" questions.</p>

<p>Here is a simple model I have:</p>

<pre><code>CREATE TABLE ks.table1 (
name text,
timestamp int,
device text,
value int,
PRIMARY KEY (md_name, timestamp, device)
)
</code></pre>

<p>Basically I view my data as datasets with name <code>name</code>, each dataset is a kind of sparse 2D matrix (rows = <code>timestamps</code>, columns = <code>device</code>) containing <code>value</code>.</p>

<p>As the problem and the queries can be pretty symmetric (ie. is my ""matrix"" the best representation, or should I use the transposed ""matrix"") I couldn't decide easily what clustering key I should put first. It makes a bit more sense the way I did: for each timestamp I have a set of data (values for each devices present at that timestamp).</p>

<p>The usual query is then</p>

<pre><code>select * from cycles where md_name = 'xyz';
</code></pre>

<p>It targets a single partition, that will be super fast, easy enough. If there's a large amount of data my users could do something like this instead:</p>

<pre><code>select * from cycles where md_name = 'xyz' and timestamp &lt; n;
</code></pre>

<p>However I'd like to be able to ""transpose"" the problem and do this:</p>

<pre><code> select * from cycles where md_name = 'xyz' and device='uvw';
</code></pre>

<p>That means I have to create a secondary index on <code>device</code>.</p>

<p>But (and that's where the question starts""), this index is a bit different from usual indexes, as it is used for queries inside a single partition. Create the index allows to do the same on multiple partitions:</p>

<pre><code>select * from cycles where device='uvw'
</code></pre>

<p>Which is not necessary in my case.</p>

<ul>
<li>Can I improve my model to support such queries without too much duplication?</li>
<li>Is there something like a ""per-partition index""?</li>
</ul>
",<cassandra>,"<p>The index would allow you to do queries like this:</p>

<pre><code>select * from cycles where md_name='xyz' and device='uvw'
</code></pre>

<p>But that would return all timestamps for that device in the xyz partition.</p>

<p>So it sounds like maybe you want two views of the data.  Once based on name and time range, and one based on name, device, and time range.</p>

<p>If that's what you're asking, then you probably need two tables. If you're using C* 3.0, then you could use the materialized views feature to create the second view. If you're on an earlier version, then you'd have to create the two tables and do a write to each table in your application.</p>
",['table']
34491215,34495325,2015-12-28 09:53:26,order by clause not working in Cassandra query,"<p>I have created a table layer using following code:</p>

<pre><code>CREATE TABLE layer (
    layer_name text,
    layer_position text,
    PRIMARY KEY (layer_name, layer_position)
) WITH CLUSTERING ORDER BY (layer_position DESC)
</code></pre>

<p>I use the below query to fetch data from the layer table in descending order(layer):</p>

<pre><code>$select = new Cassandra\SimpleStatement(&lt;&lt;&lt;EOD
                        select * from layer ORDER BY layer_position DESC
EOD
                      ); 

$result = $session-&gt;execute($select);
</code></pre>

<p>But this query is not working.  Please can anyone help me?</p>
",<cassandra>,"<p>Simply put, Cassandra only enforces sort order <em>within</em> a partition key.</p>

<pre><code>PRIMARY KEY (layer_name, layer_position)
) WITH CLUSTERING ORDER BY (layer_position DESC)
</code></pre>

<p>In this case, <code>layer_name</code> is your partition key.  If you specify <code>layer_name</code> in your WHERE clause, your results for that value of <code>layer_name</code> will be ordered by <code>layer_position</code>.</p>

<pre><code>SELECT * FROM layer WHERE layer_name = 'layer1';
</code></pre>

<p>You don't need to specify ORDER BY.  All ORDER BY really can do at the query level is apply a different sort direction (ascending vs. descending).</p>

<p>Cassandra works this way, because it is designed to read data in whatever order it is sorted on disk.  Your partition keys are sorted by hashed token value, which is why results from an unbound WHERE clause appear to be ordered randomly.</p>

<p><strong>EDIT</strong></p>

<blockquote>
  <p>I have to fetch data using <code>state_id</code> column and it should be order by <code>layer_position</code>.</p>
</blockquote>

<p>Cassandra tables are optimized for a specific query.  While this results in high performance, the drawback is that query flexibility is limited.  The way to solve for this, is to duplicate your data into an additional table designed to serve that particular query.</p>

<pre><code>CREATE TABLE layer_by_state_id (
    layer_name text,
    layer_position text,
    state_id text,
    PRIMARY KEY (state_id, layer_position, layer_name)
) WITH CLUSTERING ORDER BY (layer_position DESC, layer_name ASC);
</code></pre>

<p>This table will allow queries like this to work:</p>

<pre><code>SELECT * FROM layer WHERE state_id='thx1138';
</code></pre>

<p>And the results will be sorted by <code>layer_position</code>, within the requested <code>state_id</code>.</p>

<p>Now I am making a couple of assumptions that you will want to investigate:</p>

<ul>
<li>I am assuming that <code>state_id</code> is a good partitioning key.  Meaning that it has high-enough cardinality to offer good distribution in the cluster, but low-enough cardinality that it returns enough CQL rows to make sorting worthwhile.</li>
<li>I am assuming that the combination of <code>state_id</code> and <code>layer_position</code> is <em>not</em> enough to uniquely identify each row.  Therefore I am ensuring uniqueness by adding <code>layer_name</code> as an additional clustering key.  You may or may not need this, but I'm guessing that you will.</li>
<li>I am assuming that using <code>state_id</code> as a partitioning key will not exhibit unbound growth so as to approach Cassandra's limit of 2 billion cells per partition.  If that is the case, you may need to add an additional partition ""bucket.""</li>
</ul>
",['table']
34565696,34568941,2016-01-02 12:06:13,Cassandra: select only latest rows,"<p>I work with following table:</p>

<pre><code>CREATE TABLE IF NOT EXISTS lp_registry.domain (
    ownerid text,
    name1st text,
    name2nd text,
    name3rd text,
    registrar text,
    registered timestamp,
    expiration timestamp,
    updated timestamp,
    technologies list&lt;text&gt;,
    techversions list&lt;text&gt;,
    ssl boolean,
    PRIMARY KEY (
        (name1st, name2nd, name3rd), 
        registrar, ownerid, registered, expiration, updated
    )
);
</code></pre>

<p>Table isn't updated only new rows are added. Everytime crawler checks domain, new row is added.</p>

<p>I am performing this select:</p>

<pre><code>SELECT * FROM lp_registry.domain WHERE 
    registrar = 'REG-WEDOS' AND 
    ownerid = 'FORPSI-JAF-S497436' 
ALLOW FILTERING;
</code></pre>

<p>But what I want in the result are only the rows with latest 'updated' value for each unique ""name3rd.name2nd.name1st"". </p>

<p>If I were in a standard SQL database, I would use nested select with MAX or GROUP BY. However, this is not supported by Cassandra (<a href=""https://stackoverflow.com/questions/17342176/max-distinct-and-group-by-in-cassandra"">MAX(), DISTINCT and group by in Cassandra</a>). But what I should do in CQL?</p>
",<php><cassandra><cql><cassandra-2.2><nosql>,"<p>Extending onto <a href=""https://stackoverflow.com/a/34566105/4406006"">Cedric's answer</a> (which is great advice and would consider that as the answer to accept) you would get a table structure roughly like:</p>



<pre><code>CREATE TABLE IF NOT EXISTS lp_registry.domain (
    ownerid text,
    name1st text,
    name2nd text,
    name3rd text,
    registrar text,
    registered timestamp,
    expiration timestamp,
    updated timestamp,
    technologies list&lt;text&gt;,
    techversions list&lt;text&gt;,
    ssl boolean,
    PRIMARY KEY ((registrar, ownerid), updated, name1st, name2nd, name3rd)
) WITH CLUSTERING ORDER BY (updated desc);
</code></pre>

<p>When selecting data it will return rows with the most recent <code>updated</code> values within the partition for the registrar and ownerid you are querying.</p>

<p>This query would be incredibly fast because your data will be organized on disk by registrar, owner id with rows in order by updated descending.  </p>

<p>This is a key concept with cassandra in that your data is organized based on how you query it.  You lose flexibility in your queries, but you can feel comfortable that you are going to get great performance because you are retrieving data as it is organized.  This is why denormalizing your data based on your queries is vital.</p>

<p>Where things become complicated is if you wanted to retrieve the most recently updated of <em>all</em> data.  That problem is not easily solvable with cassandra unless everything shares the same partition which has its own set of problems (<a href=""https://stackoverflow.com/a/28308552/4406006"">example strategy using a 'dummy' partition key</a>).</p>
",['table']
34570367,34570751,2016-01-02 20:25:05,Cassandra 3.0 updated SSTable format,"<p>According to <a href=""https://issues.apache.org/jira/browse/CASSANDRA-8099"" rel=""noreferrer"">this</a> issue, Cassandra's storage format was updated in 3.0.</p>

<p>If previously I could use cassandra-cli to see how the SSTable is built, to get something like this:</p>

<pre><code>[default@test] list phonelists;
-------------------
RowKey: scott
=&gt; (column=, value=, timestamp=1374684062860000)
=&gt; (column=phonenumbers:bill, value='555-7382', timestamp=1374684062860000)
=&gt; (column=phonenumbers:jane, value='555-8743', timestamp=1374684062860000)
=&gt; (column=phonenumbers:patricia, value='555-4326', timestamp=1374684062860000)
-------------------
RowKey: john
=&gt; (column=, value=, timestamp=1374683971220000)
=&gt; (column=phonenumbers:doug, value='555-1579', timestamp=1374683971220000)
=&gt; (column=phonenumbers:patricia, value='555-4326', timestamp=137468397122
</code></pre>

<p>What would the internal formal look like in the latest version of Cassandra? Could you provide an example? </p>

<p>What utility can I use to see the internal representation of the table in Cassandra in a way listed above, but with a new SSTable format?</p>

<p>All that I have found on the internet is that the partition header how stores column names, row stores clustering values and that there are no duplicated values. </p>

<p>How can I look into it?</p>
",<cassandra><storage><cassandra-2.0><cassandra-cli><cassandra-3.0>,"<p>Prior to 3.0 <a href=""https://docs.datastax.com/en/cassandra/1.2/cassandra/tools/toolsSstable2JsonUtilsTOC.html"" rel=""nofollow noreferrer"">sstable2json</a> was a useful utility for getting an understanding of how data is organized in SSTables.   This feature is not currently present in cassandra 3.0, but there will be an alternative eventually.  Until then myself and Chris Lohfink have developed an alternative to sstable2json (<a href=""https://github.com/tolbertam/sstable-tools"" rel=""nofollow noreferrer"">sstable-tools</a>) for Cassandra 3.0 which you can use to understand how data is organized.  There is some talk about bringing this into cassandra proper in <a href=""https://issues.apache.org/jira/browse/CASSANDRA-7464"" rel=""nofollow noreferrer"">CASSANDRA-7464</a>.</p>

<blockquote>
  <p>A key differentiator between the storage format between older verisons of Cassandra and Cassandra 3.0 is that an SSTable was previously a representation of partitions and their cells (identified by their clustering and column name) whereas with Cassandra 3.0 an SSTable now represents partitions and their rows. </p>
</blockquote>

<p>You can read about these changes in more detail by visiting this <a href=""http://www.datastax.com/2015/12/storage-engine-30"" rel=""nofollow noreferrer"">blog post</a> by the primary developer of these changes who does a great job explaining it in detail.</p>

<p>The largest benefit you will see is that in the general case your data size will shrink (in some cases by a large factor), as a lot of the overhead introduced by CQL has been eliminated by some key enhancements.</p>

<p>Here's an example showing the difference between C* 2 and 3.</p>

<p>Schema:</p>

<pre><code>create keyspace demo with replication = {'class': 'SimpleStrategy', 'replication_factor': 1};
use demo;
create table phonelists (user text, person text, phonenumbers text, primary key (user, person));
insert into phonelists (user, person, phonenumbers) values ('scott', 'bill', '555-7382');
insert into phonelists (user, person, phonenumbers) values ('scott', 'jane', '555-8743');
insert into phonelists (user, person, phonenumbers) values ('scott', 'patricia', '555-4326');
insert into phonelists (user, person, phonenumbers) values ('john', 'doug', '555-1579');
insert into phonelists (user, person, phonenumbers) values ('john', 'patricia', '555-4326');
</code></pre>

<p>sstable2json C* 2.2 output:</p>

<pre><code>[
{""key"": ""scott"",
 ""cells"": [[""bill:"","""",1451767903101827],
           [""bill:phonenumbers"",""555-7382"",1451767903101827],
           [""jane:"","""",1451767911293116],
           [""jane:phonenumbers"",""555-8743"",1451767911293116],
           [""patricia:"","""",1451767920541450],
           [""patricia:phonenumbers"",""555-4326"",1451767920541450]]},
{""key"": ""john"",
 ""cells"": [[""doug:"","""",1451767936220932],
           [""doug:phonenumbers"",""555-1579"",1451767936220932],
           [""patricia:"","""",1451767945748889],
           [""patricia:phonenumbers"",""555-4326"",1451767945748889]]}
]
</code></pre>

<p>sstable-tools toJson C* 3.0 output:</p>

<pre><code>[
  {
    ""partition"" : {
      ""key"" : [ ""scott"" ]
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""clustering"" : [ ""bill"" ],
        ""liveness_info"" : { ""tstamp"" : 1451768259775428 },
        ""cells"" : [
          { ""name"" : ""phonenumbers"", ""value"" : ""555-7382"" }
        ]
      },
      {
        ""type"" : ""row"",
        ""clustering"" : [ ""jane"" ],
        ""liveness_info"" : { ""tstamp"" : 1451768259793653 },
        ""cells"" : [
          { ""name"" : ""phonenumbers"", ""value"" : ""555-8743"" }
        ]
      },
      {
        ""type"" : ""row"",
        ""clustering"" : [ ""patricia"" ],
        ""liveness_info"" : { ""tstamp"" : 1451768259796202 },
        ""cells"" : [
          { ""name"" : ""phonenumbers"", ""value"" : ""555-4326"" }
        ]
      }
    ]
  },
  {
    ""partition"" : {
      ""key"" : [ ""john"" ]
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""clustering"" : [ ""doug"" ],
        ""liveness_info"" : { ""tstamp"" : 1451768259798802 },
        ""cells"" : [
          { ""name"" : ""phonenumbers"", ""value"" : ""555-1579"" }
        ]
      },
      {
        ""type"" : ""row"",
        ""clustering"" : [ ""patricia"" ],
        ""liveness_info"" : { ""tstamp"" : 1451768259908016 },
        ""cells"" : [
          { ""name"" : ""phonenumbers"", ""value"" : ""555-4326"" }
        ]
      }
    ]
  }
]
</code></pre>

<p>While the output is larger (that is more of a consequence of the tool).  The key differences you can see are:</p>

<ol>
<li>Data is now a collection of Partitions and their Rows (which include cells) instead of a collection of Partitions and their Cells.</li>
<li>Timestamps are now at the row level (liveness_info) instead of at the cell level.  If some row cells differentiate in their timestamps, the new storage engine does delta encoding to save space and associated the difference at the cell level.  This also includes TTLs.  As you can imagine this saves a lot of space if you have a lot of non-key columns as the timestamp does not need to be repeated.</li>
<li>The clustering information (in this case we are clustered on 'person') is now present at the Row level instead of cell level, which saves a bunch of overhead as the clustering column values don't have to be at the cell level.</li>
</ol>

<p>I should note that in this particular example data case the benefits of the new storage engine aren't completely realized since there is only 1 non-clustering column.</p>

<p>There are a number of other improvements not shown here (like the ability to store row-level range tombstones).</p>
",['table']
34575509,34580848,2016-01-03 10:28:36,Search in user defined type with Apache Cassandra,"<p>In this example:</p>

<pre><code>CREATE TYPE address (
    street text,
    city text,
    zip_code int,
    phones set&lt;text&gt;
)

CREATE TABLE users (
    id uuid PRIMARY KEY,
    name text,
    addresses map&lt;string, address&gt;
)
</code></pre>

<p>How can I query users with <code>city = newyork</code> or find a user with a specific phone number.</p>
",<cassandra><cql3><user-defined-types>,"<p>This is not really a problem of querying a user-defined type: imagine that <code>address</code> would be a single <code>text</code> column and that <code>addresses</code> would contain a single address (ie. <code>addresses TEXT</code>); the problem would be the same.</p>

<p>Your user table is not meant to be query-able by anything else than the primary key, which in this case is the partition key, which is a <code>UUID</code> which makes it quasi useless.</p>

<p>If you want to query the users by name I would denormalize (that implies some duplication) and make a <code>users_by_name</code> table:</p>

<pre><code>CREATE TABLE users_by_name(
  name TEXT,
  id UUID,
  addresses whatever,
  PRIMARY KEY((name), id)
)
</code></pre>

<p>where the users are stored by <code>name</code> (they should be unique) and the results will be retrieved sorted by <code>id</code> (<code>id</code> is the clustering key part of the primary key).</p>

<p>Same goes for query by addresses: </p>

<pre><code>CREATE TABLE users_by_name(
  city TEXT,
  street TEXT,
  name TEXT,
  id UUID,
  PRIMARY KEY((city), street)
)
</code></pre>

<p>You might think that it does not really solve your problem, but it looks like you designed your data model from a relational DB (SQL) point of view, this is not the goal with Cassandra.</p>
",['table']
34635744,34636066,2016-01-06 14:39:18,Columns ordering in Cassandra,"<p>When I create a table in CQL, is it necessary to be exact for the order of column that are <strong>NOT</strong> in the primary_key and <strong>NOT</strong> clustering columns : </p>

<pre><code>CREATE TABLE user (
    a ascii,
    b ascii,
    c ascii,
    PRIMARY KEY (a)
);
</code></pre>

<p>Is it equivalent to ?</p>

<pre><code>CREATE TABLE user (
    a ascii,
    c ascii, &lt;-- switched
    b ascii, &lt;-- switched
    PRIMARY KEY (a)
);
</code></pre>

<p>Thank you for your help</p>
",<cassandra><cql>,"<p>Both of those statements will fail, because of:</p>

<ul>
<li>The extra comma.</li>
<li>You have not provided a primary key definition.</li>
</ul>

<p>Assuming you had those fixed, then the answer is still ""yes they are the same.""  </p>

<p>Cassandra applies its own order to your columns at table creation time.  Consider this table as I have typed it:</p>

<pre><code>CREATE TABLE testorder ( 
  acolumn text,
  jcolumn text,
  dcolumn text,
  bcolumn text,
  apkey text,
  bpkey text,
  ackey text,
  bckey text,
  PRIMARY KEY ((bpkey,apkey),bckey,ackey));
</code></pre>

<p>After creating it, I'll describe the table so you can see the order that Cassandra has applied to the columns.</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; desc table testorder ;

CREATE TABLE stackoverflow.testorder (
    bpkey text,
    apkey text,
    bckey text,
    ackey text,
    acolumn text,
    bcolumn text,
    dcolumn text,
    jcolumn text,
    PRIMARY KEY ((bpkey, apkey), bckey, ackey)
) WITH CLUSTERING ORDER BY (bckey ASC, ackey ASC)
</code></pre>

<p>Essentially, Cassandra will order the partition keys and the clustering keys (ordered by their precedence in the PRIMARY KEY definition), and then the columns follow in ascending order.</p>
",['table']
34643547,34644910,2016-01-06 21:54:18,CQL partial key filter,"<p>I have a table in Cassandra where the key (now looking from a business perspective) is structure like this.
Example of a key + values:</p>

<pre><code>Key (exists of 6 columns)             Date/value
A | B | C | D | E | F |    -&gt; 2000-01 : 100, 2000-02 : 220, ....
A | B | C | D | X | F |    -&gt; 2000-01 : 100, 2000-02 : 233, ....
A | B | C | D | Y | F |    -&gt; 2000-01 : 111, 2000-02 : 210, ....
A | Z | C | D | E | F |    -&gt; 2000-01 : 122, 2000-02 : 230, ....
</code></pre>

<p>Each key has a values per certain date or month (the amount of those date/value records per key is very low. Arround 200 for now). However, the amount of the keys is high.</p>

<p>From a business side, it's very unusal to query only for one specific key like A B C D E F. The user will apply only partial filtering like: </p>

<pre><code>* * C D * *
</code></pre>

<p>In this case query should return all 6 records. He might also want to filter on the date/month, however, given the low amount of data this is a lower priority.</p>

<p>Since CQL does not allow partial table key filtering (beside the ALLOW FILTERING), I'm not sure how to structure my table. Any ideas? Or is this a case which does not fit well into Cassandra?</p>

<p>THank you</p>
",<cassandra><cql>,"<p>When modeling with Cassandra, you'll need to create a table for each way you want to query your data.  So if you want to query by <code>D=some_val</code>, you'll need another table that can answer that query - that is, it'll have to have D as it's partition key.</p>

<p>Your other option is to do full table scans and manually filter, or to use something like Spark to work with your entire dataset.  I wrote a blog post a little while ago that shows what you can do with Spark &amp; Cassandra.  <a href=""http://rustyrazorblade.com/2015/07/cassandra-pyspark-dataframes-revisted/"" rel=""nofollow"">http://rustyrazorblade.com/2015/07/cassandra-pyspark-dataframes-revisted/</a></p>
",['table']
34651000,34653725,2016-01-07 09:03:22,NoHostAvailable exception connecting to Cassandra from python,"<p>I am obtaining an exception when trying to connect to my local instance of Cassandra from Python. I can connect to Cassandra with no problems using cqlsh. The version I am running is Cassandra 3.01 on ubuntu:</p>

<pre><code>cqlsh
Connected to Test Cluster at 127.0.0.1:9042.
[cqlsh 5.0.1 | Cassandra 3.0.1 | CQL spec 3.3.1 | Native protocol v4]
</code></pre>

<p>The exception I obtain is below:</p>

<pre><code>ERROR:cassandra.cluster:Control connection failed to connect, shutting down Cluster:
Traceback (most recent call last):
  File ""cassandra/cluster.py"", line 840, in   cassandra.cluster.Cluster.connect (cassandra/cluster.c:11146)
  File ""cassandra/cluster.py"", line 2088, in cassandra.cluster.ControlConnection.connect (cassandra/cluster.c:36955)
  File ""cassandra/cluster.py"", line 2123, in cassandra.cluster.ControlConnection._reconnect_internal (cassandra/cluster.c:37811)
 NoHostAvailable: ('Unable to connect to any servers', {'127.0.0.1':       InvalidRequest(u'code=2200 [Invalid query] message=""unconfigured table schema_keyspaces""',), 'localhost': InvalidRequest(u'code=2200 [Invalid query] message=""unconfigured table schema_keyspaces""',)
</code></pre>

<p>I have checked my cassandra.yaml file and it looks ok:</p>

<pre><code>egrep 'rpc_port:|native_transport_port:' /etc/cassandra/cassandra.yaml
native_transport_port: 9042
rpc_port: 9160
</code></pre>

<p>Anything else I can look at ? Suggestions are most welcome.</p>
",<python-2.7><cassandra>,"<p>It looks like you are attempting to connect to a 3.0.1 server using an older install of cqlsh or you are (somehow) using an older python driver.</p>

<p>The error message you are getting:</p>

<pre><code>(u'code=2200 [Invalid query] message=""unconfigured table schema_keyspaces""',)
</code></pre>

<p>indicates that the client driver is attempting to get table metadata from the <code>schema_keyspaces</code> table which pre-dates 3.0. This information is now held in the <code>system_schema.keyspaces</code> table.</p>
",['table']
34703638,34704092,2016-01-10 08:48:23,Cassandra CQL: Batch select,"<p>Hi I have following table in Cassandra:</p>

<pre><code>* ---------------------------------------------------------------------------
* Note:
* 'curr_pos' is always fixed, so we can put it into cluster key and order
* In each crawler iteration 'prev_pos', 'domain_*' are updated
* -------------------------------------------------------------------------
* Patterns:
* &lt;domain_name3rd&gt;.&lt;domain_name2nd&gt;.&lt;domain_name1st&gt;
* --------------------------------------------------------------------------
CREATE TABLE IF NOT EXISTS lp_registry.keyword_position (
    engine text,
    keyword text,
    updated timestamp,
    domain_name1st text,
    domain_name2nd text,
    domain_name3rd text,
    prev_pos int,
    curr_pos int,
    PRIMARY KEY ((engine, keyword), curr_pos)   
);
</code></pre>

<p>In the top-level application I have a lists with circa hundreds of keywords.</p>

<p><strong>What I need?</strong></p>

<p>For fixed engine and keyword list i want to select all domains and their position.</p>

<p><strong>Update</strong>: The result given by application will be a NxM matrix for each engine, with N user defined keywords and M user defined domains. In each cell will be position of domain for specific keyword.</p>

<p><strong>What I am confused with?</strong></p>

<p>I need to post N selects depending on size of the list with keywords. In other words, I need iterate through keywords in the app and in the each iteration send select to DB.</p>

<p>I expect that N won't be greater than 100, but still i think that this is too many queries. </p>

<p><strong>My question</strong></p>

<p>Can I pack these selects into a single batch? How?</p>
",<php><cassandra><cql><cassandra-2.2><nosql>,"<p>It is not really a problem of batch query but a problem with the design of your table.</p>

<p>If the query you're describing is a ""core"" query of your application then you should design the table in a way that this is a single query, <em>ie.</em> <code>engine</code> and <code>keyword</code> should be clustering keys and not partition keys.</p>

<p>To give more specific advice: how do you obtain the list of engines and keywords, is there some that is logically grouping them? That could be the partition key of your table.</p>
",['table']
34730779,34731094,2016-01-11 20:48:14,How to store microsecond level timestamps in cassandra?,"<p>I'm trying to store data in cassandra which contains microsecond level timestamps.
Cassandra's docs say that the 'timestamp' data type can store milliseconds since epoch but several messages on the internet seem to imply that cassandra can natively store microsecond timestamps.</p>

<p>What is the best way practice for storing microsecond level times in cassandra? Should I just leave out the date part and store a long?</p>

<p>I'm trying to sore a columsn which look like this:
2015-11-18 07:30:46.700824</p>

<p>I get the following error:</p>

<p>ErrorMessage code=2200 [Invalid query] message=""unable to coerce '2015-11-18 07:30:18.261543' to a  formatted date (long)""</p>

<p>Aborting import at record #1. Previously inserted records are still present, and some records after that may be present as well.</p>

<p>My cassandra version:
[cqlsh 5.0.1 | Cassandra 2.1.11 | CQL spec 3.2.1 | Native protocol v3]</p>

<p>EDIT:
Here is an example of microsecond confusion in Cassandra's own docs:</p>

<p>""CAS and new features in CQL such as DROP COLUMN assume that cell timestamps are microseconds-since-epoch""</p>

<p><a href=""https://docs.datastax.com/en/upgrade/doc/upgrade/cassandra/upgradeChangesC_c.html"" rel=""nofollow"">https://docs.datastax.com/en/upgrade/doc/upgrade/cassandra/upgradeChangesC_c.html</a></p>

<p>Another: <a href=""https://issues.apache.org/jira/browse/CASSANDRA-8297"" rel=""nofollow"">https://issues.apache.org/jira/browse/CASSANDRA-8297</a></p>

<p>EDIT2:
I should mention that I intend to query this using spark. From what I understand, spark parses its own flavor of sql and translates it to cassandra (although I'm using CassandraContext in zeppelin). Anything which might help or hinder my search for microsecond level timestmaps?</p>
",<cassandra>,"<p>You can use bigint or a timeuuid. Type 1 uuid's have 100ns precision so it can cover you. Some utilities, libraries, convenience functions may not give you what you need though so be prepared to write some uuid functions.</p>
",['precision']
34730897,34731825,2016-01-11 20:55:32,How do I bind a dict as a preparedquery parameter?,"<p>I want to bind a <code>dict</code> to a parameter of a prepared statement but I can't seem to figure out the syntax. What's confusing me is that it works if I use <a href=""https://datastax.github.io/python-driver/getting_started.html#passing-parameters-to-cql-queries"" rel=""nofollow"">positional parameters</a> without a prepared statement.</p>

<p>Take a look at this example:</p>

<pre><code>from cassandra.cluster import Cluster


def works():
    id = '1'
    mydict = {'count': 1, 'value': 3}

    updateStmt = ""insert into test.prep_test (id, mydict) values (%s, %s);""

    session.execute(updateStmt, (id, mydict))


def doesntwork():
    id = '2'
    mydict = {'count': 1, 'value': 3}

    updateStmt = ""insert into test.prep_test (id, mydict) values (?, ?);""
    prep = session.prepare(updateStmt)

    session.execute(prep, [id, mydict])


if __name__ == ""__main__"":
    cluster = Cluster(['127.0.0.1'])
    session = cluster.connect('test')

    session.execute(
        'create table if not exists test.prep_test (  id ascii, mydict MAP&lt;ascii,decimal&gt;, PRIMARY KEY (id));')

    works()
    doesntwork()

    session.shutdown()
    cluster.shutdown()
</code></pre>

<p>The <code>works()</code> method inserts data just fine. However, the <code>doesntwork()</code> method fails with the following error:</p>

<pre><code>Traceback (most recent call last):
  File ""/new.py"", line 31, in &lt;module&gt;
    doesntwork()
  File ""/new.py"", line 20, in doesntwork
    session.execute(prep, [id, mydict])
  File ""cassandra/cluster.py"", line 1569, in cassandra.cluster.Session.execute (cassandra/cluster.c:26912)
  File ""cassandra/cluster.py"", line 1619, in cassandra.cluster.Session.execute_async (cassandra/cluster.c:27219)
  File ""cassandra/cluster.py"", line 1632, in cassandra.cluster.Session._create_response_future (cassandra/cluster.c:27553)
  File ""cassandra/query.py"", line 411, in cassandra.query.PreparedStatement.bind (cassandra/query.c:5947)
  File ""cassandra/query.py"", line 536, in cassandra.query.BoundStatement.bind (cassandra/query.c:7678)
TypeError: Received an argument of invalid type for column ""mydict"". Expected: &lt;class 'cassandra.io.asyncorereactor.MapType(AsciiType, DecimalType)'&gt;, Got: &lt;type 'dict'&gt;; (Non-Decimal type received for Decimal value)

Process finished with exit code 1
</code></pre>
",<python><python-2.7><cassandra>,"<p>When you run it in works() case, the driver under the covers is using string substitution to produce the following query, and submit it to C*</p>

<pre><code>'''insert into test.prep_test1 (id, mydict) values ('1', {'count': 1, 'value': 3});'''
</code></pre>

<p>This is basically a dumb string substitution.</p>

<p>In the second case, multiple messages are passing between the server and client. Firstly a statement is prepared. Context about this prepared statement is shared with the C* server. The validation here is going to be more strict, because the context associated with the query is known to driver, and server. </p>

<p>You can work around this by using the appropriate type, in the query. Which in this case is the would be to bind mydict to Decimal values as suggested above.</p>

<pre><code>mydict = {'count': Decimal(1), 'value': Decimal(3)}
</code></pre>

<p>Alternatively you could use an int type in your table like this.</p>

<pre><code> 'create table if not exists test.prep_test (  id ascii, mydict MAP&lt;ascii,int&gt;, PRIMARY KEY (id));')
</code></pre>

<p>You could argue that the driver should be able to map from int, or float types to decimal, but that's not the way it works as of right now.</p>
",['table']
34756326,34758535,2016-01-13 00:23:11,How to change Cassandra indexes to allow query,"<p>I'm using Cassandra to store pageviews in a very simple matter. However I'm unable to perform the queries I want due to indices which I set up wrongfully.</p>

<pre><code>CREATE TABLE my_site.pageviews (
    url text,
    createdat timestamp,
    userid text,
    PRIMARY KEY ((url, createdat, userid))
)
</code></pre>

<p>I found out the hard way that I wasn't able to query, unless I specified all parts of the primary key in a query.</p>

<p>How do I need to configure my PRIMARY KEY to allow for these queries?:</p>

<pre><code>SELECT * FROM pageviews WHERE url = ? AND createdat &gt; ?

SELECT * FROM pageviews WHERE userid = ? AND createdat &gt; ?
</code></pre>

<p>Any guidance would be greatly appreciated!</p>
",<indexing><cassandra>,"<p>For the types of lookups you will need to use two tables that will look like this:</p>

<pre><code>CREATE TABLE my_site.pageviews_by_url (
    url text,
    createdat timestamp,
    userid text,
    PRIMARY KEY ((url), createdat, userid)
)

CREATE TABLE my_site.pageviews_by_userid (
    url text,
    createdat timestamp,
    userid text,
    PRIMARY KEY ((userid),createdat,url)
)
</code></pre>

<p>If you'll notice the subtle difference. Table ""pageviews_by_url"" has only 'url' as a partition key and the other columns as clustering keys. Table ""pageviews_by_userid"" is mostly the same table but with 'userid' as the partition key. On a SELECT query, using an = on the partition key and then the > on the first clustering column, will get you the results you are looking for when you do the following queries:</p>

<pre><code>SELECT * FROM pageviews WHERE url = ? AND createdat &gt; ?

SELECT * FROM pageviews WHERE userid = ? AND createdat &gt; ?
</code></pre>

<p>When inserting data into these tables, it's a good case to use a BATCH statement. Insert to both tables at the same time. </p>
",['table']
34757922,35213418,2016-01-13 03:30:02,Could not retrieve endpoint ranges: java.lang.IllegalArgumentException,"<p>I am tyring to load sstables to cassandra using sstableloader utility. But I am getting the following error.</p>

<pre><code>&gt; java.lang.IllegalArgumentException
java.lang.RuntimeException: Could not retrieve endpoint ranges: 
    at org.apache.cassandra.tools.BulkLoader$ExternalClient.init(BulkLoader.java:338)
    at org.apache.cassandra.io.sstable.SSTableLoader.stream(SSTableLoader.java:156)
    at org.apache.cassandra.tools.BulkLoader.main(BulkLoader.java:106)
Caused by: java.lang.IllegalArgumentException
    at java.nio.Buffer.limit(Buffer.java:275)
    at org.apache.cassandra.utils.ByteBufferUtil.readBytes(ByteBufferUtil.java:543)
    at org.apache.cassandra.serializers.CollectionSerializer.readValue(CollectionSerializer.java:124)
    at org.apache.cassandra.serializers.MapSerializer.deserializeForNativeProtocol(MapSerializer.java:101)
    at org.apache.cassandra.serializers.MapSerializer.deserializeForNativeProtocol(MapSerializer.java:30)
    at org.apache.cassandra.serializers.CollectionSerializer.deserialize(CollectionSerializer.java:50)
    at org.apache.cassandra.db.marshal.AbstractType.compose(AbstractType.java:68)
    at org.apache.cassandra.cql3.UntypedResultSet$Row.getMap(UntypedResultSet.java:287)
    at org.apache.cassandra.config.CFMetaData.fromSchemaNoTriggers(CFMetaData.java:1824)
    at org.apache.cassandra.config.CFMetaData.fromThriftCqlRow(CFMetaData.java:1117)
    at org.apache.cassandra.tools.BulkLoader$ExternalClient.init(BulkLoader.java:330)
    ... 2 mor
</code></pre>

<p>the command I am using to load the sstable is</p>

<pre><code>$bin/sstableloader -d nodename -u username -pw password path/to/sstable/keyspacename/tablename
</code></pre>

<p>this was working a few days back .I am not sure whats changed and how to debug it ? 
I am using datastax. 
I am loading the sstable from the same node which is in the cluster.i.e my source and destination node are same.
Has someone seen this error before ?
Cassandra version : 2.1 
Any Help is appreciated.</p>
",<cassandra><datastax><datastax-enterprise><bulkloader>,"<p>I had this problem again, so debugged it a little for the root cause. The problem is if at any time you have altered your cassandra table by dropping some column. it triggers a bug for sstableLoader. That why dropping the table and creating it again works.</p>
",['table']
34760379,34761839,2016-01-13 07:06:57,Why is my data insertion in my Cassandra database sometimes stable and sometimes slow?,"<p>This is my query if the current data ID is present or absent in the Cassandra database:</p>

<pre class=""lang-python prettyprint-override""><code>row = session.execute(""SELECT * FROM articles where id = %s"", [id]) 
</code></pre>

<p>Resolved messages in Kafka, then determine whether or not this message exists in the Cassandra database if it does not exist, then it should perform an insert operation, if it does exist, it should not be inserted in the data.</p>

<pre class=""lang-python prettyprint-override""><code>messages = consumer.get_messages(count=25)

if len(messages) == 0:
    print 'IDLE'
    sleep(1)
    continue

for message in messages:
    try:
        message = json.loads(message.message.value)
        data = message['data']
        if data:
            for article in data:
                source = article['source']
                id = article['id']
                title = article['title']
                thumbnail = article['thumbnail']
                #url = article['url']
                text = article['text']
                print article['created_at'],type(article['created_at'])
                created_at = parse(article['created_at'])
                last_crawled = article['last_crawled']
                channel = article['channel']#userid
                category = article['category']
                #scheduled_for = created_at.replace(minute=created_at.minute + 5, second=0, microsecond=0)
                scheduled_for=(datetime.utcnow() + timedelta(minutes=5)).replace(second=0, microsecond=0)
                row = session.execute(""SELECT * FROM articles where id = %s"", [id])
                if len(list(row))==0:
                #id parse base62
                    ids = [id[0:2],id[2:9],id[9:16]]
                    idstr=''
                    for argv in ids:
                        num = int(argv)
                        idstr=idstr+encode(num)
                    url='http://weibo.com/%s/%s?type=comment' % (channel,idstr)
                    session.execute(""INSERT INTO articles(source, id, title,thumbnail, url, text, created_at, last_crawled,channel,category) VALUES (%s,%s, %s, %s, %s, %s, %s, %s, %s, %s)"", (source, id, title,thumbnail, url, text, created_at, scheduled_for,channel,category))
                    session.execute(""INSERT INTO schedules(source,type,scheduled_for,id) VALUES (%s, %s, %s,%s) USING TTL 86400"", (source,'article', scheduled_for, id))
                    log.info('%s %s %s %s %s %s %s %s %s %s' % (source, id, title,thumbnail, url, text, created_at, scheduled_for,channel,category))

    except Exception, e:
        log.exception(e)
        #log.info('error %s %s' % (message['url'],body))
        print e
        continue
</code></pre>

<p>I have one ID which only has one unique table row, which I want to be like this. As soon as I add different scheduled_for times for the unique ID my system crashes. Add this <code>if len(list(row))==0:</code> is the right thought but my system is very slow after that.</p>

<p>This is my table description:</p>

<pre class=""lang-none prettyprint-override""><code>DROP TABLE IF EXISTS schedules;

CREATE TABLE schedules (
 source text,
 type text,
 scheduled_for timestamp,
 id text,
 PRIMARY KEY (source, type, scheduled_for, id)
);
</code></pre>

<p>This scheduled_for is changeable. Here is also a concrete example:</p>

<pre class=""lang-none prettyprint-override""><code>Hao article 2016-01-12 02:09:00+0800 3930462206848285
Hao article 2016-01-12 03:09:00+0801 3930462206848285
Hao article 2016-01-12 04:09:00+0802 3930462206848285
Hao article 2016-01-12 05:09:00+0803 3930462206848285
</code></pre>

<p>Here is my article CQL schema:</p>

<pre class=""lang-none prettyprint-override""><code>CREATE TABLE crawler.articles (
    source text,
    created_at timestamp,
    id text,
    category text,
    channel text,
    last_crawled timestamp,
    text text,
    thumbnail text,
    title text,
    url text,
    PRIMARY KEY (source, created_at, id)
) WITH CLUSTERING ORDER BY (created_at DESC, id ASC)
AND bloom_filter_fp_chance = 0.01
AND caching = '{""keys"":""ALL"", ""rows_per_partition"":""ALL""}'
AND comment = ''
AND compaction = {'sstable_size_in_mb': '160', 'enabled': 'true', 'unchecked_tombstone_compaction': 'false', 'tombstone_compaction_interval': '86400', 'tombstone_threshold': '0.2', 'class': 'org.apache.cassandra.db.compaction.LeveledCompactionStrategy'}
AND compression = {'sstable_compression': 'org.apache.cassandra.io.compress.LZ4Compressor'}
AND dclocal_read_repair_chance = 0.1
AND default_time_to_live = 604800
AND gc_grace_seconds = 864000
AND max_index_interval = 2048
AND memtable_flush_period_in_ms = 0
AND min_index_interval = 128
AND read_repair_chance = 0.0
AND speculative_retry = '99.0PERCENTILE';

CREATE INDEX articles_id_idx ON crawler.articles (id);
CREATE INDEX articles_url_idx ON crawler.articles (url);
</code></pre>
",<python><cassandra><scrapy><apache-kafka>,"<p>Looking at your SCHEMA and the way you use it I could assume that secondary index on ID field is creating problems and slowing down queries. You can check more details why secondary indexes are bad on many places just googling it (this <a href=""https://pantheon.io/blog/cassandra-scale-problem-secondary-indexes"" rel=""nofollow"">source</a> is a good start, also <a href=""https://docs.datastax.com/en/cql/3.1/cql/ddl/ddl_when_use_index_c.html"" rel=""nofollow"">DataStax documentation page</a>). Basically when you use secondary index in 5 node cluster you must hit each node to find item you are looking for and when using primary key each node knows which node holds data.</p>

<p>Secondary indexes are particularly bad if you use data with high cardinality (performance drops when you add more items) and you use ID which is different for each article. They are ok when you use low cardinality such as index some data by days of week (you knwo there will be only 7 days in a week so you can predict size of index table) or category in your case if you have finite number of categories.</p>

<p>I would advice to create one more table, <code>article_by_id</code> which will be reverse index to your article table. You can use <a href=""http://www.datastax.com/dev/blog/lightweight-transactions-in-cassandra-2-0"" rel=""nofollow"">Lightweight Transaction</a> and do <code>INSERT ... IF NOT EXISTS</code> first to that table and if operation returns <code>true</code> (meaning insert went through so record was not present previously) you can do regular INSERT to your <code>articles</code> table and if it return <code>false</code> (meaning data was not inserted because it already exists) you can skip INSERT to <code>articles</code> table.</p>

<p>Here is table (I would suggest to use UUID instead of text for ID but I created table based on your article table):</p>

<pre><code>CREATE TABLE article_by_id (
    id text,
    source text,
    created_at timestamp,
    PRIMARY KEY (id)
) WITH comment = 'Article by id.';
</code></pre>

<p>This way you can always find all parts of your key based on just ID. If ID is your input parameter selecting from this table will give you source and created_at.</p>

<p>Here is insert query which will return true or false:</p>

<pre><code>INSERT INTO article_by_id(id, source, created_at) VALUES (%s,%s, %s) IF NOT EXISTS; 
</code></pre>

<p>And more tip, if you can find key based on some non changeable data in your entity than you do not need second table. In example if source and created_at uniquely identifies article in your system and never change you can remove id and use your original table.</p>
",['table']
34763277,34764824,2016-01-13 09:48:24,How to model cassandra in this particular situations?,"<p>if I have table structure below, how can i query by </p>

<pre><code>""source = 'abc' and created_at &gt;= '2016-01-01 00:00:00'""?

CREATE TABLE articles (
    id text,
    source text,
    created_at timestamp,
    category text,
    channel text,
    last_crawled timestamp,
    text text,
    thumbnail text,
    title text,
    url text,
    PRIMARY KEY (id)
)
</code></pre>

<p>I would like to model my system according to this:
<a href=""http://www.ebaytechblog.com/2012/07/16/cassandra-data-modeling-best-practices-part-1/"" rel=""nofollow"">http://www.ebaytechblog.com/2012/07/16/cassandra-data-modeling-best-practices-part-1/</a></p>

<p>Edit:</p>

<p>What we are doing is very similar to what you are proposing. The difference is our primary key doesn't have brackets around source: 
<code>PRIMARY KEY (source, created_at, id)</code>. We also have two other indexes: </p>

<pre><code>CREATE INDEX articles_id_idx ON crawler.articles (id); 
CREATE INDEX articles_url_idx ON crawler.articles (url); 
</code></pre>

<p>Our system is really slow like this. What do you suggest?</p>

<p>Thanks for your replies!</p>
",<cassandra>,"<p>Given the table structure</p>

<pre><code>CREATE TABLE articles (
    id text,
    source text,
    created_at timestamp,
    category text,
    channel text,
    last_crawled timestamp,
    text text,
    thumbnail text,
    title text,
    url text,
    PRIMARY KEY ((source),created_at, id)
)
</code></pre>

<p>You can issue the following queries:</p>

<pre><code>SELECT * FROM articles WHERE source=xxx // Give me all article given the source xxx

SELECT * FROM articles WHERE source=xxx AND created_at &gt; '2016-01-01 00:00:00'; // Give me all articles whose source is xxx and created after 2016-01-01 00:00:00
</code></pre>

<p>The couple (created_at,id) in the primary key is here to guarantee article unicity. Indeed, it is possible to have, at the same created_at time, 2 different articles</p>
",['table']
34790674,34791781,2016-01-14 13:20:07,What are the implications of using lightweight transactions?,"<p>In particular I was looking at <a href=""http://docs.datastax.com/en/cassandra/latest/cassandra/dml/dmlLtwtTransactions.html"" rel=""noreferrer"">this page</a> where it says:</p>

<blockquote>
  <p>If lightweight transactions are used to write to a row within a partition, only lightweight transactions for both read and write operations should be used.</p>
</blockquote>

<p>I'm confused as to what using LWTs for read operations looks like. Specifically how this relates to per-query consistency (and serialConsistency) levels.</p>

<p>The <a href=""http://docs.datastax.com/en/cassandra/latest/cassandra/dml/dmlConfigConsistency.html"" rel=""noreferrer"">description</a> for <code>SERIAL</code> read consistency raises further questions:</p>

<blockquote>
  <p>Allows reading the current (and possibly uncommitted) state of data without proposing a new addition or update.</p>
</blockquote>

<p>That suggests that using <code>SERIAL</code> for reads is not ""using a LWT"".</p>

<p>But then </p>

<ul>
<li>How does Cassandra know to check for in-progress transactions when you do a read?</li>
<li>What is the new update that is proposed while you're trying to read, and how does this affect the read?</li>
<li>How would that work if the consistency you're reading at (say <code>ONE</code> for example) is less than the serialConsistency used for writing?</li>
<li>Once you use a LWT on a table (or row?, or column?), are all non-<code>SERIAL</code> reads forced to take the penalty of participating in quorums and the transaction algorithm?</li>
<li>Does the requirement actually apply to the whole row, or just the columns involved in the conditional statement?</li>
</ul>

<p>If I ignore this advice and make both serial and non-serial reads/writes. In what way to the LWTs fail?</p>
",<transactions><cassandra><datastax-java-driver>,"<blockquote>
  <p>How does Cassandra know to check for in-progress transactions when you
  do a read?</p>
</blockquote>

<p>This is exactly what the <code>SERIAL</code> consistency level indicates. It makes sure that a query will only return results after all pending transactions have been fully executed.</p>

<blockquote>
  <p>What is the new update that is proposed while you're trying to read,
  and how does this affect the read?</p>
</blockquote>

<p>I think what the doc is trying to say is that the read will be handled just like a LWT - just without make any updates on it's own. </p>

<blockquote>
  <p>How would that work if the consistency you're reading at (say ONE for example) is less than the serialConsistency used for writing?</p>
</blockquote>

<p>Reads using <code>SERIAL</code> will always imply <code>QUORUM</code> as consistency level. Reading with <code>ONE</code> will not provide you any guarantees provided by <code>SERIAL</code> and you can end up reading stalled data. </p>

<blockquote>
  <p>Once you use a LWT on a table (or row?, or column?), are all non-SERIAL reads forced to take the penalty of participating in quorums and the transaction algorithm?</p>
</blockquote>

<p>No. You can use non-SERIAL consistency levels for your queries and have them executed with the exact same performance characteristics as any other non-serial queries. </p>

<blockquote>
  <p>Does the requirement actually apply to the whole row, or just the columns involved in the conditional statement?</p>
</blockquote>

<p>No, I think you should be fine as long as you use different columns for serial reads/writes (including conditions) and regular reads/writes.</p>

<blockquote>
  <p>If I ignore this advice and make both serial and non-serial reads/writes. In what way to the LWTs fail?</p>
</blockquote>

<p>If you execute regular writes, not being executed as part of a LWT, those writes will be applied at any time, without interfering at all with the consensus process of LWTs. As a consequence, regular writes can in theory change a value that is part of a LWT condition at a time between evaluating the condition and applying the update, which is a potential cause for inconsistencies you wanted to avoid using LWTs. </p>
",['table']
34808811,34809260,2016-01-15 10:16:40,Cassandra's performance: data size and hardware,"<p>I need a high performance database for multiple concurrent read/write operations on a large data table and I don't know if Cassandra is a good candidate or not. Thus it would be great if you can help me to clarify my below questions. Let's say I have a table with 5 million of rows and 5 millions of columns.</p>

<p>1.Is cassandra's performance linear to the processing power of hardware?  </p>

<p>2.When I need to search for 1 column to see if it is existed or not, if not, then I want to insert a new one to the current table. Is this operation fast?</p>

<p>3.If the current response time of read/write operations is slow, what are the possible ways I can improve it without changing the structure of my current table?</p>

<p>Additional information:
<br>a. Transaction control is not important.
<br>b. Replication is depends on use cases. For the table that have multiple concurrent read/write operations, replication is not needed. For the table that have multiple concurrent read, replication is needed. </p>

<p>Thank you very much. </p>
",<cassandra>,"<blockquote>
  <p>1.Is cassandra's performance linear to the processing power of hardware?</p>
</blockquote>

<p>Cassandra overall performance is rather linear to the number of machines. For 1 machine, if you're using spinning disk, officially it is not recommended to exceed 1Tb/machine. The limit for SSD is higher, around 3Tb/machine. At least that's what recommended for Cassandra 2.1 and 2.2. With Cassandra 3.0 and the storage engine rewrite, those figures may be higher because server density has been improved.</p>

<blockquote>
  <p>2.When I need to search for 1 column to see if it is existed or not, if not, then I want to insert a new one to the current table. Is this operation fast?</p>
</blockquote>

<p>Lookup of data using <strong>primary key</strong> is quite fast thanks to a lot of data structure to optimize disk access (bloom filter, partition key cache, partition sample ... see <a href=""http://www.slideshare.net/doanduyhai/cassandra-introduction-apache-con-2014-budapest/48"">http://www.slideshare.net/doanduyhai/cassandra-introduction-apache-con-2014-budapest/48</a>)</p>

<p>If you're not accessing data by <strong>primary key</strong>, it will result in sequential scan on a lot of data and then performance is no longer guaranteed</p>

<blockquote>
  <p>3.If the current response time of read/write operations is slow, what are the possible ways I can improve it without changing the structure of my current table?</p>
</blockquote>

<p>It should be the other way around. Design your table structure and data model to have fast read (write operations are always fast with Cassandra). Appropriate hardware (SSD) and memory (for page cache) will also improve the read/write operations. Apart from those parameters above, the other tuning knobs (key cache size, bloom filter fp chance ...) only give marginal improvement</p>

<blockquote>
  <p>b. Replication is depends on use cases. For the table that have multiple concurrent read/write operations, replication is not needed. </p>
</blockquote>

<p>Without replication, data lost is possible with hardware failure, are you sure data lost is acceptable for a table that should serve read &amp; write ? </p>
",['table']
34871464,34871831,2016-01-19 08:08:30,How to programmatically determine the number of nodes in a Cassandra Cluster?,"<p>Is there a way to determine the number of nodes in a Cassandra cluster without first having a context?</p>

<p>I am trying to get that number to make sure that the user does not give me a replicating factor that is too large (i.e. says 10 with only 9 nodes.)</p>

<p><strong>Important:</strong> At this point, the only interface I have is thrift in C.</p>

<p><strong>Note:</strong> I looked into using the describe_ring() but unfortunately, the function forces you to have a valid context (so it describes the ring for that context and not the number of existing nodes in a Cassandra cluster.)</p>
",<c><cassandra><thrift><cassandra-2.0>,"<p>You can look at the system table using the Thrift protocol:  <strong>system.peers</strong>. Here are listed <strong>all others nodes</strong> and their information, but not the local node. By counting the number of nodes in <strong>system.peers</strong>, the total node count is entries_count_in_peers + 1</p>

<p>Below is the structure (CQL) of the <strong>system.peers</strong> table</p>

<pre><code>CREATE TABLE system.peers (
    peer inet PRIMARY KEY,
    data_center text,
    host_id uuid,
    preferred_ip inet,
    rack text,
    release_version text,
    rpc_address inet,
    schema_version uuid,
    tokens set&lt;text&gt;
)
</code></pre>

<p>There is one <strong>partition</strong> (row key in Thrift terminology) per node</p>
","['table', 'rpc_address', 'rack']"
34889485,34903856,2016-01-20 00:31:07,How do I define multiple table definitions and multiple column specs for cassandra-stress profile file?,"<pre><code># Keyspace Name
keyspace: demo1

# The CQL for creating a keyspace (optional if it already exists)
keyspace_definition: |
  CREATE KEYSPACE demo1;
# Table name
table: sample_test

# The CQL for creating a table you wish to stress (optional if it already exists)
table_definition: |
    CREATE TABLE sample_test (
        key1 blob PRIMARY KEY,
        value1 blob
    )

### Column Distribution Specifications ###

columnspec:
  - name: hash
    size: fixed(96)       #domain names are relatively short

  - name: body
    size: gaussian(100..300)    #the body of the blog post can be long
    population: uniform(1..10M)  #10M possible domains to pick from
</code></pre>

<p>Now how do I define another table within the same keyspace? All examples talk about just defining one table. I tried also defining another table definition and its column spec just like above but then I get the following error ""com.datastax.driver.core.exceptions.InvalidQueryException: Batch too large""</p>
",<cassandra><cassandra-2.1><cassandra-stress>,"<p>Unfortunately you cannot. A stress profile can contain only one <code>table_definition</code> and it can only have one table defined in it. It even verifies that theres only one definition matching the <code>table</code> declaration. You could try running two instances of the stress tool at same time to get that behavior, its inconvenient but I think its the only workaround available short of writing own benchmark (which is tricky).</p>
",['table']
34924308,34930782,2016-01-21 12:48:56,Are dummy partition keys always bad?,"<p>I can't find much on the subject of dummy partition keys in Cassandra, but what I can find tends to side with the idea that you should avoid them altogether. By dummy, I mean a column whose only purpose is to contain the same value for all rows, thereby putting all data on 1 node and giving the lowest possible cardinality. For example:</p>

<pre><code>dummy  | id   | name
-------------------------
0      | 01   | 'Oliver'
0      | 02   | 'James'
0      | 03   | 'Nicholls'
</code></pre>

<p>The two main points in regards to why you should avoid dummy partition keys are:</p>

<p>1) You end up with data ""hot-spots"". There is a lot of data stored on 1 node so there's more traffic around that node and you have poor distribution around the cluster.</p>

<p>2) Partition space is finite. If you put all data on one partition, it will eventually be incapable of storing any more data.</p>

<p>I can understand these points and I agree that you definitely want to avoid those situations, so I put this idea out of my mind and tried to think of a good partition key for my table. The table in question stores sites and there are two common ways that table gets queried in our system. Either a single site is requested or all sites are requested.</p>

<p>This puts me in a bit of an awkward situation, because the table is either queried on nothing or the site ID, and making a unique field the partition key would give me very high cardinality and high latency on queries that request all sites.</p>

<p>So I decided that I'd just choose an arbitrary field that would give relatively low cardinality, even though it doesn't reflect how the data will actually be queried, just because it's better than having a cardinality that is either excessively high or excessively low. This approach also has problems though.</p>

<p>I could partition my data on column x, but we have numerous clients, all of whom use our system differently, so x for 1 client could give the results I'm after, but could give awful results for another.</p>

<p>At this point I'm running out of options. I need a field in my table that will be consistent for all clients, however this field doesn't exist, so I'm now considering having a new field that will contain a random number from 1-3 and then partitioning on that field, which is essentially just a dummy field. The only difference is that I want to randomise the values a little bit as to avoid hot-spots and unbounded row growth.</p>

<p>I know this is a data-modelling question and it varies from system to system, and of course there are going to be situations where you have to choose the lesser of two evils (there is no perfect solution), but what I'm really focussed on with this question is:</p>

<p>Are dummy partition keys something that should outright never be a consideration in Cassandra, or are there situations in which they're seen as acceptable? If you think the former, then how would you approach this situation?</p>
",<cassandra>,"<blockquote>
<p>I can't find much on the subject of dummy partition keys in Cassandra, but what I can find tends to side with the idea that you should avoid them altogether.</p>
</blockquote>
<p>I'm going to go out on a limb and guess that your search has yielded my article <a href=""https://www.datastax.com/blog/we-shall-have-order"" rel=""nofollow noreferrer"">We Shall Have Order!</a>, where I made my position on the use of &quot;dummy&quot; partition keys quite clear.  Bearing that in mind, I'll try to provide some alternate solutions.</p>
<p>I see two potential problems to solve here.  The first:</p>
<blockquote>
<p>I need a field in my table that will be consistent for all clients, however this field doesn't exist</p>
</blockquote>
<p>Typically this is solved by duplicating your data into another query table.  That's the best way to serve multiple, varying query patterns.  If you have one client (service?) that needs to query that table by site id, then you could have that table duplicated into a table called <code>sites_by_id</code>.</p>
<pre><code>CREATE TABLE sites_by_id (
  id BIGINT,
  name TEXT,
  PRIMARY KEY (id));
</code></pre>
<p>The other problem is this query pattern:</p>
<blockquote>
<p>all sites are requested</p>
</blockquote>
<p>Another common Cassandra anti-pattern is that of unbound SELECTs (SELECT query without a WHERE clause).  I am sure you understand why these are bad, as they require all nodes/partitions to be read for completion (which is probably why you are looking into a &quot;dummy&quot; key).  But as the table supporting these types of queries increases in size, they will only get slower and slower over time...regardless of whether you execute an unbound SELECT or use a &quot;dummy&quot; key.</p>
<p>The solution here is to perform a re-examination of your data model, and business requirements.  Perhaps your data can be split up into sites by region or country?  Maybe your client really only needs the sites that have been updated for this year?  Obtaining some more details on the client's query requirements may help you find a good partitioning key for them to use.  Otherwise, if they really <em>do</em> need all of them all of the time, then doanduyhai's suggestion of using Spark will better fit your use case.</p>
",['table']
34928489,34928768,2016-01-21 16:01:59,Is Cassandra's hash for same value across multiple tables?,"<p>I have a multi-tenant application where tenantId would be part of every query, so I am putting it into the partition key for all tables.</p>

<p>EXAMPLES:</p>

<pre><code>CREATE TABLE users {
tenantId text,
user text,
active boolean,
PRIMARY_KEY (tenantId, user)
}

CREATE TABLE roles {
tenantId text,
rolename text,
PRIMARY_KEY (tenantId, rolename)
}
</code></pre>

<p><strong>Now, imagine 100s of tables like these...</strong></p>

<p><strong>My question is:</strong></p>

<p>Will Cassandra hash the tenantId 'foo' to point ALL data from ALL tables to the same node and make it a <em>uber</em> hotspot or will it evenly distribute each table &amp; tenant data around the cluster evenly?</p>
",<cassandra><cassandra-2.0>,"<p>Simple answer, the token value (hash of the partition key) is the same and it does not depend on the table name or whatever. The reason is that we use the same partition (<strong>Murmur3</strong>) within the whole cluster.</p>

<p>So in your case, yes, if your partition key is <em>tenantId</em>, all data from one customer will be distributed to the same replicas, and this applies for <strong>all tables</strong> having this partition key</p>
",['table']
34940989,34941454,2016-01-22 07:12:31,How to convert int column to float/double column in Cassandra database table,"<p>I am using cassandra database in production.I have one column field in<br>
    a cassandra table e.g coin_deducted  is int data type. </p>

<p>I need to convert coin_deducted in float/double data type. 
    But I tried to change data type by using alter<br>
    table command but cassandra is throwing incompatible issue while<br>
    converting int to float. Is there any way to do this?</p>

<p>e.g: currently it is showing like: </p>

<pre><code>   user_id | start_time | coin_deducted (int)                    
   122     | 26-01-01   | 12

I want to be 

  user_id | start_time | coin_deducted (float)                    
   122     | 26-01-01   | 12.0
</code></pre>

<p>Is it possible to copy entire one column field into new added column 
   field in same table?</p>
",<cassandra><cassandra-2.0>,"<p>Changing type of column is possible only if old type and new type are compatible. From documentation:</p>

<blockquote>
  <p>To change the storage type for a column, the type you are changing to
  and from must be compatible.</p>
</blockquote>

<p>One more proof that this cannot be done is when you write statement:</p>

<pre><code>ALTER TABLE table_name ALTER int_column TYPE float;
</code></pre>

<p>it will tell you that types are incompatible. This is also logical since float is broader type than int (has decimal) and database would not know what to put on decimal space. Here is a list of <a href=""https://stackoverflow.com/questions/31880381/cassandra-alter-column-type-which-types-are-compatible"">compatible</a> types which can be altered one to another without problems.</p>

<p><strong>Solution 1</strong></p>

<p>You can do it on application level, create one more column in that table which is float and create background job which will loop through all records and copy your <code>int</code> value to new <code>float</code> column.</p>

<p>We created <a href=""https://github.com/smartcat-labs/cassandra-migration-tool-java"" rel=""nofollow noreferrer"">cassandra migration tool</a> for DATA and SCHEMA migrations for cases like this, you add it as dependency and can write SCHEMA migration which will add new column and add DATA migration which will fire in background and copy values from old column to new column. Here is a <a href=""https://github.com/smartcat-labs/spring-cassandra-showcase-application"" rel=""nofollow noreferrer"">link</a> to Java example application to see usage.</p>

<p><strong>Solution 2</strong></p>

<p>If you do not have application level and want to do this purely in CQL you can use <a href=""https://docs.datastax.com/en/cql/3.1/cql/cql_reference/copy_r.html"" rel=""nofollow noreferrer"">COPY</a> command to extract data to CSV, create new table with float, sort manually int values in CSV and return data to new table.</p>
",['table']
34944323,34950340,2016-01-22 10:23:06,"Does NetworkTopologyStrategy behave exactly as SimpleStrategy for single rack, single datacenter?","<p>I'm trying to understand <a href=""https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/locator/NetworkTopologyStrategy.java"" rel=""nofollow""><code>NetworkTopologyStrategy</code></a>. Does it behave exactly as <code>SimpleStrategy</code> for a single rack and a single datacenter?</p>
",<cassandra><cassandra-2.0><datastax><datastax-enterprise>,"<p>Yes. SimpleStrategy finds the first replica using the primary key token, then places replicas on subsequent nodes in their order along the token ring. NetworkTopologyStrategy does the same thing, but also skips nodes while looking for unique racks within each datacenter. If it does not find enough, the replicas are placed on nodes in the order they were skipped. With a single rack this results in the same placement as SimpleStrategy.</p>
",['rack']
34948651,34949149,2016-01-22 14:08:03,Can a column in a cassandra table be altered to make it static?,"<p>I have a table in the cassandra database with a few static columns and I wish to add another column to it that is static. Is there a way to alter the table so that the new column is static?</p>
",<cassandra><cql>,"<p>Let's say your keyspace is <code>key</code> and the table schema is :</p>

<pre><code> CREATE TABLE p (   
     k text,   
     s text STATIC,   
     i int,   
     PRIMARY KEY (k, i) ); 
</code></pre>

<p>Then you can execute </p>

<pre><code> ALTER TABLE p ADD f text STATIC;
</code></pre>
",['table']
34996158,34996904,2016-01-25 15:25:12,Cassandra batch isolation guarantee,"<p>I have a question regarding Cassandra batch isolation:</p>

<p>Our cluster consist of a single datacenter, replication factor of 3, reading and writing in LOCAL_QUORUM.
We must provide a news feed resembling an 'after' trigger, to notify clients about CRUD events of data in the DB.
We thought of performing the actual operation, and inserting an event on another table (also in another partition), within a batch. Asynchronously, some process would read events from event table and send them through an MQ.</p>

<p>Because we're writing to different partitions, and operation order is not necessarily maintained in a batch operation; is there a chance our event is written, and our process read it before our actual data is persisted?</p>

<p>Could the same happen in case our batch at last fails?</p>

<p>Regards,
Alejandro </p>
",<cassandra><datastax><cassandra-2.2>,"<p>From ACID properties, Cassandra can provide ACD. Therefore, don't expect Isolation in its classical sense.</p>

<p>Batching records will provide you with Atomicity. So it does guarantee that all or none of the records within a batch are written. However, because it doesn't guarantee Isolation, you can end up having some of the records persisted and others not (e.g. wrote to your queue table, but not master table).</p>

<p>Cassandra <a href=""http://docs.datastax.com/en/cql/3.1/cql/cql_reference/batch_r.html"" rel=""nofollow"">docs</a> explain how it works:</p>

<blockquote>
  <p>To achieve atomicity, Cassandra first writes the serialized batch to the batchlog system table that consumes the serialized batch as blob data. When the rows in the batch have been successfully written and persisted (or hinted) the batchlog data is removed. There is a performance penalty for atomicity.</p>
</blockquote>

<p>Finally, using Cassandra table as MQ is considered <a href=""http://www.datastax.com/dev/blog/cassandra-anti-patterns-queues-and-queue-like-datasets"" rel=""nofollow"">anti-pattern</a>. </p>
",['table']
35005734,35076202,2016-01-26 01:46:00,Constant timeouts in Cassandra after adding second node,"<p>I'm trying to migrate a moderately large swath of data (~41 million rows) from an SQL database to Cassandra.  I've previously done a trial-run using half the dataset, and everything worked exactly as expected.</p>

<p>The problem is, now that I'm trying the complete migration Cassandra is throwing constant timeout errors.  For instance:</p>

<pre><code>[INFO] [talledLocalContainer] com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: /127.0.0.1:10112 (com.datastax.driver.core.exceptions.DriverException: Timed out waiting for server response))
[INFO] [talledLocalContainer]   at com.datastax.driver.core.exceptions.NoHostAvailableException.copy(NoHostAvailableException.java:84)
[INFO] [talledLocalContainer]   at com.datastax.driver.core.DefaultResultSetFuture.extractCauseFromExecutionException(DefaultResultSetFuture.java:289)
[INFO] [talledLocalContainer]   at com.datastax.driver.core.DefaultResultSetFuture.getUninterruptibly(DefaultResultSetFuture.java:205)
[INFO] [talledLocalContainer]   at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:52)
[INFO] [talledLocalContainer]   at com.mycompany.tasks.CassandraMigrationTask.execute(CassandraMigrationTask.java:164)
[INFO] [talledLocalContainer]   at org.quartz.core.JobRunShell.run(JobRunShell.java:202)
[INFO] [talledLocalContainer]   at org.quartz.simpl.SimpleThreadPool$WorkerThread.run(SimpleThreadPool.java:573)
[INFO] [talledLocalContainer] Caused by: com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: /127.0.0.1:10112 (com.datastax.driver.core.exceptions.DriverException: Timed out waiting for server response))
[INFO] [talledLocalContainer]   at com.datastax.driver.core.RequestHandler.sendRequest(RequestHandler.java:108)
[INFO] [talledLocalContainer]   at com.datastax.driver.core.RequestHandler$1.run(RequestHandler.java:179)
[INFO] [talledLocalContainer]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[INFO] [talledLocalContainer]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[INFO] [talledLocalContainer]   at java.lang.Thread.run(Thread.java:745)
</code></pre>

<p>I've tried increasing the timeout values in <code>cassandra.yaml</code>, and that increased the amount of time that the migration was able to run before dying to a timeout (roughly in proportion to the increase in the timeout).  </p>

<p>Prior to changing the timeout settings, my stack-trace looked more like:</p>

<pre><code>[INFO] [talledLocalContainer] com.datastax.driver.core.exceptions.WriteTimeoutException: Cassandra timeout during write query at consistency ONE (1 replica were required but only 0 acknowledged the write)
[INFO] [talledLocalContainer]   at com.datastax.driver.core.exceptions.WriteTimeoutException.copy(WriteTimeoutException.java:54)
[INFO] [talledLocalContainer]   at com.datastax.driver.core.DefaultResultSetFuture.extractCauseFromExecutionException(DefaultResultSetFuture.java:289)
[INFO] [talledLocalContainer]   at com.datastax.driver.core.DefaultResultSetFuture.getUninterruptibly(DefaultResultSetFuture.java:205)
[INFO] [talledLocalContainer]   at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:52)
[INFO] [talledLocalContainer]   at com.mycompany.tasks.CassandraMigrationTask.execute(CassandraMigrationTask.java:164)
[INFO] [talledLocalContainer]   at org.quartz.core.JobRunShell.run(JobRunShell.java:202)
[INFO] [talledLocalContainer]   at org.quartz.simpl.SimpleThreadPool$WorkerThread.run(SimpleThreadPool.java:573)
[INFO] [talledLocalContainer] Caused by: com.datastax.driver.core.exceptions.WriteTimeoutException: Cassandra timeout during write query at consistency ONE (1 replica were required but only 0 acknowledged the write)
[INFO] [talledLocalContainer]   at com.datastax.driver.core.exceptions.WriteTimeoutException.copy(WriteTimeoutException.java:54)
[INFO] [talledLocalContainer]   at com.datastax.driver.core.Responses$Error.asException(Responses.java:99)
[INFO] [talledLocalContainer]   at com.datastax.driver.core.DefaultResultSetFuture.onSet(DefaultResultSetFuture.java:140)
[INFO] [talledLocalContainer]   at com.datastax.driver.core.RequestHandler.setFinalResult(RequestHandler.java:249)
[INFO] [talledLocalContainer]   at com.datastax.driver.core.RequestHandler.onSet(RequestHandler.java:433)
[INFO] [talledLocalContainer]   at com.datastax.driver.core.Connection$Dispatcher.messageReceived(Connection.java:697)
[INFO] [talledLocalContainer]   at com.datastax.shaded.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
[INFO] [talledLocalContainer]   at com.datastax.shaded.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
[INFO] [talledLocalContainer]   at com.datastax.shaded.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
[INFO] [talledLocalContainer]   at com.datastax.shaded.netty.channel.Channels.fireMessageReceived(Channels.java:296)
[INFO] [talledLocalContainer]   at com.datastax.shaded.netty.handler.codec.oneone.OneToOneDecoder.handleUpstream(OneToOneDecoder.java:70)
</code></pre>

<p>My timeout settings are currently:</p>

<pre><code># How long the coordinator should wait for read operations to complete
read_request_timeout_in_ms: 30000
# How long the coordinator should wait for seq or index scans to complete
range_request_timeout_in_ms: 30000
# How long the coordinator should wait for writes to complete
write_request_timeout_in_ms: 30000
# How long the coordinator should wait for counter writes to complete
counter_write_request_timeout_in_ms: 30000
# How long a coordinator should continue to retry a CAS operation
# that contends with other proposals for the same row
cas_contention_timeout_in_ms: 1000
# How long the coordinator should wait for truncates to complete
# (This can be much longer, because unless auto_snapshot is disabled
# we need to flush first so we can snapshot before removing the data.)
truncate_request_timeout_in_ms: 60000
# The default timeout for other, miscellaneous operations
request_timeout_in_ms: 20000
</code></pre>

<p>...which gets me about 1.5m rows inserted before the timeout happens.  The original timeout settings were:</p>

<pre><code># How long the coordinator should wait for read operations to complete
read_request_timeout_in_ms: 5000
# How long the coordinator should wait for seq or index scans to complete
range_request_timeout_in_ms: 10000
# How long the coordinator should wait for writes to complete
write_request_timeout_in_ms: 2000
# How long the coordinator should wait for counter writes to complete
counter_write_request_timeout_in_ms: 5000
# How long a coordinator should continue to retry a CAS operation
# that contends with other proposals for the same row
cas_contention_timeout_in_ms: 1000
# How long the coordinator should wait for truncates to complete
# (This can be much longer, because unless auto_snapshot is disabled
# we need to flush first so we can snapshot before removing the data.)
truncate_request_timeout_in_ms: 60000
# The default timeout for other, miscellaneous operations
request_timeout_in_ms: 10000
</code></pre>

<p>...which caused the timeouts to happen approximately every 300,000 rows.  </p>

<p>The only significant change that's occurred between when I had my successful run and now is that I added a second node to the Cassandra deployment.  So intuitively I'd think the issue would have something to do with the propagation of data from the first node to the second (as in, there's <code>&lt;some process&gt;</code> that scales linearly with the amount of data inserted and which isn't used when there's only a single node).  But I'm not seeing any obvious options that might be useful for configuring/mitigating this.  </p>

<p>If it's relevant, I'm using batch statements during the migration, typically with between 100 and 200 statements/rows per batch, at most.  </p>

<p>My keyspace was originally set up <code>WITH REPLICATION =
  { 'class' : 'SimpleStrategy', 'replication_factor' : 2 }</code>, but I altered it to be <code>WITH REPLICATION =
  { 'class' : 'SimpleStrategy', 'replication_factor' : 1 }</code> to see if that would make any difference.  It didn't.  </p>

<p>I also tried explicitly setting <code>ConsistencyLevel.ANY</code> on all my insert statements (and also the enclosing batch statements).  That also made no difference.  </p>

<p>There doesn't seem to be anything interesting in Cassandra's log on either node, although the first node is certainly showing more 'ops' than the second:</p>

<p><strong>First node - 454317 ops</strong></p>

<pre><code>INFO  [SlabPoolCleaner] 2016-01-25 19:46:08,806 ColumnFamilyStore.java:905 - Enqueuing flush of assetproperties_flat: 148265302 (14%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:15] 2016-01-25 19:46:08,807 Memtable.java:347 - Writing Memtable-assetproperties_flat@350387072(20.557MiB serialized bytes, 454317 ops, 14%/0% of on/off-heap limit)
INFO  [MemtableFlushWriter:15] 2016-01-25 19:46:09,393 Memtable.java:382 - Completed flushing /var/cassandra/data/itb/assetproperties_flat-e83359a0c34411e593abdda945619e28/itb-assetproperties_flat-tmp-ka-32-Data.db (5.249MiB) for commitlog position ReplayPosition(segmentId=1453767930194, position=15188257)
</code></pre>

<p><strong>Second node - 2020 ops</strong></p>

<pre><code>INFO  [BatchlogTasks:1] 2016-01-25 19:46:33,961 ColumnFamilyStore.java:905 - Enqueuing flush of batchlog: 4923957 (0%) on-heap, 0 (0%) off-heap
INFO  [MemtableFlushWriter:22] 2016-01-25 19:46:33,962 Memtable.java:347 - Writing Memtable-batchlog@796821497(4.453MiB serialized bytes, 2020 ops, 0%/0% of on/off-heap limit)
INFO  [MemtableFlushWriter:22] 2016-01-25 19:46:33,963 Memtable.java:393 - Completed flushing /var/cassandra/data/system/batchlog-0290003c977e397cac3efdfdc01d626b/system-batchlog-tmp-ka-11-Data.db; nothing needed to be retained.  Commitlog position was ReplayPosition(segmentId=1453767955411, position=18567563)
</code></pre>

<p>Has anyone encountered a similar issue, and if so, what was the fix?  </p>

<p>Would it be advisable to just take the second node offline, run the migration with just the first node, and then run <code>nodetool repair</code> afterwards to get the second node back in sync? </p>

<p><strong>Edit</strong></p>

<p>Answers to questions from comments:</p>

<ol>
<li><p>I'm using the datastax Java driver, and have a server-side task (<a href=""http://quartz-scheduler.org/api/2.2.0/"" rel=""noreferrer"">Quartz job</a>) that uses the ORM layer (hibernate) to lookup the next chunk of data to migrate, write it into Cassandra, and then purge it from the SQL database.  I'm getting a connection to Cassandra using the following code:</p>

<pre><code>public static Session getCassandraSession(String keyspace) {
    Session session = clusterSessions.get(keyspace);
    if (session != null &amp;&amp; ! session.isClosed()) {
        //can use the cached session
        return session;
    }

    //create a new session for the specified keyspace
    Cluster cassandraCluster = getCluster();
    session = cassandraCluster.connect(keyspace);

    //cache and return the session
    clusterSessions.put(keyspace, session);
    return session;
}

private static Cluster getCluster() {
    if (cluster != null &amp;&amp; ! cluster.isClosed()) {
        //can use the cached cluster
        return cluster;
    }

    //configure socket options
    SocketOptions options = new SocketOptions();
    options.setConnectTimeoutMillis(30000);
    options.setReadTimeoutMillis(300000);
    options.setTcpNoDelay(true);

    //spin up a fresh connection
    cluster = Cluster.builder().addContactPoint(Configuration.getCassandraHost()).withPort(Configuration.getCassandraPort())
                .withCredentials(Configuration.getCassandraUser(), Configuration.getCassandraPass()).withSocketOptions(options).build();

    //log the cluster details for confirmation
    Metadata metadata = cluster.getMetadata();
    LOG.debug(""Connected to Cassandra cluster: "" + metadata.getClusterName());
    for ( Host host : metadata.getAllHosts() ) {
        LOG.debug(""Datacenter:  "" + host.getDatacenter() + ""; Host:  "" + host.getAddress() + ""; Rack: "" + host.getRack());
    }

    return cluster;
}
</code></pre>

<p>The part with the <code>SocketOptions</code> is a recent addition, as the latest timeout error sounded like it was coming from the Java/client side rather than from within Cassandra itself.</p></li>
<li><p>Each batch inserts no more than 200 records.  Typical values are closer to 100.</p></li>
<li><p>Both nodes have the same specs:  </p>

<ul>
<li>Intel(R) Xeon(R) CPU E3-1230 V2 @ 3.30GHz</li>
<li>32GB RAM</li>
<li>256GB SSD (primary), 2TB HDD (backups), both in RAID-1 configurations</li>
</ul></li>
<li><p><strong>First node:</strong></p>

<pre><code>Pool Name                    Active   Pending      Completed   Blocked  All time blocked
CounterMutationStage              0         0              0         0                 0
ReadStage                         0         0          58155         0                 0
RequestResponseStage              0         0         655104         0                 0
MutationStage                     0         0         259151         0                 0
ReadRepairStage                   0         0              0         0                 0
GossipStage                       0         0          58041         0                 0
CacheCleanupExecutor              0         0              0         0                 0
AntiEntropyStage                  0         0              0         0                 0
MigrationStage                    0         0              0         0                 0
Sampler                           0         0              0         0                 0
ValidationExecutor                0         0              0         0                 0
CommitLogArchiver                 0         0              0         0                 0
MiscStage                         0         0              0         0                 0
MemtableFlushWriter               0         0             80         0                 0
MemtableReclaimMemory             0         0             80         0                 0
PendingRangeCalculator            0         0              3         0                 0
MemtablePostFlush                 0         0            418         0                 0
CompactionExecutor                0         0           8979         0                 0
InternalResponseStage             0         0              0         0                 0
HintedHandoff                     0         0              2         0                 0
Native-Transport-Requests         1         0        1175338         0                 0

Message type           Dropped
RANGE_SLICE                  0
READ_REPAIR                  0
PAGED_RANGE                  0
BINARY                       0
READ                         0
MUTATION                     0
_TRACE                       0
REQUEST_RESPONSE             0
COUNTER_MUTATION             0
</code></pre>

<p><strong>Second node:</strong></p>

<pre><code>Pool Name                    Active   Pending      Completed   Blocked  All time blocked
CounterMutationStage              0         0              0         0                 0
ReadStage                         0         0          55803         0                 0
RequestResponseStage              0         0              1         0                 0
MutationStage                     0         0         733828         0                 0
ReadRepairStage                   0         0              0         0                 0
GossipStage                       0         0          56623         0                 0
CacheCleanupExecutor              0         0              0         0                 0
AntiEntropyStage                  0         0              0         0                 0
MigrationStage                    0         0              0         0                 0
Sampler                           0         0              0         0                 0
ValidationExecutor                0         0              0         0                 0
CommitLogArchiver                 0         0              0         0                 0
MiscStage                         0         0              0         0                 0
MemtableFlushWriter               0         0            394         0                 0
MemtableReclaimMemory             0         0            394         0                 0
PendingRangeCalculator            0         0              2         0                 0
MemtablePostFlush                 0         0            428         0                 0
CompactionExecutor                0         0           8883         0                 0
InternalResponseStage             0         0              0         0                 0
HintedHandoff                     0         0              1         0                 0
Native-Transport-Requests         0         0             70         0                 0

Message type           Dropped
RANGE_SLICE                  0
READ_REPAIR                  0
PAGED_RANGE                  0
BINARY                       0
READ                         0
MUTATION                     0
_TRACE                       0
REQUEST_RESPONSE             0
COUNTER_MUTATION             0
</code></pre></li>
<li><p>The output of <code>nodetool ring</code> was very long.  Here's a <code>nodetool status</code> instead:</p>

<pre><code>Datacenter: DC1
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address         Load       Tokens  Owns    Host ID                               Rack
UN  204.11.xxx.1  754.66 MB  1024    ?       8cf373d8-0b3e-4fd3-9e63-fdcdd8ce8cd4  RAC1
UN  208.66.xxx.2  767.78 MB  1024    ?       42e1f336-84cb-4260-84df-92566961a220  RAC2
</code></pre></li>
<li><p>I increased all of Cassandra's timeout values by a factor of 10, and also set the Java driver's read timeout settings to match, and now I'm up to <s>8m</s> 29.4m inserts with no issues.  In theory if the issue scales linearly with the timeout values I should be good up until around 15m inserts (which is at least good enough that I don't need to constantly babysit the migration process waiting for each new error).</p></li>
</ol>
",<java><cassandra><timeout>,"<p>Okay, so I was able to get the timeout errors to stop by doing two things.  First, I increased Cassandra's timeout values on both hosts, as follows:</p>

<pre><code># How long the coordinator should wait for read operations to complete
read_request_timeout_in_ms: 30000
# How long the coordinator should wait for seq or index scans to complete
range_request_timeout_in_ms: 30000
# How long the coordinator should wait for writes to complete
write_request_timeout_in_ms: 30000
# How long the coordinator should wait for counter writes to complete
counter_write_request_timeout_in_ms: 30000
# How long a coordinator should continue to retry a CAS operation
# that contends with other proposals for the same row
cas_contention_timeout_in_ms: 1000
# How long the coordinator should wait for truncates to complete
# (This can be much longer, because unless auto_snapshot is disabled
# we need to flush first so we can snapshot before removing the data.)
truncate_request_timeout_in_ms: 60000
# The default timeout for other, miscellaneous operations
request_timeout_in_ms: 20000
</code></pre>

<p>I suspect those values are unnecessarily large, but those are what I had in place when everything started working.</p>

<p>The second part of the solution was to adjust the client timeout in my Java code, as follows:</p>

<pre><code>//configure socket options
SocketOptions options = new SocketOptions();
options.setConnectTimeoutMillis(30000);
options.setReadTimeoutMillis(300000);
options.setTcpNoDelay(true);

//spin up a fresh connection (using the SocketOptions set up above)
cluster = Cluster.builder().addContactPoint(Configuration.getCassandraHost()).withPort(Configuration.getCassandraPort())
            .withCredentials(Configuration.getCassandraUser(), Configuration.getCassandraPass()).withSocketOptions(options).build();
</code></pre>

<p>With those two changes, the timeout errors stopped and the data migration completed without issue.  </p>

<p>As @MarcintheCloud rightly points out in the comments above, increasing the timeout values may only have the effect of masking the underlying problem.  But that's good enough in my case since 1) the underlying problem only surfaces under very high load, 2) I only need to run the migration process once, and 3) once the data has been migrated, the actual load levels are orders of magnitude lower than what's experienced during the migration.</p>

<p>However, understanding the underlying cause still seems worthwhile.  So what was it?  Well I've got two theories:</p>

<ol>
<li><p>As @MarcintheCloud posits, perhaps 1024 is too many tokens to reasonably use with Cassandra.  And perhaps as a consequence of that the deployment gets a bit flaky under heavy load.</p></li>
<li><p>My alternative theory has to do with network chatter between the two nodes.  In my deployment, the first node runs the app-server instance, the first Cassandra instance, and the primary SQL database.  The second node runs the second Cassandra instance and also a replica SQL database that is kept in sync with the primary database in near-real-time.  </p>

<p>Now, the migration process essentially does two things concurrently; it writes data into Cassandra, and it deletes data from the SQL database.  Both of those actions generate changesets that need to propagate over the network to the second node.  </p>

<p>So my theory is that if changes are happening quickly enough on the first node (since the SSD <em>does</em> allow very high IO throughput), the network transfers of the SQL and Cassandra changelogs (and/or the subsequent IO ops on the second node) may occasionally contend with each other, introducing additional latency into the replication process(es) and potentially leading to timeouts.  It seems plausible that with enough contention, one process or the other might get blocked for several seconds at a time, which is enough to trigger timeout errors at Cassandra's default settings.</p></li>
</ol>

<p>Those are the plausible theories I can think of.  Though no real way of testing to confirm which (if any) is correct.</p>
",['auto_snapshot']
35010995,35014015,2016-01-26 09:52:21,Cassandra nodes appearing in different datacenters,"<p>I am having trouble with three nodes in Cassandra, each of them in an individual computer, as I am trying to set up my first Cassandra structure. I have set up everything as in the Datastax documentation, and I have the same configuration in the different cassandra.yaml of each machine (changing the relative ips). The thing is that after configuring everything, each computer sees each other as DN, and each machine (localhost) appears as UN, with the difference that in the .101 computer I can see two different datacenters, while in the other computers only one datacenter appears. </p>

<p>So in my 192.168.1.101 machine when I type </p>

<pre><code>sudo nodetool status
</code></pre>

<p>I get this output: </p>

<pre><code>Datacenter: DC1
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address        Load       Tokens       Owns    Host ID                               Rack
DN  192.168.1.200  ?          256          ?       968d5d1e-a113-40ce-9521-e392a927ea5e  r1
DN  192.168.1.102  ?          256          ?       fc5c2dbe-8834-4040-9e77-c3d8199b6767  r1
Datacenter: dc1
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address        Load       Tokens       Owns    Host ID                               Rack
UN  127.0.0.1      446.13 KB  256          ?       6d28d540-2b44-4522-8612-b5f70a3d7d52  rack1
</code></pre>

<p>While when I type ""nodetool status"" in one of the other two machines, I get this output: </p>

<pre><code>Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address        Load       Tokens       Owns    Host ID                               Rack
DN  192.168.1.200  ?          256          ?       968d5d1e-a113-40ce-9521-e392a927ea5e  rack1
UN  127.0.0.1      506,04 KB  256          ?       fc5c2dbe-8834-4040-9e77-c3d8199b6767  rack1
DN  192.168.1.101  ?          256          ?       6d28d540-2b44-4522-8612-b5f70a3d7d52  rack1
</code></pre>

<p>In OpsCenter I can only see my 192.168.1.101 machine:</p>

<p><a href=""https://i.stack.imgur.com/DBQod.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DBQod.png"" alt=""OpsCenter displaying only one node""></a></p>

<p>... Which makes me think that something's odd in the yaml file of this machine and the others, but I have checked several times and it seems that the configuration is the same in the other computers. Enpoint_snitch is set to ""GossipingPropertyFileSnitch"". </p>

<p>Any tips on how to solve the reason why all the other nodes appear as Down Normal and why I am getting two datacenters would be highly appreaciated. It's driving me crazy! </p>

<p>Thanks for reading.</p>
",<cassandra><yaml><nosql>,"<p>It looks like some of the installed nodes were dead, so I deleted the nodes that were not the local machine in each of the nodes, ie: </p>

<pre><code>nodetool removenode 968d5d1e-a113-40ce-9521-e392a927ea5e
nodetool removenode fc5c2dbe-8834-4040-9e77-c3d8199b6767
</code></pre>

<p>and after that I got the right output when I executed nodetools status </p>

<pre><code>[machine1]~$ sudo nodetool status
Datacenter: dc1
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address    Load       Tokens       Owns    Host ID                               Rack
UN  127.0.0.1  286.93 KB  256          ?       6d28d540-2b44-4522-8612-b5f70a3d7d52  rack1


[machine2]~$ sudo nodetool status
Datacenter: dc1
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address    Load       Tokens       Owns    Host ID                               Rack
UN  127.0.0.1  268.45 KB  256          ?       fc5c2dbe-8834-4040-9e77-c3d8199b6767  rack1
</code></pre>

<p>And made sure that the parameters cluster_name, seeds, listen_address and rpc_address were right. </p>

<pre><code>cluster_name: 'Test Cluster'
seeds: ""192.168.1.101, 192.168.1.102""
listen_address: 192.168.1.101
rpc_address: 192.168.1.101
</code></pre>

<p>Changing listen_address and rpc_address to the corresponding ip of each machine in their corresponding cassandra.yaml file.</p>

<p>After that I got the right output (I am using only 2 machines for the nodes now):</p>

<pre><code>Datacenter: dc1
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address        Load       Tokens       Owns (effective)  Host ID                               Rack
UN  192.168.1.101  309.13 KB  256          51.6%             6d28d540-2b44-4522-8612-b5f70a3d7d52  rack1
UN  192.168.1.102  257.15 KB  256          48.4%             fc5c2dbe-8834-4040-9e77-c3d8199b6767  rack1
</code></pre>
","['rpc_address', 'listen_address']"
35014213,35022239,2016-01-26 12:47:37,How to enable streaming from Cassandra to Spark?,"<p>I have the following <strong>spark job</strong>:</p>

<pre><code>from __future__ import print_function

import os
import sys
import time
from random import random
from operator import add
from pyspark.streaming import StreamingContext
from pyspark import SparkContext,SparkConf
from pyspark.streaming.kafka import KafkaUtils
from pyspark.sql import SQLContext, Row
from pyspark.streaming import StreamingContext
from pyspark_cassandra import streaming,CassandraSparkContext

if __name__ == ""__main__"":

    conf = SparkConf().setAppName(""PySpark Cassandra Test"")
    sc = CassandraSparkContext(conf=conf)
    stream = StreamingContext(sc, 2)

    rdd=sc.cassandraTable(""keyspace2"",""users"").collect()
    #print rdd
    stream.start()
    stream.awaitTermination()
    sc.stop() 
</code></pre>

<p>When I run this, it gives me the following <strong>error</strong>:</p>

<pre><code>ERROR StreamingContext: Error starting the context, marking it as stopped
java.lang.IllegalArgumentException: requirement failed: \
No output operations registered, so nothing to execute
</code></pre>

<p>the <strong>shell script</strong> I run:</p>

<pre><code>./bin/spark-submit --packages TargetHolding:pyspark-cassandra:0.2.4 example
s/src/main/python/test/reading-cassandra.py
</code></pre>

<p>Comparing spark streaming with kafka, I have this line missing from the above code:</p>

<pre><code>kafkaStream = KafkaUtils.createStream(stream, 'localhost:2181', ""name"", {'topic':1})
</code></pre>

<p>where I'm actually using <code>createStream</code> but for cassandra, I can't see anything like this on the docs. How do I start the streaming between spark streaming and cassandra?</p>

<p><strong>Versions</strong>:</p>

<pre><code>Cassandra v2.1.12
Spark v1.4.1
Scala 2.10
</code></pre>
",<apache-spark><cassandra><pyspark><spark-streaming><datastax>,"<p>To create DStream out of a Cassandra table, you can use a <code>ConstantInputDStream</code> providing the RDD created out of the Cassandra table as input. This will result in the RDD being materialized on each DStream interval. </p>

<p>Be warned that large tables or tables that continuously grow in size will negatively impact performance of your Streaming job.</p>

<p>See also: <a href=""https://stackoverflow.com/questions/32451614/reading-from-cassandra-using-spark-streaming/32480168#32480168"">Reading from Cassandra using Spark Streaming</a>  for an example.</p>
",['table']
35014263,35014303,2016-01-26 12:50:25,How can cassandra choose which node on cluster to perform read/write operation?,"<p>I'm using Cassandra datastax java API, and I wonder how it chooses which node on the cluster to write or read data. For example, if I execute (via Session) the write operation 1000 times, will all the operations go to the same node? Or it will choose the node to perform the execution randomly ?</p>

<p>If it allowed us to perform read/write on the node we want, following on specific patter (e.g. round robin writing...), which is the best option we can choose to speed up the writing process?</p>

<p>Thank you very much. </p>
",<cassandra><datastax-java-driver>,"<p>The partitioning key of the record defines which node is responsible for storing the data, and for doing queries on it. That is why you need to give the full partitioning key in <code>WHERE</code> criteria, or do a full table scan.</p>

<p>So essentially, you do not choose the node yourself explicitly, but implicitly by specifying the partitioning key in read/write operations.</p>
",['table']
35017426,35020512,2016-01-26 15:25:02,Get cassandra tables creation date,"<p>How can I get the creation date and time of a cassandra table?</p>

<p>I tried to use cqlsh DESC TABLE but there is no information about the creation time stamp... </p>
",<cassandra><cqlsh>,"<p>Depending on your version of Cassandra, you can check the schema tables.  Each table gets a unique ID when it is created, and that ID gets written to the schema tables.  If you query the <code>WRITETIME</code> of that ID, it should give you a UNIX timestamp (in microseconds) of when it was created.</p>

<p>Cassandra 2.2.x and down:</p>

<pre><code>&gt; SELECT keyspace_name, columnfamily_name, writetime(cf_id) 
    FROM system.schema_columnfamilies 
    WHERE keyspace_name='stackoverflow' AND columnfamily_name='book';

 keyspace_name | columnfamily_name | writetime(cf_id)
---------------+-------------------+------------------
 stackoverflow |              book | 1446047871412000

(1 rows)
</code></pre>

<p>Cassandra 3.0 and up:</p>

<pre><code>&gt; SELECT keyspace_name, table_name, writetime(id) 
    FROM system_schema.tables 
    WHERE keyspace_name='stackoverflow' AND table_name='book';

 keyspace_name | table_name | writetime(id)
---------------+------------+------------------
 stackoverflow |       book | 1442339779097000

(1 rows)
</code></pre>
",['table']
35030817,35031038,2016-01-27 07:06:38,cassandra query for list the data using timestamp,"<p>I am very new to Cassandra. I have one table with the following columns CustomerId, Timestamp, Action,ProductId. I need to select the CustomerId and from date - to date using time stamp.I dont know how to do this in cassandra any help will be appreciated.</p>
",<cassandra>,"<p>First of all could you should remember that you should plan what queries will be executed in future and make table keys according to it.
If you have keys as (customerId, date) then your query can be for example:</p>

<pre><code>SELECT * FROM products WHERE customerId= '1' AND date &lt; 1453726670241 AND date &gt; 1453723370048;
</code></pre>

<p>Please, see <a href=""http://docs.datastax.com/en/latest-cql/cql/cql_using/useAboutCQL.html"" rel=""nofollow"">http://docs.datastax.com/en/latest-cql/cql/cql_using/useAboutCQL.html</a></p>
",['table']
35088677,35089068,2016-01-29 15:56:23,Insert now() using Cassandra's Java Object-mapping API,"<p>What is the equivalent of:</p>

<pre><code>INSERT INTO table (myColumn) VALUES (now())
</code></pre>

<p>using the Cassandra object-mapping api?</p>
",<cassandra><datastax><datastax-java-driver>,"<p>The <code>@Computed</code> annotation doesnt look like it would work unfortunately.</p>

<p>You can also set the value of your object to a type1 uuid. The jre doesnt have standard function for it but you can use the <a href=""https://docs.datastax.com/en/drivers/java/2.0/com/datastax/driver/core/utils/UUIDs.html#timeBased--"" rel=""nofollow"">java driver util</a>, <a href=""http://mediashelf.github.io/uuid-datepath-idmapper/generate_uuids.html"" rel=""nofollow"">JUG</a>, <a href=""https://github.com/apache/cassandra/blob/cassandra-2.1/src/java/org/apache/cassandra/utils/UUIDGen.java#L70"" rel=""nofollow"">cassandra-all</a> or even write one yourself. This would be a little different because your setting the time as the time of creation as opposed to coordinator setting time of when it receives the request but with ORM's abstractions you tend to lose some control. </p>

<p>Alternatively there is nothing preventing you from issuing CQL statements while still using the object mapping api. Maybe even <a href=""https://docs.datastax.com/en/latest-java-driver/common/drivers/reference/accessorAnnotatedInterfaces.html"" rel=""nofollow"">adding a query to a method</a> on your object to do it ie:</p>

<pre><code>@Query(""UPDATE table SET myColumn = now() WHERE ...."")
public ResultSet setNow()
</code></pre>
",['table']
35135392,35140812,2016-02-01 16:25:35,Nullable timestamp with order by,"<p>We've recently decided to migrate an application to Cassandra (from Oracle) because it may help with performance, and as I have a decent Oracle background, I gotta admit I struggle with the Cassandra ""way of thinking"".</p>

<p>Basically i'm having a table with ~15 fields, among those dates. One of these dates is used for ""ordering"", so I need to be able to do ""order by"" on it. At the same time though, this field can be nullable.</p>

<p>Now i've figured putting that field as a primary key lets me actually do the order-by part, but I can't assign the null value to it anymore...</p>

<p>Any ideas ?</p>
",<cassandra>,"<p>You are correct in that you cannot query by NULL values in Cassandra.  There's a really good reason for that; which is that NULL values don't really exist.  That row simply does not contain a value for the ""NULL"" column.  So the CQL interface abstracts that with the ""NULL"" output, because that's easier to explain to people.</p>

<p>Cassandra also does not allow NULLs (or an absence of a column value) in its key fields.  So the best you can do in this case, is to come up with a timestamp constant that you (and your application) recognize to be NULL without breaking anything.  So consider this example table structure:</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; CREATE TABLE eventsByMonth (
  monthBucket text,
  eventTime timestamp,
  event text,
  PRIMARY KEY (monthBucket,eventTime))
  WITH CLUSTERING ORDER BY (eventTime DESC);
</code></pre>

<p>Next I'll insert some values to test with:</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; INSERT INTO eventsByMonth (monthBucket,eventTime,event)
                  VALUES ('201509','2015-09-19 00:00:00','Talk Like A Pirate Day');
aploetz@cqlsh:stackoverflow&gt; INSERT INTO eventsByMonth (monthBucket,eventTime,event)
                  VALUES ('201509','2015-09-25 00:00:00','Hobbit Day');
aploetz@cqlsh:stackoverflow&gt; INSERT INTO eventsByMonth (monthBucket,eventTime,event)
                  VALUES ('201509','2015-09-19 21:00:00','dentist appt');
aploetz@cqlsh:stackoverflow&gt; INSERT INTO eventsByMonth (monthBucket,eventTime,event)
                  VALUES ('201503','2015-03-14 00:00:00','Pi Day');
</code></pre>

<p>Let's say that I have two events that I want to keep track of, but I don't know the <code>eventTime</code>s, so instead of INSERTing a NULL, I'll just specify a zero.  For the sake of the example, I'll put one in September 2015 and the other in October 2015:</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; INSERT INTO eventsByMonth (monthBucket,eventTime,event)
                  VALUES ('201510',0,'Some random day I want to keep track of');
aploetz@cqlsh:stackoverflow&gt; INSERT INTO eventsByMonth (monthBucket,eventTime,event)
                  VALUES ('201509',0,'Some other random day I want to keep track of');
</code></pre>

<p>Now when I query for September of 2015, I'll get the following output:</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; SELECT * FROM eventsbymonth WHERe monthbucket = '201509';

 monthbucket | eventtime                | event
-------------+--------------------------+-----------------------------------------------
      201509 | 2015-09-25 00:00:00-0500 |                                    Hobbit Day
      201509 | 2015-09-19 21:00:00-0500 |                                  dentist appt
      201509 | 2015-09-19 00:00:00-0500 |                        Talk Like A Pirate Day
      201509 | 1969-12-31 18:00:00-0600 | Some other random day I want to keep track of

(4 rows)
</code></pre>

<p>Notes:</p>

<ul>
<li>This is probably something you want to avoid doing, if possible.</li>
<li>INSERT/UPDATE (Upsert) with a ""NULL"" value is the same as a DELETE operation, and creates tombstone(s).</li>
<li>Upserting a zero (0) as a TIMESTAMP defaults to 1970-01-01 00:00:00 UTC.  My current timezone offset is -0600, which is why the value of 1969-12-31 18:00:00 appears.</li>
<li>I don't need to specify an <code>ORDER BY</code> clause in my query, because the defined clustering order is what I want.  It is a good idea to configure this as per your query requirements, because all <code>ORDER BY</code> can really do is enforce ASCending or DESCending.  You cannot specify a column in your <code>ORDER BY</code> that differs from your table's defined clustering order.</li>
<li>An advantage of using a zero TIMESTAMP, is that all rows containing that key are ordered at the bottom of the result set (DESCending order), so you'll always know where to look for them.</li>
<li>Not sure what your partitioning key is, but I used <code>monthBucket</code> for mine.  FYI- ""bucketing"" is a Cassandra modeling technique used when working with time series data, to evenly distribute data in your cluster.</li>
</ul>
",['table']
35143455,35148017,2016-02-02 01:20:39,Remove WRITETIME of a Cassandra column,"<p><strong>Background:</strong></p>

<p>So a machine has a mis-configured date-time to be 2017 (instead of 2016), and Cassandra ran for a day.  After the date-time was corrected, the WRITETIME of many fields are still 2017, and all INSERT/UPDATE would failed silently.</p>

<p><strong>Question:</strong></p>

<p>Is there any way to remove/clear WRITETIME?  Or better yet, fix the entire DB?</p>
",<cassandra>,"<p>You're facing the ""write barrier"" issue. 3 possibles solutions:</p>

<ol>
<li>trash your DB and rebuild it from scratch, brute force solution, not recommended if you do not have a backup of your data</li>
<li>use <code>sstable2json</code> to convert data files to JSON, fix the timestamp and use <code>json2sstable</code> to recreate back data files. It may cost you a lot of time and effort to do the parsing of timestamp </li>
<li>use Apache Spark to read from the current tables and write to a new table (same name) but in a different keyspace. Look at this source code sample: <a href=""https://github.com/doanduyhai/Cassandra-Spark-Demo/blob/master/src/main/scala/usecases/MigrateAlbumsData.scala#L58-L60"" rel=""nofollow"">https://github.com/doanduyhai/Cassandra-Spark-Demo/blob/master/src/main/scala/usecases/MigrateAlbumsData.scala#L58-L60</a></li>
</ol>
",['table']
35153421,35194508,2016-02-02 12:22:22,date range query in cassandra,"<p>I am new to Cassandra. </p>

<p>We have table structure is like this</p>

<p><code>CREATE TABLE keyspace.events (
    id bigint,
    msg_time bigint,
    status int,
    uuid timeuuid,
    message text,
    PRIMARY KEY (id, msg_time, status, uuid)
) WITH CLUSTERING ORDER BY (msg_time ASC, status ASC, uuid ASC)
CREATE INDEX timestamp ON hh_keyspace.game_events (msg_time);</code></p>

<p>We insert data with TTL of 32 days. Analytics team wants only last 1 day of data. Query with msg_time > '' ALLOW FILTERING will have huge performance impact. </p>

<p>Analytics team run the query daily. Are there any other ways to get the data. </p>
",<cassandra><cql>,"<p>I realized that the best way is to duplicate the data because writes are cheap in Cassandra. We write to another table with different key structure. </p>

<p>Ref: </p>

<p>1) <a href=""http://blog.websudos.com/2014/08/16/a-series-on-cassandra-part-1-getting-rid-of-the-sql-mentality/"" rel=""nofollow"">http://blog.websudos.com/2014/08/16/a-series-on-cassandra-part-1-getting-rid-of-the-sql-mentality/</a> (Section - 4. Duplicate data and maintain consistency at application level)</p>

<p>2) <a href=""http://blog.websudos.com/2014/08/23/a-series-on-cassandra-part-2-indexes-and-keys/"" rel=""nofollow"">http://blog.websudos.com/2014/08/23/a-series-on-cassandra-part-2-indexes-and-keys/</a> (Section - Secondary indexes) </p>
",['table']
35185890,35305373,2016-02-03 19:25:14,Columns not found in class error,"<p>I'm trying to pull certain data out of a cassandra table, and then write it back to a different table in cassandra.</p>

<p>This is what I have:</p>

<pre><code>JavaRDD&lt;MeasuredValue&gt; mvRDD = javaFunctions(sc).cassandraTable(""SB1000_47130646"", ""Measured_Value"", mapRowTo(MeasuredValue.class))
  .where (""\""Time_Key\"" IN (1601823,1601824)"")
  .select(""Time_Key"",""Start_Frequency"",""Bandwidth"", ""Power"");  
</code></pre>

<p>Then I write back to a new table with:</p>

<pre><code>javaFunctions(mvRDD).writerBuilder(""spark_reports"",""SB1000_47130646"", mapToRow(MeasuredValue.class)).withColumnSelector(someColumns(""Time_Key"", ""Start_Frequency"", ""Bandwidth"", ""Power"")).saveToCassandra();
</code></pre>

<p>My MeasuredValue Class looks like this:</p>

<pre><code>public static class MeasuredValue implements Serializable {


public MeasuredValue() { }

public MeasuredValue(Long Time_Key, Double Start_Frequency, Double Bandwidth, Float Power) {
    this.Time_Key = Time_Key;
    this.Start_Frequency = Start_Frequency;
    this.Bandwidth = Bandwidth;
    this.Power = Power;

}
private Long Time_Key;
public Long gettime_key() { return Time_Key; }
public void settime_key(Long Time_Key) { this.Time_Key = Time_Key; }

private Double Start_Frequency;
public Double getstart_frequency() { return Start_Frequency; }
public void setstart_frequency(Double Start_Frequency) { this.Start_Frequency = Start_Frequency; }

private Double Bandwidth;
public Double getbandwidth() { return Bandwidth; }
public void setbandwidth(Double Bandwidth) { this.Bandwidth = Bandwidth; }

private Float Power;    
public Float getpower() { return Power; }
public void setpower(Float Power) { this.Power = Power;
}
</code></pre>

<p>The error I get when running is:</p>

<pre><code>Exception in thread ""main"" java.lang.IllegalArgumentException: requirement failed: Columns not found in class com.neutronis.spark_reports.Spark_Reports$MeasuredValue: [Time_Key, Start_Frequency, Bandwidth, Power]
</code></pre>
",<java><cassandra><apache-spark-sql><datastax-java-driver><spark-dataframe>,"<p>I discovered that this is due to the fact that the getters/setters follow the JAVA naming scheme as far as capital letters and variables.  Since the columns in my table were camel case I had too reconfigure the column names to a proper all lower case naming convention for it to work correctly.</p>

<p>In order to use capital letters I had to use a HashMap:</p>

<pre><code> HashMap&lt;String,String&gt; colmap = new HashMap&lt;String,String&gt;();
    colmap.put( ""start_frequency"", ""Start_Frequency"" );
    colmap.put( ""bandwith"", ""Bandwidth"" );
    colmap.put( ""power"", ""Power"" );
    RowReaderFactory&lt;MeasuredValue&gt; mapRowTo = mapRowTo(MeasuredValue.class, colmap);
</code></pre>
",['table']
35200166,35202461,2016-02-04 11:45:46,Cassandra - Declare specific field from user-defined type as primary key,"<p>I want to declare specific field from user-defined type as primary key.
Assume I have this :</p>

<pre><code>CREATE TYPE entity (
  entity_id TEXT,
  entity_type TEXT
);

CREATE TABLE some_object_by_entity_id (
  someId TEXT,
  mytext TEXT,
  entity FROZEN&lt;entity&gt;,
  PRIMARY KEY ((entity.entity_id), transaction_id)
) WITH CLUSTERING ORDER BY (transaction_id ASC);

...
</code></pre>

<p>now I want to make somehow the entity_id from entity (which is a user-defined type) to be my primary key, but Cassandra gives me syntax error.
Am I able to do so with any other syntax ?</p>
",<cassandra><cassandra-2.0>,"<p>You can't do that, try to duplicate the entity_id as a simple column of your table:</p>

<pre><code>CREATE TABLE some_object_by_entity_id (
  entity_id TEXT,
  someId TEXT,
  mytext TEXT,
  entity FROZEN&lt;entity&gt;,
  PRIMARY KEY ((entity_id), transaction_id)
) WITH CLUSTERING ORDER BY (transaction_id ASC);
</code></pre>

<p>The drawback of this solution is you need to keep in sync your table entity_id and the entity_id value inside the frozen entity manually from the application code.</p>
",['table']
35228503,35229842,2016-02-05 16:05:26,Cassandra: can I index and query along a third dimension?,"<p>I want to put a third dimension criteria on queries in Cassandra. It already alows efficient 2-d queries because it is not simply a key-value store, but actually a key-key value store. That is:</p>

<p>Simple key-value store:</p>

<p><a href=""https://i.stack.imgur.com/4omb3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4omb3.png"" alt=""enter image description here""></a></p>

<p>Key-key-value store:</p>

<p><a href=""https://i.stack.imgur.com/k1qXc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/k1qXc.png"" alt=""enter image description here""></a></p>

<p>So the attraction with Cassandra is that given a value for keyA, I can perform very efficient range queries along keyB, because they are contiguously stored. </p>

<p>Now is it possibe, given keyA and keyB, to also have an index along a third dimension, say keyC, so that I can limit which values are returned based on keyC?</p>

<p>So essentially:</p>

<p><a href=""https://i.stack.imgur.com/DgBjc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DgBjc.png"" alt=""enter image description here""></a></p>

<p>Basically given keyA, say keyA-1, and a range of KeyB, say keyB-2 thru keyB-4, I want only to return the values corresponding with keyC-3, shown green above. </p>

<p>I know this is possible because even a simple key-value store can do it with multiple indices. The question is, is it <em>efficient</em>. Could I still perform really fast range queries along keyB?</p>

<p>My use case is time series, where I want to store minute-resolution, and daily-resolution data for the same series. So keyA would be the series I want, keyB would be the day, and keyC would be the minute. I want to do this because storing everything as minute would mean if I needed the daily data, it would mean getting far too much data out and over the network (24*60 minutes per day and I only want one of them), into memory, and lots of client-side aggregation. </p>

<p>I know I could store minute and daily in separate tables, but that would limit my flexibility somewhat, not to mention the cleanliness of the schema. </p>

<p>If this is not easy/efficient in Cassandra, is this possible in RIAK TS?</p>
",<cassandra><cassandra-2.0><riak>,"<blockquote>
  <p>Basically given keyA, say keyA-1, and a range of KeyB, say keyB-2 thru keyB-4, I want only to return the values corresponding with keyC-3, shown green above.</p>
</blockquote>

<p>Yes it is possible with the following table structure</p>

<pre><code>CREATE TABLE data (
     keyA text,
     keyC text,
     keyB int,
     val double,
     PRIMARY KEY ((keyA), keyC, keyB)
);

 SELECT * FROM data WHERE keyA='xxx' AND keyC='yyy' AND keyB&gt;=aaa AND keyB&lt;=bbb;
</code></pre>

<p>The abstraction for this table can be seen as:</p>

<pre><code>Map&lt;KeyA,SortedMap&lt;KeyC,SortedMap&lt;KeyB,val&gt;&gt;&gt;  
</code></pre>

<blockquote>
  <p>So keyA would be the series I want, keyB would be the day, and keyC would be the minute</p>
</blockquote>

<p>So essentially, with the above table, you can answer to the query: <em>Give me all values for a serie S (keyA), for the minute M (keyC) and for day (keyB) between X and Y</em> very very efficiently because it results in sequential scan...</p>

<p>The only problem now is that the partition key, which is only base on the serie ID (keyA) will grow arbitrary very large.</p>

<p>One solution is to split it by year, e.g. having a composite partition key like <code>PRIMARY KEY((keyA, year), keyC, keyB)</code>. This would impose an extra constraint on your query: <em>you must to provide serie ID AND the year every time</em></p>
",['table']
35255768,35274459,2016-02-07 16:09:04,How do I get the last value of primary key in cassandra(php)?,"<p>How can I get the last value of primary key in Cassandra-php? I meant, we have a function to get the ID generated in last query <code>mysql_insert_id</code> in <strong>PHP</strong> for <strong>MYSQL</strong>. Likewise, is there anything for <strong>Cassandra</strong>? Can you please help me for this problem? Suppose this is my sample table, how can i get the last value of primary key? </p>

<pre><code>      userId                    | BookId (primary key) |  Genrecode
      --------------------------------------------------------------
      22                        | 9a9fa429c3494137     |  ART4
      56                        | 9a9fa429b3496137     |  45RT
      89                        | 9a9ga429a3496132     |  ER68
      20                        | 249ga429a9096542     |  QW3Y
      29                        | 249kg429a2393652     |  QWE5
      12                        | i55oa429a9093462     |  9ER4
      08                        | e4235k594ik9654r     |  WRUO
</code></pre>
",<php><cassandra><thrift><cql><nosql>,"<p>As Alar mentioned, there are no auto-increment/last_insert_id() and the linked article goes into why. </p>

<p>That being said, problems like these turn into a data model problem. </p>

<blockquote>
  <p>How can I get the last value of primary key in Cassandra(-php)</p>
</blockquote>

<p>I suggest creating a separate table in Cassandra to keep track of keys created. Something like: 
<code>PRIMARY KEY((day),timestamp) and sort timestamp in DESC</code> </p>

<p>This will allow you to: 
<code>SELECT key FROM key_log WHERE day=today limit 1</code> 
To get the most recent key created for that specific day. Depending on the frequency of keys generated, you may have to change the partition key to distribute the data more evenly. </p>

<p>I find it's usually a good idea to keep track of generated keys like this either way because you never know what application logic you'll eventually need to cover and this saves you from an app heavy data migration. </p>
",['table']
35273798,35279705,2016-02-08 15:56:10,How to compute summary statistic on Cassandra table with Spark DataFrame?,"<p>I'm trying to get the min, max mean of some Cassandra/SPARK data but I need to do it with JAVA. </p>

<pre><code>import org.apache.spark.sql.DataFrame;
import static org.apache.spark.sql.functions.*;

DataFrame df = sqlContext.read()
        .format(""org.apache.spark.sql.cassandra"")
        .option(""table"",  ""someTable"")
        .option(""keyspace"", ""someKeyspace"")
        .load();

df.groupBy(col(""keyColumn""))
        .agg(min(""valueColumn""), max(""valueColumn""), avg(""valueColumn""))
        .show();
</code></pre>

<p><strong>EDITED to show working version:</strong>
Make sure to put "" around the someTable and someKeyspace</p>
",<java><scala><apache-spark><cassandra><statistics>,"<p>Just import your data as a <code>DataFrame</code> and apply required aggregations:</p>

<pre><code>import org.apache.spark.sql.DataFrame;
import static org.apache.spark.sql.functions.*;

DataFrame df = sqlContext.read()
        .format(""org.apache.spark.sql.cassandra"")
        .option(""table"", someTable)
        .option(""keyspace"", someKeyspace)
        .load();

df.groupBy(col(""keyColumn""))
        .agg(min(""valueColumn""), max(""valueColumn""), avg(""valueColumn""))
        .show();
</code></pre>

<p>where <code>someTable</code> and <code>someKeyspace</code> store table name and keyspace respectively. </p>
",['table']
35276696,35277690,2016-02-08 18:29:23,Cassandra database model,"<p>I just switched to Cassandra and I have such items to model:</p>

<p>1.House - <a href=""http://www.remax1stclass.com/houses-for-sale/palatine/60067/1025-north-sterling-avenue-211"" rel=""nofollow"">Here is its view</a>
which has 2.<a href=""http://www.remax1stclass.com/houses-for-sale/palatine/60067/1025-north-sterling-avenue-211"" rel=""nofollow"">city</a>, 3.<a href=""http://www.remax1stclass.com/houses-for-sale/palatine/60067"" rel=""nofollow"">zip</a> and 4.<a href=""http://www.remax1stclass.com/houses-for-sale/palatine/single-family-home"" rel=""nofollow"">property type</a> </p>

<p>I will also need <a href=""http://www.remax1stclass.com/houses-for-sale"" rel=""nofollow"">all cities view</a> and I need all property types and zips for search form completions (not implemented yet).</p>

<p>To the best of my knowledge I should create 4 tables and make Primary keys ...""foreign keys"", should not I? </p>

<p>I have also small request - like Harry Truman “GIVE me a one-handed economist,” demanded a frustrated American president. “All my economists say, ‘on the one hand...on the other'” :-).</p>

<p>I do not have enough experience with Cassandra to make a choice if I get ""or you can do in this way, or that way"", so just give me 1 best schema and I will implement it. </p>

<p>Thank you</p>
",<cassandra><nosql>,"<blockquote>
  <p>To the best of my knowledge I should create 4 tables and make Primary keys ...""foreign keys"", should not I?</p>
</blockquote>

<p>You definitely do not want to do that.  First of all, foreign keys do not exist in Cassandra.  Secondly, what you're talking about is modeling from a relational standpoint.  With Cassandra, you don't want data for one query spread across multiple tables, because that could be spread across multiple nodes.  And querying multiple nodes introduces more network time into the equation, which is slow.</p>

<p>In Cassandra, you want to take a query-based modeling approach.  Sometimes that can mean one table for each query.  Bearing that in mind, I am hearing that you need to query your properties two different ways:</p>

<ul>
<li>By ""house"" (MLS?)</li>
<li>By city</li>
</ul>

<p>Essentially, you should have a table to serve each of those queries:</p>

<pre><code>CREATE TABLE housesbymls (
    mls text,
    city text,
    price bigint,
    propertytype text,
    state text,
    street text,
    year bigint,
    zip text,
    PRIMARY KEY (mls));

CREATE TABLE housesByCity (
  mls text,
  street text,
  city text,
  state text,
  zip text,
  propertyType text,
  price bigint,
  year bigint,
  PRIMARY KEY ((state,city),zip,mls));
</code></pre>

<p>After upserting some data, I can query by MLS:</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; SELECT * FROM housesByMLS WHERE mls='09110857';

 mls      | city     | price  | propertytype     | state | street                         | year | zip
----------+----------+--------+------------------+-------+--------------------------------+------+-------
 09110857 | Palatine | 104900 | Condominium Unit |    IL | 1025 North Serling Avenue, 211 | 1978 | 60067

(1 rows)
</code></pre>

<p>And I can query by state/city or state/city/zip:</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; SELECT * FROM housesByCity
    WHERE state='IL' AND city='Palatine';
</code></pre>

<p>or:</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; SELECT * FROM housesByCity
    WHERE state='IL' AND city='Palatine' AND zip='60067';
</code></pre>

<p>Both of those return:</p>

<pre><code> state | city     | zip   | mls      | price  | propertytype     | street                         | year
-------+----------+-------+----------+--------+------------------+--------------------------------+------
    IL | Palatine | 60067 | 09110857 | 104900 | Condominium Unit | 1025 North Serling Avenue, 211 | 1978

(1 rows)
</code></pre>

<p>The idea behind the PRIMARY KEY structure on this one, is that <code>state</code> and <code>city</code> make up the partitioning key (which helps Cassandra figure out where to put that row in the cluster) so they are both required.  Then, as cities can have multiple <code>zip</code> codes, you can also focus your query on that.  PRIMARY KEYs in Cassandra are unique, so I put <code>mls</code> on the end to ensure uniqueness.</p>
",['table']
35303036,35304587,2016-02-09 21:59:31,What is the nature of Cassandra indexes created by DSE when using Solr integration?,"<p>When integrating Solr with Cassandra using DSE software, adding a Solr core for a column family creates indexes on all the top level fields that are indexed in Solr schema. With the example CF and Solr schema outlined <a href=""http://www.datastax.com/dev/blog/tuple-and-udt-support-in-dse-search"" rel=""nofollow"">here</a>, there are a bunch of indexes generated:</p>

<pre><code>cassandra@cqlsh:demo1&gt; desc demo;

CREATE TABLE demo1.demo (
    id text PRIMARY KEY,
    friends list&lt;frozen&lt;name&gt;&gt;,
    magic_numbers frozen&lt;tuple&lt;int, int, int&gt;&gt;,
    name frozen&lt;name&gt;,
    solr_query text,
    status text
[skipped]
CREATE CUSTOM INDEX demo1_demo_friends_index ON demo1.demo (friends) USING 'com.datastax.bdp.search.solr.Cql3SolrSecondaryIndex';
CREATE CUSTOM INDEX demo1_demo_magic_numbers_index ON demo1.demo (magic_numbers) USING 'com.datastax.bdp.search.solr.Cql3SolrSecondaryIndex';
CREATE CUSTOM INDEX demo1_demo_name_index ON demo1.demo (name) USING 'com.datastax.bdp.search.solr.Cql3SolrSecondaryIndex';
CREATE CUSTOM INDEX demo1_demo_solr_query_index ON demo1.demo (solr_query) USING 'com.datastax.bdp.search.solr.Cql3SolrSecondaryIndex';
CREATE CUSTOM INDEX demo1_demo_status_index ON demo1.demo (status) USING 'com.datastax.bdp.search.solr.Cql3SolrSecondaryIndex';
</code></pre>

<p>What I would like to understand is whether these indexes are just true Solr indexes, and just ""show up"" in Cassandra output because there is some integration that is going on, or they are actually ""full Cassandra indexes"" (for the lack of a better name, but I'm talking an index I can create using <code>CREATE INDEX</code> CQL statement). The concern is if they are Cassandra indexes, then they will create a performance problem as the corresponding data is likely to have high cardinality.</p>

<p>If they are not ""full Cassandra indexes"", then I'm wondering why there are their issues creating Solr cores over frozen fields. I.e. if I create a column family of:</p>

<pre><code>cassandra@cqlsh:demo1&gt; CREATE TABLE demo2 ( 
  ""id"" VARCHAR PRIMARY KEY, 
  ""name"" frozen&lt;Name&gt;, 
 ""friends"" frozen&lt;list&lt;Name&gt;&gt; );
</code></pre>

<p>Solr core creation (<code>dsetool create_core</code> with <code>generateResources=true</code>) fails with:</p>

<pre><code>WARN  [demo1.demo2 Index WorkPool scheduler thread-0] 2016-02-09 13:57:14,781  WorkPool.java:672 - Listener com.datastax.bdp.search.solr.AbstractSolrSecondaryIndex$SSIIndexPoolListener@69442bb
6 failed for pool demo1.demo2 Index with exception: SolrCore 'demo1.demo2' is not available due to init failure: org.apache.cassandra.exceptions.InvalidRequestException: Frozen collections cur
rently only support full-collection indexes. For example, 'CREATE INDEX ON &lt;table&gt;(full(&lt;columnName&gt;))'.
org.apache.solr.common.SolrException: SolrCore 'demo1.demo2' is not available due to init failure: org.apache.cassandra.exceptions.InvalidRequestException: Frozen collections currently only su
pport full-collection indexes. For example, 'CREATE INDEX ON &lt;table&gt;(full(&lt;columnName&gt;))'.
        at org.apache.solr.core.CoreContainer.getCore(CoreContainer.java:742) ~[solr-uber-with-auth_2.0-4.10.3.1.287.jar:4.10.3.1.287]
        at com.datastax.bdp.search.solr.core.CassandraCoreContainer.getCore(CassandraCoreContainer.java:171) ~[dse-search-4.8.4.jar:4.8.4]
        at com.datastax.bdp.search.solr.AbstractSolrSecondaryIndex.getCore(AbstractSolrSecondaryIndex.java:546) ~[dse-search-4.8.4.jar:4.8.4]
        at com.datastax.bdp.search.solr.AbstractSolrSecondaryIndex$SSIIndexPoolListener.onBackPressure(AbstractSolrSecondaryIndex.java:1467) ~[dse-search-4.8.4.jar:4.8.4]
</code></pre>

<p>(this, of course, works just fine following the examples in the blog that uses the list of frozen fields, and not the frozen list of fields).</p>
",<cassandra><datastax><datastax-enterprise>,"<blockquote>
  <p>What I would like to understand is whether these indexes are just true Solr indexes, and just ""show up"" in Cassandra output because there is some integration that is going on, or they are actually ""full Cassandra indexes""</p>
</blockquote>

<p>DSE Search indexes use Cassandra's secondary index API to provide a bridge between the Cassandra write path and the Solr document update machinery. They are not ""full Cassandra indexes"" in the sense you've mentioned in your question, even though you see multiple index entries in your table description. Each one of those entries represents a single indexed field in the <em>same</em> Solr core.</p>

<blockquote>
  <p>I'm wondering why there are their issues creating Solr cores over frozen fields.</p>
</blockquote>

<p>Were you able to follow the <a href=""http://www.datastax.com/dev/blog/tuple-and-udt-support-in-dse-search"" rel=""nofollow"">blog post</a> you mentioned to completion, or do you observe your error there as well? If you can follow it to the end without errors, perhaps we can isolate your problem using that as a baseline. (I'm assuming you've used <code>dsetool create_core</code> with <code>generateResources=true</code> to create the core in question.)</p>
",['table']
35355294,35355465,2016-02-12 05:16:46,Failing to connect to Cassandra through Phantom (NoHostAvailableException),"<p>Am getting the following error code when trying to insert into Cassandra through <a href=""https://github.com/websudos/phantom"" rel=""nofollow"">Phantom</a> from a Scala application.</p>

<p>Cassandra version is that bundled with: dsc-cassandra-3.0.1</p>

<blockquote>
  <p>[error] (run-main-0) com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: localhost/127.0.0.1:9042 (com.datastax.driver.core.exceptions.InvalidQueryException: unconfigured table schema_keyspaces), localhost/0:0:0:0:0:0:0:1:9042 (com.datastax.driver.core.TransportException: [localhost/0:0:0:0:0:0:0:1:9042] Cannot connect))
  com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: localhost/127.0.0.1:9042 (com.datastax.driver.core.exceptions.InvalidQueryException: unconfigured table schema_keyspaces), localhost/0:0:0:0:0:0:0:1:9042 (com.datastax.driver.core.TransportException: [localhost/0:0:0:0:0:0:0:1:9042] Cannot connect))</p>
</blockquote>

<p>I have read through other such questions in StackOverflow but have not found a resolution to my issue. </p>

<p>Additionally I have not noticed the presence of the following in any of the other error logs:</p>

<p>Am I correct in reading <code>localhost/127.0.0.1:9042</code></p>

<p>Doesn't this boil down to <code>127.0.0.1/127.0.0.1:9402</code> - which would explain why it can't find the correct port opening.</p>

<p>Going down this track now, trying to figure out if that's a thing.</p>

<p>Have ensured Cassandra is running.</p>

<blockquote>
  <p>I also ran sudo lsof -i -P | grep -i ""listen"" with the following output (just pulling out the java ones):</p>
</blockquote>

<pre><code>java 4053 dan_mi_sun 85u IPv4 0xdbcce7039c377b9d 0t0 TCP localhost:7199 (LISTEN)
java 4053 dan_mi_sun 86u IPv4 0xdbcce703986952cd 0t0 TCP localhost:53680 (LISTEN)
java 4053 dan_mi_sun 92u IPv4 0xdbcce7039869b46d 0t0 TCP localhost:7002 (LISTEN)
java 4053 dan_mi_sun 145u IPv4 0xdbcce7039c37846d 0t0 TCP localhost:9042 (LISTEN)
</code></pre>

<p>Any thoughts on what the issue could be?</p>

<p>Have found this, but not sure if it is relevant:</p>

<p><a href=""https://datastax-oss.atlassian.net/browse/JAVA-897"" rel=""nofollow"">https://datastax-oss.atlassian.net/browse/JAVA-897</a></p>

<p>In case it is of use here is the <code>build.sbt</code></p>

<pre><code>name := ""SuperChain""

organization := ""org.dyne.danielsan""

version := ""0.1.0-SNAPSHOT""

scalaVersion := ""2.11.7""

crossScalaVersions := Seq(""2.10.4"", ""2.11.2"")

resolvers ++= Seq(
  ""Typesafe repository snapshots"" at ""http://repo.typesafe.com/typesafe/snapshots/"",
  ""Typesafe repository releases"" at ""http://repo.typesafe.com/typesafe/releases/"",
  ""Sonatype repo""                    at ""https://oss.sonatype.org/content/groups/scala-tools/"",
  ""Sonatype releases""                at ""https://oss.sonatype.org/content/org.dyne.danielsan.superchain.data.cassandra.init.repositories/releases"",
  ""Sonatype snapshots""               at ""https://oss.sonatype.org/content/org.dyne.danielsan.superchain.data.cassandra.init.repositories/snapshots"",
  ""Sonatype staging""                 at ""http://oss.sonatype.org/content/org.dyne.danielsan.superchain.data.cassandra.init.repositories/staging"",
  ""Java.net Maven2 Repository""       at ""http://download.java.net/maven/2/"",
  ""Twitter Repository""               at ""http://maven.twttr.com"",
  ""Wedsudos Bintray Repo""            at ""https://dl.bintray.com/websudos/oss-releases/""
)

libraryDependencies ++= Seq(
  ""com.websudos"" %% ""phantom-dsl"" % ""1.12.2"",
  ""org.scalatest"" %% ""scalatest"" % ""2.2.1"" % ""test"",
  ""org.scalacheck"" %% ""scalacheck"" % ""1.11.5"" % ""test""
)

initialCommands := ""import org.dyne.danielsan.superchain._""
</code></pre>
",<java><scala><cassandra><datastax><phantom-dsl>,"<p>This error:  <code>com.datastax.driver.core.exceptions.InvalidQueryException: unconfigured table schema_keyspaces</code> leads me to believe that the version of phantom you are using is not using datastax java-driver 3.0+.   Since you are connecting to a 3.0 cluster, you need a 3.0 driver that understands the schema tables (system_schema.* instead of system.schema*).  If you upgrade to phantom-dsl 1.21.0, that should fix the issue.</p>
",['table']
35368151,35371881,2016-02-12 16:56:53,No of SSTtable for given column family,"<p>Folks,</p>

<p>We were trying to evaluate CASSANDRA for one of the production application. We had few basic queries which we would like to understand before going forward.</p>

<p>WRITE :</p>

<p>Cassandra uses consistent hashing mechanism to distribute key evenly across nodes. So some key will be available on some Cassandra node.</p>

<p>We further understood that there will be internal SSTTable structure created to store this data within the node.</p>

<p>READ :</p>

<p>While performing a read client will send request to any Cassandra node cluster and based on consistent hashing Cassandra will determine where the key is located on which node.</p>

<p>Following things are not clear.</p>

<p>1) How many SSTTables are created for given key space/column family on a node ( is it some fix number or only 1)</p>

<p>2) Cassandra document describes that there is some broom filter(alternative to standard hashing) which is used to determine whether given key is present in the SSTtable or not ( What if there are 1000 SSTtables there will be 1000 bloom filter which will be checked to determine whether key is present or not.)</p>
",<cassandra>,"<p>1) Number of sstables depend on the compaction strategy and load. To get an idea check out <a href=""https://en.wikipedia.org/wiki/Log-structured_merge-tree"" rel=""nofollow"">log structured merge trees</a> to have a basic understanding then look at the different compaction strategies (size tiered, leveled, date tiered).</p>

<p>2) Yes there is 1 bloom filter per sstable to give a probabilistic membership of a partition existing in that sstable. Size of bloom filter depends on the number of partitions and the target false positives percentage. They are kept off heap and are generally small, so less a concern now a days than as earlier versions.</p>

<p>Checking out the dynamo and big table papers may help in understanding the principals behind the clustering and storage. There is a lot of free resources on the read/write path and too much to fully go over in a stack overflow question so I would recommend going through some material at the <a href=""https://academy.datastax.com"" rel=""nofollow"">datastax academy</a> or some presentations on youtube.</p>
",['table']
35372052,35469106,2016-02-12 20:56:15,Integer to UUID conversion using padded 0's,"<p>I have a question regarding UUID generation.  </p>

<p>Typically, when I'm generating a UUID I will use a random or time based generation method. </p>

<p>HOWEVER, I'm migrating legacy data from MySQL over to a C* datastore and I need to change the legacy (auto-incrementing) integer IDs to UUIDS.  Instead of creating another denormalized table with the legacy integer IDs as the primary key and all the data duplicated, I was wondering what folks thought about padding 0's onto the front of the integer ID to form a UUID.  Example below.</p>

<p>*Something important to note is that the legacy IDs highest values will never top 1 million, so overflow isn't really an issue.</p>

<p>The idea would look like this:</p>

<p>Legacy ID: 123456    --->    UUID: 00000000-0000-0000-0000-000000123456</p>

<p>This would be done using some string concats and the UUID.fromString(""00000000-0000-0000-0000-000000123456"" method.  </p>

<p>Does this seem like a bad pattern to anyone?  I'm not a huge fan of the idea, gives me a bad taste in my mouth, but I don't have a technical reason for why haha.</p>

<p>As far as collisions go, the probability of a collision occurring is still ridiculously low.  So I'm not worried about increasing collisions.  I suppose it just seems like bad practice to me, that its ""too easy"".</p>
",<java><mysql><cassandra><uuid><nosql>,"<p>I've decided to go in a different direction from doanduyhai's answer.    </p>

<p>In order to maintain data consistency, we decided to fully de-normalize the data and create another table in C* that is keyed on our legacy IDs.  When migrating the objects from our legacy into C*, they are assigned a new randomly generated UUID, which will be their new primary ID for the future.  The legacy IDs will be kept around until such a time that we decide they are no longer needed.  Upon that time, we can cleanly drop the legacy ID table and be done with them.  </p>

<p>This solution allowed for a cleaner break from our legacy ID system in the future, and allowed us to prevent the use of strange custom made UUIDs.  I also wasn't a huge fan of having the ID field as a blob type that could have multiple types of data stored in it since, in the future, we plan on only wanting UUIDs to be there.</p>
",['table']
35392430,35394190,2016-02-14 13:37:34,Cassandra - Delete not working,"<p>Sometimes; when I perform a DELETE; it doesn't work.</p>

<p>My config : [cqlsh 5.0.1 | Cassandra 3.0.3 | CQL spec 3.4.0 | Native protocol v4]</p>

<pre><code>cqlsh:my_db&gt; SELECT * FROM conversations  WHERE user_id=120 AND conversation_id=2 AND peer_type=1;

user_id | conversation_id | peer_type | message_map
---------+-----------------+-----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 120 |               2 |         1 | {0: {real_id: 68438, date: 1455453523, sent: True}, 1: {real_id: 68437, date: 1455453520, sent: True}, 2: {real_id: 68436, date: 1455453517, sent: True}, 3: {real_id: 68435, date: 1455453501, sent: True}, 4: {real_id: 68434, date: 1455453500, sent: True}, 5: {real_id: 68433, date: 1455453499, sent: True}, 6: {real_id: 68432, date: 1455453498, sent: True}, 7: {real_id: 68431, date: 1455453494, sent: True}, 8: {real_id: 68430, date: 1455453480, sent: True}}

(1 rows)
cqlsh:my_db&gt; DELETE message_map FROM conversations WHERE user_id=120 AND conversation_id=2 AND peer_type=1;
cqlsh:my_db&gt; SELECT * FROM conversations  WHERE user_id=120 AND conversation_id=2 AND peer_type=1;

user_id | conversation_id | peer_type | message_map
---------+-----------------+-----------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 120 |               2 |         1 | {0: {real_id: 68438, date: 1455453523, sent: True}, 1: {real_id: 68437, date: 1455453520, sent: True}, 2: {real_id: 68436, date: 1455453517, sent: True}, 3: {real_id: 68435, date: 1455453501, sent: True}, 4: {real_id: 68434, date: 1455453500, sent: True}, 5: {real_id: 68433, date: 1455453499, sent: True}, 6: {real_id: 68432, date: 1455453498, sent: True}, 7: {real_id: 68431, date: 1455453494, sent: True}, 8: {real_id: 68430, date: 1455453480, sent: True}}

(1 rows)
</code></pre>

<p>CQLSH doesn't return me any error on the DELETE instruction, but it's like if it wasn't taken in account.</p>

<p>Do you know why ?</p>

<p>NB : This is my table definition :</p>

<pre><code>CREATE TABLE be_telegram.conversations (
user_id bigint,
conversation_id int,
peer_type int,
message_map map&lt;int, frozen&lt;message&gt;&gt;,
PRIMARY KEY (user_id, conversation_id, peer_type)
) WITH CLUSTERING ORDER BY (conversation_id ASC, peer_type ASC)
AND bloom_filter_fp_chance = 0.01
AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
AND comment = ''
AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
AND crc_check_chance = 1.0
AND dclocal_read_repair_chance = 0.1
AND default_time_to_live = 0
AND gc_grace_seconds = 864000
AND max_index_interval = 2048
AND memtable_flush_period_in_ms = 0
AND min_index_interval = 128
AND read_repair_chance = 0.0
AND speculative_retry = '99PERCENTILE';
</code></pre>
",<cassandra><cqlsh>,"<p>A <code>DELETE</code> statement removes one or more columns from one or more rows in a table, or it removes the entire row if no columns are specified. Cassandra applies selections within the same <code>partition key</code> atomically and in isolation.</p>

<p>When a column is deleted, it is not removed from disk immediately. The deleted column is marked with a <code>tombstone</code> and then removed after the configured grace period has expired. The optional <code>timestamp</code> defines the new <code>tombstone</code> record.</p>

<h1>About deletes in <strong>Cassandra</strong></h1>

<p>The way Cassandra deletes data differs from the way a relational database deletes data. A relational database might spend time scanning through data looking for expired data and throwing it away or an administrator might have to partition expired data by month, for example, to clear it out faster. Data in a Cassandra column can have an optional expiration date called TTL (time to live).</p>

<h3><strong>Facts about deleted data to keep in mind are:</strong></h3>

<ol>
<li>Cassandra does not immediately remove data marked for deletion from
disk. The deletion occurs during compaction.</li>
<li>If you use the <a href=""https://docs.datastax.com/en/cql/3.1/cql/cql_reference/tabProp.html?scroll=tabProp__moreCompaction"" rel=""noreferrer"">sized-tiered</a> or <a href=""https://docs.datastax.com/en/cql/3.0/cql/cql_reference/tabProp.html?scroll=tabProp__moreCompaction"" rel=""noreferrer"">date-tiered</a> compaction strategy, you
can drop data immediately by <a href=""https://docs.datastax.com/en/cassandra/2.0/cassandra/tools/toolsCompact.html"" rel=""noreferrer"">manually starting the compaction
process</a>. Before doing so, understand the documented disadvantages of
the process.</li>
<li>A deleted column can reappear if you do not run <a href=""https://docs.datastax.com/en/cassandra/2.0/cassandra/tools/toolsRepair.html"" rel=""noreferrer"">node repair</a>
routinely.</li>
</ol>

<h3>Why deleted data can reappear</h3>

<blockquote>
  <p>Marking data with a tombstone signals Cassandra to retry sending a
  delete request to a replica that was down at the time of delete. If
  the replica comes back up within the grace period of time, it
  eventually receives the delete request. However, if a node is down
  longer than the grace period, the node can miss the delete because the
  tombstone disappears after gc_grace_seconds. Cassandra always attempts
  to replay missed updates when the node comes back up again. After a
  failure, it is a best practice to run node repair to <a href=""https://docs.datastax.com/en/cassandra/2.0/cassandra/operations/ops_repair_nodes_c.html"" rel=""noreferrer"">repair
  inconsistencies</a> across all of the replicas when bringing a node back
  into the cluster. If the node doesn't come back within
  gc_grace,_seconds, remove the node, wipe it, and bootstrap it again.</p>
</blockquote>

<p>In your case, <code>compaction</code> is <code>sized-tiered</code>. So please try compaction process.</p>

<blockquote>
  <h3>Compaction</h3>
  
  <p>Periodic compaction is essential to a healthy Cassandra database
  because Cassandra does not insert/update in place. As inserts/updates
  occur, instead of overwriting the rows, Cassandra writes a new
  timestamped version of the inserted or updated data in another
  SSTable. Cassandra manages the accumulation of SSTables on disk using
  compaction.</p>
  
  <p>Cassandra also does not delete in place because the SSTable is
  immutable. Instead, Cassandra marks data to be deleted using a
  tombstone. Tombstones exist for a configured time period defined by
  the <a href=""https://docs.datastax.com/en/cql/3.1/cql/cql_reference/cql_storage_options_c.html"" rel=""noreferrer"">gc_grace_seconds</a> value set on the table. During compaction, there
  is a temporary spike in disk space usage and disk I/O because the old
  and new SSTables co-exist. This diagram depicts the compaction
  process:</p>
</blockquote>

<p><a href=""https://i.stack.imgur.com/oRX1H.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/oRX1H.png"" alt=""enter image description here""></a></p>

<blockquote>
  <p>Compaction merges the data in each SSTable data by partition key,
  selecting the latest data for storage based on its timestamp.
  Cassandra can merge the data performantly, without random IO, because
  rows are sorted by partition key within each SSTable. After evicting
  tombstones and removing deleted data, columns, and rows, the
  compaction process consolidates SSTables into a single file. The old
  SSTable files are deleted as soon as any pending reads finish using
  the files. Disk space occupied by old SSTables becomes available for
  reuse.</p>
  
  <p>Data input to SSTables is sorted to prevent random I/O during SSTable
  consolidation. After compaction, Cassandra uses the new consolidated
  SSTable instead of multiple old SSTables, fulfilling read requests
  more efficiently than before compaction. The old SSTable files are
  deleted as soon as any pending reads finish using the files. Disk
  space occupied by old SSTables becomes available for reuse.</p>
</blockquote>

<p>so try this </p>

<pre><code>nodetool &lt;options&gt; repair

options are:
( -h | --host ) &lt;host name&gt; | &lt;ip address&gt;
( -p | --port ) &lt;port number&gt;
( -pw | --password ) &lt;password &gt;
( -u | --username ) &lt;user name&gt;
-- Separates an option and argument that could be mistaken for a option.
keyspace is the name of a keyspace.
table is one or more table names, separated by a space.
</code></pre>

<p>This command starts the compaction process on tables that use the <code>SizeTieredCompactionStrategy</code> or <code>DateTieredCompactionStrategy</code>. You can specify a keyspace for compaction. If you do not specify a <code>keyspace</code>, the <code>nodetool</code> command uses the <code>current keyspace</code>. You can specify one or more tables for <code>compaction</code>. If you do not specify a table(s), compaction of all tables in the keyspace occurs. This is called a <strong>major compaction</strong>. If you do specify a table(s), compaction of the specified table(s) occurs. This is called a <strong>minor compaction</strong>. A major compaction consolidates all existing SSTables into a single SSTable. During compaction, there is a temporary spike in disk space usage and disk I/O because the old and new SSTables co-exist. A major compaction can cause considerable disk I/O.</p>
",['table']
35403691,35404165,2016-02-15 07:29:09,Lost updates in Cassandra,"<p>I am facing problems of lost writes while updating a row in Cassandra. Here's my schema:</p>

<pre><code>create table balances(
id bigint,
balance decimal,
last_transaction_id bigint,
update_timestamp timestamp,
type varchar,
is_balance_valid boolean, 
primary key (wallet_id)
) 
</code></pre>

<p>Total nodes in cluster: 3 in Local DC
Replication factor: 2
Cassandra Version: 2.1.8</p>

<p>I update the value of column ""<strong>balance</strong>"" every time user does a transaction by reading the previously set value, adding the transaction amount and issuing the update. I am using Java, Datastax driver (2.1.5).</p>

<p>Once out of about half a million transactions, one particular update would fail. This happens usually when the user has done two transactions in quick succession, down to few milliseconds. Here are logs:</p>

<blockquote>
  <p>Transaction #1</p>
  
  <p>10 Feb 2016 18:15:16,984 -[pool-11-thread-1]-  INFO -
  ScratchpadMasterStreamProcessor.processMessage(62) - Printing str id:
  1466140282Scratchpad id: 9127013322</p>
  
  <p>10 Feb 2016 18:15:16,986 -[pool-11-thread-1]- DEBUG -
  SclwBalanceUpdater.updateBalance(43) - <strong>Current balance: 0.0</strong></p>
  
  <p>10 Feb 2016 18:15:16,986 -[pool-11-thread-1]- DEBUG -
  SclwBalanceUpdater.updateBalance(44) - <strong>Deviation : 200.0</strong></p>
  
  <p>10 Feb 2016 18:15:16,986 -[pool-11-thread-1]- DEBUG -
  UserBalanceManager.updateWalletBalance(70) - Updating user..510978682</p>
  
  <p>10 Feb 2016 18:15:16,987 -[pool-11-thread-1]- DEBUG -
  SclwBalanceUpdater.updateBalance(51) - <strong>Final Balance: 200.0</strong></p>
  
  <p>10 Feb 2016 18:15:16,987 -[pool-11-thread-1]- DEBUG -
  ScratchpadMasterStreamProcessor.processMessage(79) - Balance Update
  was successful for wallet 510978682</p>
  
  <p>Transaction #2 </p>
  
  <p>10 Feb 2016 18:18:19,157 -[pool-11-thread-1]-  INFO -
  ConsumerThread.run(82) - Event Recieved</p>
  
  <p>10 Feb 2016 18:18:19,159 -[pool-11-thread-1]- DEBUG -
  SclwBalanceUpdater.updateBalance(43) - <strong>Current balance: 200.0</strong></p>
  
  <p>10 Feb 2016 18:18:19,159 -[pool-11-thread-1]- DEBUG -
  SclwBalanceUpdater.updateBalance(44) - <strong>Deviation : 50.0</strong></p>
  
  <p>10 Feb 2016 18:18:19,159 -[pool-11-thread-1]- DEBUG -
  UserBalanceManager.updateWalletBalance(70) - Updating user..510978682</p>
  
  <p>10 Feb 2016 18:18:19,160 -[pool-11-thread-1]- DEBUG -
  SclwBalanceUpdater.updateBalance(51) - <strong>Final Balance: 250.0</strong></p>
  
  <p>10 Feb 2016 18:18:19,160 -[pool-11-thread-1]- DEBUG -
  ScratchpadMasterStreamProcessor.processMessage(79) - Balance Update
  was successful for wallet 510978682</p>
  
  <p>Transaction #3 (This is lost)</p>
  
  <p>10 Feb 2016 18:18:19,160 -[pool-11-thread-1]-  INFO -
  ScratchpadMasterStreamProcessor.processMessage(62) - Printing str id:
  1466162182Scratchpad id: 9127117934</p>
  
  <p>10 Feb 2016 18:18:19,161 -[pool-11-thread-1]- DEBUG -
  SclwBalanceUpdater.updateBalance(43) - <strong>Current balance: 250.0</strong></p>
  
  <p>10 Feb 2016 18:18:19,161 -[pool-11-thread-1]- DEBUG -
  SclwBalanceUpdater.updateBalance(44) - <strong>Deviation : -250.0</strong></p>
  
  <p>10 Feb 2016 18:18:19,161 -[pool-11-thread-1]- DEBUG -
  UserBalanceManager.updateWalletBalance(70) - Updating user..510978682</p>
  
  <p>10 Feb 2016 18:18:19,162 -[pool-11-thread-1]- DEBUG -
  SclwBalanceUpdater.updateBalance(51) - <strong>Final Balance: 0.0</strong></p>
  
  <p>10 Feb 2016 18:18:19,162 -[pool-11-thread-1]- DEBUG -
  ScratchpadMasterStreamProcessor.processMessage(79) - Balance Update
  was successful for wallet 510978682</p>
  
  <p>Transaction #4 Read stale balance, oops</p>
  
  <p>10 Feb 2016 18:18:23,140 -[pool-11-thread-1]-  INFO -
  ConsumerThread.run(82) - Event Recieved</p>
  
  <p>10 Feb 2016 18:18:23,140 -[pool-11-thread-1]-  INFO -
  ScratchpadMasterStreamProcessor.processMessage(62) - Printing str id:
  1466162730Scratchpad id: 9127120830</p>
  
  <p>10 Feb 2016 18:18:23,141 -[pool-11-thread-1]- DEBUG -
  SclwBalanceUpdater.updateBalance(43) - <strong>Current balance: 250.0</strong></p>
  
  <p>10 Feb 2016 18:18:23,141 -[pool-11-thread-1]- DEBUG -
  SclwBalanceUpdater.updateBalance(44) - <strong>Deviation : 200.0</strong></p>
  
  <p>10 Feb 2016 18:18:23,141 -[pool-11-thread-1]- DEBUG -
  UserBalanceManager.updateWalletBalance(70) - Updating user..510978682</p>
  
  <p>10 Feb 2016 18:18:23,142 -[pool-11-thread-1]- DEBUG -
  SclwBalanceUpdater.updateBalance(51) - <strong>Final Balance: 450.0</strong></p>
  
  <p>10 Feb 2016 18:18:23,142 -[pool-11-thread-1]- DEBUG -
  ScratchpadMasterStreamProcessor.processMessage(79) - Balance Update
  was successful for wallet 510978682</p>
</blockquote>

<p>I have set consistency level to LOCAL_QUORUM for both reads and writes, and the three cassandra node servers have same time (using NTP). What could be the problem?</p>
",<java><cassandra><updates><datastax><consistency>,"<p>First, please take a look on Codo's comment that describes very well <em>why</em> do you have a problem. </p>

<p>However I'd like to suggest a solution without moving to other DB. 
You can use counter type for your <code>balance</code> field. The update statement of counter works differently. It sends to cassandra command to increase/decrease the field by cirtain value, so you will not have a problem of inconsistency. </p>

<p>The counter based solution however is not good for all applications. For example it is limited to integer type. Probably more common solution is to create a kind of transaction programmatically: save update requests in separate table and create asynchronous procedure that aggregates all update requests done during certain period of time and applies them to the <code>balance</code> value. </p>
",['table']
35432959,35433111,2016-02-16 12:43:11,How to perform efficient SELECT * queries in C* (Cassandra),"<p>With a lot of data in a table the SELECT * queries seem to be inefficient.
How is it possible to solve this problem?</p>

<p>Are there any ideas for a design of C* infrastructure or a design of table itself?</p>

<p>Might there be any special query structure to perform select all elements more efficient?</p>
",<database><performance><cassandra><architecture><nosql>,"<blockquote>
  <p>With a lot of data in a table the SELECT * queries seem to be inefficient.  How is it possible to solve this problem?</p>
</blockquote>

<p>With a relational database, SELECT * FROM without restriction == <strong>full table scan</strong></p>

<p>With a distributed database like <strong>Cassandra</strong>, SELECT * FROM without restriction == <strong>full CLUSTER scan</strong>, possibly on a cluster of 100+ machines ....</p>

<p>Long story short, it is <strong>not designed</strong> for full table scan.</p>

<p>If you need to scan through all the data from a table, use <strong>Apache Spark</strong> with the <strong>Spark/Cassandra</strong> connector to do the job.</p>

<blockquote>
  <p>Might there be any special query structure to perform select all elements more efficient?</p>
</blockquote>

<p>No, magic doesn't exist. And if someone ever finds a way to perform full table scan in distributed database extremely fast, he'll be millionaire already.</p>
",['table']
35466325,35466546,2016-02-17 19:48:27,Paging Large Queries: total number,"<p>Regarding Cassandra and paging. I might guess the answer but just to be sure;</p>

<p>I know how to ask for a pagesize, but is it possible to get the eventually total number for a query. Like you query <code>select * from tableName</code> with a pagesize of 10, but if you did not use paging you would get 100. Is it possible to get the number 100 when using pagesize 10?</p>

<p>Note: Just if it is of any use, I am using <code>gocql</code>.</p>
",<cassandra><cassandra-2.0><gocql>,"<p>Plain answer is no, getting number 100 in your example means knowing the result of ""SELECT count(*) FROM table"", which is a perf killer query.</p>

<p>The best thing you can get is an <strong>estimate</strong> of number of partitions per node using <strong>nodetool</strong> or calling directly <strong>JMX Beans</strong>. But it won't get you the estimate of <strong>CQL row</strong> (because in 1 partition there may be N rows if your table has clustering columns)</p>
",['table']
35532569,35534622,2016-02-21 05:10:59,Exception while running Spring boot cassandra project,"<p>I downloaded the spring initializer project by selecting few dependencies for web, security, validation and spring-boot cassandra. When I try to run the ./gradlew bootrun I'm getting the following exception during startup</p>

<blockquote>
  <p>org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'session' defined in class path resource [org/springframework/boot/autoconfigure/data/cassandra/CassandraDataAutoConfiguration.class]: Invocation of init method failed; nested exception is com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: localhost/127.0.0.1:9042 (com.datastax.driver.core.exceptions.InvalidQueryException: unconfigured table schema_keyspaces))
      at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1578) ~[spring-beans-4.2.4.RELEASE.jar:4.2.4.RELEASE]
      at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:545) ~[spring-beans-4.2.4.RELEASE.jar:4.2.4.RELEASE]
      at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:482) ~[spring-beans-4.2.4.RELEASE.jar:4.2.4.RELEASE]
      at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:306) ~[spring-beans-4.2.4.RELEASE.jar:4.2.4.RELEASE]
      at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230) ~[spring-beans-4.2.4.RELEASE.jar:4.2.4.RELEASE]
      at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:302) ~[spring-beans-4.2.4.RELEASE.jar:4.2.4.RELEASE]
      at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:197) ~[spring-beans-4.2.4.RELEASE.jar:4.2.4.RELEASE]
      at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java[spring-beans-4.2.4.RELEASE.jar:4.2.4.RELEASE]
      at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:839) ~[spring-context-4.2.4.RELEASE.jar:4.2.4.RELEASE]
      at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:538) ~[spring-context-4.2.4.RELEASE.jar:4.2.4.RELEASE]
      at org.springframework.boot.context.embedded.EmbeddedWebApplicationContext.refresh(EmbeddedWebApplicationContext.java:118) ~[spring-boot-1.3.2.RELEASE.jar:1.3.2.RELEASE]
      at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:766) [spring-boot-1.3.2.RELEASE.jar:1.3.2.RELEASE]
      at org.springframework.boot.SpringApplication.createAndRefreshContext(SpringApplication.java:361) [spring-boot-1.3.2.RELEASE.jar:1.3.2.RELEASE]
      at org.springframework.boot.SpringApplication.run(SpringApplication.java:307) [spring-boot-1.3.2.RELEASE.jar:1.3.2.RELEASE]
      at org.springframework.boot.SpringApplication.run(SpringApplication.java:1191) [spring-boot-1.3.2.RELEASE.jar:1.3.2.RELEASE]
      at org.springframework.boot.SpringApplication.run(SpringApplication.java:1180) [spring-boot-1.3.2.RELEASE.jar:1.3.2.RELEASE]
      at com.anirudh.myretail.CassandratestApplication.main(CassandratestApplication.java:10) [main/:na]
  Caused by: com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: localhost/127.0.0.1:9042 (com.datastax.driver.core.exceptions.InvalidQueryException: unconfigured table schema_keyspaces))
      at com.datastax.driver.core.ControlConnection.reconnectInternal(ControlConnection.java:240) ~[cassandra-driver-core-2.1.9.jar:na]
      at com.datastax.driver.core.ControlConnection.connect(ControlConnection.java:86) ~[cassandra-driver-core-2.1.9.jar:na]
      at com.datastax.driver.core.Cluster$Manager.init(Cluster.java:1429) ~[cassandra-driver-core-2.1.9.jar:na]
      at com.datastax.driver.core.Cluster.init(Cluster.java:162) ~[cassandra-driver-core-2.1.9.jar:na]
      at com.datastax.driver.core..connectAsync(Cluster.java:341) ~[cassandra-driver-core-2.1.9.jar:na]
      at com.datastax.driver.core.Cluster.connectAsync(Cluster.java:314) ~[cassandra-driver-core-2.1.9.jar:na]
      at com.datastax.driver.core.Cluster.connect(Cluster.java:252) ~[cassandra-driver-core-2.1.9.jar:na]
      at org.springframework.cassandra.config.CassandraCqlSessionFactoryBean.afterPropertiesSet(CassandraCqlSessionFactoryBean.java:82) ~[spring-cql-1.3.2.RELEASE.jar:na]
      at org.springframework.data.cassandra.config.CassandraSessionFactoryBean.afterPropertiesSet(CassandraSessionFactoryBean.java:43) ~[spring-data-cassandra-1.3.2.RELEASE.jar:na]
      at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1637) ~[spring-beans-4.2.4.RELEASE.jar:4.2.4.RELEASE]
      at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1574) ~[spring-beans-4.2.4.RELEASE.jar:4.2.4.RELEASE]</p>
</blockquote>
",<cassandra><spring-boot>,"<blockquote>
  <p>com.datastax.driver.core.exceptions.InvalidQueryException: unconfigured table schema_keyspaces</p>
</blockquote>

<p>The Cassandra Java Driver Spring Data is using is <strong>2.1.9</strong></p>

<p>The Cassandra version you're using is probably <strong>3.x</strong>. Since <strong>3.x</strong> all the meta data has been moved to keyspace <strong>system_schema</strong></p>

<p>The solution to your issue are:</p>

<ul>
<li>downgrade your version to Cassandra <strong>2.2.x</strong> or <strong>2.1.x</strong></li>
<li>wait for Spring Data Cassandra to upgrade the driver version to
<strong>3.0.0</strong></li>
<li>if you don't want to wait, use <a href=""https://github.com/doanduyhai/Achilles/wiki"" rel=""nofollow noreferrer"">Achilles 5.2.0</a> which is an object
mapper that support Cassandra <strong>3.x</strong>. <em>Note: I'm the creator of <strong>Achilles</em></strong></li>
<li>if you don't want to wait, use the <a href=""https://github.com/datastax/java-driver/tree/3.0/manual/object_mapper/using"" rel=""nofollow noreferrer"">driver-mapper</a> module that
comes with the Cassandra Java Driver</li>
</ul>
",['table']
35551677,35555996,2016-02-22 10:50:56,cassandra - how to perform table query?,"<p>I am trying to perform a query using 2 tables:</p>

<pre><code>CREATE TABLE users(
  id_ UUID PRIMARY KEY,
  username text,
  email text,
  );

CREATE TABLE users_by_email(
  id UUID,
  email text PRIMARY KEY
)
</code></pre>

<p>In this cas, how to perform a query by email? </p>
",<database><cassandra><cql>,"<p>I am assuming that you also want <code>username</code> returned in the query.  You cannot JOIN tables in Cassandra.  So to do that, you will have to add that column to your <code>users_by_email</code> table:</p>

<pre><code>CREATE TABLE users_by_email(
  id UUID,
  email text PRIMARY KEY,
  username text,
);
</code></pre>

<p>Then, simply query that table by email address.</p>

<pre><code>&gt; SELECT id, email, username FROM users_by_email WHERE email='mreynolds@serenity.com';

 id                                   | email                  | username
--------------------------------------+------------------------+----------
 d8e57eb4-c837-4bd7-9fd7-855497861faf | mreynolds@serenity.com |      Mal

(1 rows)
</code></pre>
",['table']
35614504,35637472,2016-02-24 22:28:21,Cassandra Modeling for filter and range queries,"<p>I'm trying to model a database of users.  These users have various vital statistics: age, sex, height, weight, hair color, etc.</p>

<p>I want to be able to write queries like these:</p>

<p>get all users 5'1"" to 6'0"" tall with red hair who weigh more than 100 pounds</p>

<p>or </p>

<p>get all users who are men who are 6'0"" are ages 31-37 and have black hair</p>

<p>How can I model my data in order to make these queries?  Let's assume this database will hold billions of users.  I can't think of an approach that wouldn't require me to make MANY requests or cluster the data on VERY few nodes.</p>

<p>EDIT:</p>

<p>Just a little more background, let's assume this thought problem is to build a dating website. The site should allow users to filter people based on the aforementioned criteria (age, sex, height, weight, hair, etc.). These filters are optional, and you can have as many as you want. This site has 2 billion users. Is that something that can be achieved through data modeling alone?</p>

<p>IF I UNDERSTAND THINGS CORRECTLY
If I have 2 billion users and I create both of the tables mentioned in the first answer (assuming options of male and female for sex, and blonde, brown, red for hair color), I will, for the first table, be putting at most 2 billion records on one node if everyone has blonde hair. Best case scenario, 2/3 billion records on three nodes. In the second case, I will be putting 2/5 billion records on each node in the best case with the same worst case. Am I wrong? Shouldn't the partition keys be more unique than that? </p>
",<filter><cassandra><modeling>,"<p>Just to reiterate the end of the conversation:</p>

<p>""Your understanding is correct and you are correct in stating that partition keys should be more unique than that. Each partition had a maximum size of 2GB but a practical limit is lower. In practice you would want your data partitioned into far smaller chunks that the table above. Given the ad-hoc nature of your queries in your example I do not think you would be able to practically do this by data modelling alone. I would suggest looking at using a Solr index on a table. This would allow you a robust search capability. If you use Datastax you are even able to query this via CQL""</p>

<p>Cassandra <strong>alone</strong> is not a good candidate for <strong>this sort</strong> of complex filtering across a very large data set.  </p>
",['table']
35616962,35617612,2016-02-25 02:11:02,Is cassandra a row column database?,"<p>Im trying to learn cassandra but im confused with the terminology.</p>

<p>Many instances it says the row stores key/value pairs.</p>

<p>but, when I define a table its more like declaring a SQL table ie; you create a table and specify the column names and data types.</p>

<p>Can someone clarify this?</p>
",<cassandra>,"<p>Cassandra is a column based NoSQL database.  While yes at its lowest level it does store simple key-value pairs it stores these key-value pairs in collections.  This grouping of keys and collections is analogous to rows and columns in a traditional relational model.  Cassandra tables contain a schema and can be referenced (with restrictions) using a SQL-like language called CQL.  </p>

<p>In your comment you ask about Apples being stored in a different table from oranges.  The answer to that specific question is No it will be in the same table.  However Cassandra tables have an additional concept call the Partition Key that doesn't really have an analgous concept in the relational world.  Take for example the following table definition</p>

<p><code>CREATE TABLE fruit_types {
   fruit text,
   location text,
   cost float,
   PRIMARY KEY ((fruit), location)
}</code></p>

<p>In this table definition you will notice that we are defining the schema for the table.  You will also notice that we are defining a <code>PRIMARY KEY</code>.  This primary key is similar but not exactly like a relational concept.  In Cassandra the <code>PRIMAY KEY</code> is made up of two parts the <code>PARTITION KEY</code> and <code>CLUSTERING COLUMNS</code>.  The <code>PARTITION KEY</code> is the first fields specified in the <code>PRIMARY KEY</code> and can contain one or more fields delimitated by parenthesis.  The purpose of the <code>PARTITION KEY</code> is to be hashed and used to define the node that owns the data and is also used to physically divide the information on the disk into files.  The <code>CLUSTERING COLUMNS</code> make up the other columns listed in the <code>PRIMARY KEY</code> and amongst other things are used for defining how the data is physically stored on the disk inside the different files as specified by the <code>PARTITION KEY</code>.  I suggest you do some additional reading on the <code>PRIMARY KEY</code> here if your interested in more detail:</p>

<p><a href=""https://docs.datastax.com/en/cql/3.0/cql/ddl/ddl_compound_keys_c.html"" rel=""nofollow"">https://docs.datastax.com/en/cql/3.0/cql/ddl/ddl_compound_keys_c.html</a></p>
",['table']
35640399,35642152,2016-02-25 23:18:54,Update performance cassandra,"<p>I have a SQL table thats being modelled for cassandra to run different queries.</p>

<p>Person </p>

<pre><code> id primary key,
 fname,
 lname,
 age
</code></pre>

<p>All the fields can be queried so im creating multiple tables</p>

<p>Person_fname</p>

<pre><code> fname primary key,
 lname,
 age,
 id
</code></pre>

<p>Person_lname </p>

<pre><code> lname primary key,
 fname,
 age,
 id
</code></pre>

<p>Person_age</p>

<pre><code> age primary key,
 lname,
 age,
 fname 
</code></pre>

<p>Questions :- </p>

<pre><code>1. If first_name is updated for a particular person, Should I need to update all the tables?
2. What would be the performance impact in CASSANDRA because of this update
</code></pre>
",<cassandra>,"<p>For your questuion #1, since you mean updating column values in different tables, no doubt, you have to update all the tables.</p>

<p>For question #2, first you could do batch update, and the batch update performance on several tables is best when their partition is on the same node, one way to ensure this is to use the same column as partition key of these tables, e.g. person_id.</p>

<p>But your table design above for cassandra looks completely wrong. One thing in cassandra you need to remember is for the same primary key value, you could only have one row in a table, and insert or update with the same value of the primary key, has the same insertOrUpdate effect.</p>

<p>e.g. for your second table, it is not possible to have two rows with a same fname value 'tom'. If you insert two rows with fname='tom', the second insert row actually does update on the row inserted by the first insert.</p>
",['table']
35669381,35672817,2016-02-27 11:53:26,DSE Query optimization and use cases,"<p>Should I use <strong>Solr</strong> for all of my reading activities and then <strong>Cassandra</strong> for all writes to maximise the performance of DSE? or can I read using <strong>Cassandra</strong> but obviously on a key value basis for select activities? </p>
",<php><solr><cassandra><datastax><datastax-enterprise>,"<p>Cassandra is a write-optimised database and so reads may be slow, but Solr should be used a crutch or a 'nitro-boost' if you will, and not as the go-to method for reading. Because if your reads are slow, it may be because the DB design is fundamentally flawed and that could be dangerous for scaling as well as maintenance.</p>

<p>Maximizing the performance of a DSE should be based on the pattern of your reads and writes. For example if your users table is only used for login and a couple of other times for profile related data, you don't need Solr for that. Some duplicate tables with different keys should suffice.</p>

<p>However if your app is an ERP that requires user data at all times, Solr indexing for faster reads should be considered.</p>

<p>And to reiterate, if your reads are slow, check if a better db design can solve the issue.</p>
",['table']
35719985,35724305,2016-03-01 10:08:17,Range query on secondary index in cassandra,"<p>I am using cassandra 2.1.10.
So First I will clear that I know secondary index are anti-pattern in cassandra.But for testing purpose I was trying following:</p>

<pre><code>CREATE TABLE test_topology1.tt (
    a text PRIMARY KEY,
    b timestamp
) WITH bloom_filter_fp_chance = 0.01
    AND caching = '{""keys"":""ALL"", ""rows_per_partition"":""NONE""}'
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy'}
    AND compression = {'sstable_compression': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99.0PERCENTILE';
CREATE INDEX idx_tt ON test_topology1.tt (b);
</code></pre>

<p>When I run following query it gives me error.</p>

<pre><code>cqlsh:test_topology1&gt; Select * from tt where b&gt;='2016-04-29 18:00:00' ALLOW FILTERING;
InvalidRequest: code=2200 [Invalid query] message=""No secondary indexes on the restricted columns support the provided operators: 'b &gt;= &lt;value&gt;'""
</code></pre>

<p>while this <a href=""http://www.datastax.com/dev/blog/a-deep-look-to-the-cql-where-clause"" rel=""noreferrer"">Blog</a> says that allow filtering can be used to query secondary index.
Cassandra is installed on windows machine.</p>
",<cassandra><cassandra-2.0><cql3>,"<p>Range queries on secondary index columns are not allowed in Cassandra up to and including 2.2.x. However, as the post <em><a href=""http://www.datastax.com/dev/blog/a-deep-look-to-the-cql-where-clause"" rel=""nofollow noreferrer"">A deep look at the CQL WHERE clause</a></em> points out, they are allowed on non-indexed columns, if filtering is allwed:</p>
<blockquote>
<p>Direct queries on secondary indices support only =, CONTAINS or
CONTAINS KEY restrictions.</p>
<p>[..]</p>
<p>Secondary index queries allow you to restrict the returned results
using the =, &gt;, &gt;=, &lt;= and &lt;, CONTAINS and CONTAINS KEY restrictions
on non-indexed columns using filtering.</p>
</blockquote>
<p>So, given the table structure and index</p>
<pre><code>CREATE TABLE test_secondary_index (
     a text PRIMARY KEY,
     b timestamp,
     c timestamp 
);
CREATE INDEX idx_inequality_test ON test_secondary_index (b);
</code></pre>
<p>the following query fails because the inequality test is done on the indexed column:</p>
<pre><code>SELECT * FROM  test_secondary_index WHERE b &gt;= '2016-04-29 18:00:00' ALLOW FILTERING ;
InvalidRequest: code=2200 [Invalid query] message=&quot;No secondary indexes on the restricted columns support the provided operators: 'b &gt;= &lt;value&gt;'&quot;
</code></pre>
<p>But the following works because the inequality test is done on a non-indexed column:</p>
<pre><code>SELECT * FROM  test_secondary_index WHERE b = '2016-04-29 18:00:00' AND c &gt;= '2016-04-29 18:00:00' ALLOW FILTERING ;

 a | b | c
---+---+---

(0 rows)
</code></pre>
<p>This still works if you add another index on column <code>c</code>, but also still requires the <code>ALLOW FILTERING</code> term, which to me means that the index on column c is not used in this scenario.</p>
",['table']
35729937,35730403,2016-03-01 17:54:21,Cassandra order and clustering key,"<p>I have this table:</p>

<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE custumer_events_service.events_by_websiteId_time(
    ""event_id"" text,
    ""currentTime"" timestamp,
    ""websiteId"" varchar,

    OTHER COLUMNS ...

    PRIMARY KEY(event_id, websiteId, currentTime)
)
</code></pre>

<p>In this case, would I get 10000 rows ordered by <code>currentime</code> when I execute this query:</p>

<pre class=""lang-sql prettyprint-override""><code>SELECT * FROM events_by_websiteid_time WHERE websiteid='xxxx' LIMIT 10000 ALLOW FILTERING;
</code></pre>

<p>Or did I have to add <code>WITH CLUSTERING ORDER BY (currentTime DESC);</code> at the end?</p>
",<database><cassandra><cql><cql3>,"<p>Cassandra can only enforce a sort order within a partition.  As you are using <code>ALLOW FILTERING</code> to avoid having to provide your partition key (<code>event_id</code>) your result set will be ordered by the hashed token values of each <code>event_id</code>, and then by <code>websiteid</code> and <code>currentTime</code>.</p>

<p>To get your results to be ordered by <code>currentTime</code>, you would need to either create a new query table or alter the PRIMARY KEY definition (and perhaps the CLUSTERING ORDER) of your existing table.  If you decide to create a new query table, it would have to look something like this:</p>

<pre><code>CREATE TABLE custumer_events_service.events_by_websiteId_time_eventid(
  event_id text,
  currentTime timestamp,
  websiteId varchar,

OTHER COLUMNS ...

  PRIMARY KEY (websiteid,currentTime,event_id))
WITH CLUSTERING ORDER BY (currentTime DESC, event_id ASC);
</code></pre>

<p>That would allow this query:</p>

<pre><code>SELECT * FROM events_by_websiteid_time_eventid WHERE websiteid='xxxx' LIMIT 10000;
</code></pre>

<p>...to work as you expect.</p>
",['table']
35741765,35741994,2016-03-02 08:17:54,"what's the difference among row key, primary key and index in cassandra?","<p>I'm so confused.
When to use them and how to determine which one to use?
If a column is index/primary key/row key, could it be duplicated?</p>

<p>I want to create a column family to store some many-to-many info, for example, one column is the given name and the other is surname. One given name can related to many surnames, and one surname could have different given names.</p>

<p>I need to query surnames by a given name, and the given names by a specified surname too.</p>

<p>How to create the table?</p>

<p>Thanks!</p>
",<cassandra>,"<p>Cassandra is a NoSQL database, and as such has no such concept of many-to-many relationships. Ideally a table should not have anything other than a primary key. In your case the right way to model it in Cassandra is to create two tables, one with name as the primary key and the other with surname as the primary key</p>

<p>When you need to query by either key, you need to query the table that has that key as the primary key</p>

<p>EDIT:
From the Cassandra docs:</p>

<blockquote>
  <p>Cassandra's built-in indexes are best on a table having many rows that
  contain the indexed value. The more unique values that exist in a
  particular column, the more overhead you will have, on average, to
  query and maintain the index. For example, suppose you had a races
  table with a billion entries for cyclists in hundreds of races and
  wanted to look up rank by the cyclist. Many cyclists' ranks will share
  the same column value for race year. The race_year column is a good
  candidate for an index.</p>
  
  <p>Do not use an index in these situations: </p>
  
  <ul>
  <li>On high-cardinality columns for a query of a huge volume of records    for a small number of results. </li>
  <li>In tables that use a counter column On a frequently updated or deleted column. </li>
  <li>To look for a row in a large partition unless narrowly queried.</li>
  </ul>
</blockquote>
",['table']
35748817,35749672,2016-03-02 13:38:17,"InvalidRequest: code=2200 [Invalid query] message=""unconfigured table openeventstream""","<p>I'm trying to run CQL from Cassandra in Python. But I got this error, </p>

<pre><code>InvalidRequest: code=2200 [Invalid query] message=""unconfigured table openeventstream""
</code></pre>

<p>Do anyone have an idea what's going on?</p>

<p>Code:</p>

<pre><code>session = cluster.connect()
session.set_keyspace('meetup')
rows = session.execute('SELECT *  FROM meetup.openEventStream')
for x in rows:
    print x
</code></pre>
",<python><cassandra><cql>,"<p>Cassandra converts unquoted text to lowercase so you need to put your table name inside quotes:</p>

<pre><code>rows = session.execute('SELECT *  FROM meetup.""openEventStream""')
</code></pre>
",['table']
35760443,35760537,2016-03-02 23:40:17,Lazy cassandra load with spark,"<p>I want to know if is a good practice to load a cassandra table in a Lazy mode for then use a where clause.</p>

<p>For example:</p>

<pre><code>Lazy val table = sparkContext.cassandraTable[Type](keyspace,tableName)
</code></pre>

<p>---other part of the code---</p>

<pre><code>table.where(""column = ?"",param)
</code></pre>

<p>Thanks!</p>
",<scala><apache-spark><cassandra>,"<p>All RDD's are lazy by default. They won't actually do anything until you call an action. So don't add lazy as this will just delay the creation of the metadata around your RDD and not actually effect execution.</p>

<p>Example</p>

<pre><code>val table = sparkContext.cassandraTable[Type](keyspace,tableName)
val tableWithWhere = table.where(""x = 5"")
val tableTransformed = table.map( x:Type =&gt; turnXIntoY(x) )
//nothing has happened in C* or Spark on executors yet
tableTransformed.collect // This causes spark to start doing work
</code></pre>
",['table']
35771245,35771739,2016-03-03 11:42:24,Select columns from CassandraRow in Scala,"<p>How to select columns from CassandraRow dynamically. the columns are not fixed. example code below.</p>

<pre><code>var columnList = ""column1, column2, column3"" // this is generated dynamically 
SparkContextFunctions(sc)
  .cassandraTable(ckeyspaceName, tableName)
  .select(columnList)
</code></pre>
",<scala><apache-spark><cassandra>,"<p>For example like this:</p>

<pre><code>val columns = Seq(""column1"", ""column2"", ""column3"").map(ColumnName(_))
val table = SparkContextFunctions(sc).cassandraTable(ckeyspaceName, tableName)
table.select(columns: _*)
</code></pre>
",['table']
35779664,35779966,2016-03-03 18:03:55,Query about DSE search,"<p>In order for my data to become searchable in solr which key space should I use for all of my Cassandra tables?</p>

<p>I have the following keyspaces after starting solr:</p>

<pre><code>        system_traces
        solr_admin
        system
        dse_system
</code></pre>
",<java><php><cassandra><datastax-enterprise>,"<p>You shouldn't use any of those keyspaces for creating your own tables. You need to create your own keyspace, then create your table in that keyspace and then create a solr core on the keyspace on the table.</p>

<p>How you do this will depend on the version of DataStax Enterprise you are running. On 4.8 you can to this through <code>dsetool</code> with the following command:</p>

<pre><code>dsetool create_core keyspace.table generateResources=true reindex=true
</code></pre>

<p>You will then be able to perform search queries against that table.</p>

<p><a href=""https://docs.datastax.com/en/datastax_enterprise/4.8/datastax_enterprise/srch/srchOverview.html"" rel=""nofollow"">Here</a> is the current search documentation for Solr search on DataStax Enterprise.</p>
",['table']
35840493,35848746,2016-03-07 09:46:36,Finding distinct values of non Primary Key column in CQL Cassandra,"<p>I use the following code for creating table:</p>

<pre><code>CREATE KEYSPACE mykeyspace
WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };
USE mykeyspace;
CREATE TABLE users (
  user_id int PRIMARY KEY,
  fname text,
  lname text
);
INSERT INTO users (user_id,  fname, lname)
  VALUES (1745, 'john', 'smith');
INSERT INTO users (user_id,  fname, lname)
  VALUES (1744, 'john', 'doe');
INSERT INTO users (user_id,  fname, lname)
  VALUES (1746, 'john', 'smith');
</code></pre>

<p>I would like to find the distinct value of <code>lname</code> column (that is not a PRIMARY KEY). I would like to get the following result:</p>

<pre><code> lname
-------
 smith
</code></pre>

<p>By using <code>SELECT DISTINCT lname FROM users;</code>
However since <code>lname</code> is not a <code>PRIMARY KEY</code> I get the following error:</p>

<pre><code>InvalidRequest: code=2200 [Invalid query] message=""SELECT DISTINCT queries must
only request partition key columns and/or static columns (not lname)""
cqlsh:mykeyspace&gt; SELECT DISTINCT lname FROM users;
</code></pre>

<p>How can I get the distinct values from <code>lname</code>?</p>
",<select><cassandra><distinct><cql><cql3>,"<p>User - <em>Undefined_variable</em> - makes two good points:</p>

<ul>
<li>In Cassandra, you need to build your data model to match your query patterns.  This sometimes means duplicating your data into additional tables, to attain the desired level of query flexibility. </li>
<li><code>DISTINCT</code> only works on partition keys.</li>
</ul>

<p>So, one way to get this to work, would be to build a specific table to support that query:</p>

<pre><code>CREATE TABLE users_by_lname (
    lname text,
    fname text,
    user_id int,
    PRIMARY KEY (lname, fname, user_id)
);
</code></pre>

<p>Now after I run your INSERTs to this new query table, this works:</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; SELECT DISTINCT lname FROm users_by_lname ;

 lname
-------
 smith
   doe

(2 rows)
</code></pre>

<p>Notes: In this table, all rows with the same partition key (<code>lname</code>) will be sorted by <code>fname</code>, as <code>fname</code> is a clustering key.  I added <code>user_id</code> as an additional clustering key, just to ensure uniqueness.</p>
",['table']
35882516,35886613,2016-03-09 03:48:50,Is creating a new table from scratch to support new query a common pratice in cassandra,"<p>Currently, we have the following table, which enables us to perform query based on day.</p>

<pre><code>CREATE TABLE events_by_day(
    ...
    traffic_type text,
    device_class text,
    country text,
    ...
    yyyymmdd text,
    event_type text,
    the_datetime timeuuid,
    PRIMARY KEY((yyyymmdd, event_type), the_datetime));

create index index_country on events (country);
create index index_traffic_type on events (traffic_type);
create index index_device_class on events (device_class);
</code></pre>

<p>The following queries are being supported.</p>

<pre><code>select * from events where yymmdd = '20160303' and event_type in ('view');
select * from events where yymmdd = '20160303' and event_type in ('lead', 'view', 'sales');
select * from events where yymmdd = '20160303' and event_type = 'lead' and country = 'my' and device_class = 'smart' and traffic_type = 'WEB' ALLOW FILTERING;
</code></pre>

<p>When we need a data more than a day, we will perform the query multiple times. Say, I need ""view"" data from 1st of March 2016 till 3rd of March 2016, I will query 3 times.</p>

<pre><code>select * from events where yymmdd = '20160301' and event_type in ('view');
select * from events where yymmdd = '20160302' and event_type in ('view');
select * from events where yymmdd = '20160303' and event_type in ('view');
</code></pre>

<p>Currently, all these fit well into our requirement.</p>

<p>However, in the future, let's say we have a new requirement, we need ""view"" data from 2013 till 2016.</p>

<p>Instead of querying it 1460 times (365 days * 4 years) , is it a common practice for us to create a whole new empty table like</p>

<pre><code>CREATE TABLE events_by_year(
    ...
    traffic_type text,
    device_class text,
    country text,
    ...
    yyyy text,
    event_type text,
    the_datetime timeuuid,
    PRIMARY KEY((yyyy, event_type), the_datetime));
</code></pre>

<p>and then fill up the data with large data from <code>events_by_day</code> (which might takes several days to finish the insertion as <code>events_by_day</code> table already has many rows)?</p>
",<cassandra>,"<blockquote>
  <p>is it a common practice for us to create a whole new empty table?</p>
</blockquote>

<p>Yes it is.  This is called ""Query Based Modeling,"" and it is quite common in Cassandra.  While Cassandra scales and performs well, it does not offer much in the way of query flexibility.  So to get around that, instead of using ill-performing methods (secondary indexes, ALLOW FILTERING) to query an existing table, the table is commonly duplicated with a different PRIMARY KEY.  Basically, you are trading disk space for performance.</p>

<p>Not to self-promote or anything, but I gave a talk on this subject at the last Cassandra Summit.  You may find the slides helpful: <a href=""http://www.slideshare.net/aploetz/escaping-disco-era-data-modeling"" rel=""nofollow"">Escaping Disco Era Data Modeling</a></p>

<p>Speaking of performance, using the <code>IN</code> keyword on a partition key has been proven to be just as bad as using a secondary index.  You'll get much better performance with 3 parallel queries, as opposed to this: <code>event_type in ('lead', 'view', 'sales')</code>.</p>

<p>Additionally, your last query is using <code>ALLOW FILTERING</code> which is something you should never do on a production system, because it will result in a scan of your entire table, and several of your nodes.</p>

<p>For ideal performance, it is best to ensure that your queries target a specific data partition.  This way, you will only hit a single node, and not introduce extraneous network traffic into the equation.</p>
",['table']
35884060,35887194,2016-03-09 05:59:03,How to set auto increament ID(integer) in cassandra like SQL?,"<p>I have millions of records and now I have to fetch the last latest records. I want to save each record with record_id, and I want this record_id to auto increment when a new record gets inserted.</p>

<p>FOR EXAMPLE: Suppose I have 1000 record and first I want the latest 100 records from 901 to 1000.  Now on the second request, I want the next 100 latest record from 801 to 900. I have gone through so many links but have not found anything relevant.  Can any body give a proper solution?</p>

<p>Let's suppose table emp contains:<br>
name text,<br>
record_id int,<br> 
address text<br></p>

<p>Let's assume name is primary key and record_id is cluster key.  But please don't discuss primary key concept now because my requirement is to create id as a cluster key (I'm using cassandra 2.2.3 and cql 3.3.1).  </p>
",<cassandra>,"<p>Auto-increment IDs don't really work in Cassandra or any other distributed database.</p>

<p>Why?  Let's say that you have three nodes.  Two nodes get write requests to the same table at the same time.  One checks the table for the max ID, and gets a (example) response of 2544.  Before that new row can be written, the other node does the same process, and also gets 2544.  Now you have two rows being inserted with 2545, and in Cassandra, the last write ""wins"" so you'll lose the first write.</p>

<p>Consequently, this is also why read-before-write approaches are considered anti-patterns in Cassandra.  As Stefan suggested, a TimeUUID offers a way around this problem.</p>

<p>In Cassandra you need to design your tables to fit your query patterns.  What I'm hearing, is that you want to retrieve the last 100 updated employees.  I would create a specific table to serve that:</p>

<pre><code>CREATE TABLE employee_updates (
  datebucket text,
  record_id timeuuid,
  name text,
  address text,
  PRIMARY KEY (datebucket,record_id))
WITH CLUSTERING ORDER BY (record_id DESC);
</code></pre>

<p>Now when you query this table for the last 100 records:</p>

<pre><code>SELECT * FROM employee_udpates WHERE datebucket='20160309' LIMIT 100;
</code></pre>

<p>You can get the most-recent 100 records for that particular day.</p>

<p>Note: If ""day"" is too granular for your solution (only a few employee records get updated each day) then feel free to widen that to something more applicable.</p>

<p><strong>UPDATE:</strong></p>

<blockquote>
  <p>what if i want to previous latest 100 record that is 801 to 900</p>
</blockquote>

<p>This solution actually does have a way to ""page"" through the results.</p>

<p>Let's insert some rows into your table:</p>

<pre><code>&gt; INSERT INTO employee_updates (datebucket, record_id , address , name ) VALUES ('20160309',now(),'123 main st.','Bob Kerman');
&gt; INSERT INTO employee_updates (datebucket, record_id , address , name ) VALUES ('20160309',now(),'456 Gene ave.','Bill Kerman');
&gt; INSERT INTO employee_updates (datebucket, record_id , address , name ) VALUES ('20160309',now(),'34534 Water st.','Jebediah Kerman');
&gt; INSERT INTO employee_updates (datebucket, record_id , address , name ) VALUES ('20160309',now(),'843 Rocket dr.','Valentina Kerman');
&gt; INSERT INTO employee_updates (datebucket, record_id , address , name ) VALUES ('20160309',now(),'33476 Booster way','Isabella Kerman');
&gt; INSERT INTO employee_updates (datebucket, record_id , address , name ) VALUES ('20160309',now(),'43 Solid Rocket pl.','Helcine Kerman');
</code></pre>

<p>Now let me SELECT the top 3 most-recent for today:</p>

<pre><code>&gt; SELECT datebucket, record_id, dateof(record_id), name 
  FROm employee_updates WHERE datebucket='20160309' LIMIT 3;

 datebucket | record_id                            | system.dateof(record_id) | name
------------+--------------------------------------+--------------------------+------------------
   20160309 | 511f9150-e5db-11e5-a4ba-a52893cc9f36 | 2016-03-09 09:43:02+0000 |   Helcine Kerman
   20160309 | 2f9f3670-e5db-11e5-a4ba-a52893cc9f36 | 2016-03-09 09:42:06+0000 |  Isabella Kerman
   20160309 | 23b0dc60-e5db-11e5-a4ba-a52893cc9f36 | 2016-03-09 09:41:46+0000 | Valentina Kerman

(3 rows)
</code></pre>

<p>As I have clustered this table on <code>record_id</code> in DESCending order, I can get the next 3 records simply by querying for a <code>record_id</code> less than the last one I read.  In this case, that'd be <code>23b0dc60-e5db-11e5-a4ba-a52893cc9f36</code>:</p>

<pre><code>&gt; SELECT datebucket, record_id, dateof(record_id), name 
  FROm employee_updates WHERE datebucket='20160309' 
  AND record_id &lt; 23b0dc60-e5db-11e5-a4ba-a52893cc9f36 LIMIT 3;

 datebucket | record_id                            | system.dateof(record_id) | name
------------+--------------------------------------+--------------------------+-----------------
   20160309 | 16400100-e5db-11e5-a4ba-a52893cc9f36 | 2016-03-09 09:41:23+0000 | Jebediah Kerman
   20160309 | 0b239cf0-e5db-11e5-a4ba-a52893cc9f36 | 2016-03-09 09:41:05+0000 |     Bill Kerman
   20160309 | 00d648b0-e5db-11e5-a4ba-a52893cc9f36 | 2016-03-09 09:40:47+0000 |      Bob Kerman

(3 rows)
</code></pre>
",['table']
35889750,35905025,2016-03-09 11:03:01,Cassandra replication factor greater than number of nodes,"<p><br />
I am using the datastax java driver for Apache Cassandra (v. 2.1.9) and I am wondering what should happen when I set replication_factor greater than number of nodes. I've read somewhere that Cassandra allows for this operation, but should fail when I will try to save some data (of course it depends on the write consistency level, but I mean the case of ALL).<br />
The problem is that everything works, no exception is being thrown, even if I try to save data. Why?<br />
Maybe the pieces of information which I've read were old, for older versions of Cassandra?
One more question, whether it's true, than what would happen when I add another node to the cluster?</p>
",<java><cassandra><datastax-java-driver><cassandra-2.1>,"<p>Cassandra has a concept of ""tunable consistency"" which in part means you can control the consistency level setting for read/write operations.</p>

<p>You can read a bit more in the docs explaining <a href=""https://docs.datastax.com/en/cassandra/3.x/cassandra/dml/dmlConfigConsistency.html"" rel=""noreferrer"">consistency levels</a> and how to set them in the <a href=""https://docs.datastax.com/en/cql/3.3/cql/cql_reference/consistency_r.html"" rel=""noreferrer"">cqlsh shell</a>.</p>

<p>To learn more I suggest experimenting with the cqlsh on a single-node of Cassandra. For example we can create a keyspace with replication factor of 2 and load some data into it:</p>

<pre><code>cqlsh&gt; create keyspace test with replication = {'class': 'SimpleStrategy', 'replication_factor':2};
cqlsh&gt; create table test.keys (key int primary key, val int);
cqlsh&gt; insert into test.keys (key, val) values (1, 1);
cqlsh&gt; select * from test.keys;

 key | val
-----+-----
   1 |   1 
</code></pre>

<p>Everything works fine because the default consistency level is ONE, so only 1 node had to be online. Now try the same but setting it to ALL:</p>

<pre><code>cqlsh&gt; CONSISTENCY ALL;
Consistency level set to ALL.
cqlsh&gt; insert into test.keys (key, val) values (2, 2);
Traceback (most recent call last):
  File ""resources/cassandra/bin/cqlsh.py"", line 1324, in perform_simple_statement
    result = future.result()
  File ""resources/cassandra/bin/../lib/cassandra-driver.zip/cassandra-driver/cassandra/cluster.py"", line 3133, in result
    raise self._final_exception
Unavailable: code=1000 [Unavailable exception] message=""Cannot achieve consistency level ALL"" info={'required_replicas': 2, 'alive_replicas': 1, 'consistency': 'ALL'}

cqlsh&gt; select * from test.keys;
Traceback (most recent call last):
  File ""resources/cassandra/bin/cqlsh.py"", line 1324, in perform_simple_statement
    result = future.result()
  File ""resources/cassandra/bin/../lib/cassandra-driver.zip/cassandra-driver/cassandra/cluster.py"", line 3133, in result
    raise self._final_exception
Unavailable: code=1000 [Unavailable exception] message=""Cannot achieve consistency level ALL"" info={'required_replicas': 2, 'alive_replicas': 1, 'consistency': 'ALL'}
</code></pre>

<p>Neither reads nor writes will work because the 2nd node doesn't exist. In fact the error message will give a helpful clue that two replicas were needed but only one was available.</p>

<p>Once you have an understanding using cqlsh, you can apply the same using the Java drivers, depending on what your application needs.</p>
",['table']
35945636,40871211,2016-03-11 16:47:44,cassandra-cli 'list' in cassandra 3.0,"<p>I want to view the ""rowkey"" with its stored data in cassandra 3.0. I know, the depreciated cassandra-cli had the 'list'-command. However, in cassandra 3.0, I cannot find the replacement for the 'list'-command. Anyone knows the new cli-command for 'list'?</p>
",<cassandra><cassandra-cli>,"<p>You can use <code>sstabledump</code> utility as @chris-lohfink suggested. How to use it? Create keyspace, table in it populate some data:</p>

<pre><code>cqlsh&gt; CREATE KEYSPACE IF NOT EXISTS minetest WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };

cqlsh&gt; CREATE TABLE object_coordinates (
   ... object_id int PRIMARY KEY,
   ... coordinate text
   ... );

cqlsh&gt; use minetest;

cqlsh:minetest&gt; insert into object_coordinates (object_id, coordinate) values (564682,'59.8505,34.0035');
cqlsh:minetest&gt; insert into object_coordinates (object_id, coordinate) values (1235,'61.7814,40.3316');
cqlsh:minetest&gt; select object_id, coordinate, writetime(coordinate) from object_coordinates;

 object_id | coordinate      | writetime(coordinate)
-----------+-----------------+-----------------------
      1235 | 61.7814,40.3316 |      1480436931275615
    564682 | 59.8505,34.0035 |      1480436927707627

(2 rows)
</code></pre>

<p><code>object_id</code> is a primary (partition key) key, <code>coordinate</code> is clustering one.</p>

<p>Flush changes to disk:</p>

<pre><code># nodetool flush
</code></pre>

<p>Find sstable on disk and analyze it:</p>

<pre><code># cd /var/lib/cassandra/data/minetest/object_coordinates-e19d4c40b65011e68563f1a7ec2d3d77

# ls
backups  mc-1-big-CompressionInfo.db  mc-1-big-Data.db  mc-1-big-Digest.crc32  mc-1-big-Filter.db  mc-1-big-Index.db  mc-1-big-Statistics.db  mc-1-big-Summary.db  mc-1-big-TOC.txt

# sstabledump mc-1-big-Data.db
[
  {
    ""partition"" : {
      ""key"" : [ ""1235"" ],
      ""position"" : 0
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 18,
        ""liveness_info"" : { ""tstamp"" : ""2016-11-29T16:28:51.275615Z"" },
        ""cells"" : [
          { ""name"" : ""coordinate"", ""value"" : ""61.7814,40.3316"" }
        ]
      }
    ]
  },
  {
    ""partition"" : {
      ""key"" : [ ""564682"" ],
      ""position"" : 43
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 61,
        ""liveness_info"" : { ""tstamp"" : ""2016-11-29T16:28:47.707627Z"" },
        ""cells"" : [
          { ""name"" : ""coordinate"", ""value"" : ""59.8505,34.0035"" }
        ]
      }
    ]
  }
]
</code></pre>

<p>Or with <code>-d</code> flag:</p>

<pre><code># sstabledump mc-1-big-Data.db -d
[1235]@0 Row[info=[ts=1480436931275615] ]:  | [coordinate=61.7814,40.3316 ts=1480436931275615]
[564682]@43 Row[info=[ts=1480436927707627] ]:  | [coordinate=59.8505,34.0035 ts=1480436927707627
</code></pre>

<p>Output says that <code>1235</code> and <code>564682</code> and saves coordinates in those partitions.</p>

<p>Link to doc <a href=""http://www.datastax.com/dev/blog/debugging-sstables-in-3-0-with-sstabledump"" rel=""nofollow noreferrer"">http://www.datastax.com/dev/blog/debugging-sstables-in-3-0-with-sstabledump</a></p>

<p>PS. <code>sstabledump</code> is provided by <code>cassandra-tools</code> package in ubuntu.</p>
",['table']
35971955,35974589,2016-03-13 15:14:30,Query spark on JSON object stored on Cassandra DB,"<p>I built the structure on cassandra DB to store the time series data of the OS data like services, process and other information. To understand how to works Cassandra about storing JSON data and retrieval the data by CQL queries with condition I prefered to simplify the model. Because in the total model DB I'll have the <em>TYPE</em> more complex than report_object like hashMap of array of hashMap for example:
<em>Type</em> <code>NETSTAT--&gt; Object[n] --&gt; {host:192.168.0.23, protocol: TCP ,LocalAddress : 0.0.0.0}</code>
so the Type NETSTAT will have a list of hashMaps that will contain the fields key -> value. 
For simplify I have choosen to show the following schema:</p>

<pre><code>CREATE TYPE report_object (RTIME varchar, RMINORVER int, RUSER varchar, RLANG varchar, RSCRIPT varchar, RMAJORVER int, RHOST varchar, RPATH varchar);
CREATE TABLE test (
REPORTUUID uuid PRIMARY KEY,
report frozen&lt;report_object&gt;);
</code></pre>

<p>Inside the table I injectioned the JSON data with the followed query inside java class:</p>

<pre><code>INSERT INTO test JSON '{""REPORTUUID"": ""9fb21fb9-333e-4017-ab77-0fa6ee1e20e3"" ,""REPORT"":{""RTIME"":""6/MAR/2016 6:0:0 PM"",""RMINORVER"":0,""RUSER"":""Administrator"",""RLANG"":""vbs"",""RSCRIPT"":""Main"",""RMAJORVER"":5,""RHOST"":""WIN-SAPV9MUEMNS"",""RPATH"":""C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\IXP000.TMP""}}';
</code></pre>

<p>I inectioned other data with the query above.
The questions to clarify my concepts are:
- I would like to do the queries with conditions that check inside TYPE defined, is it possible with CQL or is necessary to use spark SQL? </p>

<ul>
<li>Is design DB model right for the purpose (Because I have passed from RDBMS to DB NoSQL) ?</li>
</ul>
",<json><apache-spark><cassandra><time-series>,"<p>To be able to query User Defined Type using Cassandra you'll have to create an index first:</p>

<pre><code>CREATE INDEX on test.test(report);
</code></pre>

<p>but it allows only a predicate based on a full document:</p>

<pre><code>SELECT * FROM test
WHERE report=fromJson('{""RTIME"":""6/MAR/2016 6:0:0 PM"",""RMINORVER"":0,""RUSER"":""Administrator"",""RLANG"":""vbs"",""RSCRIPT"":""Main"",""RMAJORVER"":5,""RHOST"":""WIN-SAPV9MUEMNS"",""RPATH"":""C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\IXP000.TMP""}');
</code></pre>

<p>You'll find more details and explanation in <a href=""https://stackoverflow.com/q/33840105/1560062"">how to filter cassandra query by a field in user defined type</a></p>

<p>When exposed using Spark these values can be filtered using filter on <code>CassandraTableScanRDD</code>:</p>

<pre><code>val rdd = sc.cassandraTable(""test"", ""test"")
rdd.filter(row =&gt;
  row.getUDTValue(""report"").getString(""rscript"") == ""Main"")
</code></pre>

<p>or <code>where</code> / <code>filter</code> on a <code>DataFrame</code>:</p>

<pre><code>df.where($""report.rscript"" === ""Main"")
</code></pre>

<p>Although query like this using Spark a whole table has to be fetched before data can be filtered. While it is not clear what exactly you are trying to achieve but it is rather unlikely this will be an useful structure in general.</p>
",['table']
35992290,35993699,2016-03-14 15:58:49,How do I add two column values in a table with CQL?,"<p>I am needing to add two values together to create a third value with CQL. Is there any way to do this? My table has the columns <code>number_of_x</code> and <code>number_of_y</code> and I am trying to create <code>total</code>. I did an update on the table with a set command as follows:</p>

<pre><code>UPDATE my_table
SET total = number_of_x + number_of_y ;
</code></pre>

<p>When I run that I get the message back saying:</p>

<pre><code>no viable alternative at input ';'.
</code></pre>
",<cassandra><cql>,"<p>Per <a href=""https://docs.datastax.com/en/cql/3.3/cql/cql_reference/update_r.html"" rel=""nofollow"">docs</a>. assignment is one of:</p>

<pre><code>column_name = value
set_or_list_item = set_or_list_item ( + | - ) ...
map_name = map_name ( + | - ) ...
map_name = map_name ( + | - ) { map_key : map_value, ... } 
column_name [ term ] = value
counter_column_name = counter_column_name ( + | - ) integer
</code></pre>

<p>And you cannot mix counter and non counter columns in the same table so what you are describing is impossible in a single statement. But you can do a read before write:</p>

<pre><code>CREATE TABLE my_table ( total int, x int, y int, key text PRIMARY KEY )
INSERT INTO my_table (key, x, y) VALUES ('CUST_1', 1, 1);
SELECT * FROM my_table WHERE key = 'CUST_1';

 key    | total | x | y
--------+-------+---+---
 CUST_1 |  null | 1 | 1

UPDATE my_table SET total = 2 WHERE key = 'CUST_1' IF x = 1 AND y = 1;

 [applied]
-----------
      True

SELECT * FROM my_table WHERE key = 'CUST_1';

 key    | total | x | y
--------+-------+---+---
 CUST_1 |     2 | 1 | 1
</code></pre>

<p>The <code>IF</code> clause will handle concurrency issues if x or y was updated since the <code>SELECT</code>. You can than retry again if <code>applied</code> is <code>False</code>.</p>

<p>My recommendation however in this scenario is for your application to just read both <code>x</code> and <code>y</code>, then do addition locally as it will perform MUCH better.</p>

<p>If you really want C* to do the addition for you, there is a <a href=""http://docs.datastax.com/en/cql/3.3/cql/cql_using/useQueryStdAggregate.html"" rel=""nofollow"">sum aggregate function</a> in 2.2+ but it will require updating your schema a little:</p>

<pre><code>CREATE TABLE table_for_aggregate (key text, type text, value int, PRIMARY KEY (key, type));

INSERT INTO table_for_aggregate (key, type, value) VALUES ('CUST_1', 'X', 1);
INSERT INTO table_for_aggregate (key, type, value) VALUES ('CUST_1', 'Y', 1);

SELECT sum(value) from table_for_aggregate WHERE key = 'CUST_1';

 system.sum(value)
-------------------
                 2
</code></pre>
",['table']
36012240,36013384,2016-03-15 13:08:24,ControlConnection warning in datastax java driver,"<p>I am using cassandra 2.0.9 and datastax java driver 2.0.5 to query.
I had set rpc_address as 0.0.0.0 in cassandra. Sometimes I am getting this warning message from the client </p>

<pre><code>4411 [Cassandra Java Driver worker-1] WARN com.datastax.driver.core.ControlConnection - Found host with 0.0.0.0 as rpc_address, using listen_address (/192.168.100.175) to contact it instead. If this is incorrect you should avoid the use of 0.0.0.0 server side.
</code></pre>

<p>I cannot find why this warning occured sometimes only. How can I solve this?</p>
",<cassandra><cassandra-2.0><datastax-java-driver>,"<p>The driver uses the <code>rpc_address</code> set in the <code>system.peers</code> table to find the address to connect to. When you configure the rpc address to <code>0.0.0.0</code> thats what gets put in the system table so the driver cant know for sure what IP to connect to.</p>

<p>If possible you should just set it to its actual IP <code>192.168.100.175</code>, but if thats incorrect you may just want to add the actual address of more of your nodes to the host list the driver initially is provided.</p>
",['table']
36044078,36044943,2016-03-16 18:30:02,What are the pitfalls of Cassandra materialised views and IN queries?,"<p>Let's say I have a table</p>

<pre><code>CREATE TABLE events (         
  stream_id text,         
  type text,         
  data text,         
  timestamp timestamp,         
  PRIMARY KEY (stream_id, timestamp)
);
</code></pre>

<p>The request pattern is that I need to get all events by <code>stream_id</code>.<br>
e.g. <code>SELECT * FROM events WHERE stream_id = 'A-1';</code></p>

<p>Then I need to get all events given a set of <code>type</code>s. So I create a MV:</p>

<pre><code>CREATE MATERIALIZED VIEW events_by_type AS
  SELECT * FROM events
  WHERE type IS NOT NULL AND
  timestamp IS NOT NULL 
  PRIMARY KEY (type, stream_id, timestamp)
  WITH CLUSTERING ORDER BY (timestamp DESC);
</code></pre>

<p>The request is like<br>
<code>SELECT * FROM events_by_type WHERE type IN ('T1', 'T2);</code></p>

<p>What are the pitfalls with this query patterns and data model?
If any, is it possible to improve it?</p>
",<events><cassandra><data-modeling><materialized-views>,"<p>Only pitfall I can think of that you may hit is the consistency with the view is not reflected in the consistency level of the write to the base table. So if you need stronger consistency (quorum on reads &amp; writes) you may run into issues.</p>

<p>One concern is that your partitions are unbounded. On current versions if your building larger than 100mb or so partitions you can start having misc performance issues (works, but will sometimes require GC tuning to keep things moving). This is improving recently but you should break up your partitions some. i.e.</p>

<pre><code>CREATE TABLE events (         
  stream_id text,
  time_bucket text,
  type text,         
  data text,         
  timestamp timestamp,         
  PRIMARY KEY ((stream_id, time_bucket), timestamp)
);

CREATE MATERIALIZED VIEW events_by_type AS
  SELECT * FROM events WHERE
    type IS NOT NULL AND
    time_bucket IS NOT NULL AND
    timestamp IS NOT NULL 
PRIMARY KEY ((type, time_bucket), stream_id, timestamp)
WITH CLUSTERING ORDER BY (timestamp DESC);
</code></pre>

<p>It adds a little complexity in your <code>time_bucket</code> needs to be known. You can either predefine the buckets to something like daily (ie <code>2016-10-10 00:00:00</code>) or create a new table that maps the possible time_buckets for a type or stream_id.</p>
",['table']
36048660,36050458,2016-03-16 23:06:23,Cassandra partition key for time series data,"<p>I'm testing Cassandra as time series database.</p>

<p>I create data model as below:</p>

<pre><code>CREATE KEYSPACE sm WITH replication = {
  'class': 'SimpleStrategy',
  'replication_factor': 1
};

USE sm;

CREATE TABLE newdata (timestamp timestamp,
  deviceid int, tagid int,
  decvalue decimal,
  alphavalue text,
  PRIMARY KEY (deviceid,tagid,timestamp));
</code></pre>

<p>In the Primary key, I set deviceid as the partition key which mean all data with same device id will write into one node (does it mean one machine or one partition. Each partition can have max 2 billion rows) also if I query data within the same node, the retrieval will be fast, am I correct?  I’m new to Cassandra and a bit confused about the partition key and clustering key.</p>

<p>Most of my query will be as below:</p>

<ul>
<li>select lastest timestamp of know deviceid and tagid</li>
<li>Select decvalue of known deviceid and tagid and timestamp</li>
<li>Select alphavalue of known deviceid and tagid and timestamp</li>
<li>select * of know deviceid and tagid with time range </li>
<li>select * of known deviceid with time range</li>
</ul>

<p>I will have around 2000 deviceid, each deviceid will have 60 tagid/value pair. I'm not sure if it will be a wide rows of deviceid, timestamp, tagid/value, tagid/value....  </p>
",<php><cassandra><cql><cqlsh>,"<blockquote>
  <p>I’m new to Cassandra and a bit confused about the partition key and clustering key.</p>
</blockquote>

<p>It sounds like you understand partition keys, so I'll just add that your partition key helps Cassandra figure out where (which token range) in the cluster to store your data.  Each node is responsible for several primary token ranges (assuming vnodes).  When your data is written to a data partition, it is sorted by your clustering keys.  This is also how it is stored on-disk, so remember that your clustering keys determine the order in which your data is stored on disk.</p>

<blockquote>
  <p>Each partition can have max 2 billion rows</p>
</blockquote>

<p>That's not exactly true.  Each partition can support up to 2 billion <em>cells</em>.  A cell is essentially a column name/value pair.  And your clustering keys add up to a single cell by themselves.  So compute your cells by counting the column values that you store for each CQL row, and add one more if you use clustering columns.</p>

<p>Depending on your wide row structure you will probably have a limitation of far fewer than 2 billion rows.  Additionally, that's just the storage limitation.  Even if you managed to store 1 million CQL rows in a single partition, querying that partition would return so much data that it would be ungainly and probably time-out.</p>

<blockquote>
  <p>if I query data within the same node, the retrieval will be fast, am I correct?</p>
</blockquote>

<p>It'll at least be faster than multi-key queries that hit multiple nodes.  But whether or not it will be ""fast"" depends on other things, like how wide your rows are, and how often you do things like deletes and in-place updates.</p>

<blockquote>
  <p>Most of my query will be as below:</p>

<pre><code>select lastest timestamp of know deviceid and tagid
Select decvalue of known deviceid and tagid and timestamp
Select alphavalue of known deviceid and tagid and timestamp
select * of know deviceid and tagid with time range
select * of known deviceid with time range
</code></pre>
</blockquote>

<p>Your current data model can support all of those queries, except for the last one.  In order to perform a range query on <code>timestamp</code>, you'll need to duplicate your data into a new table, and build a PRIMARY KEY to support that query pattern.  This is called ""query-based modeling.""  I would build a query table like this:</p>

<pre><code>CREATE TABLE newdata_by_deviceid_and_time (
  timestamp timestamp,
  deviceid int,
  tagid int,
  decvalue decimal,
  alphavalue text,
  PRIMARY KEY (deviceid,timestamp));
</code></pre>

<p>That table can support a range query on <code>timestamp</code>, while partitioning on <code>deviceid</code>.</p>

<p>But the biggest problem I see with either of these models, is that of ""unbounded row growth.""  Basically, as you collect more and more values for your devices, you will approach the 2 billion cell limit per partition (and again, things will probably get slow way before that).  What you need to do, is use a modeling technique called ""time bucketing.""</p>

<p>For the example, I'll say that I determined that bucketing by month would keep me well under the 2 billion cells limit <em>and</em> allow for the type of date range flexibility that I needed.  If so, I would add an additional partition key <code>monthbucket</code> and my (new) table would look like this:</p>

<pre><code>CREATE TABLE newdata_by_deviceid_and_time (
  timestamp timestamp,
  deviceid int,
  tagid int,
  decvalue decimal,
  alphavalue text,
  monthbucket text,
  PRIMARY KEY ((deviceid,monthbucket),timestamp));
</code></pre>

<p>Now when I wanted to query for data in a specific device and date range, I would also specify the <code>monthbucket</code>:</p>

<pre><code>SELECT * FROM newdata_by_deviceid_and_time
WHERE deviceid='AA23' AND monthbucket='201603'
AND timestamp &gt;= '2016-03-01 00:00:00-0500'
AND timestamp &lt; '2016-03-16 00:00:00-0500';
</code></pre>

<p>Remember, <code>monthbucket</code> is just an example.  For you, it may make more sense to use quarter or even year (assuming that you don't store too many values per <code>deviceid</code> in a year).</p>
",['table']
36079921,36090155,2016-03-18 08:51:42,Is Cassandra not suitable for the task or I'm misunderstanding documentation?,"<p>I was planning to substitute Oracle SQL on this task:</p>

<p>-several billions rows (n * 1000 millions)</p>

<p>-100% operations are simple selects</p>

<p>but there are 10 different criteria for selecting data. And they are combined also. E.g. </p>

<pre><code>search1 - ""select ... where name = 'x' and birth = 'y'""
search2 - ""select ... where name = 'x' and phone = 'y'""
</code></pre>

<p>et c</p>

<p>Surprisingly, found that it is a huge pain to make such queries in Cassandra.
Especially, concerning 2 billion cell limit and that we do not plan huge cluster. At first it will work on 1 server, so this division to partitions will bring no profit, but will need time to overcome. </p>

<p>So, the questions are:</p>

<p>-will one (or maybe two, if Oracle also will be slow) server with Cassandra be dramatically faster than one RDBMS server?</p>

<p>-does right (not ugly) data model for my task exist or it is simply not for Cassandra?</p>

<p>-maybe other NOSQL database will suit much better?</p>
",<cassandra><nosql>,"<p>Cassandra CQL is not very good for doing table scans or ad hoc queries.</p>

<p>Cassandra CQL works best when you are doing transactional queries that target either one row or a set of clustered rows (within a partition).</p>

<p>So in your example, Cassandra would work if you wanted to partition the data by name, and then work on one name at a time. This would scale to billions of names by adding more nodes.</p>

<p>But then if you want to search all names for particular selection criteria, then it's a table scan and you'd have to pair Cassandra with something like spark to have that be efficient.</p>

<p>So for your use case you are probably better off with a relational database than Cassandra. If you wanted to use Cassandra you'd probably need to create several tables with duplicated data, where each table used a key designed for to work for one or two of your different query types.</p>
",['table']
36126512,36263000,2016-03-21 08:53:06,How to know no of rows Inserted using Spark In cassandra,"<p>I'm inserting into cassandra using Spark.</p>

<pre><code>CassandraJavaUtil.javaFunctions(newRDD)
            .writerBuilder(""dmp"", ""dmp_user_user_profile_spark1"", mapToRow(UserSetGet.class)).saveToCassandra();
            logger.info(""DataSaved"");
</code></pre>

<p>My question is if RDD has 5k rows, and while inserting into Cassandra for some reason the job fails.</p>

<p>Will there be rollback for the rows that were Inserted out of 5k </p>

<p>and if not , how will I know how many rows were actually inserted , Such that I can start my job again from the failed row.</p>
",<java><apache-spark><cassandra><spark-cassandra-connector>,"<p>Simple answer, No, there will not be automatic rollback.</p>

<p>Whatever data spark was able to save into cassandra, will be persisted into cassandra.</p>

<p>And no, there is no simple way to know till what dataset, spark job was able to save successfully. Infact, only way i can think of is, to read data from cassandra, join and filter out from your resultset, based on key.</p>

<p>To be honest, that seems quite and overhead if data is huge to make humongous join. In most cases, you can simply re-run the job on spark, and let it save to cassandra table again.
Since, in cassandra update and inserts work same way. It won't be a problem.</p>

<p>Only place this can be problematic is, if you are dealing with counter tables.</p>

<p>Update :
For this specific scenario, You can split your rdd into batches of your size, and then try to save them.
That way, if you fail on one rdd, you will know which rdd failed. If not that set, you should be able to pick up from next rdd for sure.</p>
",['table']
36133127,36139750,2016-03-21 14:03:17,How to configure cassandra for remote connection,"<p>I am trying to configure Cassandra Datastax Community Edition for remote connection on windows, </p>

<p>Cassandra Server is installed on a Windows 7 PC, <strong>With the local CQLSH it connects perfectly to the local server.</strong></p>

<p>But when i try to connect <strong>with CQLSH from another PC in the same Network, i get this error message:</strong> </p>

<blockquote>
  <p>Connection error: ('Unable to connect to any servers', {'MYHOST':
  error(10061, ""Tried connecting to [('HOST_IP', 9042)]. Last error: No
  connection could be made because the target machine actively refused
  it"")})</p>
</blockquote>

<p>So i am wondering how to configure correctly (what changes should i make on cassandra.yaml config file) the Cassandra server to allow  remote connections.</p>

<p>Thank you in advance!</p>
",<cassandra><cqlsh><cassandra-cli><nosql>,"<p>Remote access to Cassandra is via its thrift port for Cassandra 2.0. In Cassandra 2.0.x, the default cqlsh listen port is 9160 which is defined in cassandra.yaml by the rpc_port parameter. By default, Cassandra 2.0.x and earlier enables Thrift by configuring start_rpc to true in the cassandra.yaml file. </p>

<p>In Cassandra 2.1, the cqlsh utility uses the native protocol. In Cassandra 2.1, which uses the Datastax python driver, the default cqlsh listen port is 9042.</p>

<p>The cassandra node should be bound to the IP address of your server's network card - it shouldn't be 127.0.0.1 or localhost which is the loopback interface's IP, binding to this will prevent direct remote access. To configure the bound address, use the rpc_address parameter in cassandra.yaml. Setting this to 0.0.0.0 will listen on all network interfaces.</p>

<p>Have you checked that the remote machine can connect to the Cassandra node? Is there a firewall between the machines? You can try these steps to test this out:</p>

<p><strong>1) Ensure you can connect to that IP from the server you are on:</strong></p>

<p>$ ssh user@xxx.xxx.xx.xx</p>

<p><strong>2) Check the node's status and also confirm it shows the same IP:</strong></p>

<p>$nodetool status</p>

<p><strong>3) Run the command to connect with the IP (only specify the port if you are not using the default):</strong></p>

<p>$ cqlsh xxx.xxx.xx.xx</p>
",['rpc_address']
36166382,36166383,2016-03-22 22:23:48,How to expire each element of a collection by setting an individual time-to-live (TTL) property in Cassandra?,"<p>How to expire each element of a collection by setting an individual time-to-live (TTL) property in Cassandra?<br></p>

<p>the documentation is here, but I cannot find an example.
(<a href=""https://docs.datastax.com/en/cql/3.3/cql/cql_using/useExpire.html"" rel=""nofollow"">https://docs.datastax.com/en/cql/3.3/cql/cql_using/useExpire.html</a>)</p>
",<cassandra><ttl>,"<p><br>
If you want to have different TTL in a same column collection (set, list, map) of cassandra.<br><br>
Do like in this example:<br><br>
There is a table -> tableName<br>
whih one column (col1) primary key of text type<br>
A column(col2) of type set &lt;long><br></p>

<pre><code>UPDATE tableName USING TTL 30 SET col2=col2+{11} WHERE col1=-10;
UPDATE tableName USING TTL 88 SET col2=col2+{22} WHERE col1=-10;
</code></pre>

<p>In the example I am upserting to values to the set, {11} with TTL=30 and {22} with TTL=88.<br>
When one element exeeds the TTL it is automaticaly deleted.<br>
When all the elements in the set exceeds the TTL and the set is empty, the row is also deleted.</p>
",['table']
36181112,36181836,2016-03-23 14:35:50,Describe Cassandra ColumnFamily,"<p>I am connecting to a Cassandra database and am trying to find out what attributes an entry in a column family has. I am unable to use the cqlsh command line utility. After creating the connection I have:</p>

<pre><code>myCF = ColumnFamily(myConncetion, ""user"")
for u in myCF.get_range():
    u_item = u[1]
    name = u_item.get('FIRST_NAME')
    print name
</code></pre>

<p>This prints the names of all users. I would like to know what other attributes besides ""FIRST_NAME"" a u_item has. Is there a built in function that can describe its attributes? Thanks in advance.</p>
",<python><cassandra>,"<p>Which version of Cassandra are you running?  If you wanted to go through the table programmatically, you could try querying the schema from the system tables:</p>

<p>If you're on Cassandra >= 3.x+:</p>

<pre><code>SELECT column_name FROM system_schema.columns 
    WHERE keyspace_name='yourkeyspacename' AND table_name='yourtablename';
</code></pre>

<p>If you're on Cassandra &lt;= 2.2.x;</p>

<pre><code>SELECT column_name FROM system.schema_columns 
    WHERE keyspace_name='yourkeyspacename' AND columnfamily_name ='yourtablename';
</code></pre>

<blockquote>
  <p>How can I execute this Select statement from a python script?</p>
</blockquote>

<p>If I'm running on Cassandra 2.2.5, I can take the above code and make it work in a Python script (<code>cassConnect.py</code>) like this:</p>

<pre><code>from cassandra.cluster import Cluster
from cassandra.auth import PlainTextAuthProvider

import sys

hostname=sys.argv[1]
username=sys.argv[2]
password=sys.argv[3]
keyspace=sys.argv[4]
table=sys.argv[5]

nodes = []
nodes.append(hostname)

auth_provider = PlainTextAuthProvider(username=username, password=password)
cluster = Cluster(nodes,auth_provider=auth_provider)
session = cluster.connect(keyspace)

pStatement = session.prepare(""""""
    SELECT column_name FROM system.schema_columns WHERE keyspace_name=? AND columnfamily_name=?;
"""""")

rows = session.execute(pStatement,[keyspace,table])
for row in rows:
    print row[0]

$ python cassConnect.py 127.0.0.1 aploetz bacon stackoverflow user
email
first
last
username
</code></pre>
",['table']
36181280,36203619,2016-03-23 14:43:34,Is this type of counter table definition valid?,"<p>I want to create a table with wide partitions (or, put another way, a table which has no value columns (non primary key columns)) that enables the number of rows in any of its partitions to be efficiently procured. Here is a simple definition of such a table</p>

<pre><code>CREATE TABLE IF NOT EXISTS test_table
(
    partitionKeyCol         timestamp
    clusteringCol           timeuuid
    partitionRowCountCol    counter    static
    PRIMARY KEY             (partitionKeyCol, clusteringCol)
)
</code></pre>

<p>The problem with this definition, and others structured like it, is that their validity cannot be clearly deduced from the information contained in the docs.</p>

<p><strong>What the docs do state</strong> (with regards to counters):</p>

<ul>
<li><p>A counter column can neither be specified as part of a table's <code>PRIMARY KEY</code>, nor used to create an <code>INDEX</code></p></li>
<li><p>A counter column can only be defined in a <em>dedicated counter table</em> (which I take to be a table which solely has counter columns defined as its value columns)</p></li>
</ul>

<p><strong>What the docs do not state</strong> (with regards to counters):</p>

<ul>
<li><p>The ability of a table to have a static counter column defined for it (given the unique write path of counters, I feel that this is worth mentioning)</p></li>
<li><p>The ability of a table, which has zero value columns defined for it (making it a <em>dedicated counter table</em>, given my understanding of the term), to also have a static counter column defined for it</p></li>
</ul>

<p>Given the information on this subject that is present in (and absent from) the docs, such a definition appears to be valid. However, I'm not sure how that is possible,  given that the updates to <code>partitionRowCountCol</code> would require use of a write path different from that used to insert <code>(partitionKeyCol, clusteringCol)</code> tuples.</p>

<p>Is this type of counter table definition valid? If so, how are writes to the table carried out?</p>
",<cassandra><cql><cql3>,"<p>It looks like a table with this structure can be defined, but I'm struggling to find a good use case for it. It seems there is no way to actually write to that clustering column.</p>

<pre><code>CREATE TABLE test.test_table (
    a timestamp,
    b timeuuid,
    c counter static,
    PRIMARY KEY (a, b)
);

cassandra@cqlsh:test&gt; insert into test_table (a,b,c) VALUES (unixtimestampof(now()), now(), 3);
InvalidRequest: code=2200 [Invalid query] message=""INSERT statements are not allowed on counter tables, use UPDATE instead""
cassandra@cqlsh:test&gt; update test_table set c = c + 1 where a=unixtimestampof(now());
cassandra@cqlsh:test&gt; update test_table set c = c + 1 where a=unixtimestampof(now());
cassandra@cqlsh:test&gt; select * from test_table;

 a                        | b    | c
--------------------------+------+---
 2016-03-24 15:04:31+0000 | null | 1
 2016-03-24 15:04:37+0000 | null | 1

(2 rows)
cassandra@cqlsh:test&gt; update test_table set c = c + 1 where a=unixtimestampof(now()) and b=now();
InvalidRequest: code=2200 [Invalid query] message=""Invalid restrictions on clustering columns since the UPDATE statement modifies only static columns""
cassandra@cqlsh:test&gt; insert into test_table (a,b) VALUES (unixtimestampof(now()), now());
InvalidRequest: code=2200 [Invalid query] message=""INSERT statements are not allowed on counter tables, use UPDATE instead""
cassandra@cqlsh:test&gt; update test_table set b = now(), c = c + 1 where a=unixtimestampof(now());
InvalidRequest: code=2200 [Invalid query] message=""PRIMARY KEY part b found in SET part""
</code></pre>

<p>What is it you're trying to model?</p>
",['table']
36210321,36210877,2016-03-24 21:30:24,Comparing Cassandra structure with Relational Databases,"<p>A few days ago I read about wide-column stored type of NoSQL and
exclusively Apache-Cassandra.</p>
<p>What I understand is that  Cassandra consist of:</p>
<p>A keyspace(like database in relational databases) and supporting many column families or tables (Same as table in relational databases) and unlimited rows.</p>
<p>From Stackoverflow tags:</p>
<blockquote>
<p>A wide column store is a type of key-value database. It uses tables, rows, and columns, but unlike a relational database, the names and format of the columns can vary from row to row in the same table.</p>
</blockquote>
<p>In Cassandra all of the rows (in a table) should have a row key then each row key can have multiple columns.
I read about differences in implementation and storing data of Relational database and NoSQL (Cassandra).</p>
<p>But I don't understand the difference between structure:</p>
<p>Imagine a scenario which I have a table (or column family in Cassandra):</p>
<p>When I execute a query (CQL) like this :</p>
<pre><code>select * from users;
</code></pre>
<p>It gives me the result as you can see :</p>
<pre><code>lastname  | age  | city          | email               
----------+------+---------------+----------------------
      Doe |   36 | Beverly Hills | janedoe@email.com       
    Jones |   35 |        Austin | bob@example.com        
    Byrne |   24 |     San Diego | robbyrne@email.com         
    Smith |   46 |    Sacramento | null                    
   Jones2 | null |        Austin | bob@example.com       
</code></pre>
<p>So I perform the above scenario in relational database (MS SQL) with the following query:</p>
<pre><code>select * from [users] 
</code></pre>
<p>And the result is:</p>
<pre><code>lastname  | age  | city          | email               
----------+------+---------------+----------------------
      Doe |   36 | Beverly Hills | janedoe@email.com       
    Jones |   35 |        Austin | bob@example.com        
    Byrne |   24 |     San Diego | robbyrne@email.com         
    Smith |   46 |    Sacramento | NULL                    
   Jones2 | NULL |        Austin | bob@example.com       
</code></pre>
<p>I know that Cassandra supports dynamic column and I can perform this by using sth like:</p>
<pre><code>ALTER TABLE users ADD website varchar;
</code></pre>
<p>But it is available in relational model for example in mssql the above code can be implemented too. Something like:</p>
<pre><code>ALTER TABLE users ADD website varchar(MAX);
</code></pre>
<p>What I see is that the first select and second select result is the same.
In Cassandra , they just give a row key (lastname) as a standalone object but it is same as a unique field (like ID or a text) in mssql (and all relational databases) and I see the type of column in Cassandra is static (in my example <code>varchar</code>) unlike what it describes in Stackoverflow tag.</p>
<p>So my questions is:</p>
<ol>
<li><p>Is there any misunderstanding in my imagination about Cassandra?!</p>
</li>
<li><p>So what is different between two structure ?! I show you the result is same.</p>
</li>
<li><p>Is there any special scenarios (JSON like) that cannot be implemented in relational databases but Cassandra supports? (For example I know that nested column doesn't support in Cassandra.)</p>
</li>
</ol>
<p>Thank you for reading.</p>
",<cassandra><wide-column-store>,"<p>We have to look at more complex example to see the differences :)</p>

<p>For start:</p>

<ul>
<li>column family term was used in older Thrift API   </li>
<li>in newer CQL API,
the term table is used</li>
</ul>

<p>Table is defined as ""two-dimensional view of a multi-dimensional column family"".</p>

<p>The term ""wide-rows"" was related mainly to the Thrift API. In cql it is defined a bit differently, but underneath looks the same. </p>

<p>Comparing SQL and CQL. In SQL table is a set of rows. In simple example it looks like in CQL it is the same, but it is not. CQL table is a set of partitions, where each partition can be just a single row (e.g. when you don't have a clustering key) or multiple rows. Partition containing multiple rows is in Thrift therminology named ""wide-row"". To see how it is stored underneath, please read e.g. part about composite-keys from <a href=""http://www.planetcassandra.org/blog/composite-keys-in-apache-cassandra/"" rel=""noreferrer"">here</a>.</p>

<p>There are more differences: </p>

<ul>
<li>CQL can have static columns which are stored on partition level - it
seems that every row in partition have a common value, but really it
is a single value stored on upper level. It can be used also to model 1:N relations</li>
<li>In CQL you can have collection type columns - set, list, map</li>
<li>Column can contain a user defined type (you can define e.g. <code>address</code> as type, and reuse this type in many places), or collection
can be a collection of user defined types</li>
<li>But also CQL does not support JOINs which are available in SQL, and you have to structure your tables very carefully, since they have to
be strictly query oriented (in cassandra you can't query data by any
column value, secondary indexes also have many limitations). It is
usually said that in relational model you model tables clearly basing
on data, when in cassandra you model basing on queries.</li>
</ul>

<p>I hope I was able to make it a bit more clear for you. I recommend watching some vidoes (or reading slides) from <a href=""https://academy.datastax.com/courses/ds201-cassandra-core-concepts"" rel=""noreferrer"">Datastax Core Concepts Course</a> as solid introduction to Cassandra.</p>
",['table']
36217451,36241271,2016-03-25 09:38:39,What is the main difference between partition and column family in Cassandra,"<p>I can't figure out if in the implementation of Apache Cassandra the notion of partition and family column is the same!? It seems that Cassandra is no longer of column family databases but more likely a tabular partitioned database. Can some please explain. I'm following this <a href=""https://pdfs.semanticscholar.org/22c6/740341ef13d3c5ee52044a4fbaad911f7322.pdf"" rel=""nofollow"">paper work</a></p>
",<cassandra><cql><nosql>,"<p>No.</p>

<p>A columnfamily, now called Table (since CQL took over thrift), is a table which is going to be saved on all nodes in your Cassandra cluster.</p>

<p>How the data of a table is broken down on nodes is the work of the partitioner, so the partitioning mechanism has nothing to do with the concept of a table since from the outside you are not supposed to know whether your data is saved on node 1 or node 2 or node 3...</p>

<p>Finally, the partitioner is defined for a cluster as a whole. This, in part, defines things such as whether your rows will be sorted (which is not a good idea because then the number of rows saved on a given node will not be well balanced.)</p>

<p>For additional information, you may want to search for the word ""partition"" on this page:</p>

<p><a href=""http://wiki.apache.org/cassandra/Operations"" rel=""nofollow"">http://wiki.apache.org/cassandra/Operations</a></p>
","['partitioner', 'table']"
36242493,36246069,2016-03-27 00:27:22,Are sorted columns in Cassandra using just one set of nodes? (one set = repeat factor),"<p>Using older versions of Cassandra, we were expected to create our own sorted rows using a <em>special</em> row of columns, because columns are saved sorted in Cassandra.</p>

<p>Is Cassandra 3.0 with CQL using the same concept when you create a <code>PRIMARY KEY</code>?</p>

<p>Say, for example, that I create a table like so:</p>

<pre><code>CREATE TABLE my_table (
    created_on timestamp,
    ...,
    PRIMARY KEY (created_on)
);
</code></pre>

<p>Then I add various entries like so:</p>

<pre><code>INSERT INTO my_table (created_on, ...) VALUES (1, ...);
...
INSERT INTO my_table (created_on, ...) VALUES (9, ...);
</code></pre>

<p>How does Cassandra manage the sort on the <code>PRIMARY KEY</code>? Will that happens on all nodes, or only one set (what I call a set is the number of replicates, so if you have a cluster of 100 nodes with a replication factor of 4, would the primary key appear on 100 nodes, 25, or just 4? With older versions, it would only be on 4 nodes.)</p>
",<cassandra><primary-key><cql><cql3>,"<p>In your case the primary key is the partition key, which used to be the row key. Which means the data your are inserting will be present on 4 out of 100 nodes if the replication factor is set to 4.</p>

<p>In CQL you can add more columns to the primary key, which are called clustering keys. When querying C* with CQL the result set might contain more than one row for a partition key. Those rows are logical and are stored in the partition of which they share the partition key (but vary in their clustering key values). The data in those logical rows is replicated as the partition is.</p>

<p>Have a look at the example for possible primary keys in the official documentation of the <a href=""https://docs.datastax.com/en/cql/3.1/cql/cql_reference/create_table_r.html"" rel=""nofollow"">CREATE TABLE</a> statement.</p>

<p><strong>EDIT</strong> (row sorting):</p>

<p>C* keeps the partitions of a table in the order of their partition key values' hash code. The ordering is therefor not straight forward and results for range queries by partition key values are not what you would expect them to be. But as partitions are in fact ordered you still can do server side pagination with the help of the <a href=""http://docs.datastax.com/en//cql/3.1/cql/cql_using/paging_c.html"" rel=""nofollow"">token function</a>.
That said, you could employ the <a href=""https://docs.datastax.com/en/cassandra/2.1/cassandra/architecture/architecturePartitionerBOP_c.html"" rel=""nofollow"">ByteOrderedPartitioner</a> to achieve lexical ordering of your partitions. But it is very easy to create hotspots with that partitioner and it is generally discouraged to use it.</p>

<p>The rows of a given partition are ordered by the actual values of their clustering keys. Range queries on those behave as you'd expect them to.</p>
","['partitioner', 'table']"
36267519,36287571,2016-03-28 17:32:22,saveToCassandra works with Cassandra Lucene plugin?,"<p>I am implementing the example on Lucene plugin for Cassandra page (<a href=""https://github.com/Stratio/cassandra-lucene-index"" rel=""nofollow"">https://github.com/Stratio/cassandra-lucene-index</a>) and when I try to save the data using saveToCassandra I get the exception NoSuchElementException.
 If I use CassandraConnector.withSessionDo I am able to add elements into Cassandra and no exception is raised.</p>

<p>The tables:</p>

<pre><code>CREATE KEYSPACE demo
WITH REPLICATION = {'class' : 'SimpleStrategy', 'replication_factor': 1};
USE demo;
CREATE TABLE tweets (
    id INT PRIMARY KEY,
    user TEXT,
    body TEXT,
    time TIMESTAMP,
    latitude FLOAT,
    longitude FLOAT
);

CREATE CUSTOM INDEX tweets_index ON tweets ()
USING 'com.stratio.cassandra.lucene.Index'
WITH OPTIONS = {
    'refresh_seconds' : '1',
    'schema' : '{
        fields : {
            id    : {type : ""integer""},
            user  : {type : ""string""},
            body  : {type : ""text"", analyzer : ""english""},
            time  : {type : ""date"", pattern : ""yyyy/MM/dd"", sorted : true},
            place : {type : ""geo_point"", latitude:""latitude"", longitude:""longitude""}
        }
    }'
};
</code></pre>

<p>The code :</p>

<pre><code>import org.apache.spark.{SparkConf, SparkContext, Logging}
import com.datastax.spark.connector.cql.CassandraConnector
import com.datastax.spark.connector._

object App extends Logging{
    def main(args: Array[String]) {

        // Get the cassandra IP and create the spark context
        val cassandraIP = System.getenv(""CASSANDRA_IP"");
        val sparkConf = new SparkConf(true)
                        .set(""spark.cassandra.connection.host"", cassandraIP)
                        .set(""spark.cleaner.ttl"", ""3600"")
                        .setAppName(""Simple Spark Cassandra Example"")


        val sc = new SparkContext(sparkConf)

        // Works
        CassandraConnector(sparkConf).withSessionDo { session =&gt;
           session.execute(""INSERT INTO demo.tweets(id, user, body, time, latitude, longitude) VALUES (19, 'Name', 'Body', '2016-03-19 09:00:00-0300', 39, 39)"")
        }

        // Does not work
        val demo = sc.parallelize(Seq((9, ""Name"", ""Body"", ""2016-03-29 19:00:00-0300"", 29, 29)))
        // Raises the exception
        demo.saveToCassandra(""demo"", ""tweets"", SomeColumns(""id"", ""user"", ""body"", ""time"", ""latitude"", ""longitude""))

    }
}
</code></pre>

<p>The exception:</p>

<pre><code>16/03/28 14:15:41 INFO CassandraConnector: Connected to Cassandra cluster: Test Cluster
Exception in thread ""main"" java.util.NoSuchElementException: Column  not found in demo.tweets
    at com.datastax.spark.connector.cql.StructDef$$anonfun$columnByName$2.apply(Schema.scala:60)
    at com.datastax.spark.connector.cql.StructDef$$anonfun$columnByName$2.apply(Schema.scala:60)
    at scala.collection.Map$WithDefault.default(Map.scala:52)
    at scala.collection.MapLike$class.apply(MapLike.scala:141)
    at scala.collection.AbstractMap.apply(Map.scala:58)
    at com.datastax.spark.connector.cql.TableDef$$anonfun$9.apply(Schema.scala:153)
    at com.datastax.spark.connector.cql.TableDef$$anonfun$9.apply(Schema.scala:152)
    at scala.collection.TraversableLike$WithFilter$$anonfun$map$2.apply(TraversableLike.scala:722)
    at scala.collection.immutable.Map$Map1.foreach(Map.scala:109)
    at scala.collection.TraversableLike$WithFilter.map(TraversableLike.scala:721)
    at com.datastax.spark.connector.cql.TableDef.&lt;init&gt;(Schema.scala:152)
    at com.datastax.spark.connector.cql.Schema$$anonfun$com$datastax$spark$connector$cql$Schema$$fetchTables$1$2.apply(Schema.scala:283)
    at com.datastax.spark.connector.cql.Schema$$anonfun$com$datastax$spark$connector$cql$Schema$$fetchTables$1$2.apply(Schema.scala:271)
    at scala.collection.TraversableLike$WithFilter$$anonfun$map$2.apply(TraversableLike.scala:722)
    at scala.collection.immutable.Set$Set4.foreach(Set.scala:137)
    at scala.collection.TraversableLike$WithFilter.map(TraversableLike.scala:721)
    at com.datastax.spark.connector.cql.Schema$.com$datastax$spark$connector$cql$Schema$$fetchTables$1(Schema.scala:271)
    at com.datastax.spark.connector.cql.Schema$$anonfun$com$datastax$spark$connector$cql$Schema$$fetchKeyspaces$1$2.apply(Schema.scala:295)
    at com.datastax.spark.connector.cql.Schema$$anonfun$com$datastax$spark$connector$cql$Schema$$fetchKeyspaces$1$2.apply(Schema.scala:294)
    at scala.collection.TraversableLike$WithFilter$$anonfun$map$2.apply(TraversableLike.scala:722)
    at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:153)
    at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:306)
    at scala.collection.TraversableLike$WithFilter.map(TraversableLike.scala:721)
    at com.datastax.spark.connector.cql.Schema$.com$datastax$spark$connector$cql$Schema$$fetchKeyspaces$1(Schema.scala:294)
    at com.datastax.spark.connector.cql.Schema$$anonfun$fromCassandra$1.apply(Schema.scala:307)
    at com.datastax.spark.connector.cql.Schema$$anonfun$fromCassandra$1.apply(Schema.scala:304)
    at com.datastax.spark.connector.cql.CassandraConnector$$anonfun$withClusterDo$1.apply(CassandraConnector.scala:121)
    at com.datastax.spark.connector.cql.CassandraConnector$$anonfun$withClusterDo$1.apply(CassandraConnector.scala:120)
    at com.datastax.spark.connector.cql.CassandraConnector$$anonfun$withSessionDo$1.apply(CassandraConnector.scala:110)
    at com.datastax.spark.connector.cql.CassandraConnector$$anonfun$withSessionDo$1.apply(CassandraConnector.scala:109)
    at com.datastax.spark.connector.cql.CassandraConnector.closeResourceAfterUse(CassandraConnector.scala:139)
    at com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:109)
    at com.datastax.spark.connector.cql.CassandraConnector.withClusterDo(CassandraConnector.scala:120)
    at com.datastax.spark.connector.cql.Schema$.fromCassandra(Schema.scala:304)
    at com.datastax.spark.connector.writer.TableWriter$.apply(TableWriter.scala:275)
    at com.datastax.spark.connector.RDDFunctions.saveToCassandra(RDDFunctions.scala:36)
    at com.webradar.spci.spark.cassandra.App$.main(App.scala:27)
    at com.webradar.spci.spark.cassandra.App.main(App.scala)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)
    at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)    16/03/28 14:15:41 INFO CassandraConnector: Connected to Cassandra cluster: Test Cluster
Exception in thread ""main"" java.util.NoSuchElementException: Column  not found in demo.tweets
    at com.datastax.spark.connector.cql.StructDef$$anonfun$columnByName$2.apply(Schema.scala:60)
    at com.datastax.spark.connector.cql.StructDef$$anonfun$columnByName$2.apply(Schema.scala:60)
    at scala.collection.Map$WithDefault.default(Map.scala:52)
    at scala.collection.MapLike$class.apply(MapLike.scala:141)
    at scala.collection.AbstractMap.apply(Map.scala:58)
    at com.datastax.spark.connector.cql.TableDef$$anonfun$9.apply(Schema.scala:153)
    at com.datastax.spark.connector.cql.TableDef$$anonfun$9.apply(Schema.scala:152)
    at scala.collection.TraversableLike$WithFilter$$anonfun$map$2.apply(TraversableLike.scala:722)
    at scala.collection.immutable.Map$Map1.foreach(Map.scala:109)
    at scala.collection.TraversableLike$WithFilter.map(TraversableLike.scala:721)
    at com.datastax.spark.connector.cql.TableDef.&lt;init&gt;(Schema.scala:152)
    at com.datastax.spark.connector.cql.Schema$$anonfun$com$datastax$spark$connector$cql$Schema$$fetchTables$1$2.apply(Schema.scala:283)
    at com.datastax.spark.connector.cql.Schema$$anonfun$com$datastax$spark$connector$cql$Schema$$fetchTables$1$2.apply(Schema.scala:271)
    at scala.collection.TraversableLike$WithFilter$$anonfun$map$2.apply(TraversableLike.scala:722)
    at scala.collection.immutable.Set$Set4.foreach(Set.scala:137)
    at scala.collection.TraversableLike$WithFilter.map(TraversableLike.scala:721)
    at com.datastax.spark.connector.cql.Schema$.com$datastax$spark$connector$cql$Schema$$fetchTables$1(Schema.scala:271)
    at com.datastax.spark.connector.cql.Schema$$anonfun$com$datastax$spark$connector$cql$Schema$$fetchKeyspaces$1$2.apply(Schema.scala:295)
    at com.datastax.spark.connector.cql.Schema$$anonfun$com$datastax$spark$connector$cql$Schema$$fetchKeyspaces$1$2.apply(Schema.scala:294)
    at scala.collection.TraversableLike$WithFilter$$anonfun$map$2.apply(TraversableLike.scala:722)
    at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:153)
    at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:306)
    at scala.collection.TraversableLike$WithFilter.map(TraversableLike.scala:721)
    at com.datastax.spark.connector.cql.Schema$.com$datastax$spark$connector$cql$Schema$$fetchKeyspaces$1(Schema.scala:294)
    at com.datastax.spark.connector.cql.Schema$$anonfun$fromCassandra$1.apply(Schema.scala:307)
    at com.datastax.spark.connector.cql.Schema$$anonfun$fromCassandra$1.apply(Schema.scala:304)
    at com.datastax.spark.connector.cql.CassandraConnector$$anonfun$withClusterDo$1.apply(CassandraConnector.scala:121)
    at com.datastax.spark.connector.cql.CassandraConnector$$anonfun$withClusterDo$1.apply(CassandraConnector.scala:120)
    at com.datastax.spark.connector.cql.CassandraConnector$$anonfun$withSessionDo$1.apply(CassandraConnector.scala:110)
    at com.datastax.spark.connector.cql.CassandraConnector$$anonfun$withSessionDo$1.apply(CassandraConnector.scala:109)
    at com.datastax.spark.connector.cql.CassandraConnector.closeResourceAfterUse(CassandraConnector.scala:139)
    at com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:109)
    at com.datastax.spark.connector.cql.CassandraConnector.withClusterDo(CassandraConnector.scala:120)
    at com.datastax.spark.connector.cql.Schema$.fromCassandra(Schema.scala:304)
    at com.datastax.spark.connector.writer.TableWriter$.apply(TableWriter.scala:275)
    at com.datastax.spark.connector.RDDFunctions.saveToCassandra(RDDFunctions.scala:36)
    at com.webradar.spci.spark.cassandra.App$.main(App.scala:27)
    at com.webradar.spci.spark.cassandra.App.main(App.scala)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)
    at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
    at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
    at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
    at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
    at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
</code></pre>

<p>EDITED:
Versions</p>

<ul>
<li>Spark 1.6.0 </li>
<li>Cassandra 3.0.3 </li>
<li>Lucene plugin 3.0.3.1</li>
<li>For Jar creation I used maven-assembly-plugin to get a fat JAR.</li>
</ul>

<p>If I remove the custom index I am able to use saveToCassandra</p>
",<apache-spark><cassandra><lucene><cassandra-lucene-index>,"<p>It seems that the problem is caused by a problem in the Cassandra Spark driver, and not in the plugin.</p>

<p>Since <a href=""https://issues.apache.org/jira/browse/CASSANDRA-10217"" rel=""nofollow"">CASSANDRA-10217</a>  Cassandra 3.x per-row indexes don't require to be created on a fake column anymore. Thus, from Cassandra 3.x the ""<strong>CREATE CUSTOM INDEX %s ON %s(%s)</strong>"" column-based syntax is replaced with the new ""<strong>CREATE CUSTOM INDEX %s ON %s()</strong>"" row-based syntax.  However, DataStax Spark driver doesn't seem to support this new feature yet.</p>

<p>When ""<em>com.datastax.spark.connector.RDDFunctions.saveToCassandra</em>"" is called it tries to load the table schema and the index schema related to a table column. Since this new index syntax does not have the fake-column anymore it results in a NoSuchElementException due to an empty column name. </p>

<p>However, saveToCassandra works well if you execute the same example with prior fake column syntax:</p>

<pre><code>CREATE KEYSPACE demo
WITH REPLICATION = {'class' : 'SimpleStrategy', 'replication_factor': 1};
USE demo;
CREATE TABLE tweets (
    id INT PRIMARY KEY,
    user TEXT,
    body TEXT,
    time TIMESTAMP,
    latitude FLOAT,
    longitude FLOAT,
    lucene TEXT
);



CREATE CUSTOM INDEX tweets_index ON tweets (lucene)
USING 'com.stratio.cassandra.lucene.Index'
WITH OPTIONS = {
    'refresh_seconds' : '1',
    'schema' : '{
        fields : {
            id    : {type : ""integer""},
            user  : {type : ""string""},
            body  : {type : ""text"", analyzer : ""english""},
            time  : {type : ""date"", pattern : ""yyyy/MM/dd"", sorted : true},
            place : {type : ""geo_point"", latitude:""latitude"", longitude:""longitude""}
        }
    }'
};
</code></pre>
",['table']
36312134,36313617,2016-03-30 14:38:13,Cassandra multiple column sorting design,"<p>Assuming an object as following:</p>

<pre><code>performance {
    userid,
    date,
    score1,
    score2,
    score3,
    ...
}
</code></pre>

<p>I want to be able to query <code>performance</code>s sorted on either of the score fields. Should I create different tables for each <code>score</code> field as compound key or is there a better way to do this?</p>

<p>I hesitate to duplicate <code>performance</code> objects for each <code>score</code> field, as there may be many <code>score</code> fields.</p>
",<cassandra>,"<p>In the <em>performace</em> table, you could consider to create secondary indexes on <em>score1</em>, <em>score2</em> and <em>score3</em>. However, Cassandra's built-in secondary indexes are best on a column family having many rows that contain the indexed value. So, you have to balance your use case, avoid to use secondary indexes to query a huge volume of records for a small number of results. <em>In the purpose of sorting, this is not a good solution.</em></p>

<p>In best practice, using roughly one table per query pattern is recommended. Data duplication is fine. It also helps to identify the most frequent query patterns and isolate the less frequent. Some queries might be executed only a few thousand times, while others a billion times. Also consider which queries are sensitive to latency and which are not. It is recommended to think and balance two high-level rules while modelling: spreading data evenly around cluster and minimising the number of partitions read.</p>

<p>Please see also: <a href=""http://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling"" rel=""nofollow"">Basic Rules of Cassandra Data Modeling</a></p>
",['table']
36342531,36344778,2016-03-31 19:54:47,Efficient way to store a JSON string in a Cassandra column?,"<p>Cassandra newbie question. I'm collecting some data from a social networking site using REST calls. So I end up with the data coming back in JSON format. </p>

<p>The JSON is only one of the columns in my table. I'm trying to figure out what the ""best practice"" is for storing the JSON string. </p>

<p>First I thought of using the <strong>map</strong> type, but the JSON contains a mix of strings, numerical types, etc. It doesn't seem like I can declare wildcard types for the map key/value. The JSON string can be quite large, probably over 10KB in size. I could potentially store it as a string, but it seems like that would be inefficient. I would assume this is a common task, so I'm sure there are some general guidelines for how to do this.</p>

<p>I know Cassandra has native support for JSON, but from what I understand, that's mostly used when the entire JSON map matches 1-1 with the database schema. That's not the case for me. The schema has a bunch of columns and the JSON string is just a sort of ""payload"". Is it better to store the JSON string as a <strong>blob</strong> or as <strong>text</strong>? BTW, the Cassandra version is 2.1.5.</p>

<p>Any hints appreciated. Thanks in advance. </p>
",<cassandra><cassandra-2.0>,"<p>In the Cassandra Storage engine there's really not a big difference between a blob and a text, since Cassandra stores text as blobs essentially. And yes the ""native"" JSON support you speak of is only for when your data model matches your JSON model, and it's only in Cassandra 2.2+.</p>

<p>I would store it as a text type, and you shouldn't have to implement anything to compress your JSON data when sending the data (or handle uncompressing). Since Cassandra's Binary Protocol supports doing <a href=""http://www.datastax.com/dev/blog/binary-protocol"" rel=""noreferrer"">transport compression</a>. Also make sure your table is storing the <a href=""http://docs.datastax.com/en/cassandra/2.1/cassandra/operations/ops_config_compress_t.html"" rel=""noreferrer"">data compressed</a> with the same compression algorithm (I suggest using LZ4 since it's the fastest algo implmeneted) to save on doing compression for each read request. Thus if you configure storing the data compressed and use transport compression, you don't even have to implement either yourself. </p>

<p>You didn't say which Client Driver you're using, but here's the documentation on how to setup Transport Compression for <a href=""https://datastax.github.io/java-driver/2.1.7/features/compression/"" rel=""noreferrer"">Datastax Java Client Driver</a>.</p>
",['table']
36369110,36376029,2016-04-02 04:13:12,Query a table in different ways or orderings in Cassandra,"<p>I've recently started to play around with Cassandra. My understanding is that in a Cassandra table you define 2 keys, which can be either single column or composites:</p>

<ol>
<li>The Partitioning Key: determines how to distribute data across nodes</li>
<li>The Clustering Key: determines in which order the records of a same partitioning key (i.e. within a same node) are written. This is also the order in which the records will be read.</li>
</ol>

<p>Data from a table will always be sorted in the same order, which is the order of the clustering key column(s). So a table must be designed for a specific query.</p>

<p>But what if I need to perform 2 different queries on the data from a table. What is the best way to solve this when using Cassandra ?</p>

<h1>Example Scenario</h1>

<p>Let's say I have a simple table containing posts that users have written :</p>

<pre><code>CREATE TABLE posts (
  username varchar,
  creation timestamp,
  content varchar,
  PRIMARY KEY ((username), creation)
);
</code></pre>

<p>This table was ""designed"" to perform the following query, which works very well for me:</p>

<pre><code>SELECT * FROM posts WHERE username='luke' [ORDER BY creation DESC];
</code></pre>

<h2>Queries</h2>

<p>But what if I need to get all posts regardless of the username, in order of time:</p>

<p><strong>Query (1):</strong> <code>SELECT * FROM posts ORDER BY creation;</code></p>

<p>Or get the posts in alphabetical order of the content:</p>

<p><strong>Query (2):</strong> <code>SELECT * FROM posts WHERE username='luke' ORDER BY content;</code></p>

<p>I know that it's not possible given the table I created, but what are the alternatives and best practices to solve this ?</p>

<h2>Solution Ideas</h2>

<p>Here are a few ideas spawned from my imagination (just to show that at least I tried):</p>

<ul>
<li>Querying with the IN clause to select posts from many users. This could help in Query (1). When using the IN clause, you can fetch globally sorted results if you disable paging. But using the IN clause quickly leads to bad performance when the number of usernames grows.</li>
<li>Maintaining full copies of the table for each query, each copy using its own PRIMARY KEY adapted to the query it is trying to serve.</li>
<li>Having a main table with a UUID as partitioning key. Then creating smaller copies of the table for each query, which only contain the (key) columns useful for their own sort order, and the UUID for each row of the main table. The smaller tables would serve only as ""sorting indexes"" to query a list of UUID as result, which can then be fetched using the main table.</li>
</ul>

<p>I'm new to NoSQL, I would just want to know what is the correct/durable/efficient way of doing this.</p>
",<sorting><cassandra><nosql>,"<p>Question 1:</p>

<p>Depending on your use case I bet you could model this with time buckets, depending on the range of times you're interested in.</p>

<p>You can do this by making the primary key a year,year-month, or year-month-day depending on your use case (or finer time intervals)</p>

<p>The basic idea is that you bucket changes for what suites your use case. For example:</p>

<ul>
<li>If you often need to search these posts over months in the past, then you may want to use the year as the PK.</li>
<li>If you usually need to search the posts over several days in the past, then you may want to use a year-month as the PK.</li>
<li>If you usually need to search the post for yesterday or a couple of days, then you may want to use a year-month-day as your PK.</li>
</ul>

<p>I'll give a fleshed out example with yyyy-mm-dd as the PK:</p>

<p>The table will now be:</p>

<pre><code>CREATE TABLE posts_by_creation (
  creation_year int,
  creation_month int,
  creation_day int,
  creation timeuuid,
  username text,  -- using text instead of varchar, they're essentially the same
  content text,
  PRIMARY KEY ((creation_year,creation_month,creation_day), creation)
)
</code></pre>

<p>I changed creation to be a timeuuid to guarantee a unique row for each post creation event. If we used just a timestamp you could theoretically overwrite an existing post creation record in here.</p>

<p>Now we can then insert the Partition Key (PK): creation_year, creation_month, creation_day based on the current creation time:</p>

<pre><code>INSERT INTO posts_by_creation (creation_year, creation_month, creation_day, creation, username, content) VALUES (2016, 4, 2, now() , 'fromanator', 'content update1';
INSERT INTO posts_by_creation (creation_year, creation_month, creation_day, creation, username, content) VALUES (2016, 4, 2, now() , 'fromanator', 'content update2';
</code></pre>

<p>now() is a CQL function to generate a timeUUID, you would probably want to generate this in the application instead, and parse out the yyyy-mm-dd for the PK and then insert the timeUUID in the clustered column.</p>

<p>For a usage case using this table, let's say you wanted to see all of the changes today, your CQL would look like:</p>

<pre><code>SELECT * FROM posts_by_creation WHERE creation_year = 2016 AND creation_month = 4 AND creation_day = 2;
</code></pre>

<p>Or if you wanted to find all of the changes today after 5pm central:</p>

<p>SELECT * FROM posts_by_creation WHERE creation_year = 2016 AND creation_month = 4 AND creation_day = 2 AND creation >= minTimeuuid('2016-04-02 5:00-0600') ;</p>

<p>minTimeuuid() is another cql function, it will create the smallest possible timeUUID for the given time, this will guarantee that you get all of the changes from that time.</p>

<p>Depending on the time spans you may need to query a few different partition keys, but it shouldn't be that hard to implement. Also you would want to change your creation column to a timeuuid for your other table.</p>

<p>Question 2:</p>

<p>You'll have to create another table or use materialized views to support this new query pattern, just like you thought.</p>

<p>Lastly if your not on Cassandra 3.x+ or don't want to use materialized views you can use Atomic batches to ensure data consistency across your several de-normalized tables (that's what it was designed for). So in your case it would be a BATCH statement with 3 inserts of the same data to 3 different tables that support your query patterns.</p>
",['table']
36381274,36381451,2016-04-03 03:21:02,Cassandra Defining Primary key and alternatives,"<p>Here is a simple example of the user table in cassandra. What is best strategy to create a primary key.</p>

<p>My requirements are </p>

<ol>
<li>search by uuid</li>
<li>search by username</li>
<li>search by email</li>
</ol>

<p>All the keys mentioned will be high cardinality keys. Also at any moment I will be having only one of them to search</p>

<p>PRIMARY KEY(uid,username,email)</p>

<p>What if I have only the username ?, Then the above primary key is not use ful. I am not able visualize a solution to achieve this using compound primary key?</p>

<p>what are other options? should we go with a new table with username to uid, then search the user table. ?</p>

<p>From all articles out there on the internet recommends not to create  secondary index for high cardinality keys</p>

<pre><code>CREATE TABLE medicscity.user (
    uid uuid,
    fname text,
    lname text,
    user_id text,
    email_id text,
    password text,
    city text,
    state_id int,
    country_id int,
    dob timestamp,
    zipcode text,
    PRIMARY KEY (??)
) 
</code></pre>

<p>How do we solve this kind of situation ?</p>
",<cassandra><datastax>,"<p>Yes, you need to go with duplicate tables.</p>

<p>If ever in Cassandra you face a situation in which you will have to query a table based on column1, column2 or column3 independently. You will have to duplicate the tables.</p>

<p>Now, how much duplication you have to use, is individual choice.</p>

<p>Like, in this example, you can either duplicate table with full data. 
Or, you can simply create a new table column1 (partition), column2, column 3 as primary key in main table.
Create a new table with primary key of column1, column2, column3 and partition key on column2.
Another one with same primary key and partition key on column3.</p>

<p>So, your data duplicate will be row, but in this case you will end up querying data twice. One from duplicate table, and one from full fledged table.</p>

<p>Big data technology, is there to speed up computation and let your system scale horizontally, and it comes at the expense of disk/storage. I mean just look at everything, even its base of replication factor does duplication of data.</p>
",['table']
36436652,36437122,2016-04-05 20:42:09,Does Cassandra 2.1 insert performance depends on affected columns?,"<p>Environment: Cassandra 2.1, DataStax Driver 2.1.9, single node cluster with DSE 4.8</p>

<p>I created a table:</p>

<pre><code>create table calc_data_test2(
    data_set_id uuid,svod_type text,section text,index_code text,value_type text,data_hash text,c1 text,c2 text,c3 text,c4 text,c5 text,c6 text,c7 text,c8 text,c9 text,c10 text,c11 text,c12 text,c13 text,c14 text,c15 text,c16 text,c17 text,c18 text,c19 text,c20 text,c21 text,c22 text,c23 text,c24 text,c25 text,c26 text,c27 text,c28 text,c29 text,c30 text,c31 text,c32 text,c33 text,c34 text,c35 text,c36 text,c37 text,c38 text,c39 text,c40 text,c41 text,c42 text,c43 text,c44 text,c45 text,c46 text,c47 text,c48 text,c49 text,c50 text,c51 text,c52 text,c53 text,c54 text,c55 text,c56 text,c57 text,c58 text,c59 text,c60 text,c61 text,c62 text,c63 text,c64 text,c65 text,c66 text,c67 text,c68 text,c69 text,c70 text,c71 text,c72 text,c73 text,c74 text,c75 text,c76 text,c77 text,c78 text,c79 text,c80 text,c81 text,c82 text,c83 text,c84 text,c85 text,c86 text,c87 text,c88 text,c89 text,c90 text,c91 text,c92 text,c93 text,c94 text,c95 text,c96 text,c97 text,c98 text,c99 text,c100 text,se1 text,se2 text,data_value double,
    primary key ((data_set_id))
);
</code></pre>

<p>Then I made a few experiments with async inserts into the table. There were 1000000 inserts in the same table with 50 parallel request for each case. The difference in the count of affected columns. Here is the results:</p>

<ul>
<li>85 columns - 143860 ms</li>
<li>65 columns - 108564 ms</li>
<li>45 columns - 78213 ms</li>
<li>25 columns - 68447 ms</li>
<li>5 columns - 49812 ms</li>
</ul>

<p>The details below.</p>

<hr>

<p>Insert of 85 columns:</p>

<pre><code>&gt;java -jar store-utils-cli.jar -pt ""insert into csod.calc_data_test2(data_set_id, svod_type,section,index_code,value_type,c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,c20,c21,c22,c23,c24,c25,c26,c27,c28,c29,c30,c31,c32,c33,c34,c35,c36,c37,c38,c39,c40,c41,c42,c43,c44,c45,c46,c47,c48,c49,c50,c51,c52,c53,c54,c55,c56,c57,c58,c59,c60,c61,c62,c63,c64,c65,c66,c67,c68,c69,c70,c71,c72,c73,c74,c75,c76,c77,c78,c79,c80) VALUES(now(), '58','9281','7611','367','7371','8353','4269','134','5884','6794','3147','7639','7798','7890','8547','4212','8630','5962','8686','4482','372','7218','6070','5525','1381','9816','5721','3632','5364','3980','6635','9641','518','6394','2560','1202','5595','7466','1507','7783','9586','6724','9169','9673','7867','8509','6889','3540','5994','4290','1925','8924','4704','4987','803','4291','4987','1111','4934','9885','6441','8212','9349','6852','6628','42','6713','3696','3316','8122','3288','3845','6063','5430','2052','5121','3343','6362','8724','2184','1380','5828','3723','8185');"" 1000000 --cassandra.connection.requests.max.local=50
22:56:40,398  INFO ru.croc.rosstat.csod.store.cassandra.connection.CassandraCluster:-1 - Connection to CassandraSettings$Connection(nodes:[csodx01.lab.croc.ru], port:9042, keyspace:csod, requests:CassandraSettings$Connection$Requests(fetchSize:1000, batchSize:2000, consistencyLevel:LOCAL_QUORUM, max:CassandraSettings$Connection$Requests$Max(local:50, remote:20, retry:CassandraSettings$Connection$Requests$Max$Retry(enabled:true, read:10, write:10, unavailable:5)))) established

Entering: Overall process
Entering: Prebuilding of statements
Leaving [1086 ms]: Prebuilding of statements
Entering: Executing statements async
Leaving [143860 ms][6951.202558042542 ops/s]: Executing statements async
Leaving [144954 ms]: Overall process
</code></pre>

<p>Insert of 65 columns:</p>

<pre><code>&gt;java -jar store-utils-cli.jar -pt ""insert into csod.calc_data_test2(data_set_id, svod_type,section,index_code,value_type,c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,c20,c21,c22,c23,c24,c25,c26,c27,c28,c29,c30,c31,c32,c33,c34,c35,c36,c37,c38,c39,c40,c41,c42,c43,c44,c45,c46,c47,c48,c49,c50,c51,c52,c53,c54,c55,c56,c57,c58,c59,c60) VALUES(now(), '58','9281','7611','367','7371','8353','4269','134','5884','6794','3147','7639','7798','7890','8547','4212','8630','5962','8686','4482','372','7218','6070','5525','1381','9816','5721','3632','5364','3980','6635','9641','518','6394','2560','1202','5595','7466','1507','7783','9586','6724','9169','9673','7867','8509','6889','3540','5994','4290','1925','8924','4704','4987','803','4291','4987','1111','4934','9885','6441','8212','9349','6852');"" 1000000 --cassandra.connection.requests.max.local=50
00:28:27,393  INFO ru.croc.rosstat.csod.store.cassandra.connection.CassandraCluster:-1 - Connection to CassandraSettings$Connection(nodes:[csodx01.lab.croc.ru], port:9042, keyspace:csod, requests:CassandraSettings$Connection$Requests(fetchSize:1000, batchSize:2000, consistencyLevel:LOCAL_QUORUM, max:CassandraSettings$Connection$Requests$Max(local:50, remote:20, retry:CassandraSettings$Connection$Requests$Max$Retry(enabled:true, read:10, write:10, unavailable:5)))) established

Entering: Overall process
Entering: Prebuilding of statements
Leaving [847 ms]: Prebuilding of statements
Entering: Executing statements async
Leaving [108564 ms][9211.15655281677 ops/s]: Executing statements async
Leaving [109413 ms]: Overall process
</code></pre>

<p>Insert of 45 columns:</p>

<pre><code>&gt;java -jar store-utils-cli.jar -pt ""insert into csod.calc_data_test2(data_set_id, svod_type,section,index_code,value_type,c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,c20,c21,c22,c23,c24,c25,c26,c27,c28,c29,c30,c31,c32,c33,c34,c35,c36,c37,c38,c39,c40) VALUES(now(), '58','9281','7611','367','7371','8353','4269','134','5884','6794','3147','7639','7798','7890','8547','4212','8630','5962','8686','4482','372','7218','6070','5525','1381','9816','5721','3632','5364','3980','6635','9641','518','6394','2560','1202','5595','7466','1507','7783','9586','6724','9169','9673');"" 1000000 --cassandra.connection.requests.max.local=50
00:33:19,972  INFO ru.croc.rosstat.csod.store.cassandra.connection.CassandraCluster:-1 - Connection to CassandraSettings$Connection(nodes:[csodx01.lab.croc.ru], port:9042, keyspace:csod, requests:CassandraSettings$Connection$Requests(fetchSize:1000, batchSize:2000, consistencyLevel:LOCAL_QUORUM, max:CassandraSettings$Connection$Requests$Max(local:50, remote:20, retry:CassandraSettings$Connection$Requests$Max$Retry(enabled:true, read:10, write:10, unavailable:5)))) established

Entering: Overall process
Entering: Prebuilding of statements
Leaving [845 ms]: Prebuilding of statements
Entering: Executing statements async
Leaving [78213 ms][12785.598302072545 ops/s]: Executing statements async
Leaving [79060 ms]: Overall process
</code></pre>

<p>Insert of 25 columns:</p>

<pre><code>&gt;java -jar store-utils-cli-1.2.0-SNAPSHOT.jar -pt ""insert into csod.calc_data_test2(data_set_id, svod_type,section,index_code,value_type,c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13,c14,c15,c16,c17,c18,c19,c20) VALUES(now(), '58','9281','7611','367','7371','8353','4269','134','5884','6794','3147','7639','7798','7890','8547','4212','8630','5962','8686','4482','372','7218','6070','5525');"" 1000000 --cassandra.connection.requests.max.local=50
00:39:29,337  INFO ru.croc.rosstat.csod.store.cassandra.connection.CassandraCluster:-1 - Connection to CassandraSettings$Connection(nodes:[csodx01.lab.croc.ru], port:9042, keyspace:csod, requests:CassandraSettings$Connection$Requests(fetchSize:1000, batchSize:2000, consistencyLevel:LOCAL_QUORUM, max:CassandraSettings$Connection$Requests$Max(local:50, remote:20, retry:CassandraSettings$Connection$Requests$Max$Retry(enabled:true, read:10, write:10, unavailable:5)))) established

Entering: Overall process
Entering: Prebuilding of statements
Leaving [885 ms]: Prebuilding of statements
Entering: Executing statements async
Leaving [68447 ms][14609.844112963314 ops/s]: Executing statements async
Leaving [69339 ms]: Overall process
</code></pre>

<p>And insert of 5 columns:</p>

<pre><code>&gt;java -jar store-utils-cli-1.2.0-SNAPSHOT.jar -pt ""insert into csod.calc_data_test2(data_set_id, svod_type,section,index_code,value_type) VALUES(now(), '58','9281','7611','367');"" 1000000 --cassandra.connection.requests.max.local=50
00:43:35,293  INFO ru.croc.rosstat.csod.store.cassandra.connection.CassandraCluster:-1 - Connection to CassandraSettings$Connection(nodes:[csodx01.lab.croc.ru], port:9042, keyspace:csod, requests:CassandraSettings$Connection$Requests(fetchSize:1000, batchSize:2000, consistencyLevel:LOCAL_QUORUM, max:CassandraSettings$Connection$Requests$Max(local:50, remote:20, retry:CassandraSettings$Connection$Requests$Max$Retry(enabled:true, read:10, write:10, unavailable:5)))) established

Entering: Overall process
Entering: Prebuilding of statements
Leaving [968 ms]: Prebuilding of statements
Entering: Executing statements async
Leaving [49812 ms][20075.483819160043 ops/s]: Executing statements async
Leaving [50782 ms]: Overall process
</code></pre>

<p>Is it really true that number of affected columns in an insert has so big impact on the performance? I haven't find any info about such dependency yet. May be I'm doing something wrong?</p>

<p>All meaningfull code for the inserts is here:</p>

<pre><code>override fun run(args: Array&lt;String?&gt;) {
    if (args.isEmpty() || args.size &lt; 2){
        System.err.println(""You should specify a query and a number of iterations: ${args.toList()}"")
        return
    }

    val query: String? = args[0]
    val iterationCount: Long = args[1]!!.toLong()

    // get the session
    val session: Session = cassandraCluster.connection().driverSession
    // prepare the query
    val preparedQuery: PreparedStatement = session.prepare(query)

    MeasureTime(""Overall process"").use {
        // create bound statements 
        val statements = MeasureTime(""Prebuild statements"").use {
            (1..iterationCount).map { BoundStatement(preparedQuery) }
        }

        // execute async
        MeasureTime(""Execute statements async"", iterationCount).use {
            val phaser = Phaser(1)
            statements.map { statement -&gt;
                phaser.register()
                session.executeAsync(statement).withCallback({
                    phaser.arriveAndDeregister()
                }, { err -&gt;
                    System.err.println(err)
                    phaser.arriveAndDeregister()
                })
            }
            // block until all tasks are done
            phaser.arriveAndAwaitAdvance()
        }
    }
}

// extension method for convenience
private fun &lt;T&gt; ListenableFuture&lt;T&gt;.withCallback(onSuccessCallback: (T?) -&gt; Unit, onFailureCallback: (Throwable?) -&gt; Unit): ListenableFuture&lt;T&gt; {
    Futures.addCallback(this, object: FutureCallback&lt;T&gt; {
        override fun onSuccess(p0: T?) {
            onSuccessCallback(p0)
        }

        override fun onFailure(p0: Throwable?) {
            onFailureCallback(p0)
        }
    })
    return this
}

class MeasureTime(val message: String, val operationCount: Long? = null): Closeable {
    private val startTime: Long

    init {
        startTime = System.nanoTime()
        System.out.println(""Entering: $message"")
    }

    override fun close() {
        val endTime = System.nanoTime()
        val elapsed = (endTime - startTime)/1000000
        val opStats = if (operationCount != null) {
            val f = operationCount/elapsed.toDouble()*1000
            ""[$f ops/s]""
        } else """"
        val message = ""Leaving [$elapsed ms]$opStats: $message""
        System.out.println(message)
    }
}
</code></pre>

<p>I believe it's not a problem for Java-man to understand what's going on in the kotlin code.</p>
",<java><performance><cassandra><cassandra-2.1>,"<p>If you're inserting more data in the second table than in the 1st one (c1 to c100 + couple of other cols) then it's normal that the insert is slower.</p>

<p>Now, even if you insert the same amount of data (in term of bytes count) in both tables, the insert in the 2nd table will still be a little bit slower because of:</p>

<ol>
<li><p>the overhead of meta data. You have more columns so there are more objects to create in memory to store them</p></li>
<li><p>the CPU consumption to serialize a lot of columns instead of a few of them</p></li>
<li><p>and maybe I forget other parameters</p></li>
</ol>
",['table']
36505461,36505965,2016-04-08 17:10:50,Is it possible to read data only from a single node in a Cassandra cluster with a replication factor of 3?,"<p>I know that Cassandra have different read consistency levels but I haven't seen a consistency level which allows as read data by key only from one node. I mean if we have a cluster with a replication factor of 3 then we will always ask all nodes when we read. Even if we choose a consistency level of one we will ask all nodes but wait for the first response from any node. That is why we will load not only one node when we read but 3 (4 with a coordinator node). I think we can't really improve a read performance even if we set a bigger replication factor.<br><br>
Is it possible to read really only from a single node?</p>
",<cassandra><cluster-computing><replication><database-replication><consistency>,"<p>Are you using a Token-Aware Load Balancing Policy?</p>

<p>If you are, <em>and</em> you are querying with a consistency of LOCAL_ONE/ONE, a read query should only contact a single node.</p>

<p>Give the article <a href=""http://www.datastax.com/dev/blog/ideology-and-testing-of-a-resilient-driver"" rel=""nofollow"">Ideology and Testing of a Resilient Driver</a> a read.  In it, you'll notice that using the TokenAwarePolicy has this effect:</p>

<blockquote>
  <p>""For cases with a single datacenter, the TokenAwarePolicy chooses the primary replica to be the chosen coordinator in hopes of cutting down latency by avoiding the typical coordinator-replica hop.""</p>
</blockquote>

<p>So here's what happens.  Let's say that I have a table for keeping track of <a href=""http://wiki.kerbalspaceprogram.com/wiki/Kerbonaut"" rel=""nofollow"">Kerbalnauts</a>, and I want to get all data for ""Bill.""  I would use a query like this:</p>

<pre><code>SELECT * FROM kerbalnauts WHERE name='Bill';
</code></pre>

<p>The driver hashes my partition key value (name) to the token of <strong>4639906948852899531</strong> (<code>SELECT token(name) FROM kerbalnauts WHERE name='Bill';</code> returns that value).  If I am working with a 6-node cluster, then my primary token ranges will look like this:</p>

<pre><code>node   start range              end range
1)     9223372036854775808 to  -9223372036854775808
2)    -9223372036854775807 to  -5534023222112865485
3)    -5534023222112865484 to  -1844674407370955162
4)    -1844674407370955161 to   1844674407370955161
5)     1844674407370955162 to   5534023222112865484
6)     5534023222112865485 to   9223372036854775807
</code></pre>

<p>As node 5 is responsible for the token range containing the partition key ""Bill,"" my query will be sent to node 5.  As I am reading at a consistency of LOCAL_ONE, there will be no need for another node to be contacted, and the result will be returned to the client...having only hit a single node.</p>

<p><em>Note: Token ranges computed with:</em></p>

<pre><code>python -c'print [str(((2**64 /5) * i) - 2**63) for i in range(6)]'
</code></pre>
",['table']
36597022,36597486,2016-04-13 11:29:59,Cassandra + Spark for Real time analytics,"<p>I am working on an application for ""Real Time Rendering of Big Data (Spatial data)"".
With the help of Spark Streaming + Spark SQL + WebSocket, i am able to render pre defined queries on dashboard. But i want to fetch data with interactive queries and ad hoc queries.</p>

<p>For that purpose i am trying to implement it with ""Spark Streaming + Cassandra"". These queries required aggregation and filter on huge amount of data.</p>

<p>I am new to Cassandra and Spark, So i am confused about below approachs, which will be better\faster:</p>

<ol>
<li>Spark Streaming -> Filtering (Spark) -> Save to Cassandra ->Interactive Query -> UI (Dashboard)</li>
<li>Spark Streaming -> Filtering (Spark) -> Save to Cassandra ->Spark SQL  -> Interactive Query -> UI (Dashboard)</li>
</ol>

<p>Will Cassandra be fast enough to give result in real time ? Or should i create an RDD from Cassandra to perform interactive queries over it.</p>

<p>One of the query is:</p>

<pre><code>""SELECT *  FROM PERFORMANCE.GEONAMES A  INNER JOIN  
(SELECT max(GEONAMEID) AS MAPINFO_ID FROM  PERFORMANCE.GEONAMES
where longitude between %LL_LONG% and %UR_LONG% 
and latitude between %LL_LAT% and %UR_LAT%  
and %WHERE_CLAUSE% GROUP BY LEFT(QUADKEY, %QUAD_TREE_LEVEL%)  )
AS B ON A.GEONAMEID = B.MAPINFO_ID""
</code></pre>

<p>Any inputs or suggestions will be appreciated. Thanks,</p>

<p>Thanks @doanduyhai for suggesting SASI secondary index, it really made a huge difference. </p>
",<apache-spark><cassandra><spark-streaming><spark-dataframe>,"<blockquote>
  <p>Will Cassandra be fast enough to give result in real time ? Or should i create an RDD from Cassandra to perform interactive queries over it.</p>
</blockquote>

<p>It depends on how much <strong>filtering</strong> you're doing up-front and the number of machines in your cluster. If your Cassandra table has 1Tb of data and you query fetches 100Gb of data in memory, assuming a cluster of 10 machines, it means loading 1Gb in memory it's manageable but the query will never be sub-minute.</p>

<p>Now, if you filter enough to fetch only 100Mb total out of the Cassandra table, it means 10Mb/machine and it's possible to have latency of the order of seconds.</p>

<p>How to filter data early in Cassandra ?</p>

<ol>
<li>Use the new <a href=""https://github.com/apache/cassandra/blob/trunk/doc/SASI.md"" rel=""nofollow"">SASI secondary index</a> (wait for Cassandra 3.5 released this week because 2 critical bugs have been discovered)</li>
<li>Use <a href=""http://docs.datastax.com/en/datastax_enterprise/4.8/datastax_enterprise/srch/srchIntro.html"" rel=""nofollow"">DSE Search</a> to filter early with Solr</li>
<li>Use <a href=""https://github.com/Stratio/cassandra-lucene-index"" rel=""nofollow"">Stratio Lucene secondary index</a> </li>
</ol>
",['table']
36606244,37251436,2016-04-13 18:08:02,sstableloader to update existing columns,"<p>Hi I am new to cassandra and was checking on sstableloader. </p>

<p>By the function of cassandra if any insert happens on the key which is already in Cassandra table the values will be updated. I tried using the same concept using sstableloader using MAP.</p>

<p>But instead of updating only the columns required it is updating the entire row. Is there any way we can insert only the columns required and leave the rest as was earlier.</p>
",<cassandra><cassandra-2.0>,"<p>@root545, thank you for your interest.</p>

<p>So basically I was doing things wrong. Cassandra always inserts based on the key, if the y is already present then it will override the values for that key.</p>

<p>When you want to override any value for an existing key, Just provide the insert statement as insert into table(keys present in table, the values you want to override).</p>

<p>for example if we have a table like below.</p>

<p>create table example (
key1 text,
key2 text,
value1 text,
value2 text,
value3 text)
primary key(key1,key2)</p>

<p>then in order to update the value3 alone we need to provide the insert statement as</p>

<p>insert into example(key1,key2,value3) values (?,?,?)</p>

<p>and create the sstablewriter as</p>

<p>CQLSSTableWriter cqlssTableWriter = CQLSSTableWriter.builder().inDirectory().forTable().using().withPartitioner(new Murmur3Partitioner()).build();</p>

<p>This should do</p>
",['table']
36633568,36633930,2016-04-14 20:40:42,Fetch data from cassandra by portion,"<p>In cassandra db, I have a table with 10 millions rows. If I use normal select query, I can only have 1 connection to cassandra to fetch data. Is it possible to do select query with row number ? </p>

<pre><code>E.g. Select * from abc where row = x to row = y ?
</code></pre>

<p>So I can have 10 different threads, so each of them will be responsible to fetch 1 million rows. If this is possible, should I have number of threads equal to the number of nodes I have in my cluster?</p>

<p>I'm connecting to my cassandra cluster by using java datastax driver.</p>
",<java><cassandra><datastax>,"<blockquote>
  <p>Is it possible to do select query with row number ?</p>
</blockquote>

<p>No, but it is possible to execute a range query based on the hashed token value of your partition key.</p>

<p>Let's say that you have a six node cluster.  The Murmur3 primary token ranges for a 6 node cluster look like this:</p>

<pre><code>node   start range              end range
1)     9223372036854775808 to  -9223372036854775808
2)    -9223372036854775807 to  -5534023222112865485
3)    -5534023222112865484 to  -1844674407370955162
4)    -1844674407370955161 to   1844674407370955161
5)     1844674407370955162 to   5534023222112865484
6)     5534023222112865485 to   9223372036854775807
</code></pre>

<p>If I have a table called <code>abc</code> with a partition key of <code>pkey</code>, and wanted to query all rows for that table on node 2, my query would look like this:</p>

<pre><code>SELECT * FROM abc
  WHERE token(pkey) &gt;   9223372036854775808
    AND token(pkey) &lt;= -5534023222112865485;
</code></pre>

<p>If an entire row's worth is too many (and I'm guessing that it will be), you can work on bisecting your token range until the number of rows becomes manageable.  Remember, selecting 10 million rows isn't something that Cassandra was designed to be particularly good at, so it might take a few tries.</p>

<p>As for thread count, that may also take some trial and error.  But one thread per node sounds like a reasonable starting point.</p>
",['table']
36700859,36706463,2016-04-18 17:44:19,Does the same partition key in different cassandra tables add up to cell theoretical limit?,"<p>It is known that a Cassandra partition has a theoretical limit of 2 billion cells. But how does that work in a situation like this one below:</p>

<pre><code>create table table1 (
    some_id int PRIMARY KEY,
    some_name text
);

create table table2 (
    other_id int PRIMARY KEY,
    other_name text
);
</code></pre>

<p>Assume we have 1 billion cells in partition (some_id = 1) on table1.
If we had another 1 billion cells in partition (other_id = 1) on table2, would those add up to the 2 billion theoretical limit?</p>

<p>In other words, are equal partition keys in different tables stored together?</p>
",<cassandra>,"<p>Different tables have different partitions. This makes the structure of any particular partition homogenous (it will always follow the proscribed schema of a single table) which allows for optimizations. </p>

<p>If you look at the storage engine under the hood you'll see that every table even has it's own directory structure making it clear that a partition from one table will never interact with the partition of another. (see /var/lib/cassandra/)</p>
",['table']
36741702,36809977,2016-04-20 10:58:56,Cassandra TTL not working,"<p>I am using datastax with cassandra. I want a row to be automatically deleted after 15 minutes of its insertion. But the row still remains. </p>

<p>My code is below:</p>

<pre><code>Insert insertStatement = QueryBuilder.insertInto(keySpace, ""device_activity"");
        insertStatement.using(QueryBuilder.ttl(15* 60));
        insertStatement.value(""device"", UUID.fromString(persistData.getSourceId()));
        insertStatement.value(""lastupdatedtime"", persistData.getLastUpdatedTime());
        insertStatement.value(""devicename"", persistData.getDeviceName());
        insertStatement.value(""datasourcename"", persistData.getDatasourceName());
</code></pre>

<p>The table consist of 4 columns : device (uuid), datasourcename(text), devicename(text), lastupdatedtime (timestamp).</p>

<p>If I query the TTL of some field it shows me 4126 seconds which is wrong.
//Select TTL(devicename) from device_activity; // Gives me 4126 seconds</p>
",<cassandra><datastax><ttl>,"<p>In the below link, the explanation of TTL is provided.</p>

<p><a href=""https://docs.datastax.com/en/cql/3.1/cql/cql_using/use_expire_c.html"" rel=""nofollow"">https://docs.datastax.com/en/cql/3.1/cql/cql_using/use_expire_c.html</a></p>

<p>""TTL data has a precision of one second, as calculated on the server. Therefore, a very small TTL probably does not make much sense. Moreover, the clocks on the servers should be synchronized; otherwise reduced precision could be observed because the expiration time is computed on the primary host that receives the initial insertion but is then interpreted by other hosts on the cluster.""</p>

<p>After reading this i could resolve by setting proper time on the corresponding node(machine.)</p>
",['precision']
36764437,36766364,2016-04-21 08:49:34,Data Replication In Cassandra,"<p>I am trying to understand data replication in Cassandra. In my case, I have to store a huge number of records into a single table based on yymmddhh primary key partition.</p>

<p>I have two data centers (DC1 and DC2) and I created a keyspace using below CQL.</p>

<pre><code>CREATE KEYSPACE db1 WITH REPLICATION = { 'class' : 'NetworkTopologyStrategy', 'DC1' : 1, 'DC2' : 1 };
</code></pre>

<p>And then created a new table tbl_data using below CQL</p>

<pre><code>CREATE TABLE db1.tbl_data (
        yymmddhh varchar,
        other_details text,
        PRIMARY KEY (yymmddhh)
    ) WITH read_repair_chance = 0.0;
</code></pre>

<p>Now, I can see that the above keyspace ""db1"" and table ""tbl_data"" created successfully. I have few millions of rows to insert, I am assuming that all rows will be stored on both servers i.e. DC1 and DC2 since replication factor is 1 of both data centers.</p>

<p>Suppose, after some time I need to add more nodes since number of records can increase to billions, so in that case one data center can't handle that huge number of records due to disk space limitation. </p>

<p><strong>a)</strong> So, how can I divide data into different nodes and can add new nodes on demand?</p>

<p><strong>b)</strong> Do I need to alter keyspace ""db1"" to put name of new data centers in the list?</p>

<p><strong>c)</strong> How the current system will work horizontally?</p>

<p><strong>d)</strong> I am connecting Cassandra using nodejs driver by using below code. Do I need to put ip address of all nodes here in code? What If I keep increasing the number of nodes on demand, do I need to change the code every time?</p>

<pre><code>var client = new cassandra.Client({ contactPoints: ['ipaddress_of_node1'], keyspace: 'db1' });
</code></pre>

<p>From all above examples you can see that my basic requirement is to store a huge number of records into a single table spreading data to different servers where I should be able to add new servers if data volume increases.</p>
",<cassandra>,"<p>a) If you add new nodes to the data center, the data will be automatically shared between the nodes. With replication factor 1 and default settings, it should be ~50% on each node, though it might take a bit to redistribute data between the nodes after adding a new node. 'nodetool status ' can show you which node owns how much of that keyspace.</p>

<p>b) Yes, I do believe you have to (though not 100% on this).</p>

<p>c) Horizontally with your setup it'll scale linearly (assuming the machines are equal and have the same num_tokens value) by distributing data as according to 1 divided on number of nodes (1 node = 100%, 2 = 50%, 3 = 33%, etc.), both throughput and storage capacity will scale.</p>

<p>d) No, assuming the nodejs driver works like the C++ and Python drivers of Cassandra (it should!), after connecting to Cassandra it'll be aware of the other nodes in the cluster.</p>
",['num_tokens']
36765885,36769104,2016-04-21 09:49:39,Tables already created to insert into a cassandra keyspace to test,"<p>I want to test my cluster a little, how data replicates, etc.</p>

<p>I have a cassandra cluster formed by 5 machines ( centos 7 &amp; cassie 3.4 on them).</p>

<p>Are there anywhere tables already created for testing that I can import in my db in some keyspace?</p>

<p>If yes, please be kind enough and explain me how to import them into a keyspace and where from to take them.</p>
",<cassandra>,"<p>You can use Cassandra-stress. This is great to create data for your style of table and also has some default tables.</p>

<p><a href=""http://docs.datastax.com/en/cassandra_win/3.0/cassandra/tools/toolsCStress.html"" rel=""nofollow"">http://docs.datastax.com/en/cassandra_win/3.0/cassandra/tools/toolsCStress.html</a></p>

<p>I highly recommend it. </p>
",['table']
36781157,36781808,2016-04-21 21:54:51,How to make the query to work?,"<p>I have Cassandra version 2.0, and in it I am totally new in it, so the question...
I have table <code>T1</code>, with columns with names: 1,2,3...14 (for simplicity);
<code>Partitioning key</code> is column <code>1</code> , <code>2</code>;
<code>Clustering key</code> is column <code>3</code>, <code>1</code> , <code>5</code>;
I need to perform following query:</p>

<pre><code>SELECT 1,2,7 FROM T1 where 2='A';
</code></pre>

<p>Column <code>2</code> is a flag, so values are repeating.
I get the following error:</p>

<pre><code>Unable to execute CQL query: Partitioning column 2 cannot be restricted because the preceding column 1 is either not restricted or is restricted by a non-EQ relation
</code></pre>

<p>So what is the right way to do it? I really need to get the data that already filtered. Thanks.</p>
",<cassandra><cassandra-2.0><cql><cql3>,"<p>So, to make sure I understand your schema, you have defined a table <code>T1</code>:</p>

<pre><code>CREATE TABLE T1 (
  1 INT,
  2 INT,
  3 INT,
  ...
  14 INT,
  PRIMARY ((1, 2), 3, 1, 5)
);
</code></pre>

<p>Correct?</p>

<p>If this is the case, then Cassandra <strong>cannot</strong> find the data to answer your CQL  query:</p>

<pre><code>SELECT 1,2,7 FROM T1 where 2 = 'A';
</code></pre>

<p>because your query has not provided a value for column ""1"", without which Cassandra cannot compute the partition key (which requires, per your composite <code>PRIMARY KEY</code> definition, both columns ""1"" <em>and</em> ""2""), and without that, it cannot determine where to look on which nodes in the ring.  By including ""2"" in your <em>partition key</em>, you are telling Cassandra that that data is <strong>required</strong> for determine where to store (and thus, where to <em>read</em>) that data.</p>

<p>For example, given your schema, this query <em>should</em> work:</p>

<pre><code>SELECT 7 FROM T1 WHERE 1 = 'X' AND 2 = 'A';
</code></pre>

<p>since you are providing both values of your partition key.</p>

<p>@Caleb Rockcliffe has good advice, though, regarding the need for other, secondary/supplemental lookup mechanisms if the above table definition is a big part of your workload.  You may need to find some way to <em>first</em> lookup the values for ""1"" and ""2"", <em>then</em> issue your query.  <em>E.g.</em>:</p>

<pre><code>CREATE TABLE T1_index (
  1 INT,
  2 INT,
  PRIMARY KEY (1, 2);
);
</code></pre>

<p>Given a value for ""1"", the above will provide <em>all</em> of the possible ""2"" values, through which you can then iterate:</p>

<pre><code>SELECT 2 FROM T1_index WHERE 1 = 'X';
</code></pre>

<p>And then, for each ""1"" and ""2"" combination, you can <em>then</em> issue your query against table <code>T1</code>:</p>

<pre><code>SELECT 7 FROM T1 WHERE 1 = 'X' AND 2 = 'A';
</code></pre>

<p>Hope this helps!</p>
",['table']
36801555,36803271,2016-04-22 19:06:27,news feed like time-series data on cassandra,"<p>I am making a website and I want to store all users posts in one table ordered by the time they post it. the cassandra data model that I made is this</p>

<pre><code>CREATE TABLE Posts(
   ID uuid,
   title text,
   insertedTime timestamp,
   postHour int,
   contentURL text,
   userID text,
   PRIMARY KEY (postHour, insertedTime)
) WITH CLUSTERING ORDER BY (insertedTime DESC);
</code></pre>

<p>The question I'm facing is, when a user visits the posts page, it fetches the most recent ones by querying </p>

<pre><code>SELECT * FROM Posts WHERE postHour = ?;
</code></pre>

<p>? = current hour</p>

<p>so far when the user scrolls down ajax requests are made to get more posts from the server. Javascript keeps track of postHour of the lastFetched item and sends back to the server along with the cassandra PagingState when requesting for new posts.</p>

<p>but this approach will query more than 1 partition when user scrolls down.
I want to know whether this model would perform without a problem, is there any other model that I can follow.</p>

<p>Someone please point me in the right direction.
Thank You.</p>
",<cassandra><time-series><data-modeling><nosql>,"<p>That's a good start but a few pointers:</p>

<ol>
<li><p>You'll probably need more than just the <code>postHour</code> as the partition key. I'm guessing you don't want to store all the posts <em>regardless of the day</em> together and then page through them. What you're probably are after here is:</p>

<pre><code>PRIMARY KEY ((postYear, postMonth, postDay, postHour), insertedTime)
</code></pre></li>
<li><p>But there's still a problem. Your <code>PRIMARY KEY</code> has to uniquely identify a row (in this case a post). I'm going to guess it's possible, although not likely, that two users might make a post with the same <code>insertedTime</code> value. What you really need then is to add the <code>ID</code> to make sure they are unique:</p>

<pre><code>PRIMARY KEY ((postYear, postMonth, postDay, postHour), insertedTime, ID)
</code></pre></li>
<li><p>At this point, I'd consider just combining your <code>ID</code> and <code>insertedTime</code> columns into a single <code>ID</code> column of type <code>timeuuid</code>. With those changes, your final table looks like:</p>

<pre><code>CREATE TABLE Posts(
  ID timeuuid,
  postYear int,
  postMonth int,
  postDay int,
  postHour int,
  title text,
  contentURL text,
  userID text,
  PRIMARY KEY ((postYear, postMonth, postDay, postHour), ID)
) WITH CLUSTERING ORDER BY (ID DESC);
</code></pre>

<p>Whatever programming language you're using should have a way to generate a <code>timeuuid</code> from the inserted time and then extract that time from a <code>timeuuid</code> value if you want to show it in the UI or something. (Or you could use the <a href=""http://docs.datastax.com/en/cql/3.3/cql/cql_reference/timeuuid_functions_r.html"" rel=""nofollow"">CQL timeuuid functions</a> for doing the converting.)</p></li>
</ol>

<p>As to your question about querying multiple partitions, yes, that's totally fine to do, but you could run into trouble if you're not careful. For example, what happens if there is a 48 hour period with no posts? Do you have to issue 48 queries that return empty results before finally getting some back on your 49th query? (That's probably going to be really slow and a crappy user experience.)</p>

<p>There are a couple things you could do to try and mitigate that:</p>

<ol>
<li>Make your partitions less granular. For example, instead of doing posts by hour, make it posts by <em>day</em>, or posts by <em>month</em>. If you know that those partitions won't get too large (i.e. users won't make so many posts that the partition gets huge), that's probably the easiest solution.</li>
<li><p>Create a second table to keep track of which partitions actually have posts in them. For example, if you were to stick with posts by hour, you could create a table like this:</p>

<pre><code>CREATE TABLE post_hours (
  postYear int,
  postMonth int,
  postDay int,
  postHour int,
  PRIMARY KEY (postYear, postMonth, postDay, postHour)
);
</code></pre>

<p>You'd then insert into this table (using a <a href=""http://docs.datastax.com/en/cql/3.3/cql/cql_using/useBatchTOC.html"" rel=""nofollow"">Batch</a>) anytime a user adds a new post. You can then query this table <em>first</em> before you query the <code>Posts</code> table to figure out which partitions have posts and should be queried (and thus avoid querying a whole bunch of empty partitions).</p></li>
</ol>
",['table']
36826036,36834762,2016-04-24 16:46:45,"CQL: Select all rows, distinct partition key","<p>I have a table </p>

<pre><code>    CREATE TABLE userssbyprofit (
    userid text,
    profit double,
    dateupdated timeuuid,
    PRIMARY KEY (userid, profit, dateupdated)
) WITH CLUSTERING ORDER BY (profit DESC, dateupdated DESC)
</code></pre>

<p>Userid can be used to lookup the full user details in another table. This table will provide a history of the users profits. Taking the most recent one to find their current profit amount.</p>

<p>How do I retrieve the 10 most profitable users with their profit amount. I want it to be distinct based on the userID</p>

<p>Thanks.</p>
",<cassandra><cassandra-2.0><cql><cql3><phantom-dsl>,"<p>You need to create one more table or view which have only user id and profit . New table or view will have user id order by profit with desc order .</p>
",['table']
36838670,36839955,2016-04-25 10:59:21,Modelling of hierarchical entities in Cassandra,"<p>I am new to Cassandra and i did an online search about modelling of hierarchical entities in Cassandra but i couldn't find anything.</p>

<p>I tried classical relational db approach in which you store parent ids but that don't seem to work in nosql world.</p>

<p>So basically i have hierarchical entities (parent-child relationship), how should i create my tables for efficient querying on Cassandra?</p>

<p>I have packages and products. I put products in packages and i can also put packages in packages which contain products and packages and so on.</p>
",<cassandra><data-modeling><cql><hierarchical-data>,"<p>To have a good understanding of how Cassandra modelling works, you should first see some tutorials on <strong>datastax academy</strong> or see some videos online.  </p>

<p>In any case, what you need to think about is how your data will be accessed.  </p>

<p>For example, let's say you only needs to have all the products and packages in a package, then you will only need a package table, with a column for the packages and one for the products.  </p>

<p>Now if you want to see in which packages lies a product, the model above is wrong, and you need to make another table that will store every packages that contain a given product.  </p>

<p>I think you see where I am going: it's impossible to find a good model if we don't know how it will be used.</p>

<p>Also, look at this more in-depth example: <a href=""http://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling"" rel=""nofollow"">http://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling</a></p>
",['table']
36872271,36898839,2016-04-26 17:52:48,Cassandra data modeling query,"<p>Im currently learning apache cassandra and im new in nosql data modeling. At the moment im trying to build a system where i have the following fields:</p>

<pre><code> latitude | time_forecast       | longitude  | forecast_request | rh                 | swflx              | temp
----------+---------------------+------------+------------------+--------------------+--------------------+--------------------
    41.45 | 2016-04-26 17:00:00 | -8.6166667 |  2016-04-26_0000 | 0.6161368489265442 |  397.4789733886719 | 290.44512939453125
    41.45 | 2016-04-26 18:00:00 | -8.6166667 |  2016-04-26_0000 |  0.673031210899353 |  261.1000061035156 |  289.3315734863281
    41.45 | 2016-04-26 19:00:00 | -8.6166667 |  2016-04-26_0000 | 0.8489508032798767 | 27.700000762939453 |  286.8634948730469
    41.25 | 2016-04-26 17:00:00 | -8.6166667 |  2016-04-26_0000 | 0.6555368900299072 |  395.1789855957031 |  289.8011169433594
    41.25 | 2016-04-26 18:00:00 | -8.6166667 |  2016-04-26_0000 | 0.7271312475204468 | 203.39999389648438 |  288.5975646972656
    41.25 | 2016-04-26 19:00:00 | -8.6166667 |  2016-04-26_0000 | 0.8572507500648499 | 30.989999771118164 |  286.7254943847656
   41.456 | 2016-04-26 17:00:00 | -8.6166667 |  2016-04-26_0000 | 0.6161368489265442 |  397.4789733886719 | 290.44512939453125
   41.456 | 2016-04-26 18:00:00 | -8.6166667 |  2016-04-26_0000 |  0.673031210899353 |  261.1000061035156 |  289.3315734863281
   41.456 | 2016-04-26 19:00:00 | -8.6166667 |  2016-04-26_0000 | 0.8489508032798767 | 27.700000762939453 |  286.8634948730469
</code></pre>

<p>This is how i created the table:</p>

<pre><code>create table if not exists forecast 
(   
    latitude varchar,
    longitude varchar,
    time_forecast varchar,
    forecast_request varchar,       
    swflx varchar,      
    temp varchar,
    rh varchar,

    PRIMARY KEY(latitude, time_forecast, longitude)
)

WITH CLUSTERING ORDER BY (time_forecast ASC);
</code></pre>

<hr>

<p>With this model , <code>time_forecast</code> cannot be my primary key because it will not be unique.</p>

<p>This was the only way i could insert data without making upserts but the problem im facing now is that i can't query the database only by <code>time_forecast</code>.</p>

<p>In this example, for each coordinate (latitude and longitude) im making a forecast for the next 3 hours and i can't avoid using a composite key.</p>

<p><strong>How would you model the database in a way i could retrieve the coordinates and variables from only a range of <code>time_forecast</code> value</strong>?</p>
",<database-design><cassandra><nosql>,"<blockquote>
  <p>How would you model the database in a way i could retrieve the coordinates and variables from only a range of time_forecast value?</p>
</blockquote>

<pre><code>create table if not exists forecast 
(   
    location text,
    latitude varchar,
    longitude varchar,
    time_forecast varchar,
    forecast_request varchar,       
    swflx varchar,      
    temp varchar,
    rh varchar,

    PRIMARY KEY((location), time_forecast)
) WITH CLUSTERING ORDER BY (time_forecast ASC);
</code></pre>

<p>Then you can query by range of time_forecast:</p>

<pre><code>SELECT * FROM forecast
WHERE location = 'California'
AND time_forecast &gt;= 'xxx' 
AND time_forecast &lt;= 'yyy';
</code></pre>
",['table']
36914347,36920786,2016-04-28 12:13:36,How to use sstableloader?,"<p>i use Cassandra 3.4 on some centos 7 machines.</p>

<p>I have 2 clusters:</p>

<p>Cluster 1 with 2 DC , DC1 has 2 machines 192.168.0.171/192.168.172, DC2 has 1 machine 192.168.0.173. Cluster 1 has some data on it, on one keyspace with  replication 2 : 1.</p>

<p>Cluster 2 with 1 datacenter , DC3 has 2 machines. 192.168.0.174/192.168.0.175.</p>

<p>On second cluster, DC3, I create the keyspace : ""keyspace1"" with NetworkTopologyStrategy : DC3 : 2.</p>

<p>Streamed some cassandra-stress on 192.168.0.175 :</p>

<p>cassandra-stress write n=1000000 -node 192.168.0.175.</p>

<p>In this moment cassandra-stress should generate some garbage data.</p>

<p>Checked the /var/lib/cassandra/data/keyspace1/standard1-97a771600d4011e69a5a13282caaa658 and there i have some ma-1-big-Data.db 57 Mb, ma-2-big-Data.db 65 Mb, ma-3-big-Data.db 65 Mb.</p>

<p>My question :</p>

<p>Let`s assume the garbage data is actual data and i want to stream from Cluster 2 this data into Cluster 1.
How can i do that by using sstableloader?</p>

<p>NOTE: Please give, if possible, example with commands ( i`m quite newbie in domain :( )</p>
",<cassandra><streaming>,"<p>bin/sstableloader -d 192.168.0.171,192.168.172 /var/lib/cassandra/data/keyspace1/standard1-97a771600d4011e69a5a13282caaa658 </p>

<p>this command will load data from one cluster to another cluster</p>

<p>Note: keyspace and table should exist in both clusters, and the tables should have the same schema.</p>
",['table']
37031438,37039978,2016-05-04 14:58:52,Design data model for messaging system with Cassandra,"<p>I am new to Cassandra and trying to build a data model for messaging system. I found few solutions but none of them exactly match my requirements. There are <strong>two main requirements</strong>:</p>

<ol>
<li>Get a list of last messages for a particular user, from all other users, sorted by time.</li>
<li>Get a list of messages for one-to-one message history, sorted by time as well.</li>
</ol>

<p>I thought of something like this,</p>

<pre><code>CREATE TABLE chat (
  to_user text,
  from_user_text,
  time text,
  msg text,
  PRIMARY KEY((to_user,from_user),time) 
  ) WITH CLUSTERING ORDER BY (time DESC);
</code></pre>

<p>But this design has few issues, like I wont be able to satisfy <strong>first requirement</strong> since this design requires to pass from_user as well. And also  this would be inefficient when number of (to_user,from_user) pair increases.</p>
",<cassandra><messagebox><datamodel>,"<p>You are right. That one table won't satisfy both queries, so you will need two tables. One for each query. This is a core concept with Cassandra data modeling. Query driven design. </p>

<p>So the query looking for messages to a user:</p>

<pre><code>CREATE TABLE chat (
  to_user text,
  from_user_text,
  time text,
  msg text,
  PRIMARY KEY((to_user),time) 
  ) WITH CLUSTERING ORDER BY (time DESC);
</code></pre>

<p>Messages from a user to another user. </p>

<pre><code>CREATE TABLE chat (
  to_user text,
  from_user_text,
  time text,
  msg text,
  PRIMARY KEY((to_user),from_user,time) 
  ) WITH CLUSTERING ORDER BY (time DESC);
</code></pre>

<p>Slight difference from yours: from_user is a clustering column and not a part of the partition key. This is minimize the amount of select queries needed in application code.</p>

<p>It's possible to use the second table to satisfy both queries, but you will have to supply the 'from_user' to use a range query on time. </p>
",['table']
37072926,37074945,2016-05-06 12:51:29,Partial inserts with Cassandra and Phantom DSL,"<p>I'm building a simple Scala Play app which stores data in a Cassandra DB using the Phantom DSL driver for Scala.  One of the nice features of Cassandra is that you can do partial updates i.e. so long as you provide the key columns, you do not have to provide values for all the other columns in the table.  Cassandra will merge the data into your existing record based on the key.  </p>

<p>Unfortunately, it seems this doesn't work with Phantom DSL.  I have a table with several columns, and I want to be able to do an update, specifying values just for the key and one of the data columns, and let Cassandra merge this into the record as usual, while leaving all the other data columns for that record unchanged.</p>

<p>But Phantom DSL overwrites existing columns with null if you don't specify values in your insert/update statement.</p>

<p>Does anybody know of a work-around for this?  I don't want to have to read/write all the data columns every time, as eventually the data columns will be quite large.</p>

<p>FYI I'm using the same approach to my Phantom coding as in these examples:</p>

<p><a href=""https://github.com/thiagoandrade6/cassandra-phantom/blob/master/src/main/scala/com/cassandra/phantom/modeling/model/GenericSongsModel.scala"" rel=""nofollow"">https://github.com/thiagoandrade6/cassandra-phantom/blob/master/src/main/scala/com/cassandra/phantom/modeling/model/GenericSongsModel.scala</a></p>
",<scala><cassandra><phantom-dsl>,"<p>It would be great to see some code, but partial updates are possible with phantom. Phantom is an immutable builder, it will not override anything with null by default. If you don't specify a value it won't do anything about it.</p>

<pre><code>database.table.update.where(_.id eqs id).update(_.bla setTo ""newValue"")
</code></pre>

<p>will produce a query where only the values you've explicitly set to something will be set to null. Please provide some code examples, your problem seems really strange as queries don't keep track of table columns to automatically add in what's missing.</p>

<p><strong>Update</strong></p>

<p>If you would like to delete column values, e.g set them to <code>null</code> inside Cassandra basically, phantom offers a different syntax which does the same thing:</p>

<pre><code>database.table.delete(_.col1, _.col2).where(_.id eqs id)`
</code></pre>

<p>Furthermore, you can even delete map entries in the same fashion:</p>

<pre><code>database.table.delete(_.props(""test""), _.props(""test2"").where(_.id eqs id)
</code></pre>

<p>This assumes <code>props</code> is a <code>MapColumn[Table, Record, String, _]</code>, as the <code>props.apply(key: T</code>) is typesafe, so it will respect the keytype you define for the map column.</p>
",['table']
37089670,37089747,2016-05-07 14:21:56,Query Spark SQL from Node.js server,"<p>I'm currently using npm's <a href=""https://www.npmjs.com/package/cassandra-driver"" rel=""nofollow"">cassandra-driver</a> to query my Cassandra database from a Node.js server. Since I want to be able to write more complex queries, I'd like to use Spark SQL instead of CQL. Is there any way to create a RESTful API (or something else) so that I can use Spark SQL the same way that I currently use CQL?</p>

<p>In other words, I want to be able to send a Spark SQL query from my Node.js server to another server and get a result back.</p>

<p>Is there any way to do this? I've been searching for solutions to this problem for a while and haven't found anything yet.</p>

<p><strong>Edit:</strong> I'm able to query my database with Scala and Spark SQL from the Spark shell, so that bit is working. I just need to connect Spark and my Node.js server somehow.</p>
",<node.js><apache-spark><cassandra><apache-spark-sql>,"<p>I had a similar problem, and I solved by using <a href=""https://github.com/spark-jobserver/spark-jobserver"" rel=""nofollow"">Spark-JobServer</a>.</p>

<p>The main approach with Spark-Jobserver (SJS) usually is to create a special job that extends their SparkSQLJob such as in the following example:</p>

<pre><code>object ExecuteQuery extends SparkSQLJob {
  override def validate(sqlContext: SQLContext, config: Config): SparkJobValidation = {
    // Code to validate the parameters received in the request body
  }
  override def runJob(sqlContext: SQLContext, jobConfig: Config): Any = {
    // Assuming your request sent a { ""query"": ""..."" } in the body:
    val df = sqlContext.sql(config.getString(""query""))
    createResponseFromDataFrame(df) // You should implement this
  }
}
</code></pre>

<p>However, for this approach to work well with Cassandra, you have to use the <a href=""https://github.com/datastax/spark-cassandra-connector"" rel=""nofollow"">spark-cassandra-connector</a> and then, to load the data you will have two options:</p>

<p><strong>1)</strong> Before calling this <code>ExecuteQuery</code> via REST, you have to transfer the full data you want to query from Cassandra to Spark. For that, you would do something like (<em>code adapted from the <a href=""https://github.com/datastax/spark-cassandra-connector/blob/master/doc/14_data_frames.md"" rel=""nofollow"">spark-cassandra-connector documentation</a></em>):</p>

<pre><code>val df = sqlContext
  .read
  .format(""org.apache.spark.sql.cassandra"")
  .options(Map( ""table"" -&gt; ""words"", ""keyspace"" -&gt; ""test""))
  .load() 
</code></pre>

<p>And then register it as a table in order to SparkSQL be able to access it:</p>

<pre><code>df.registerAsTempTable(""myTable"") // As a temporary table
df.write.saveAsTable(""myTable"") // As a persistent Hive Table
</code></pre>

<p>Only after that you would be able to use the <code>ExecuteQuery</code> to query from <code>myTable</code>.</p>

<p><strong>2)</strong> As the first option can be inefficient in some use cases, there is another option. </p>

<p>The spark-cassandra-connector has a special <a href=""http://datastax.github.io/spark-cassandra-connector/ApiDocs/1.6.0-M2/spark-cassandra-connector/#org.apache.spark.sql.cassandra.CassandraSQLContext"" rel=""nofollow"">CassandraSQLContext</a> that can be used to query C* tables directly from Spark. It can be used like:</p>

<pre><code>val cc = new CassandraSQLContext(sc)
val df = cc.sql(""SELECT * FROM keyspace.table ..."")
</code></pre>

<p>However, to use a different type of context with Spark-JobServer, you need to extend <code>SparkContextFactory</code> and use it in the moment of context creation (which can be done by a POST request to <code>/contexts</code>). An example of a special context factory can be seen on <a href=""https://github.com/spark-jobserver/spark-jobserver/blob/v0.6.2/job-server-extras/src/spark.jobserver/context/SQLContextFactory.scala"" rel=""nofollow"">SJS Gitub</a>. You also have to create a <code>SparkCassandraJob</code>, extending <code>SparkJob</code> (but this part is very <a href=""https://github.com/spark-jobserver/spark-jobserver/blob/v0.6.2/job-server-extras/src/spark.jobserver/SparkSqlJob.scala"" rel=""nofollow"">easy</a>).</p>

<p>Finally, the <code>ExecuteQuery</code> job have to be adapted to use the new classes. It would be something like:</p>

<pre><code>object ExecuteQuery extends SparkCassandraJob {
  override def validate(cc: CassandraSQLContext, config: Config): SparkJobValidation = {
    // Code to validate the parameters received in the request body
  }
  override def runJob(cc: CassandraSQLContext, jobConfig: Config): Any = {
    // Assuming your request sent a { ""query"": ""..."" } in the body:
    val df = cc.sql(config.getString(""query""))
    createResponseFromDataFrame(df) // You should implement this
  }
}
</code></pre>

<p>After that, the <code>ExecuteQuery</code>job can be executed via REST with a POST request.</p>

<hr>

<p><strong>Conclusion</strong></p>

<p>Here I use the first option because I need the advanced queries available in the <code>HiveContext</code> (window functions, for example), which are not available in the <code>CassandraSQLContext</code>. However, if you don't need those kind of operations, I recommend the second approach, even if it needs some extra coding to create a new ContextFactory for SJS.</p>
",['table']
37111429,37111559,2016-05-09 08:53:28,Query all and consistency,"<p>This is a question regarding the behavior of cassandra for a select * query.
It's more for understanding, I know that normaly I should not execute such a query.</p>

<p>Assuming I have 4 Nodes with RF=2.
Following table (column family):
create table test_storage (
    id text,
    created_on TIMESTAMP,
    location int,
    data text,
    PRIMARY KEY(id)
);
I inserted 100 entries into the table.</p>

<p>Now I do a select * from test_storage via cqlsh. Doing the query multiple times I get different results, so not all entries. When changing consistency to local_quorum I always get back the complete result. Why is this so? 
I assumed, despite from the performance, that I also get for consistency one all entries since it must query the whole token range.</p>

<p>Second issue, when I add a secondary index in this case to location, and do a query like select * from test_storage where location=1 I also get random results wiht consistency one. And always correct results when changing to consistency level local_quorum. Also here I don't understand why this happens?</p>
",<cassandra><nodes><nosql>,"<blockquote>
  <p>When changing consistency to local_quorum I always get back the complete result. Why is this so?</p>
</blockquote>

<p>Welcome to the eventual consistency world. To understand it, read my slides: <a href=""http://www.slideshare.net/doanduyhai/cassandra-introduction-2016-60292046/31"" rel=""nofollow"">http://www.slideshare.net/doanduyhai/cassandra-introduction-2016-60292046/31</a></p>

<blockquote>
  <p>I assumed, despite from the performance, that I also get for consistency one all entries since it must query the whole token range</p>
</blockquote>

<p>Yes, Cassandra will query all token ranges because of the non restricted <code>SELECT *</code> but it will only request data from one replicas out of 2 (RF=2)</p>

<blockquote>
  <p>and do a query like select * from test_storage where location=1 I also get random results wiht consistency one</p>
</blockquote>

<p>Same answer as above, native Cassandra secondary index is just using a Cassandra table under the hood to store the reverse-index so the same eventual consistency rules apply there too</p>
",['table']
37111861,37118370,2016-05-09 09:14:36,"Types MapColumn, SetColumn, JsonColumn needs owner and record. What actually are these values?","<p>For example, I have </p>

<pre><code>type MapColumn[Owner &lt;: com.websudos.phantom.dsl.CassandraTable[Owner, Record], Record, K, V] = 
    com.websudos.phantom.column.MapColumn[Owner, Record, K, V]
</code></pre>

<p><code>K</code> and <code>V</code> are obvious, but owner and record?  what should I input there?</p>
",<scala><cassandra><phantom-dsl>,"<p>The whole power of phantom is its ability to map around your data model and give you back type safe results, or so I had in mind when I wrote it. The <code>Owner</code> type param is the type of the table written by the user and its needed so you can do stuff like:</p>

<pre><code>select.where(_.id eqs id)
</code></pre>

<p>Looks pretty simple, but the trick is that without the refined type param through which the compiler can ""memorize"" which columns you have arbitrarily defined inside your table, you would never be able to ""know"" in the DSL code what columns the user writes.</p>

<p>So the DSL has to know the final type of the table you will create by extending <code>CassandraTable</code>.</p>

<pre><code>case class MyRecord(id: UUID, name: String)
class MyTable extends CassandraTable[MyTable, MyRecord] {
  object id extends UUIDColumn(this) with PartitionKey[UUID]
  // MyTable is Owner and MyRecord is Record.
  object mapColumn extends MapColumn[MyTable, MyRecord, String, String](this)
}
</code></pre>

<p>So that's why all the query builders are a function from a <code>table: Owner</code> to something else. Even the above is simply shorthand notation for:</p>

<pre><code>select.where(table =&gt; table.id eqs id)
</code></pre>

<p>The <code>Record</code> type is what makes the Cassandra results type safe. By telling your table what case class it wraps around, phantom is able to map all the results back to this case class using an implicit api approach, so instead of having to deal with things like:</p>

<pre><code>res.getString(""mystring"")
</code></pre>

<p>Such things are invisible dealt with under the hood, and phantom ""knows"" which results from a Cassandra row returned belong to which field inside the <code>case class</code>. It's immensely less verbose and more efficient, since you don't really want to care how the driver deals with its internal parsing of Netty buffers and CQL message exchanges between a client and the database, you just want your record back.</p>

<p>So the <code>Record</code> in combination with the <code>fromRow</code> method are needed, and they are passed around not only in those columns, but in every single column. The only difference is that with <code>StringColumn</code>, the compiler is able to infer the type of <code>T</code> and <code>R</code> for you so you don't have to type it.</p>

<p>This is because of:</p>

<pre><code>type StringColumn[
  Owner &lt;: CassandraTable[Owner, Record],
  Record
] = com.websudos.phantom.column.PrimitiveColumn[Owner, Record, String]
</code></pre>

<p>So in reality all columns need this. Collections require an extra parameter(or two in the case of maps), provided by the user, and because of that the compiler isn't able to infer the type like with <code>StringColumn</code> or <code>BooleanColumn</code>, so you need to type them by hand.</p>

<p>In phantom 1.26.0+, this has been changed, and the extra type parameters are now also invisible, so you will be able to type the following, without having to specify <code>Owner</code> and <code>Record</code>.</p>

<pre><code>object map extends MapColumn[String, String](this)
</code></pre>
",['table']
37114455,37119894,2016-05-09 11:19:35,Reading error in cassandra,"<p>I'm having a weir error trying to read data from a Cassandra table. I have a single-node installation, with the default setup. This is the query I'm making:</p>

<pre><code>  SELECT component_id,
         reading_1,
         reading_2,
         reading_3,
         date
  FROM component_readings
  WHERE park_id=2
        AND component_id IN (479)
        AND date &gt;= '2016-04-09+0000'
        AND date &lt;= '2016-05-08+0000';
</code></pre>

<p><code>component_readings</code> is a simple table, with no clustering conditions:</p>

<pre><code>CREATE TABLE component_readings (
    park_id int,
    component_id int,
    date timestamp,
    reading_1 decimal,
    reading_2 decimal,
    ...
    PRIMARY KEY ((park_id), component_id, date)
);
</code></pre>

<p>With some <code>component_id</code> values, it works, and with another values, it fails. This is the error I'm getting:</p>

<pre><code>cassandra.ReadFailure: code=1300 [Replica(s) failed to execute read] 
message=""Operation failed - received 0 responses and 1 failures""
info={'required_responses': 1, 'received_responses': 0, 'failures': 1,
'consistency': 'LOCAL_ONE'}
</code></pre>

<p>And the cassandra's system.log shows this error:</p>

<pre><code>ERROR [SharedPool-Worker-1] 2016-05-09 15:33:58,872 StorageProxy.java:1818 - 
Scanned over 100001 tombstones during query 'SELECT * FROM xrem.component_readings
WHERE park_id, component_id = 2, 479 AND date &gt;= 2016-04-09 02:00+0200 AND date &lt;=
2016-05-08 02:00+0200 LIMIT 5000' (last scanned row partion key was ((2, 479),
2016-05-04 17:30+0200)); query aborted
</code></pre>

<p>The weird thing is that I get the error only when making the query from an external program (via the python cassandra-connector). If I make it directly in the cqlsh shell, it works perfectly.</p>

<p>My installation was cassandra 2.2, but I've upgraded to 3.5, and I get the same error.</p>
",<cassandra>,"<p>You are exceeding the <code>tombstone_failure_threshold</code>. It defaults to 100'000. You can either </p>

<ul>
<li>increase the value in the cassandra.yaml or</li>
<li>clean up your tombstones</li>
</ul>

<p>To do the latter <a href=""https://docs.datastax.com/en/cql/3.3/cql/cql_reference/alter_table_r.html"" rel=""noreferrer"">alter</a> your table and set the gc_grace_seconds to 0:</p>

<pre><code>ALTER TABLE component_readings WITH GC_GRACE_SECONDS = 0;
</code></pre>

<p>Then trigger a compaction via the nodetool. This will flush out all tombstones.</p>

<p>In your particular scenario of a one-node-cluster you could leave the GC_GRACE_SECONDS at zero. But if you do, keep in mind to undo this if you ever want to use more than one node!</p>
",['table']
37116717,37119261,2016-05-09 13:12:43,"Cassandra. Bloom filter size is > 16GB, reduce the bloom_filter_fp_chance","<p>After restoring data to Cassandra cluster (1 node) I have error:</p>

<pre><code>ERROR [CompactionExecutor:7] 2016-05-09 08:05:38,621 CassandraDaemon.java:185 - Exception in thread Thread[CompactionExecutor:7,1,main]
java.lang.UnsupportedOperationException: Bloom filter size is &gt; 16GB, reduce the bloom_filter_fp_chance
        at org.apache.cassandra.utils.obs.OffHeapBitSet.&lt;init&gt;(OffHeapBitSet.java:40) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.utils.FilterFactory.createFilter(FilterFactory.java:85) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.utils.FilterFactory.getFilter(FilterFactory.java:78) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.io.sstable.format.big.BigTableWriter$IndexWriter.&lt;init&gt;(BigTableWriter.java:470) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.io.sstable.format.big.BigTableWriter.&lt;init&gt;(BigTableWriter.java:86) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.io.sstable.format.big.BigFormat$WriterFactory.open(BigFormat.java:107) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.io.sstable.format.SSTableWriter.create(SSTableWriter.java:84) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.db.compaction.writers.DefaultCompactionWriter.&lt;init&gt;(DefaultCompactionWriter.java:52) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.db.compaction.CompactionTask.getCompactionAwareWriter(CompactionTask.java:237) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:174) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:74) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:256) ~[apache-cassandra-2.2.5.jar:2.2.5]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_71]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_71]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_71]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_71]
        at java.lang.Thread.run(Thread.java:745) [na:1.8.0_71]
</code></pre>

<p>Any ideas, how may it be fixed?</p>

<p>Average size of sstable (db file) is 524MB.
Max. size is 167G</p>

<pre><code>Bloom filter false positives: 0
Bloom filter false ratio: 0.00000
Bloom filter space used: 8409389240
Bloom filter off heap memory used: 59948996312
</code></pre>
",<cassandra><bloom-filter>,"<p>An approximation formula for bloom filter false positive chance vs space is:</p>

<p>m = n * ln(1/fpc)/ln(2)²</p>

<p>m = size in bytes
n = number of distinct partition keys
fpc = bloom filter false positive chance</p>

<p>see <a href=""http://www.slideshare.net/doanduyhai/cassandra-data-structures-and-algorithms/57"" rel=""noreferrer"">http://www.slideshare.net/doanduyhai/cassandra-data-structures-and-algorithms/57</a> for the maths details</p>

<p>To have a bloom filter of 16Gb in term of size, you probably have <strong>a lot</strong> of partitions in a single SSTable (and probably very big SSTable).</p>

<p>Can you please:</p>

<ol>
<li>give the avg and max size of your SSTables ?</li>
<li>give the bloom filter fp chance configured on the table which is causing issue (use <code>nodetool cfstats</code> or <code>nodetool tablestats</code>)</li>
</ol>
",['table']
37157688,37157999,2016-05-11 08:58:35,InvalidRequestException(why:Unknown identifier,"<p>when I tried to save a table to <strong>cassandra</strong> using <strong>persist()</strong> method and <strong>kundera</strong> framework, i receive the error:</p>

<pre><code>28462 [Thread-15-localhostAMQPbolt0-executor[2 2]] INFO  d.d.pieceDAOImpl - start to insert data
28513 [Thread-15-localhostAMQPbolt0-executor[2 2]] INFO  c.i.c.c.CassandraClientBase - Returning cql query  INSERT INTO ""pieces""(""width"",""height"",""depth"",""IdPiece"") VALUES(10.0,11.0,12.0,'1') .
28543 [Thread-15-localhostAMQPbolt0-executor[2 2]] ERROR c.i.c.c.CassandraClientBase - Error while executing query  INSERT INTO ""pieces""(""width"",""height"",""depth"",""IdPiece"") VALUES(10.0,11.0,12.0,'1')
28544 [Thread-15-localhostAMQPbolt0-executor[2 2]] ERROR o.a.s.util - Async loop died!
java.lang.RuntimeException: com.impetus.kundera.KunderaException: com.impetus.kundera.KunderaException: InvalidRequestException(why:Unknown identifier IdPiece)
        at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:448) ~[storm-core-1.0.0.jar:1.0.0]
        at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:414) ~[storm-core-1.0.0.jar:1.0.0]
        at org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:73) ~[storm-core-1.0.0.jar:1.0.0]
        at org.apache.storm.daemon.executor$fn__8226$fn__8239$fn__8292.invoke(executor.clj:851) ~[storm-core-1.0.0.jar:1.0.0]
        at org.apache.storm.util$async_loop$fn__554.invoke(util.clj:484) [storm-core-1.0.0.jar:1.0.0]
        at clojure.lang.AFn.run(AFn.java:22) [clojure-1.7.0.jar:?]
        at java.lang.Thread.run(Thread.java:745) [?:1.7.0_99]
Caused by: com.impetus.kundera.KunderaException: com.impetus.kundera.KunderaException: InvalidRequestException(why:Unknown identifier IdPiece)
        at com.impetus.kundera.persistence.EntityManagerImpl.persist(EntityManagerImpl.java:180) ~[project-0.0.1-SNAPSHOT-jar-with-dependencies.jar:?]
        at database.dao.pieceDAOImpl.insert(pieceDAOImpl.java:54) ~[project-0.0.1-SNAPSHOT-jar-with-dependencies.jar:?]
        at database.controller.DatabaseController.saveSensorEntitie(DatabaseController.java:47) ~[project-0.0.1-SNAPSHOT-jar-with-dependencies.jar:?]
        at connector.bolt.PrinterBolt.execute(PrinterBolt.java:66) ~[project-0.0.1-SNAPSHOT-jar-with-dependencies.jar:?]
        at org.apache.storm.daemon.executor$fn__8226$tuple_action_fn__8228.invoke(executor.clj:731) ~[storm-core-1.0.0.jar:1.0.0]
        at org.apache.storm.daemon.executor$mk_task_receiver$fn__8147.invoke(executor.clj:463) ~[storm-core-1.0.0.jar:1.0.0]
        at org.apache.storm.disruptor$clojure_handler$reify__7663.onEvent(disruptor.clj:40) ~[storm-core-1.0.0.jar:1.0.0]
        at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:435) ~[storm-core-1.0.0.jar:1.0.0]
        ... 6 more
</code></pre>

<p>And im sur that my idpiece is the primary key of my table.</p>

<p>my table:</p>

<pre><code>CREATE TABLE mykeyspace.pieces (
    idpiece text PRIMARY KEY,
    depth double,
    height double,
    width double
) WITH bloom_filter_fp_chance = 0.01
    AND caching = '{""keys"":""ALL"", ""rows_per_partition"":""NONE""}'
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy'}
    AND compression = {'sstable_compression': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99.0PERCENTILE';
</code></pre>

<p>my class entity</p>

<pre><code>@Entity
@Table(name = ""pieces"", schema = ""mykeyspace@cassandra_pu"")
public class PieceEntitie implements Serializable{

    @Id
    private String IdPiece;
    @Column
    private double width;
    @Column
    private double height;
    @Column
    private double depth;
</code></pre>

<p>how can i resolve this problem ?
thank you in advance</p>
",<java><cassandra><apache-storm><kundera>,"<p>In Cassandra <a href=""https://cassandra.apache.org/doc/cql3/CQL.html#identifiers"" rel=""nofollow"">quoted identifiers are case-sensitive</a>.</p>

<p>So your column name <code>""IdPiece""</code> is different then actual column <code>idpiece</code> in your table <code>pieces</code></p>
",['table']
37191751,37218301,2016-05-12 15:46:34,Unable to add another node to existing node to form a cluster. Couldn't change num_tokens to vnodes,"<p>i have installed cassandra on two individual nodes both on Amazon.when i am trying to configure nodes to form a cluster the nodes. I am receiving the following error.</p>

<p><strong><em>ERROR [main] 2016-05-12 11:01:26,402  CassandraDaemon.java:381 - Fatal configuration error
org.apache.cassandra.exceptions.ConfigurationException: Cannot change the number of tokens from 1 to 256.</em></strong></p>

<p>I using these setting in  cassandra.yaml file</p>

<p>listen_address and rpc_address to : private Ip address</p>

<p>seeds : Public Ip [Elastic Ip address]</p>

<p>num_tokens: 256</p>
",<amazon-ec2><cassandra><datastax-enterprise>,"<p>This message usually appears when num_tokens is changed after the node has been bootstrapped.</p>

<p>The solution is:</p>

<ol>
<li>Stop Cassandra on all nodes</li>
<li>Delete the data directory (inc. datafiles, commitlog and saved_caches)</li>
<li>Double check that <code>num_tokens</code> is set to <code>256</code>, <code>initial_token</code> is commented out and <code>auto_bootstrap</code> is set to <code>true</code> in cassandra.yaml</li>
<li>Start Cassandra on all nodes</li>
</ol>

<p>This will wipe your existing cluster and cause the nodes to bootstrap from scratch again.</p>

<p>Cassandra doesn't support changing between vnodes and static tokens after a datacenter is bootstrapped. If you need to change from vnodes to static tokens or vice versa in an already running cluster, you'll need to create a second datacenter using the new configuration, stream your data across, and then decomission the original nodes.</p>
",['num_tokens']
37237596,37238543,2016-05-15 11:18:05,Change Cassandra datacenter name,"<p>Is it possible to change the datacenter name in a Cassandra Cluster?
If so how do I accomplish this? I have a Dev cluster which was built with the default DC name 'Cassandra'. I would like to change this because we are going to be setting up and testing replication between DCs.</p>
",<cassandra>,"<p>Its possible you can change the snitch to GossipingFilePropertySnitch and specify the dc name and rack name in <code>cassandra-rackdc.properties</code> file, After doing that you need to restart the node, in that case you will get an error like :</p>

<pre><code>Error: Cannot start node if snitch's data center (&lt;new-datacentername&gt;) differs from previous data center (&lt;old-datacenter-name&gt;). 
Please fix the snitch configuration, decommission and rebootstrap this node or use the flag -Dcassandra.ignore_dc=true.
</code></pre>

<p>In order to avoid this need to add the below line in <code>cassandra-env.sh</code> file and restart the node. </p>

<pre><code>JVM_OPTS=\""$JVM_OPTS -Dcassandra.ignore_rack=true -Dcassandra.ignore_dc=true\""'
</code></pre>

<p>Remember, you will need downtime for your cluster to restart your datacenter in this case if doing this on production environment.</p>
","['rack', 'dc']"
37417176,37419534,2016-05-24 14:57:05,Cassandra modeling pattern,"<p>I'm new to Cassandra, and I've been reading all I can and experimenting.</p>

<p>I've come across documentation that say you can can create 1 table per query, if you like.  So if I have a ""Customer"" record that has 4 different fields that I need to query by, then I can to create 4 different tables to do that.</p>

<p>Then I came across a feature called a ""Batch"" which seems say that I can make 4 updates happen transactionally if I put them in a batch.</p>

<p>But I can't find anything clear in the documentation that pulls all of the pieces together and says ""You SHOULD create 1 table per query, and you SHOULD use a Batch to keep all of those query tables in sync.  This is the best practice.""</p>

<p>Is this the best practice?  For a newbie, I could do with a little less ""CAN"" and a little more ""SHOULD""  :)</p>
",<cassandra><data-modeling>,"<p>Have you considered using materialized views?  This is a new feature in Cassandra 3.0 that could meet your use case nicely, you can have a base table and then create a view off that table for each query.  For example, using <a href=""http://www.datastax.com/dev/blog/materialized-view-performance-in-cassandra-3-x"" rel=""noreferrer"">this blog post</a> as an example:</p>

<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE users (
    id uuid PRIMARY KEY,
    username text,
    email text,
    age int
 );

CREATE MATERIALIZED VIEW users_by_name AS 
    SELECT * FROM users 
    WHERE username IS NOT NULL
    PRIMARY KEY (username, id);
</code></pre>

<p>When you insert data into users, the data is also propagated to views.  However, it's not exactly transactional (getting a successful response for the write does not mean it's been propagated to the views yet, but they will be eventually), but it may reduce burden on the client side and should take care of any concerns about the tables/views being in sync.</p>
",['table']
37422324,37422499,2016-05-24 19:32:28,CQL Cassandra Missing mandatory PRIMARY KEY part id,"<p>I'm trying to create a record and I'm getting this error.</p>

<pre><code>[40] pry(main)&gt; a = TopicPost.new(topic_id: ""professional_safety"", pub_date: ""5a612420-21e2-11e6-bdf4-0800200c9a66"")
=&gt; #&lt;TopicPost topic_id: ""professional_safety"", pub_date: 5a612420-21e2-11e6-bdf4-0800200c9a66, publisher: nil, author_id: nil, id: nil, message: nil, name: nil, link: nil, shared_count: nil, prospect_score: nil, indico_score: nil, keywords: #&lt;Set: {}&gt;, updated_at: nil, created_at: nil, profile_image: nil, bio: nil, account_link: nil, twitter_handle: nil, favorite_count: nil, followers_count: nil, klout_score: nil, has_engagements: nil, is_retweet: nil, engaged_influencer_ids: #&lt;Set: {}&gt;, urls: #&lt;Set: {}&gt;, images: #&lt;Set: {}&gt;, feed_name: nil, feed_url: nil, description: nil, title: nil, batch_id: nil, score: nil, fcm: nil, scorelog: {}&gt;
[41] pry(main)&gt; a.save
Cql::QueryError: Missing mandatory PRIMARY KEY part id
from /home/blau08/.rvm/gems/ruby-2.1.2/gems/cql-rb-2.0.4/lib/cql/client/client.rb:545:in `execute'
[42] pry(main)&gt; 
</code></pre>
",<ruby-on-rails><cassandra><cequel>,"<p>What does your table structure look like?</p>

<p>I'm guessing that you have a composite PRIMARY KEY, and the <code>id</code> column is a part of it.  Cassandra PRIMARY KEYs are unique, so you must provide the complete key for an upsert.</p>

<p>In looking at your code, specifically the commented-out portion, I do see a clue:</p>

<pre><code>[40] pry(main)&gt; a = TopicPost.new(topic_id: ""professional_safety"", pub_date: ""5a612420-21e2-11e6-bdf4-0800200c9a66"")
=&gt; #&lt;TopicPost topic_id: ""professional_safety"", pub_date: 5a612420-21e2-11e6-bdf4-0800200c9a66, publisher: nil, author_id: nil, id: nil, message: nil, name: nil, link: nil, shared_count: nil, prospect_score: nil, indico_score: nil, keywords: #&lt;Set: {}&gt;, updated_at: nil, created_at: nil, profile_image: nil, bio: nil, account_link: nil, twitter_handle: nil, favorite_count: nil, followers_count: nil, klout_score: nil, has_engagements: nil, is_retweet: nil, engaged_influencer_ids: #&lt;Set: {}&gt;, urls: #&lt;Set: {}&gt;, images: #&lt;Set: {}&gt;, feed_name: nil, feed_url: nil, description: nil, title: nil, batch_id: nil, score: nil, fcm: nil, scorelog: {}&gt;
</code></pre>

<p>You don't appear to be providing a value for <code>id</code> in your <code>TopicPost.new</code> statement, but I do see it in your comment:</p>

<pre><code>author_id: nil, id: nil, message: nil
</code></pre>

<p>Try providing a value for <code>id</code>.  Of course, without seeing your PRIMARY KEY definition, there could be other required columns that you are not providing a value for, but start there.</p>
",['table']
37435410,37440683,2016-05-25 11:12:39,CQL get dynamic column value,"<p>I have C* 2.1.9 cluster. For it management I used thrift based driver. Also I have table <code>users</code> with <code>notes</code> dynamic created column in it. I can sure that with column really exists by <code>cassandra-cli</code>:</p>

<pre><code>    list users

    RowKey: bla-bla
    ...
    =&gt; (name=notes, value=bla-bla, timestamp=bla-bla)
    ...
</code></pre>

<p>I want to get with dynamic column value using CQL. But static CQL schema knows nothing about this column. For example:</p>

<pre><code>SELECT notes FROM users;

SyntaxException: &lt;ErrorMessage code=2000 [Syntax error in CQL query] message=""line 1:7 no viable alternative at input 'notes' (SELECT [notes]...)""&gt;
</code></pre>

<p>How can I get value of this dynamic column using CQL?</p>
",<cassandra><cql>,"<p>I added new column to my table and all work now!</p>

<pre><code>ALTER TABLE users ADD notes text
</code></pre>
",['table']
37437665,37441003,2016-05-25 12:48:22,Error while trying to connect cassandra database using spark streaming,"<p>I'm working in a project which uses Spark streaming, Apache kafka and Cassandra. 
I use streaming-kafka integration. In kafka I have a producer which sends data using this configuration: </p>

<p><code>props.put(""metadata.broker.list"", KafkaProperties.ZOOKEEPER);
props.put(""bootstrap.servers"", KafkaProperties.SERVER);
props.put(""client.id"", ""DemoProducer"");</code></p>

<p>where <code>ZOOKEEPER = localhost:2181</code>, and <code>SERVER = localhost:9092</code>.</p>

<p>Once I send data I can receive it with spark, and I can consume it too. My spark configuration is:</p>

<pre><code>SparkConf sparkConf = new SparkConf().setAppName(""org.kakfa.spark.ConsumerData"").setMaster(""local[4]"");
sparkConf.set(""spark.cassandra.connection.host"", ""localhost"");
JavaStreamingContext jssc = new JavaStreamingContext(sparkConf, new Duration(2000));
</code></pre>

<p>After that I am trying to store this data into cassandra database. But when I try to open session using this:</p>

<pre><code>CassandraConnector connector = CassandraConnector.apply(jssc.sparkContext().getConf());
Session session = connector.openSession();
</code></pre>

<p>I get the following error:</p>

<pre><code>Exception in thread ""main"" com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: localhost/127.0.0.1:9042 (com.datastax.driver.core.exceptions.InvalidQueryException: unconfigured table schema_keyspaces))
at com.datastax.driver.core.ControlConnection.reconnectInternal(ControlConnection.java:220)
at com.datastax.driver.core.ControlConnection.connect(ControlConnection.java:78)
at com.datastax.driver.core.Cluster$Manager.init(Cluster.java:1231)
at com.datastax.driver.core.Cluster.getMetadata(Cluster.java:334)
at com.datastax.spark.connector.cql.CassandraConnector$.com$datastax$spark$connector$cql$CassandraConnector$$createSession(CassandraConnector.scala:182)
at com.datastax.spark.connector.cql.CassandraConnector$$anonfun$2.apply(CassandraConnector.scala:161)
at com.datastax.spark.connector.cql.CassandraConnector$$anonfun$2.apply(CassandraConnector.scala:161)
at com.datastax.spark.connector.cql.RefCountedCache.createNewValueAndKeys(RefCountedCache.scala:36)
at com.datastax.spark.connector.cql.RefCountedCache.acquire(RefCountedCache.scala:61)
at com.datastax.spark.connector.cql.CassandraConnector.openSession(CassandraConnector.scala:70)
at org.kakfa.spark.ConsumerData.main(ConsumerData.java:80)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)
</code></pre>

<p>Regarding to cassandra, I'm using default configuration:</p>

<pre><code>start_native_transport: true
native_transport_port: 9042
- seeds: ""127.0.0.1""
cluster_name: 'Test Cluster'
rpc_address: localhost
rpc_port: 9160
start_rpc: true
</code></pre>

<p>I can manage to connect to cassandra from the command line using cqlsh localhost, getting the following message:</p>

<pre><code>Connected to Test Cluster at 127.0.0.1:9042. [cqlsh 5.0.1 | Cassandra 3.0.5 | CQL spec 3.4.0 | Native protocol v4] Use HELP for help. cqlsh&gt; 
</code></pre>

<p>I used nodetool status too, which shows me this:</p>

<p><a href=""http://pastebin.com/ZQ5YyDyB"" rel=""nofollow"">http://pastebin.com/ZQ5YyDyB</a></p>

<p>For running cassandra I invoke <code>bin/cassandra -f</code></p>

<p>What I am trying to run is this:</p>

<pre><code>try (Session session = connector.openSession()) {
        System.out.println(""dentro del try"");
        session.execute(""DROP KEYSPACE IF EXISTS test"");
        System.out.println(""dentro del try - 1"");
        session.execute(""CREATE KEYSPACE test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1}"");
        System.out.println(""dentro del try - 2"");
        session.execute(""CREATE TABLE test.users (id TEXT PRIMARY KEY, name TEXT)"");
        System.out.println(""dentro del try - 3"");
    }
</code></pre>

<p>My pom.xml file looks like that:</p>

<pre><code>&lt;dependencies&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
        &lt;artifactId&gt;spark-streaming_2.10&lt;/artifactId&gt;
        &lt;version&gt;1.6.1&lt;/version&gt;
    &lt;/dependency&gt;

    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
        &lt;artifactId&gt;spark-streaming-kafka_2.10&lt;/artifactId&gt;
        &lt;version&gt;1.6.1&lt;/version&gt;
    &lt;/dependency&gt;

    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
        &lt;artifactId&gt;spark-core_2.10&lt;/artifactId&gt;
        &lt;version&gt;1.6.1&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;com.datastax.spark&lt;/groupId&gt;
        &lt;artifactId&gt;spark-cassandra-connector-java_2.10&lt;/artifactId&gt;
        &lt;version&gt;1.6.0-M1&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;com.datastax.spark&lt;/groupId&gt;
        &lt;artifactId&gt;spark-cassandra-connector_2.10&lt;/artifactId&gt;
        &lt;version&gt;1.6.0-M2&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;com.datastax.spark&lt;/groupId&gt;
        &lt;artifactId&gt;spark-cassandra-connector_2.10&lt;/artifactId&gt;
        &lt;version&gt;1.1.0-alpha2&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;com.datastax.spark&lt;/groupId&gt;
        &lt;artifactId&gt;spark-cassandra-connector-java_2.10&lt;/artifactId&gt;
        &lt;version&gt;1.1.0-alpha2&lt;/version&gt;
    &lt;/dependency&gt;

    &lt;dependency&gt;
        &lt;groupId&gt;org.json&lt;/groupId&gt;
        &lt;artifactId&gt;json&lt;/artifactId&gt;
        &lt;version&gt;20160212&lt;/version&gt;
    &lt;/dependency&gt;
&lt;/dependencies&gt;
</code></pre>

<p>I have no idea why I can't connect to cassandra using spark, is it configuration bad or what i am doing wrong?</p>

<p>Thank you!</p>
",<java><apache-spark><cassandra><apache-kafka><spark-streaming>,"<blockquote>
  <p>com.datastax.driver.core.exceptions.InvalidQueryException:
  unconfigured table schema_keyspaces)</p>
</blockquote>

<p>That error indicates an old driver with a new Cassandra version. Looking at the POM file, we find there the spark-cassandra-connector dependency declared twice.
One uses version <code>1.6.0-m2</code>  (GOOD) and the other <code>1.1.0-alpha2</code> (old).</p>

<p>Remove the references to the old dependencies <code>1.1.0-alpha2</code> from your config:</p>

<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;com.datastax.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-cassandra-connector_2.10&lt;/artifactId&gt;
    &lt;version&gt;1.1.0-alpha2&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;com.datastax.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-cassandra-connector-java_2.10&lt;/artifactId&gt;
    &lt;version&gt;1.1.0-alpha2&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
",['table']
37459832,37461673,2016-05-26 11:39:21,What are the standard ways to sync(copy) data between some tables in Cassandra?,"<p>I'm new in Cassandra, recently I watched very good <a href=""https://academy.datastax.com/courses/ds220-data-modeling"" rel=""nofollow"">tutorials</a> on DataStax that was about data modeling. </p>

<p>As I understood, in Cassandra we always have to have different tables for queries we want to have, for example even a simple query for sorting data by time or id.</p>

<p>It means we have to have some tables for each entity that has data according to query we want to have later. Imagine we have Videos, and we may have some tables for videos entity.</p>

<p><strong>First question,</strong> as I said for each query we must have a table, we are going to have different kind of sorts on video by different columns:</p>

<p>our columns for <strong>video</strong> table are:</p>

<pre><code>video_id  |  video_title  |  video_create_year  |  director  |  timestap
</code></pre>

<p>Now should we make other tables for other sorting we may need?</p>

<p><em>We may need to sort the table by director name (ASC | DESC), video_create_year (ASC | DESC), video_title (ASC | DESC)</em></p>

<p>I am not sure, Should we make different tables for each different sorting?</p>

<p>Such as:</p>

<pre><code>videos_by_diractor_asc
videos_by_diractor_desc
videos_by_title_asc
videos_by_title_desc
</code></pre>

<p>So on...</p>

<p>Did I understand it correctly?</p>

<p><strong>Second question,</strong> if I understood it correctly, then I forgot to make a table that I may needed in our website <em>(Imagine one day I get I forgot to have video_by_title_asc)</em> then what should I do? Should I write a program and copy whole data from video table? Or there are some ways in Cassandra to copy whole data if it is necessary?</p>

<p><em>I hope the question was not confusing.</em></p>
",<database><sorting><cassandra><data-modeling><datastax>,"<p>Okay, you're understanding Cassandra partially right.</p>

<p>I hope i understand you right. Your Primary Key of this tables would look like this:</p>

<pre><code>videos_by_diractor_asc PRIMARY KEY(director)
videos_by_title_asc PRIMARY KEY(title)
</code></pre>

<p>But in this case you forgot one thing: The partition key. The partition key is the first part of the primary key. I think, in your case, the year make sense. All rows with the same partition key are always on the same node. Cassandra split your rows by the partition key. The columns after the partition keys, called column keys, are sorted. The partition keys itself are not sorted. This means: node1 can have year 2015, 1998 and 1950 and node2 2010, 1990, 1577. Cassandra evenly distribute the data between the nodes. On modelling, you have to think about one important thing: What are the expected size of my table inside one partition key. This mean, in the video case, how many rows do you expect in one year? 2 Mio? 1 bln? If you will get more than 2bln rows x column, you will have a huge problem. 2bln is the maximum size of each partition key. But remember: It's the maximum. I recommend not more than 500mio. I calculate, in the worst case, with 500mio. </p>

<p>So now we can talk about the column keys. Yes, every sorting needs a new table. And you also need a new table if you want to access data in your WHERE conditions in different orders. 
One Example:
You have this primary key
PRIMARY KEY(year, director, title)</p>

<p>The first is the partition key. This means: You always need the year in your where condition. Then your data, with the same partition key, sorted, default in ASC, by director. After the director by title. In this case you can't use this WHERE condition: WHERE year = 2016 and title = 'whatever'</p>

<p>Okay, now i will answer your main question :)
The thing about the duplicated data. In Cassandra 3.0 you can use materialized views. Yes, it's a nice feature but it has his overhead. The best solution is to write a wrapper around cassandra. This wrapper only does one thing: It handles all this duplicated data. It knows what's the best way to access data if you need it sorted by title and then by director and not sorted by director and then by title. And one thing: Have no concerns to write data 5 or more times. Cassandra is optimized for writing. It's okay to write data. But don't forgot one thing: Cassandra is a database for known queries. If you know that you will need the data really often in this sorting order, create a table for it. But if you don't know it and you create this table only for the case when: Don't create a table. For this, sometimes queries, you can use spark or another solution. </p>

<p>And one more thing: If you need only to query data by one thing, like only by title, only by director, don't use cassandra for it. This is a main feature of a key value storage.</p>
",['table']
37480135,37923696,2016-05-27 09:43:58,Cassandra: Fixed number of rows in a table,"<p>I want to create a table with fixed number of rows (lets say N), where if N+1th row was added, then 1st row would be removed. </p>

<p>This is the table, I use for storage of last N best results from graph analysis:</p>

<pre><code>CREATE TABLE IF NOT EXISTS lp_registry.best (
    value float, // best value for current graph
    verts int,   // number of vertices in graph
    edges int,   // number of edges in graph
    wid text,    // worker id       
    id timeuuid, // timeuuid
    PRIMARY KEY (wid, id)
) WITH CLUSTERING ORDER BY (id ASC);
</code></pre>

<p>I've read about <a href=""https://docs.datastax.com/en/cql/3.1/cql/cql_using/use_expire_c.html"" rel=""nofollow"">expiring data at DataStax</a>, but found only TTL expirations. So I decided to do it in following way.</p>

<p><strong>My Approach A:</strong></p>

<p>Everytime a new result is wanted to be added, id of oldest row is retrieved..</p>

<pre><code>SELECT wid, id FROM lp_registry.best LIMIT 1;
</code></pre>

<p>..as well as current number of rows..</p>

<pre><code>SELECT COUNT(*) FROM FROM lp_registry.best;
</code></pre>

<p>Consequently if count >= N, then the oldest row is removed and the newest is added...</p>

<pre><code>BEGIN BATCH
    INSERT INTO lp_registry.best (value, verts, edges, wid, id) VALUES (?, ?, ?, ? now());
    DELETE FROM lp_registry.best WHERE wid = ? AND id = ?;
APPLY BATCH;
</code></pre>

<p>This approach has problem with that first selects are not atomic operations together with the following batch. So if any other worker deleted oldest row between select and batch, or N was exceeded, then this wouldn't work.</p>

<p><strong>My Approach B:</strong></p>

<p>Same first steps ...</p>

<pre><code>SELECT wid, id FROM lp_registry.best LIMIT 1;
SELECT COUNT(*) FROM FROM lp_registry.best;
</code></pre>

<p>Then try to delete oldest row again and again until success..</p>

<pre><code>if count &lt; N {
  INSERT INTO lp_registry.best (value, verts, edges, wid, id) VALUES (?, ?, ?, ? now());
} else { 
  while not success {
    DELETE FROM lp_registry.best WHERE wid = ? AND id = ? IF EXISTS;
  }
  INSERT INTO lp_registry.best (value, verts, edges, wid, id) VALUES (?, ?, ?, ? now());
}
</code></pre>

<p>In this approach there is still trouble with exceeding N in the database, before count &lt; N is checked. </p>

<p>Can you point me to the right solution?</p>
",<cassandra><cassandra-2.2>,"<p>Here is my solution. At first we need to create a table that will store current number of rows...</p>

<pre><code>CREATE TABLE IF NOT EXISTS row_counter (
  rmax int,  // maximum allowed number of rows
  rows int,  // current number of rows
  name text, // name of table
  PRIMARY KEY (name)
);
</code></pre>

<p>Then initialize it for a given fixed-rows tables:</p>

<pre><code>INSERT INTO row_counter (name, rmax, rows) 
VALUES ('best', 100, 0);
</code></pre>

<p>These are the statements used in the following code:</p>

<pre><code>q1 = ""SELECT rows, rmax FROM row_counter WHERE name = 'best'"";
q2 = ""UPDATE row_counter SET rows = ? WHERE name = 'best' IF rows &lt; ?"";
q3 = ""SELECT wid, id FROM best LIMIT 1"";
q4 = ""DELETE FROM best WHERE wid = ? AND id = ? IF EXISTS"";
q5 = ""INSERT INTO best (vertex, value, verts, edges, wid, id) VALUES (?, ?, ?, ?, ?, now())"";

selectCounter = session.prepare(q1);
updateCounter = session.prepare(q2);
selectOldBest = session.prepare(q3);
deleteOldBest = session.prepare(q4);
insertNewBest = session.prepare(q5);
</code></pre>

<p>Solution in Java:</p>

<pre><code>// Success indicator
boolean succ = false;

// Get number of registered rows in the table with best results
Row row = session.execute(selectCounter.bind()).one();
int rows = row.getInt(""rows"") + 1;
int rmax = row.getInt(""rmax"");

// Repeatedly try to reserve empty space in table
while (!succ &amp;&amp; rows &lt;= rmax) {
  succ = session.execute(updateCounter.bind(rows, Math.min(rows, rmax))).wasApplied();
  rows = session.execute(selectCounter.bind()).one().getInt(""rows"") + 1;
}

// If there is not empty space in table, repeatedly try to make new empty space
while (!succ) {
  row = session.execute(selectOldBest.bind()).one();
  succ = session.execute(deleteOldBest.bind(row.getString(""wid""), row.getUUID(""id""))).wasApplied();
}

// Insert new row
session.execute(insertNewBest.bind(vertex, value, verts, edges, workerCode));
</code></pre>
",['table']
37480186,37515887,2016-05-27 09:45:59,DataStax Community: Inconsistent reads,"<p>We're using datastax-community-64bit_2.2.6 and DevCenter-1.4.1-win-x86 on a Windows Server 2012 (and same setup on an older Win Server 2008, that does NOT seem to experience the problem).</p>

<p>We have a time series table that is behaving VERY oddly with inconsistent reads. We have a full day of data, but data for some hours in the day is NOT loaded, when we perform queries - both through code and through DevCenter, as seen on the following screenshot:::</p>

<p>devcenter lookups::: <a href=""https://drive.google.com/file/d/0B_e9YTMgramiSTFqUGFPYVB3bkk"" rel=""nofollow"">https://drive.google.com/file/d/0B_e9YTMgramiSTFqUGFPYVB3bkk</a></p>

<p>As can be seen - the hour 7-8 cannot be loaded directly - as the hour from 9-10 can.
Loading just the hour 7-8 while using >= and &lt; is possible (the top select), which just confuses matters even more.</p>

<p>In our application it gives a lot of the hours in the day as unknown (the icon with the ?) - as no data is loaded from Cassandra....see next screenshot::</p>

<p>missing hours in application::: <a href=""http://drive.google.com/open?id=0B_e9YTMgramiTUxfNTlJYlVwUEU"" rel=""nofollow"">http://drive.google.com/open?id=0B_e9YTMgramiTUxfNTlJYlVwUEU</a></p>

<p>The hours with a green icon are the same that we are able to query in DevCenter (as the hour 9-10 was on figure 2) - while the rest are not.
Making this even more cryptic is the fact that we load the same data for trend graphs where points for ALL hours are included.</p>

<p>Has anyone ever experienced anything like this??? ....it seems data for some hours are ALWAYS selectable from Cassandra, while others have issues :/
...and of course all data is inserted the same way!!</p>
",<cassandra><datastax><datastax-startup>,"<p>So, when you query it by time range, you get the data back. When you query it by exact time match, you do not get the data for some timestamps. Correct? If it is correct, it is most likely you have got your timestamps recorded with precision higher than a second. Querying by exact timestamp match is almost never a good idea, unless you know exact timestamp value up to the required precision.</p>
",['precision']
37552985,37569722,2016-05-31 18:21:45,How to model for repeated information on many records on cassandra,"<p>I have a massively huge table with hundreds of billions of records and I mean to add a field in this table of which the same value would be repeated for millions of records. I don't know how to efficiently model this in cassandra. Allow me to elaborate:</p>

<p>I have a generic table:</p>

<pre><code>CREATE TABLE readings (
    key int,
    key2 int,
    time timestamp,
    name text,
    PRIMARY KEY ((key, key2) time)
)
</code></pre>

<p>This table has 700.000.000+ records.
I want to create a field in this table, named <code>source</code>. This field indicates where the record was gotten from (since the software has many ways of receiving the information on the <code>reading</code> table). One possible value for this field is <code>""XML: path\to\file.xml""</code> or <code>""Direct import from the X database""</code> or even <code>""Manually added""</code>, I want this to be a descriptive field, used exclusively to allow later maintenance in the database where we want to manipulate only records from a given source.</p>

<p>The queries I want to run that I can't now are: </p>

<ul>
<li>Which records on the <code>readings</code> table were gotten from a given source?</li>
<li>What is the source of a given record?</li>
</ul>

<p>A solution would be for me to create a table such as:</p>

<pre><code>CREATE TABLE readings_per_source(
    source text,
    key int,
    key2 int,
    time timestamp,
    PRIMARY KEY (source, key, key2, time)
)
</code></pre>

<p>which would allow me to execute the first query, but would also mean that I would create 700.000.000+ new records on my database with a lot of information, which would take a lot of unnecessary storage space since tens of millions of these records would have the same value for <code>source</code>. </p>

<p>If this was a relational environment, I would create a <code>source_id</code> field on the <code>readings</code> table and a <code>source</code> table with <code>id (PK)</code> and <code>name</code> fields, that would mean storing only an additional integer for each row on the <code>readings</code> table and a new table with as many records as different sources there was.</p>

<p>How does one go about modelling this in cassandra?</p>
",<cassandra><modeling><nosql>,"<p>Your schema</p>

<pre><code>CREATE TABLE readings_per_source(
    source text,
    key int,
    key2 int,
    time timestamp,
    PRIMARY KEY (source, key, key2, time)
)
</code></pre>

<p>is a very bad idea because <code>source</code> is the partition key and you can have millions of records sharing the <strong>same</strong> source e.g. having a very very wide partition --> <strong>hot spots</strong></p>

<p>For you second query, <code>What is the source of a given record?</code> is it quite trivial if you access the data using the record primary keys (key, key2). The <code>source</code> column can be added as just a regular column into the table</p>

<p>For the first query <code>Which records on the readings table were gotten from a given source?</code> it is trickier. The idea here is to fetch all the records having the same source.</p>

<p><strong>Do you realize that this query can potentially return tens of millions of records</strong> ?</p>

<p>If it's what you want to do, there is a solution, use the new SASI secondary index (read my <a href=""http://www.doanduyhai.com/blog/?p=2058"" rel=""nofollow"">blog post</a> for all details) and create an index on the <code>source</code> column</p>

<pre><code>CREATE TABLE readings (
    key int,
    key2 int,
    time timestamp,
    name text,
    source text,
    PRIMARY KEY ((key, key2), time)
)

CREATE CUSTOM INDEX source_idx ON readings(source) 
USING 'org.apache.cassandra.index.sasi.SASIIndex'
WITH OPTIONS = {
     'mode': 'PREFIX', 
     'analyzer_class': 'org.apache.cassandra.index.sasi.analyzer.NonTokenizingAnalyzer',
     'case_sensitive': 'false'
};
</code></pre>

<p>Then to fetch all records having the same source, use <strong><a href=""https://github.com/datastax/java-driver/tree/3.0/manual/paging"" rel=""nofollow"">server-side paging</a></strong> feature of the Java driver (or any other Datastax driver)</p>
",['table']
37591435,37591944,2016-06-02 12:07:18,How to connect Node.js with Cassandra?,"<p>I've been trying to connect Node.js with Cassandra on localhost for 50 years (feel like it), but haven't figured out how they work together. I would appreciate any suggestion that could lead to solution.</p>

<p><strong>Project Directory:</strong></p>

<pre><code>project  - app - some files
        \- build - index.js, index.html, etc.(I start the server by ""node index.js"")
        \- node_modules - some modules
        \- signup - some files to be minified
        \- signin - some files to be minified
        \- apache-cassandra-3.0.6 - bin, conf, etc.(downloaded from tarball)
        \- package.json
        \- webpack.config.js
</code></pre>

<p>Webpack is working without any problem, so the problem doesn't exist in webpack configuration.<br>
I can insert data using cqlsh, so the problem isn't the model of data structure.</p>

<p>I believe the problem is my lack of knowledge about how to use Node.js and  Cassandra together.</p>

<p><strong>My process to connect Node.js with Cassandra:</strong></p>

<ol>
<li>go to build directory</li>
<li><code>node index.js</code></li>
<li>open localhost:3000 and find the home page, routed by express.js, displayed with no problem.</li>
<li>go to sign-up page and submit a form</li>
<li>error (no data inserted)</li>
</ol>

<hr>

<p>I'm not sure which contact point is correct, '127.0.0.1:9042' or '127.0.0.1'</p>

<pre><code>var client = new cassandra.Client({contactPoints:['127.0.0.1:9042']});
                   OR
var client = new cassandra.Client({contactPoints:['127.0.0.1']});
</code></pre>

<p>I set my router like this.</p>

<pre><code>var router = express.Router();
var insertUser = 'INSERT INTO keyspace.users (username, create_time, email, password) VALUES (?, ?, ?, ?);';

router.get""/"", function(req, res) {
  res.sendFile(__dirname + '/index.html'); // This is working.
});

router.post(""/signup"", function(req, res) {
  var username = req.body.username;
  var email = req.body.email;
  var password = req.body.password;
  client.execute(insertUser, [username, now(), email, password], 
  { prepare: true }, function(err) {
    if (err) {
      console.log(""error""); // I receive error.
    } else {
      console.log(""success"");
    }
  });
});
</code></pre>

<p>Should I keep the cassandra running in the background like this?</p>

<p><code>cd apache-cassandra-3.0.6</code> -> <code>cd bin</code> -> <code>./cassandra</code></p>

<p>What am I missing here?</p>
",<node.js><cassandra>,"<p>Based the error message you provided, it looks like you have <code>PasswordAuthenticator</code> authentication or some other authenticator set up on your Cassandra instance.   Check your 'authenticator' property in your cassandra.yaml file, is it set to <code>PasswordAuthenticator</code>, <code>AllowAllAuthenticator</code>, or something else?</p>

<p>If using <code>PasswordAuthenticator</code>, you can specify credentials by passing a <code>PlainTextAuthenticator</code> instance to your client options as <code>authProvider</code>:</p>



<pre><code>var PlainTextAuthProvider = cassandra.auth.PlainTextAuthProvider;
var client = new cassandra.Client({ contactPoints:['127.0.0.1:9042'], 
                                    authProvider: new PlainTextAuthProvider('cassandra', 'cassandra')};
</code></pre>

<p>The default user made available when using authentication is 'cassandra' with password 'cassandra', but you can change this by following <a href=""https://docs.datastax.com/en/cassandra/2.1/cassandra/security/security_config_native_authenticate_t.html"" rel=""noreferrer"">Configuring Authentication</a>.</p>
",['authenticator']
37631684,37788107,2016-06-04 15:02:58,Data Modelling in cassandra for IOT,"<p>We are trying to use Apache Cassandra in an IoT based application. We are planning to create a device  abstraction.  Any user shall be able to define a device with a series of  attributes. For each attribute, the user shall be able to define a series of properties  like  <strong>name , data type , minimum value , maximum value etc.</strong></p>

<p>Some examples of devices are given below</p>

<p><strong>Vehicle</strong></p>

<p>The vehicle can have the following attributes</p>

<ol>
<li>Speed [name :- speed , data type:- double , minimum value :- 0 , maximum value :-300]</li>
<li>Latitude [name :- speed , data :- double , minimum :- -90 , maximum :-90] </li>
<li>Longitude[name :- Longitude, data :- double , minimum :- -180 , maximum :- 180]</li>
</ol>

<p><strong>Temperature Sensor</strong></p>

<p>The temperature sensor can have the following attributes</p>

<ol>
<li>Current Temperature[name :- Current Temperation, data type:- double , minimum value :- 0 , maximum value :-300]</li>
<li>Unit  [name :- Unit , datatype:-string]</li>
</ol>

<p>In real time , each device will be sending data as key value pairs  . </p>

<p>For ex:- A <strong>Vehicle</strong> can send the following data </p>

<p>Time :- 6/4/2016 11:15:15.150 , Latitude : -1.256 , Longitude :- -180.75, Speed :- 50</p>

<p>Time :- 6/4/2016 11:15:16.150 , Latitude : -1.257 , Longitude :- -181.75, Speed :- 51</p>

<p>For ex:- A <strong>Temperature sensor</strong> can send the following data </p>

<p>Time :- 6/4/2016 11:15:15.150 , Current Temperature: 100, Unit : farenheit</p>

<p>Time :- 6/4/2016 11:15:16.150 , Latitude : 101 , Unit : farenheit</p>

<p>Since the attributes of different devices can be different , we are confused on how the model the tables in cassandra... Some of the options that came to mind are <strong>creating a table for a device, or create a single table and store the values in Map data types... We are little confused on which approach should be taken...
Any suggestions is appreciated</strong></p>
",<cassandra><time-series><device><iot>,"<p>I think the best option is to create only one table with a general purpose schema for collecting time-serie data.</p>

<p>Example CQL: </p>

<pre><code>CREATE TABLE timeline (
  device uuid,
  time timeuuid,
  key text,
  value blob,
  …
  PRIMARY KEY ((device, key), time)
);
</code></pre>

<p>Values can be store as <em>blob</em> (custom serialization), <em>map</em> or <em>numeric scalars</em>, depending on your application use case &amp; data access patterns (how to read/write/delete and if you plan to support to <strong>updates</strong>).</p>

<p>FYI useful related Datastax posts about time-series modeling:</p>

<ul>
<li><a href=""https://academy.datastax.com/resources/getting-started-time-series-data-modeling"" rel=""nofollow"">https://academy.datastax.com/resources/getting-started-time-series-data-modeling</a></li>
<li><a href=""http://www.datastax.com/dev/blog/advanced-time-series-data-modelling"" rel=""nofollow"">http://www.datastax.com/dev/blog/advanced-time-series-data-modelling</a></li>
<li><a href=""http://www.datastax.com/dev/blog/advanced-time-series-with-cassandra"" rel=""nofollow"">http://www.datastax.com/dev/blog/advanced-time-series-with-cassandra</a></li>
</ul>
",['table']
37654471,37673270,2016-06-06 09:57:09,How to update a single value in a Set Data type in Cassandra,"<p>How to update a single value in a Set Data type in Cassandra. For example below is my set values in Cassandra. Let say the column name is column1 and value is</p>

<pre><code>{'418_3', '521_4', '523_6'}
</code></pre>

<p>I want to update the value <code>'523_6'</code> to <code>'523_4'</code> so the updated value in Cassandra will be</p>

<pre><code>{'418_3', '521_4', '523_4'}
</code></pre>
",<cassandra>,"<p><code>Update table set column1=column1-{'523_6'},cloumn1=column1+{'523_4'} where id='Your Id'</code></p>

<p>The above query will remove the value from set and add another.</p>
",['table']
37656539,37657701,2016-06-06 11:43:02,Want to get count and cityname for all the rows in a table in cassandra,"<p>Any other process to get all the count with the distinct values from a table in cassandra like a group function in MySQL or Oracle</p>

<p>Example following is a table with cities:</p>

<blockquote>
  <p>cityid cityname<br>
  1      Lucknow<br>
  1      Lucknow<br>
  3      Delhi<br>
  4      Noida<br>
  5      Agra<br>
  5      Agra<br>
  5      Agra  </p>
</blockquote>

<p>I want the following output produced in cassandra:</p>

<blockquote>
  <p>count  Cityname<br>
   2    Lucknow<br>
   1    Noida<br>
   1    Delhi<br>
   3    Agra     </p>
</blockquote>
",<cassandra>,"<p>Function count(*) does not exist in cassandra. Alternativelly you can use counter <a href=""https://docs.datastax.com/en/cql/3.1/cql/cql_using/use_counter_t.html"" rel=""nofollow"">https://docs.datastax.com/en/cql/3.1/cql/cql_using/use_counter_t.html</a>.</p>

<pre><code>create table cities(
   cityname text,
   count counter,
PRIMARY KEY(cityname)
);
</code></pre>

<p>For updating the table use:</p>

<pre><code>  UPDATE cities SET count = count + 1 WHERE cityname ='Lucknow';
</code></pre>
",['table']
37664283,37819770,2016-06-06 18:20:34,Social media's like and unlike data model in Cassandra,"<p>Imagine there is a social network and here is a table for storing the like (favorite) action and unlike that is deleting from this table:</p>

<pre><code>CREATE TABLE IF NOT EXISTS post_likes(
  post_id timeuuid,
  liker_id uuid, //liker user_id
    like_time timestamp,
    PRIMARY KEY ((post_id) ,liker_id, like_time)
) WITH CLUSTERING ORDER BY (like_time DESC);
</code></pre>

<p>The above table has problem in Cassandra because when <code>liker_id</code> is the first <code>clustering_key</code>, we can't sort by the second clustering key which is <code>like_time</code>.</p>

<p>We need to sort our tables data by <code>like_time</code>, we use it when a user wants to see who liked this post and we show list of people who liked that post that sorted by time <em>(<code>like_time DESC</code>)</em></p>

<p>and we also need to delete (unlike) and we again need to have <code>post_id</code> and <code>liker_id</code></p>

<p><strong>What is your suggestion? How we can sort this table by <code>like_time</code>?</strong></p>
",<database><cassandra><datastax><datastax-enterprise><datastax-startup>,"<p><em>After more researches, I found out this solution:</em>
Picking the right data model is the hardest part of using Cassandra and here is the solution we found for likes tables in Cassandra, first of all, I have to say Cassandra's <a href=""https://docs.datastax.com/en/cassandra/1.2/cassandra/dml/dml_about_read_path_c.html"" rel=""nofollow"">read</a> and <a href=""https://wiki.apache.org/cassandra/WritePathForUsers"" rel=""nofollow"">write</a> path is amazingly fast and you don't need to be worry about writing  on your Cassandra's tables, you need to <strong>model around your queries</strong> and <em>remember, data duplication is okay. Many of your tables may repeat the same data.</em> and do not forget to <strong>spread data evenly around the cluster</strong> and <strong>minimize the number of partitions read</strong></p>

<p>Since we are using Cassandra which is NoSQL, we know one of the rules in NoSQLs is denormalization and we have to denormalize data and just think about the queries you want to have; Here for the like table data modeling we will have two tables, these tables have mainly focused on the easy read or easier to say <strong>we have focused on queries we want to have</strong>:</p>

<pre><code>CREATE TABLE IF NOT EXISTS post_likes(
    post_id timeuuid,
    liker_id uuid, //liker user_id
    like_time timestamp,
    PRIMARY KEY ((post_id) ,liker_id)
);

CREATE TABLE IF NOT EXISTS post_likes_by_time(
    post_id timeuuid,
    liker_id uuid, //liker user_id
    like_time timestamp,
    PRIMARY KEY ((post_id), like_time, liker_id)
) WITH CLUSTERING ORDER BY (like_time DESC);
</code></pre>

<p>When a user like a post, we just insert into both above tables.</p>

<hr>

<p><strong>why do we have <code>post_likes_by_time</code> table?</strong></p>

<p>In a social network, you should show list of users who liked a post, it is common that you have to sort likes by the <code>like_time DESC</code> and since you are going to sort likes by <code>like_time</code> you need to have <code>like_time</code> as clustering key to be able to sort likes by time.</p>

<p><strong>Then why do we have <code>post_likes</code> table too?</strong></p>

<p>In the <code>post_likes_by_time</code>, our clustering key is <code>like_time</code>, we also need to <strong>remove</strong> one like! We can't do that when we sorted data in our table when clustering key is <em>like_time</em>. That is the reason we also have <code>post_likes</code> table</p>

<p><strong>Why you could not only have one table and do both actions, sorting and removing on it?</strong></p>

<p>To delete one like from <code>post_likes</code> table we need to provide <code>user_id</code> (here liker_id) and <code>post_id</code> (together) and in <code>post_likes_by_time</code> we have <code>like_time</code> as clustering key and we need to sort table by <code>like_time</code>, then it should be the first clustering key and the second clustering key could be <code>liker_id</code>, and here is the point!  <code>like_time</code> is the first clustering key then for selecting or deleting by <code>liker_id</code> you also need to provide <code>like_time</code>, but you do not have <code>like_time</code> most of the times.</p>
",['table']
37670431,37682002,2016-06-07 04:21:30,RDD joinWithCassandraTable,"<p>Can anyone please help me on the below query.
I have an RDD with 5 columns. I want to join with a table in Cassandra.
I knew that there is a way to do that by using ""joinWithCassandraTable""</p>

<p>I see somewhere a syntax to use it.
Syntax:
RDD.joinWithCassandraTable(KEYSPACE, tablename, SomeColumns(""cola"",""colb""))
 .on(SomeColumns(""colc""))</p>

<p>Can anyone please send me the correct syntax??</p>

<p>I would like to actually know where to mention the column name of a table which is a key to join.</p>
",<apache-spark><cassandra><spark-cassandra-connector>,"<p>JoinWithCassandraTable works by pulling only the partition keys which match your RDD entries from C* so it only works on partition keys.</p>

<p>The documentation is here
<a href=""https://github.com/datastax/spark-cassandra-connector/blob/master/doc/2_loading.md#using-joinwithcassandratable"" rel=""nofollow"">https://github.com/datastax/spark-cassandra-connector/blob/master/doc/2_loading.md#using-joinwithcassandratable</a></p>

<p>and API Doc is here</p>

<p><a href=""http://datastax.github.io/spark-cassandra-connector/ApiDocs/1.6.0-M2/spark-cassandra-connector/#com.datastax.spark.connector.RDDFunctions"" rel=""nofollow"">http://datastax.github.io/spark-cassandra-connector/ApiDocs/1.6.0-M2/spark-cassandra-connector/#com.datastax.spark.connector.RDDFunctions</a></p>

<p>The jWCT table method can be used without the fluent api by specifying all the arguments in the method</p>

<pre><code>def joinWithCassandraTable[R](
  keyspaceName: String, 
  tableName: String, 
  selectedColumns: ColumnSelector = AllColumns, 
  joinColumns: ColumnSelector = PartitionKeyColumns)
</code></pre>

<p>But the fluent api can also be used</p>

<pre><code>joinWithCassandraTable[R](keyspace, tableName).select(AllColumns).on(PartitionKeyColumns)
</code></pre>

<p>These two calls are equivalent</p>

<p>Your example</p>

<pre><code>RDD.joinWithCassandraTable(KEYSPACE, tablename, SomeColumns(""cola"",""colb"")) .on(SomeColumns(""colc""))
</code></pre>

<p>Uses the Object from <code>RDD</code> to join against <code>colc</code> of <code>tablename</code> and only returns <code>cola</code> and <code>colb</code> as join results.</p>
",['table']
37671714,37672494,2016-06-07 06:09:02,Cassandra backup: plain copy disk files vs snapshots,"<p>We are planning to deploy a Cassandra cluster with 100 virtual nodes.
To store maximally 1TB (compressed) data on each node. We're going to use (host-) local SSD disks.
<BR><BR>
The infrustructure team is used to plainly backing up the whole partitions.
We've come across Cassandra Snapshots.
<BR><BR>
What is the difference between <strong>plainly copying the whole disk</strong> vs. <strong>Cassandra snapshots</strong>?
<BR><BR>
 - Is there a <strong>size</strong> difference? <BR><BR>
 - Using whole partition backups, also <strong>unnecessarily</strong> saves uncompressed data that are being <strong>compacted</strong>, is that the motive behind snapshots?<BR><BR></p>
",<cassandra>,"<p>There are few benefits of using snapshots:</p>

<ol>
<li>Snapshot command will flush the memtable to ssTables and then creates snapshots.</li>
<li>Nodetool can be used to restore the snapshots.</li>
<li>Incremental backup functionality can also be leveraged.</li>
<li>Snapshots create hardlink of your data so it is much faster.</li>
</ol>

<p><strong>Note:</strong> Cassandra can only restore data from a snapshot when the table schema exists. It is recommended that you also backup the schema.
In both it is to be made sure that operation (snapshot or plain copy) run at same time on all the nodes.</p>
",['table']
37734327,37737891,2016-06-09 19:09:56,How can I get a row count in Cassandra with only CQL?,"<p>I would like our developers to be able to tell how many rows (roughly) are in a table.</p>

<p>Doing ""select count (*) from table"" doesn't work because you get a timeout error unless the table is very small (under a million rows).</p>

<p>Increasing the client_timeout in ~/.cassandra/cqlshrc has never worked for us, and even if it did it's probably not a good idea to let developers run 10 minute queries against production.  :)</p>

<p>And since this a production cluster, developers do not have ssh access to the servers to run ""nodetool"" locally on the servers.</p>

<p>Running ""nodetool"" remotely requires enabling remote JMX and applying JMX security.  That seems a little much just to get an estimate of a table size.  Is there anything dangerous someone could do with that JMX access?</p>

<p>Are there any other options at all to get any kind of estimate of the number of rows in a table?</p>

<p>Thanks!  </p>
",<cassandra>,"<p>You could try exporting the table to a csv then just doing a line count of the csv. This isn't ideal performance-wise, but it won't time out like count does.</p>

<pre><code>COPY table_name TO filename_or_path.csv;
</code></pre>

<p>For more information on copy see: <a href=""http://docs.datastax.com/en/cql/3.1/cql/cql_reference/copy_r.html"" rel=""nofollow"">http://docs.datastax.com/en/cql/3.1/cql/cql_reference/copy_r.html</a></p>

<p>Another, more ""estimative"" option if you are running datastax cassandra is to install OpsCenter on to one of the nodes, which will create a UI you can expose to the developers that has a lot of useful metrics and health status. I don't think it has number or rows directly, but it does have the amount of data which could be divided by the average row size if you know this value.</p>

<p><a href=""http://www.datastax.com/products/datastax-enterprise-visual-admin"" rel=""nofollow"">http://www.datastax.com/products/datastax-enterprise-visual-admin</a></p>

<p>*EDIT: I just realized the link above is for the enterprise edition of OpsCenter, but does include a description of it. There is (or at least was) a community edition available as well.</p>
",['table']
37933885,37948563,2016-06-21 00:35:09,Phantom vs Quill for Playframework (Scala) and Cassandra,"<p>I am currently looking at using Cassandra as my database in a PlayFramework project. I was looking for a reactive driver and it seems my choices are limited to Phantom and Quill. 
My experience with nosql databases is limited to MongoDB and I have not worked with any of Quill or Phantom before. </p>

<p>Looking at the <a href=""https://github.com/getquill/quill/blob/master/CASSANDRA.md"" rel=""nofollow"">comparison here</a> , it seems one may end up writing more code in Phantom. Moreover, using a DSL to describe models seems counter-intuitive (coming from a heavy hibernate/JPA background) - but that could just be me.</p>

<p>I was wondering if someone can provide practical advice/use cases where one would excel over the other and things to watch out for in each?</p>
",<cassandra><playframework-2.5><phantom-dsl>,"<p>In a slightly biased view as the author of phantom, I have a very strong grasp of the design goals in phantom. There is an existing comparison between Quill and Phantom available on the Quill library website, which is naturally biased in the other direction.</p>

<p>Phantom aims to be the perfect choice for an application level layer, where as Quill aims to be the fanciest string generator, which is not a very useful comparison when you build large apps on top of Cassandra.</p>

<p><strong>Pros of using phantom</strong></p>

<ul>
<li><p>With respect to typesafety and how well retrofitted the DSL is to Cassandra features there is really no contest. The DSL has very ""intimate"" knowledge of your data structures and provides complete support for Cassandra features. It knows at compile time what is possible with respect to Cassandra and what isn't.</p></li>
<li><p>Quill devs argue phantom has a lot more dependencies, but that's not entirely accurate, as most of those are optional, including things like Play iteratees and streams support. What you don't want you don't get, simple as that.</p></li>
<li><p>The Quill comparison simply states: ""You could extend Phantom by extending the DSL to add new features, although it might not be a straightforward process."", which is a bit inaccurate. Being a very new player in the game, Quill is a toy when it comes to Cassandra feature support and you will often find yourself needing to add features. Phantom has its gaps without a doubt, but it's a far far more mature alternative, and the amount of times when extension is required are significantly rarer.</p></li>
<li><p>We have addressed most bugs within a number of days or weeks for more complex features, but generally everything you may require is already there, with a number of features currently not found in Quill that would take me hours even to write down.</p></li>
<li><p>I don't come from a strong JPA background, but the mapping between Cassandra and phantom is an extremely powerful layer, since it allows you to auto-generate the entire schema of the tables directly from the mapping DSL. It also allows the DSL to fully mimic Cassandra's behaviour at compile time, it will know which queries are possible with respect to your choice of a primary key and so on, quill has no such support at all.</p></li>
<li><p>Phantom has very powerful application level abstraction layers, such as connectors, databases, auto-generation of databases, things that help you run applications into productions.</p></li>
<li><p>The code behind Quill is far more complex, and while I would be the first to give strong credit to the engineering ability behind it, when I think user friendliness the story doesn't hold up quite as well.</p></li>
<li><p>Quill tries to do a lot more in one go. It's a mini engine for generation, something scalaquery tried to do some years back before they decided to focus entirely on SQL dbs and dropped supports for anything else. It's the precursor to the modern day Slick, and uses a similar QDSL quoted approach.</p></li>
<li><p>Quill is a leaking abstraction. As they aim to support a broader range of databases, they have vastly inferior support for the particularities of db specifics. An example is below:</p></li>
</ul>

<p>From the very basic examples in the comparison you read:</p>

<pre><code>val getAllByCountry = quote {
  (country: String) =&gt; query[WeatherStation]
     .filter(_.country == country)
  }
}
</code></pre>

<p>All great so far, presumably less verbose than the phantom equivalent, if we include the necessary mapping code.</p>

<pre><code>select.where(_.country eqs country).fetch()
</code></pre>

<p>But lets explore that further. What if you are trying to fetch a single country like that? Or what if you are trying to fetch <code>PagingState</code> information? Or feed in existing <code>PagingState</code> to display things over a UI.</p>

<p>This is where Quill at least in the comparison fails to give the user any real preview of what their experience will end up being. It's natural to assume that whenever you go to the page of a tool it will describe itself as the best tool in its category, as certainly we behind phantom do, but that's never the full story.</p>

<p>To be more concise, a few more cool things:</p>

<pre><code>select.where(_.country eqs country).fetchRecord()
select.where(_.country eqs country).one()
</code></pre>

<p>What about a partial select?</p>

<pre><code>select(_.country, _.city).where(_.country eqs country)
</code></pre>

<p>Phantom semantically distinguishes between all things that are possible at runtime using Cassandra and it tries above all to prevent runtime errors using compile time trickery and knowledge of the domain. How can you have the Quill equivalent?</p>

<p>Furthermore, Quill is perfectly capable of generating queries directly from a <code>case class</code>.</p>

<pre><code>  case class WeatherStation(
    country: String,
    city: String,
    stationId: String,
    entry: Int,
    value: Int
  )

  object WeatherStation {

    val getAllByCountry = quote {
      (country: String) =&gt;
        query[WeatherStation].filter(_.country == country)
    }

    val getAllByCountryAndCity = quote {
      (country: String, city: String) =&gt;
        getAllByCountry(country).filter(_.city == city)
    }

    val getAllByCountryCityAndId = quote {
      (country: String, city: String, stationId: String) =&gt;
        getAllByCountryAndCity(country, city).filter(_.stationId == stationId)
    }
  }
</code></pre>

<p>But it lacks any kind of knowledge about your schema whatsoever. What if country is not part of the primary? That query is invalid, phantom won't let you compile it, and this is just the most basic example.</p>

<p>Phantom can auto-generate your CQL from the table directly, it can generate entire databases on the fly, the pro version can even auto-migrate tables and help you deal with schema inconsistencies, and give you a very advanced UI and monitoring interface where you can upgrade and downgrade schemas on the fly.</p>

<p><strong>Cons</strong></p>

<ul>
<li><p>Phantom did indeed make it slightly less verbose to extend things like <code>TypeCodec</code>, but as of phantom 2.9.0 we have introduced an extremely powerful macro mechanism to encode types in Cassandra that doesn't rely on <code>TypeCodec</code> at all!</p></li>
<li><p>Phantom requires minimal boilerplate around defining the table DSL and by nature doesn't really work well with sharing table columns. It can be done, but it's not the most beautiful code, nor is it the worst.</p></li>
</ul>

<p><strong>Overall</strong></p>

<ul>
<li>Quill is a very nice piece of software written by very talented guys, there's 0 doubt about that.</li>
<li>It's better than phantom strictly at query generation, there's boilerplate that can be reduced through QDSLs that cannot be reduced through an EDSL, if we are fighting who's the leanest meanest string generator Quill wins.

<ul>
<li>It's a vastly inferior tool at the application layer, and it's even more unnatural for most people. Slick popularised the concepts to some extent, but some of the most basic functionalities you would want as part of your application lifecycle are not as easily addressable through a QDSL or at least it has yet to happen.</li>
<li>Phantom is far more mature, and very widely adopted, with more resources and input from the founding team, a long standing roadmap and a key partnership with Datastax that helps us stay on top of all features.</li>
</ul></li>
</ul>
",['table']
37938927,37943485,2016-06-21 08:11:25,Update denormalized data in Cassandra,"<p>Let's say, we have users who can comment videos and we want to show all comments by video with user's name.
Also user may go to his profile page and change his name.</p>

<p>Based on Cassandra data modeling practices that was covered in this answer <a href=""https://stackoverflow.com/questions/27281536/cassandra-denormalization-datamodel"">Cassandra denormalization datamodel</a>, I've create such tables:</p>

<pre><code>CREATE TABLE users (
   user_id UUID,
   first_name TEXT,
   last_name TEXT,
   PRIMARY KEY ((user_id))
); 

CREATE TABLE comments_by_video (
   video_id UUID,
   added_at TIMESTAMP,
   user_id UUID,
   comment TEXT,
   first_name TEXT,
   last_name TEXT,
   PRIMARY KEY ((video_id), added_at, user_id)
);
</code></pre>

<p>Looks awesome, we can get data that needed to show comments by video just by one query.</p>

<p>Now, let's consider such use case.</p>

<p>User created a lot of comments (like 10 000) and then decided to change his name.
Should we update all comments to change his name?
Is there a way to make it efficient?</p>
",<cassandra><data-modeling><denormalization><nosql>,"<p>Congratulation, you just enter the relationnal database zone ! </p>

<p>More seriously, this requirement is a pain with your model. Either you have to use the <code>user_id</code> to query <code>last_name</code> and <code>first name</code> in <code>users</code> table for every comments at reads, or you need to look accross all partitions and all comments to replace the <code>first_name</code> and <code>last_name</code> everywhere. There is no way to make it efficient.  </p>

<p>However, let's try a naive approach. You could create a users table, a videos table and another table that store all comments of a user like this one:   </p>

<pre><code>CREATE TABLE users_videos_comment(
    user_id uuid,
    video_id uuid,
    time timestamp,
    comment text,
    PRIMARY KEY ((user_id,video_id), time)
);
</code></pre>

<p>This is efficient for your new requirement, for a user and a video you can get all comments, so you just have to query users to look for the name, but you loose the ""one query for all comments in a video"". Also, you have to store in <code>users</code> a list of video where a user commented and on <code>videos</code> a list of users that made comments. This is difficult to maintain and will ask some more code.</p>

<p>There are maybe better ways to do it, but remember with noSQL <strong>What you loose on writes, you gain on reads</strong>  </p>

<p>If you don't mind doing a lot of writes to change the username, then keep it as it is. From this <a href=""https://stackoverflow.com/questions/23422181/cassandra-good-for-write-and-less-read-hbase-random-read-write"">post</a>, Cassandra seems better for writes anyway, so you should think of optimizing the reads.  </p>

<p>With that in mind, we can add a field in <code>users</code> that list all comments made by a user. This way, you won't have to scan across <code>comments_by_video</code> to look for every comments made by a user. This add some complexity because for any comments made by a user, you have to make two writes (<strong>and make sure it is consistent</strong>). But you have both requirements satisfied.</p>

<p>Hope it helps</p>
",['table']
37940630,37982696,2016-06-21 09:31:56,Cassandra Vnodes and token Ranges,"<p>I know that Vnodes form many token ranges for each node by setting num_tokens in cassandra.yaml file.</p>

<p>say for example (a), i have 6 nodes, each node i have set num_token=256. How many virtual nodes are formed among these 6 nodes that is, how many virtual nodes or sub token ranges contained in each physical node.</p>

<p>According to my understanding, when every node has assigned num_token as 256, then it means that all the 6 nodes contain 256 vnodes each. Is this statement true? if not then, how vnodes form the range of tokens (obviously random) in each node. It would be really convenient if someone can explain me with the example mentioned as (a).</p>

<p>what is the Ring of Vnodes signify in this url:=> <a href=""http://docs.datastax.com/en/cassandra/3.x/cassandra/images/arc_vnodes_compare.png"" rel=""noreferrer"">http://docs.datastax.com/en/cassandra/3.x/cassandra/images/arc_vnodes_compare.png</a> (taken from: <a href=""http://www.datastax.com/dev/blog/virtual-nodes-in-cassandra-1-2"" rel=""noreferrer"">http://www.datastax.com/dev/blog/virtual-nodes-in-cassandra-1-2</a> )</p>
",<cassandra>,"<p>Every partition key in Cassandra is converted to a numerical token value using the MurMur3 hash function. The token range is between -2^63 to +2^63 -1
num_token defines how many token ranges are assigned to a node. this is the same as the signed java long. Each node calculates 256 (num_tokens) random values in the token range and informs other nodes what they are, thus when a node needs to coordinate a request for a specific token it knows which nodes are responsible for it, according to the Replication Factor and DC/rack placement.
A better description for this feature would be ""automatic token range assignment for better streaming capabilities"", calling it ""virtual"" is a bit confusing.
In your case you have 6 nodes, each set with 256 token ranges so you have 6*256 token ranges and each psychical node contains 256 token ranges.</p>

<p>For example consider 2 nodes with num_tokens set to 4 and token range 0 to 100.
Node 1 calculates tokens 17, 35, 77, 92
Node 2 calculates tokens 4, 25, 68, 85
The ring shows the distribution of token ranges in this case
Node 2 is responsible for token ranges 4-17, 25-35, 68-77, 85-92 and node 1 for the rest.</p>
",['num_tokens']
38021480,38059927,2016-06-24 20:19:11,Cassandra 3.3 - cluster node - irregular behavior,"<p>I have a Cassandra Cluster conformed with the nodes <br>
{ 192.168.120.57, 192.168.120.58, 192.168.120.59 }, <strong>with replication factor of 2</strong></p>

<p>I detected the node 192.168.120.59 was behaving irregularly<br>
-> The service was running (sudo service cassandra status)<br>
-> The nodetool also told me that the node was running</p>

<pre><code>XXXX@cassandra-prod03:~$ nodetool status
Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address         Load       Tokens       Owns    Host ID                               Rack
UN  192.168.120.57  4.05 GB    256          ?       6d0f5961-a600-42c2-8ba5-5b6ebff52ceb  rack1
UN  192.168.120.58  4.4 GB     256          ?       53bc77ff-50b3-4bf0-bd55-17e2bb7c8208  rack1
UN  192.168.120.59  2.77 GB    256          ?       03d8d7fd-1d86-4034-a537-9915adb0d4b3  rack1
</code></pre>

<p><strong>BUT</strong> ->  I couldn't connect by DevCenter (the last time I tried I could and the cassandra.yml hasn't changed)</p>

<p>I looked into the log of this node (192.168.120.59), and showed:</p>

<pre><code>ERROR [HintsDispatcher:1] 2016-06-23 16:57:01,827 HintsDispatchExecutor.java:224 - Failed to dispatch hints file 6d0f5961-a600-42c2-8ba5-5b6ebff52ceb-1465184315371-1.hints: file is corrupted ({})
ERROR [HintsDispatcher:1] 2016-06-23 16:57:01,830 CassandraDaemon.java:195 - Exception in thread Thread[HintsDispatcher:1,1,main]
ERROR [HintsDispatcher:1] 2016-06-23 16:57:01,834 StorageService.java:470 - Stopping gossiper
ERROR [HintsDispatcher:1] 2016-06-23 16:57:03,842 StorageService.java:480 - Stopping native transport
</code></pre>

<p>In the other nodes, the log showed many many times:</p>

<pre><code>DEBUG [GossipTasks:1] 2016-06-24 07:56:21,551 Gossiper.java:336 - Convicting /192.168.120.59 with status shutdown - alive false
DEBUG [GossipTasks:1] 2016-06-24 07:56:22,552 Gossiper.java:336 - Convicting /192.168.120.59 with status shutdown - alive false
DEBUG [GossipTasks:1] 2016-06-24 07:56:23,552 Gossiper.java:336 - Convicting /192.168.120.59 with status shutdown - alive false
DEBUG [GossipTasks:1] 2016-06-24 07:56:24,553 Gossiper.java:336 - Convicting /192.168.120.59 with status shutdown - alive false
DEBUG [GossipTasks:1] 2016-06-24 07:56:25,553 Gossiper.java:336 - Convicting /192.168.120.59 with status shutdown - alive false
DEBUG [GossipTasks:1] 2016-06-24 07:56:26,553 Gossiper.java:336 - Convicting /192.168.120.59 with status shutdown - alive false
DEBUG [GossipTasks:1] 2016-06-24 07:56:27,554 Gossiper.java:336 - Convicting /192.168.120.59 with status shutdown - alive false
</code></pre>

<p>These are the things I have done and which didn't work:<br>
1) reboot the machine.<br>
2) nodetool repair.</p>

<pre><code>XXXXs@cassandra-prod03:~$ nodetool repair
[2016-06-23 17:07:37,582] Starting repair command #1, repairing keyspace recommendersjobs with repair options (parallelism: parallel, primary range: false, incremental: true, job threads: 1, ColumnFamilies: [], dataCenters: [], hosts: [], # of ranges: 512)
Jun 23, 2016 5:53:57 PM ClientCommunicatorAdmin Checker-run
WARNING: Failed to check the connection: java.net.SocketTimeoutException: Read timed out
Exception occurred during clean-up. java.lang.reflect.UndeclaredThrowableException
error: [2016-06-23 18:06:22,229] JMX connection closed. You should check server log for repair status of keyspace recommendersjobs(Subsequent keyspaces are not going to be repaired).
-- StackTrace --
java.io.IOException: [2016-06-23 18:06:22,229] JMX connection closed. You should check server log for repair status of keyspace recommendersjobs(Subsequent keyspaces are not going to be repaired).
at org.apache.cassandra.tools.RepairRunner.handleConnectionFailed(RepairRunner.java:97)
at org.apache.cassandra.utils.progress.jmx.JMXNotificationProgressListener.handleNotification(JMXNotificationProgressListener.java:86)
at javax.management.NotificationBroadcasterSupport.handleNotification(NotificationBroadcasterSupport.java:275)
at javax.management.NotificationBroadcasterSupport$SendNotifJob.run(NotificationBroadcasterSupport.java:352)
at javax.management.NotificationBroadcasterSupport$1.execute(NotificationBroadcasterSupport.java:337)
at javax.management.NotificationBroadcasterSupport.sendNotification(NotificationBroadcasterSupport.java:248)
at javax.management.remote.rmi.RMIConnector.sendNotification(RMIConnector.java:441)
at javax.management.remote.rmi.RMIConnector.access$1200(RMIConnector.java:121)
at javax.management.remote.rmi.RMIConnector$RMIClientCommunicatorAdmin.gotIOException(RMIConnector.java:1531)
at javax.management.remote.rmi.RMIConnector$RMINotifClient.fetchNotifs(RMIConnector.java:1352)
</code></pre>

<p>3) nodetool scrub.</p>

<p>After this I followed this page <a href=""https://docs.datastax.com/en/cassandra/1.2/cassandra/operations/ops_backup_noderestart_t.html"" rel=""nofollow"">https://docs.datastax.com/en/cassandra/1.2/cassandra/operations/ops_backup_noderestart_t.html</a></p>

<p>On the node that presented the irregular behaviour:<br>
1) Deleted all tables<br>
2) Copy and Paste the tables that were in the snapshot sub-directory<br>
3) Restarted cassandra service (<code>sudo service cassandra start</code>)<br></p>

<p>Now the Dev-Center can connect</p>

<p>4) I Ran <code>nodetool repair</code> again, and it seems that is not finishing. It has been running for more than 3 hours and is still in 0% (but for now didn't throw the other error), but I think it is not working.</p>

<pre><code>XXXX@cassandra-prod03:/var/lib/cassandra/data/recommendersjobs$ nodetool repair
[2016-06-24 12:24:13,901] Starting repair command #2, repairing keyspace recommendersjobs with repair options (parallelism: parallel, primary range: false, incremental: true, job threads: 1, ColumnFamilies: [], dataCenters: [], hosts: [], # of ranges: 512)
[2016-06-24 12:24:17,235] Repair session 7990e900-3a30-11e6-9861-e998758604bf for range [(-4211712190859502280,-4203992560184719114], (-8318576077833367495,-8293862912754480840], (6846385493332587078,6848188842228182830], (-6460899805527117383,-6451580681320322560], (-927549969226058129,-890279568362794135], (-6984061332646781425,-6971866923830433843], (4166814791483192953,4181831925595728309], (212986607214778070,288868023576418487], (656453115636318699,670358990995510048], (-3857111065907140708,-3844748631754416189], (-731643009820930522,-708535538994856284], (-3646398519882341977,-3630383383747491969], (-4215258070829115921,-4211712190859502280], (6972519766285293067,6983272857539313387], (8698322395242654268,8707318407090795417], (-1229710138608978352,-1142107267252652523], (-6043663680829718954,-6000947666680635781], (-2555062915045199657,-2518876915835081063], (700476075055800053,714305291929584762], (-9040157763677832405,-9035752029446223323], (-5508447172120753932,-5486202973508763691], (685259738113005224,700476075055800053], (-3498127715533695144,-3481009398659298196], (-6642818648542557724,-6636310914487378937], (8434600097698749124,8447388029675461333], (-9196939878235591876,-9176601682787902008], (-8392736270165548628,-8358469817741800124], (2668692603556301942,2674434559513729415], (-1868370051056061493,-1860700618558449042], (-6090102443758865946,-6081413019910708044], (-1234056662103299181,-1233526281479424892], (5500764053339166116,5524556889632227041], (-2343395791776959523,-2315566773401476532], (-1401161284995746969,-1393787270658712827], (-2204381944261735990,-2170477179223028079], (6158419946524545293,6171118529963906232], (-4713529768344258113,-4662557763507006574], (-6844075271953818452,-6821121360440227129], (-2945271950056514025,-2943444578534843676], (336935533896784347,357900664320554816], (-3228825867554770158,-3208435083108021414], (-4955223362385024239,-4912831044277064162], (4255028482041276438,4266791030997837989], (6476681916885657705,6500903072598398827], (-1233526281479424892,-1229710138608978352], (1478830670694875260,1516022971169730278], (5055515752742518226,5080231569691472194], (-8220031638992810484,-8180158125318738371], (-1644262187188908209,-1640818085920683524], (-4203992560184719114,-4201062514351678359], (-5222304299920694120,-5219839223573200521], (-8773724135260117843,-8764792082520695033], (-1355888553876954583,-1337212328034401607], (4736203535340725934,4798689233015535742], (1271210285313194340,1296150437356677365], (-3540891434049035738,-3498615337040509123], (-6469678775729021799,-6460899805527117383], (6720989190599823387,6725081429318018351], (-3498615337040509123,-3498127715533695144], (-3146863944770153663,-3146145879742664566], (1516022971169730278,1543205759172249070], (2435469497781312092,2442862581422161803], (-6451580681320322560,-6450897957558236116], (6401972194395279212,6422279168069398956], (-3185403021559141625,-3146863944770153663], (714305291929584762,737464206890275575], (-8358469817741800124,-8348341616724830280], (1175374807063094969,1214972878391460620], (-7604581392807650182,-7599009809842308024], (737464206890275575,753307058766478061], (6142712373291735013,6158419946524545293], (-4780493053770543287,-4715780574795279713], (-6475595045987094984,-6472782860721811368], (-6000947666680635781,-5984299660747839008], (1296150437356677365,1341203791801750057], (-708535538994856284,-698490751868803865], (7377888475905026189,7453466617724696329], (-6634208348348544983,-6608369916989089358], (6422279168069398956,6425564550565269465], (7864684753798709404,7865351483298068861], (-1142107267252652523,-1130763472498460650], (2661263629361374231,2668692603556301942], (5080231569691472194,5114813438584055434], (-3146145879742664566,-3133020507157500012], (-8847809439930946447,-8773724135260117843], (-7455563937756770941,-7393173741279261649], (-8001929233768101133,-7997359338916453120], (-3208435083108021414,-3185403021559141625], (-8504346858359312144,-8489394145547730314], (330577967974463458,336935533896784347], (7563807678381276197,7582113707662028389], (-8542082760751850432,-8504346858359312144], (-3554430985922725406,-3540891434049035738], (-5146821899868165131,-5132879094583817494], (-240050552784290962,-235772081518906038], (3192195406141461438,3232311876000191165], (4396701296202861119,4435447637968520477], (-6608369916989089358,-6589281199205137237], (-1981758794620356502,-1980394476029734464], (-3563262972235992519,-3554430985922725406], (-8237266126949227268,-8229088751363770412], (4603382773438618005,4650956890538380770], (7199369658829377994,7213887142216841811], (-4962133286795414966,-4960837410336448181], (5732306214090585629,5737923082195078719], (-1640818085920683524,-1638198095283336424], (2953772832389323874,2968606959459466094], (-1235736831018577452,-1234056662103299181], (-8489394145547730314,-8465866933095448418], (-8764792082520695033,-8742184848088246957], (8970234525288431416,8995803891393002810], (-8465866933095448418,-8446751642929552235], (-4836950976239959831,-4798814403290155024], (8821435912538079613,8871870554368996448], (-6636310914487378937,-6634208348348544983], (1127798841363210583,1175374807063094969], (1618461143046543312,1632362492578265392], (4994543089928849457,5055515752742518226], (1593785714944278507,1618461143046543312], (-8885929495137049667,-8847809439930946447], (7514316870399738740,7535648178040640648], (-7997359338916453120,-7884947734877215590], (6848188842228182830,6851091943952222998], (-7468624498429238352,-7467528674261593018], (-5420066870415394025,-5415525963610970555], (-1990341934536435876,-1981758794620356502], (-2119663304358780223,-2025913005077454264], (6951976975129490336,6972519766285293067], (4814028481954927072,4845532697727637101], (6905718428902118395,6951976975129490336], (2442862581422161803,2473486929810000384], (-7050886265796431130,-6984061332646781425], (899736981222314462,902246412094263205], (4181831925595728309,4186043100749193555], (5228334028316552580,5251807687845401107], (7453466617724696329,7514316870399738740], (-5313682991153179320,-5303078638925764028], (4435447637968520477,4466559239870893508], (-1326922053645316997,-1235736831018577452], (1401641821520873210,1444010990722687070], (1751424608310627366,1764837197082917798], (4501052626153108190,4556808478342919483], (4927485308468803128,4934126082298511287], (9034829150987061085,9125279489658634069], (8296589981550156872,8314630209670832253], (8448913759582599761,8531687930406385678], (-6946999157236744448,-6844075271953818452], (1444010990722687070,1478830670694875260], (8959600358987559211,8960859570137987260], (-1336371416389083059,-1326922053645316997], (-61295809418728622,-42017489632420450], (8969118191184771962,8970234525288431416], (1635093197288200819,1646180075666173338], (4934126082298511287,4936412559469748364], (1751396675459233161,1751424608310627366], (-8742184848088246957,-8729944250195454990], (-8229088751363770412,-8220031638992810484], (4598535296351629260,4603382773438618005], (-8174261525446719402,-8163507145438773646], (755491933174209966,772199007723251143], (-8624222028448607604,-8600591663798592212], (-7519020387526209046,-7508903488301733990], (753307058766478061,755491933174209966], (7582113707662028389,7704299093463733587], (8962972620592857243,8969118191184771962], (-3630383383747491969,-3629090531220364683], (-4468127887710219501,-4455300706501649674], (5852098730437702158,5901360113812253268], (-744327910454436920,-731643009820930522], (1911291563147302352,1937039485053092324], (6233932117267819795,6310871518339058610], (-890279568362794135,-887061681980670889], (-3679929070886276718,-3646398519882341977], (670358990995510048,685259738113005224], (4845532697727637101,4882596066849804097], (-6472782860721811368,-6469678775729021799], (-4715780574795279713,-4713529768344258113], (3078864489691764074,3115136329005690845], (9125279489658634069,9133717713662800417], (8960859570137987260,8962972620592857243], (9203390852073803003,-9222340996132828621], (4798689233015535742,4814028481954927072], (-6821121360440227129,-6777406043019946301], (6310871518339058610,6322362350826191135], (-4233493116436171657,-4215258070829115921], (3130822610289703870,3145528043810254358], (5114813438584055434,5123797488219630832], (-6081413019910708044,-6079621320236638978], (8447388029675461333,8448913759582599761], (-3795970094034238564,-3779568630549582632], (-8180158125318738371,-8174261525446719402], (-3844748631754416189,-3795970094034238564], (4113442287147808270,4131576074431163736], (4936412559469748364,4994543089928849457], (4466559239870893508,4501052626153108190], (-3263191959146785290,-3228825867554770158], (6766473197422691050,6766938926557171347], (-8899258035651035854,-8885929495137049667], (-5381097520433537861,-5313682991153179320], (6883419270850869410,6905718428902118395], (5303695175174121453,5306682210025644500], (-9176601682787902008,-9096402285624130055], (-5219839223573200521,-5146821899868165131], (7862240594123780934,7864684753798709404], (6425564550565269465,6447403433784268090], (8996665254881958342,9032628945801968053], (-4912831044277064162,-4902449072491610799], (-2025913005077454264,-1990341934536435876], (-7467528674261593018,-7455563937756770941], (-3293906917083373209,-3283303577982631091], (4186043100749193555,4255028482041276438], (-4902449072491610799,-4875492513338002087], (8531687930406385678,8562083491872293504], (-8600591663798592212,-8567258569010210224], (-6079621320236638978,-6077962914827619257], (-4960837410336448181,-4955223362385024239], (-7634134348451706917,-7604581392807650182], (1632362492578265392,1635093197288200819], (291067515209041750,330577967974463458], (7789905572870286790,7790287256445162065], (-8007287567032110044,-8001929233768101133], (-6971866923830433843,-6946999157236744448], (9032628945801968053,9034829150987061085], (-4798814403290155024,-4780493053770543287], (3905716352976079268,3920453505268467479], (8995803891393002810,8996665254881958342], (-1393787270658712827,-1355888553876954583], (6522490073023565012,6553480210988421066], (7162495952820208409,7199369658829377994], (4882596066849804097,4927485308468803128], (-5893410924582496350,-5858193870200257506], (8233960782654491894,8252708412304213777], (6725081429318018351,6766473197422691050], (-5303078638925764028,-5222304299920694120], (2376875294823216717,2379477119284337609], (-7393173741279261649,-7376038212607696986], (-1337212328034401607,-1336371416389083059], (902246412094263205,902703964241874282], (288868023576418487,291067515209041750], (1341203791801750057,1364316712425392743], (5199663539610295629,5228334028316552580], (6447403433784268090,6476681916885657705], (-3481009398659298196,-3391281017675748608], (4266791030997837989,4315496651682018963], (3115136329005690845,3130822610289703870], (-7599009809842308024,-7598746015885846943]] failed with error [repair #7990e900-3a30-11e6-9861-e998758604bf on recommendersjobs/postulationsbyuser, [(-4211712190859502280,-4203992560184719114], (-8318576077833367495,-8293862912754480840], (6846385493332587078,6848188842228182830], (-6460899805527117383,-6451580681320322560], (-927549969226058129,-890279568362794135], (-6984061332646781425,-6971866923830433843], (4166814791483192953,4181831925595728309], (212986607214778070,288868023576418487], (656453115636318699,670358990995510048], (-3857111065907140708,-3844748631754416189], (-731643009820930522,-708535538994856284], (-3646398519882341977,-3630383383747491969], (-4215258070829115921,-4211712190859502280], (6972519766285293067,6983272857539313387], (8698322395242654268,8707318407090795417], (-1229710138608978352,-1142107267252652523], (-6043663680829718954,-6000947666680635781], (-2555062915045199657,-2518876915835081063], (700476075055800053,714305291929584762], (-9040157763677832405,-9035752029446223323], (-5508447172120753932,-5486202973508763691], (685259738113005224,700476075055800053], (-3498127715533695144,-3481009398659298196], (-6642818648542557724,-6636310914487378937], (8434600097698749124,8447388029675461333], (-9196939878235591876,-9176601682787902008], (-8392736270165548628,-8358469817741800124], (2668692603556301942,2674434559513729415], (-1868370051056061493,-1860700618558449042], (-6090102443758865946,-6081413019910708044], (-1234056662103299181,-1233526281479424892], (5500764053339166116,5524556889632227041], (-2343395791776959523,-2315566773401476532], (-1401161284995746969,-1393787270658712827], (-2204381944261735990,-2170477179223028079], (6158419946524545293,6171118529963906232], (-4713529768344258113,-4662557763507006574], (-6844075271953818452,-6821121360440227129], (-2945271950056514025,-2943444578534843676], (336935533896784347,357900664320554816], (-3228825867554770158,-3208435083108021414], (-4955223362385024239,-4912831044277064162], (4255028482041276438,4266791030997837989], (6476681916885657705,6500903072598398827], (-1233526281479424892,-1229710138608978352], (1478830670694875260,1516022971169730278], (5055515752742518226,5080231569691472194], (-8220031638992810484,-8180158125318738371], (-1644262187188908209,-1640818085920683524], (-4203992560184719114,-4201062514351678359], (-5222304299920694120,-5219839223573200521], (-8773724135260117843,-8764792082520695033], (-1355888553876954583,-1337212328034401607], (4736203535340725934,4798689233015535742], (1271210285313194340,1296150437356677365], (-3540891434049035738,-3498615337040509123], (-6469678775729021799,-6460899805527117383], (6720989190599823387,6725081429318018351], (-3498615337040509123,-3498127715533695144], (-3146863944770153663,-3146145879742664566], (1516022971169730278,1543205759172249070], (2435469497781312092,2442862581422161803], (-6451580681320322560,-6450897957558236116], (6401972194395279212,6422279168069398956], (-3185403021559141625,-3146863944770153663], (714305291929584762,737464206890275575], (-8358469817741800124,-8348341616724830280], (1175374807063094969,1214972878391460620], (-7604581392807650182,-7599009809842308024], (737464206890275575,753307058766478061], (6142712373291735013,6158419946524545293], (-4780493053770543287,-4715780574795279713], (-6475595045987094984,-6472782860721811368], (-6000947666680635781,-5984299660747839008], (1296150437356677365,1341203791801750057], (-708535538994856284,-698490751868803865], (7377888475905026189,7453466617724696329], (-6634208348348544983,-6608369916989089358], (6422279168069398956,6425564550565269465], (7864684753798709404,7865351483298068861], (-1142107267252652523,-1130763472498460650], (2661263629361374231,2668692603556301942], (5080231569691472194,5114813438584055434], (-3146145879742664566,-3133020507157500012], (-8847809439930946447,-8773724135260117843], (-7455563937756770941,-7393173741279261649], (-8001929233768101133,-7997359338916453120], (-3208435083108021414,-3185403021559141625], (-8504346858359312144,-8489394145547730314], (330577967974463458,336935533896784347], (7563807678381276197,7582113707662028389], (-8542082760751850432,-8504346858359312144], (-3554430985922725406,-3540891434049035738], (-5146821899868165131,-5132879094583817494], (-240050552784290962,-235772081518906038], (3192195406141461438,3232311876000191165], (4396701296202861119,4435447637968520477], (-6608369916989089358,-6589281199205137237], (-1981758794620356502,-1980394476029734464], (-3563262972235992519,-3554430985922725406], (-8237266126949227268,-8229088751363770412], (4603382773438618005,4650956890538380770], (7199369658829377994,7213887142216841811], (-4962133286795414966,-4960837410336448181], (5732306214090585629,5737923082195078719], (-1640818085920683524,-1638198095283336424], (2953772832389323874,2968606959459466094], (-1235736831018577452,-1234056662103299181], (-8489394145547730314,-8465866933095448418], (-8764792082520695033,-8742184848088246957], (8970234525288431416,8995803891393002810], (-8465866933095448418,-8446751642929552235], (-4836950976239959831,-4798814403290155024], (8821435912538079613,8871870554368996448], (-6636310914487378937,-6634208348348544983], (1127798841363210583,1175374807063094969], (1618461143046543312,1632362492578265392], (4994543089928849457,5055515752742518226], (1593785714944278507,1618461143046543312], (-8885929495137049667,-8847809439930946447], (7514316870399738740,7535648178040640648], (-7997359338916453120,-7884947734877215590], (6848188842228182830,6851091943952222998], (-7468624498429238352,-7467528674261593018], (-5420066870415394025,-5415525963610970555], (-1990341934536435876,-1981758794620356502], (-2119663304358780223,-2025913005077454264], (6951976975129490336,6972519766285293067], (4814028481954927072,4845532697727637101], (6905718428902118395,6951976975129490336], (2442862581422161803,2473486929810000384], (-7050886265796431130,-6984061332646781425], (899736981222314462,902246412094263205], (4181831925595728309,4186043100749193555], (5228334028316552580,5251807687845401107], (7453466617724696329,7514316870399738740], (-5313682991153179320,-5303078638925764028], (4435447637968520477,4466559239870893508], (-1326922053645316997,-1235736831018577452], (1401641821520873210,1444010990722687070], (1751424608310627366,1764837197082917798], (4501052626153108190,4556808478342919483], (4927485308468803128,4934126082298511287], (9034829150987061085,9125279489658634069], (8296589981550156872,8314630209670832253], (8448913759582599761,8531687930406385678], (-6946999157236744448,-6844075271953818452], (1444010990722687070,1478830670694875260], (8959600358987559211,8960859570137987260], (-1336371416389083059,-1326922053645316997], (-61295809418728622,-42017489632420450], (8969118191184771962,8970234525288431416], (1635093197288200819,1646180075666173338], (4934126082298511287,4936412559469748364], (1751396675459233161,1751424608310627366], (-8742184848088246957,-8729944250195454990], (-8229088751363770412,-8220031638992810484], (4598535296351629260,4603382773438618005], (-8174261525446719402,-8163507145438773646], (755491933174209966,772199007723251143], (-8624222028448607604,-8600591663798592212], (-7519020387526209046,-7508903488301733990], (753307058766478061,755491933174209966], (7582113707662028389,7704299093463733587], (8962972620592857243,8969118191184771962], (-3630383383747491969,-3629090531220364683], (-4468127887710219501,-4455300706501649674], (5852098730437702158,5901360113812253268], (-744327910454436920,-731643009820930522], (1911291563147302352,1937039485053092324], (6233932117267819795,6310871518339058610], (-890279568362794135,-887061681980670889], (-3679929070886276718,-3646398519882341977], (670358990995510048,685259738113005224], (4845532697727637101,4882596066849804097], (-6472782860721811368,-6469678775729021799], (-4715780574795279713,-4713529768344258113], (3078864489691764074,3115136329005690845], (9125279489658634069,9133717713662800417], (8960859570137987260,8962972620592857243], (9203390852073803003,-9222340996132828621], (4798689233015535742,4814028481954927072], (-6821121360440227129,-6777406043019946301], (6310871518339058610,6322362350826191135], (-4233493116436171657,-4215258070829115921], (3130822610289703870,3145528043810254358], (5114813438584055434,5123797488219630832], (-6081413019910708044,-6079621320236638978], (8447388029675461333,8448913759582599761], (-3795970094034238564,-3779568630549582632], (-8180158125318738371,-8174261525446719402], (-3844748631754416189,-3795970094034238564], (4113442287147808270,4131576074431163736], (4936412559469748364,4994543089928849457], (4466559239870893508,4501052626153108190], (-3263191959146785290,-3228825867554770158], (6766473197422691050,6766938926557171347], (-8899258035651035854,-8885929495137049667], (-5381097520433537861,-5313682991153179320], (6883419270850869410,6905718428902118395], (5303695175174121453,5306682210025644500], (-9176601682787902008,-9096402285624130055], (-5219839223573200521,-5146821899868165131], (7862240594123780934,7864684753798709404], (6425564550565269465,6447403433784268090], (8996665254881958342,9032628945801968053], (-4912831044277064162,-4902449072491610799], (-2025913005077454264,-1990341934536435876], (-7467528674261593018,-7455563937756770941], (-3293906917083373209,-3283303577982631091], (4186043100749193555,4255028482041276438], (-4902449072491610799,-4875492513338002087], (8531687930406385678,8562083491872293504], (-8600591663798592212,-8567258569010210224], (-6079621320236638978,-6077962914827619257], (-4960837410336448181,-4955223362385024239], (-7634134348451706917,-7604581392807650182], (1632362492578265392,1635093197288200819], (291067515209041750,330577967974463458], (7789905572870286790,7790287256445162065], (-8007287567032110044,-8001929233768101133], (-6971866923830433843,-6946999157236744448], (9032628945801968053,9034829150987061085], (-4798814403290155024,-4780493053770543287], (3905716352976079268,3920453505268467479], (8995803891393002810,8996665254881958342], (-1393787270658712827,-1355888553876954583], (6522490073023565012,6553480210988421066], (7162495952820208409,7199369658829377994], (4882596066849804097,4927485308468803128], (-5893410924582496350,-5858193870200257506], (8233960782654491894,8252708412304213777], (6725081429318018351,6766473197422691050], (-5303078638925764028,-5222304299920694120], (2376875294823216717,2379477119284337609], (-7393173741279261649,-7376038212607696986], (-1337212328034401607,-1336371416389083059], (902246412094263205,902703964241874282], (288868023576418487,291067515209041750], (1341203791801750057,1364316712425392743], (5199663539610295629,5228334028316552580], (6447403433784268090,6476681916885657705], (-3481009398659298196,-3391281017675748608], (4266791030997837989,4315496651682018963], (3115136329005690845,3130822610289703870], (-7599009809842308024,-7598746015885846943]]] Validation failed in /192.168.120.58 (progress: 0%)
</code></pre>

<p>In this log appears: <br>
1) <code>failed with error [repair #7990e900-3a30-11e6-9861-e998758604bf on recommendersjobs/postulationsbyuser</code> (near the middle)<br>
2) <code>Validation failed in /192.168.120.58 (progress: 0%)</code> (in the last part)<br></p>

<p>While this repair is running (I think it is running), in the log of this node and in the others nodes of the cluster, this appears many times:</p>

<pre><code>DEBUG [ReadRepairStage:4] 2016-06-24 14:08:18,318 ReadCallback.java:235 - Digest mismatch:
org.apache.cassandra.service.DigestMismatchException: Mismatch for key DecoratedKey(-3297346077674417080, 42555f31313131303131333538) (d41d8cd98f00b204e9800998ecf8427e vs 9e7f1588ffcd53d13ab3f8bb2be0f05e)
    at org.apache.cassandra.service.DigestResolver.resolve(DigestResolver.java:85) ~[apache-cassandra-3.3.jar:3.3]
    at org.apache.cassandra.service.ReadCallback$AsyncRepairRunner.run(ReadCallback.java:226) ~[apache-cassandra-3.3.jar:3.3]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_91]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_91]
    at java.lang.Thread.run(Thread.java:745) [na:1.8.0_91]
</code></pre>

<p>NOTE 1: The cassandra was and is working because of the replication factor of 2.<br>
NOTE 2: The cassandra Cluster is being used in production, and applications are inserting and reading rows.</p>

<p><strong>What can I do to correct this irregular behaviour?<br>
I thought of creating a new node and swap it with the irregular node, what are the correct steps for doing this?</strong></p>
",<cassandra><cassandra-3.0>,"<p>When a node does not respond / is malfunctioning <br><br>
 If you can create a new VM that will repace the malfunctioning node.
See: <a href=""https://docs.datastax.com/en/cassandra/3.0/cassandra/operations/opsReplaceNode.html"" rel=""nofollow"">https://docs.datastax.com/en/cassandra/3.0/cassandra/operations/opsReplaceNode.html</a></p>

<p>If you do not want to create a new VM and use the existing malfunctioning node. <br>
<strong>IMPORTANT: NEED TO HAVE A REPLICATION FACTOR THAT ALLOWS YOU TO DELETE ALL THE DATA OF THE AFFECTED NODE</strong>.<br><br></p>

<pre><code>nodetool decommission (On that node)
</code></pre>

<p>See: <a href=""https://docs.datastax.com/en/cassandra/3.0/cassandra/tools/toolsDecommission.html"" rel=""nofollow"">https://docs.datastax.com/en/cassandra/3.0/cassandra/tools/toolsDecommission.html</a><br><br></p>

<pre><code>sudo rm -rf /var/lib/cassandra/*
</code></pre>

<p>clear/delete all data in the node
See: <a href=""https://docs.datastax.com/en/cassandra/3.0/cassandra/initialize/referenceClearCpkgData.html"" rel=""nofollow"">https://docs.datastax.com/en/cassandra/3.0/cassandra/initialize/referenceClearCpkgData.html</a><br><br></p>

<pre><code>sudo service cassandra start
</code></pre>

<p>After a time,</p>

<pre><code>nodetool repair
</code></pre>

<p><strong>If the nodetool repair FAILS:</strong><br>
For tables in keyspace:</p>

<pre><code>nodetool scrub &lt;keyspace&gt; &lt;table&gt;
</code></pre>

<p>in EACH of the nodes→ see log of the node and wait to complete<br><br></p>

<pre><code>run nodetool repair &lt;keyspace&gt; &lt;table&gt;
</code></pre>

<p>only in one node (anyone)<br><br></p>

<p><strong>NOTE:</strong> it is not necessary to run for each table, yo can run:</p>

<pre><code>nodetool scrub &lt;keyspace&gt;
</code></pre>

<p>in EACH of the nodes→ see log of the node and wait to complete<br><br></p>

<pre><code>nodetool repair &lt;keyspace&gt;
</code></pre>

<p>I first suggest trying for 1 table for seeing the log of only that table and comprehend better what is going on<br><br>
<strong>NOTE:</strong> <br>
set the loggin levels to INFO to see the important things </p>

<pre><code>nodetool setlogginglevel org.apache.cassandra INFO
</code></pre>
",['table']
38154170,38160369,2016-07-01 21:38:35,"datastax opscenter ""SchedulesNotLoaded"" error","<p>I am using Datastax OpsCenter v5.2.4 with DSE v4.8.4-1.
Since few days ago, I've been not able to retrieve the result of ""Best Practice Service"" both from API and opscenter UI.
When I try to get it, I get errors like below.</p>

<p>GUI: </p>

<blockquote>
  <p>Could not retrieve best practice rules: Scheduled jobs have not been
  loaded yet. There may be a connectivity problem with Cassandra.</p>
</blockquote>

<p>opscenterd.log</p>

<pre><code>2016-07-01 19:47:50+0000 [] ERROR: Problem while calling decorator (SchedulesNotLoaded): Scheduled jobs have not been loaded yet. There may be a connectivity problem with Cassandra.
  File ""/usr/share/opscenter/lib/py-debian/2.7/amd64/twisted/internet/defer.py"", line 1020, in _inlineCallbacks
    result = g.send(result)

  File ""/usr/lib/python2.7/dist-packages/opscenterd/WebServer.py"", line 1939, in SchedulesGetController

  File ""/usr/lib/python2.7/dist-packages/opscenterd/Schedule.py"", line 213, in getAllSchedules

  File ""/usr/lib/python2.7/dist-packages/opscenterd/Schedule.py"", line 175, in _assert_loaded
</code></pre>

<p>I've tried to restart opscenterd service as well as rebooting the opscenter machine itself but it didn't make any difference.
The error says there might be some connectivity issue, but what port/protocol is opscenter using to load these scheduled jobs? (there is no firewall between opscenter and cassandra nodes)
There is no alert in the cluster, and agents are all connected according to opscenter's GUI.</p>

<p>I couldn't find any relevant trouble shooting documentation ... how can we recover opscenter from this situation?</p>
",<cassandra><datastax><datastax-enterprise><opscenter>,"<p>Issue is around when schedules settings get in messed up state. If you schedule a one time only run, and opscenterd is down when it is scheduled to run, then on startup opscenterd dies loading that schedule.</p>

<p>If you don't have anything particularly important stored there is an easy fix. Shutdown off opscenter, use cqlsh to <code>drop keyspace ""OpsCenter"";</code> and restart opscenter.</p>

<p>Otherwise you have to hand clean up some of the schedules in <code>OpsCenter.settings</code> table that got messed up. This is fixed in 6.0, so if you upgrade it wont happen again.</p>
",['table']
38210719,38215999,2016-07-05 18:58:11,Will Cassandra avoid calculating the row MD5 if the value already is an MD5?,"<p>From various documents about Cassandra, it clearly says that it converts row keys to an MD5 before saving them in the database.</p>

<p>If my row keys already are MD5 sums, is there a way to let Cassandra know and thus avoid having it calculate the MD5 of that MD5?</p>

<p>P.S. The table I am talking about has files in it and the keys are the files MD5 sums.</p>
",<cassandra><cql3><md5sum><cassandra-3.0>,"<p>What Cassandra actually does is hash the partition key based on what the partitioner defines. The original partitioner was MD5, but modern versions of Cassandra default to Murmur3 (not QUITE murmur3, but basically murmur3).</p>

<p>In either case, yes, Cassandra hashes the partition key, because there is no way to let Cassandra know that it's already an MD5. </p>

<p>If you <strong>really</strong> want to avoid the hashing, you can look at other alternative partitioners (such as <a href=""https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/dht/ByteOrderedPartitioner.java"" rel=""nofollow"">byte ordered</a> or <a href=""https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/dht/OrderPreservingPartitioner.java"" rel=""nofollow"">order preserving</a> ), or write your own that implements <a href=""https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/dht/IPartitioner.java"" rel=""nofollow"">IPartitioner</a> . Note, though, that if you do use a different partitioner, it's used for all tables/keyspaces in the cluster. </p>
",['partitioner']
38254971,38255215,2016-07-07 20:46:26,"Datastax Cassandra Driver always attempts to connect to localhost, even though it's not configured to do so","<p>So I have the following Client code: </p>

<pre><code>def getCluster:Session = {
    import collection.JavaConversions._
    val endpoints = config.getStringList(""cassandra.server"")
    val keyspace = config.getString(""cassandra.keyspace"")

    val clusterBuilder = Cluster.builder

    endpoints.toTraversable.map { x =&gt;
      clusterBuilder.addContactPoint(x)
    }
    val cluster = clusterBuilder.build
    cluster
      .getConfiguration
      .getProtocolOptions
      .setCompression(ProtocolOptions.Compression.LZ4)
    cluster.connect(keyspace)}
</code></pre>

<p>which is shamelessly borrowed from the examples in datastax's driver documentation. </p>

<p>When I attempt to execute code with it, it always tries to connect to localhost, even though it's not configured for that... </p>

<p>In some cases, it will connect (basic reads) but for writes I get the following log message: </p>

<pre><code> 2016-07-07 11:34:31 DEBUG Connection:157 - Connection[/127.0.0.1:9042-10, inFlight=0, closed=false] Error connecting to /127.0.0.1:9042 (Connection refused: /127.0.0.1:9042)
2016-07-07 11:34:31 DEBUG STATES:404 - Defuncting Connection[/127.0.0.1:9042-10, inFlight=0, closed=false] because: [/127.0.0.1] Cannot connect
2016-07-07 11:34:31 DEBUG STATES:108 - [/127.0.0.1:9042] Connection[/127.0.0.1:9042-10, inFlight=0, closed=false] failed, remaining = 0
2016-07-07 11:34:31 DEBUG Connection:629 - Connection[/127.0.0.1:9042-10, inFlight=0, closed=true] closing connection
2016-07-07 11:34:31 DEBUG Cluster:1802 - Aborting onDown because a reconnection is running on DOWN host /127.0.0.1:9042
2016-07-07 11:34:31 DEBUG Cluster:1872 - Failed reconnection to /127.0.0.1:9042 ([/127.0.0.1] Cannot connect), scheduling retry in 512000 milliseconds
2016-07-07 11:34:31 DEBUG STATES:196 - [/127.0.0.1:9042] next reconnection attempt in 512000 ms
</code></pre>

<p>I can't figure out where/what I need to configure on the driver side (no local client, it's just the driver) to correct this issue</p>
",<scala><cassandra><datastax-java-driver>,"<p>My guess is that this is caused by configuration of the <code>cassandra.yaml</code> file on your cassandra node(s).  The two main settings that would impact this are <code>broadcast_rpc_address</code> and <code>rpc_address</code>, from <a href=""https://docs.datastax.com/en/cassandra/2.1/cassandra/configuration/configCassandra_yaml_r.html"" rel=""nofollow noreferrer"">The cassandra.yaml configuration</a> reference:</p>
<blockquote>
<p>broadcast_rpc_address</p>
<p>(Default: unset) RPC address to broadcast to drivers and other Cassandra nodes. This cannot be set to 0.0.0.0. If blank, it is set to the value of the rpc_address or rpc_interface. If rpc_address or rpc_interfaceis set to 0.0.0.0, this property must be set.</p>
<p>rpc_address</p>
<p>(Default: localhost) The listen address for client connections (Thrift RPC service and native transport).</p>
</blockquote>
<p>If you leave both of these to the defaults, <code>localhost</code> will be the default address cassandra will communicate to connect on.</p>
<p>After the driver is able to connect to a contact point, it queries the <code>system.local</code> and <code>system.peers</code> table of the contact point to determine which hosts to connect to, the addresses those tables communicate are from <code>rpc_address</code>/<code>broadcast_rpc_address</code></p>
","['rpc_address', 'table']"
38257165,38257250,2016-07-08 00:12:30,Would data be in the same partition if I use composite keys ?,"<p>Considere the following case regarding Cassandra database: I must perform a batch statement with some related data, e.g: table users and table users_by_username. I want to insert on user creation data on both tables. It's a transaction. In Cassandra documentation says the batch statement cannot reach multiple partitions. If I model the primary key as a composite key like following : </p>

<pre><code>CREATE TABLE IF NOT EXISTS user(
  id text,
  tpe text,
  username text,
  PRIMARY KEY((tpe, id))
);

CREATE TABLE IF NOT EXISTS user_by_username(
  username text,
  tpe text,
  id text,
  PRIMARY KEY((tpe, username))
);
</code></pre>

<p>Example of rows: </p>

<p>user: ('1', 'users', 'lucasrpb')
user_by_username: ('lucasrpb', 'users', '1')</p>

<p>My doubt: will data be on the same partition to be able to do the batch? </p>
",<database><cassandra><batch-processing>,"<p>Partitions are within a table, not across tables.  However, the <strong>token</strong> for data, which is used to identify which replicas will host the data, is based on the <strong>partition</strong> key (the first column in the primary key, or the first column(s) surrounded in parenthesis).</p>

<p>In your case, the <strong>partition</strong> key for 'user' is <code>(tpe, id)</code> and <code>user_by_username</code> is <code>(tpe, username)</code>.   Because of this, the token for the data will likely not be the same.</p>

<p>If the <strong>primary</strong> key for <code>user</code> was <code>(tpe, id)</code>, <code>user_by_username</code> <code>(tpe, username)</code>, the partition key for each case would be <code>tpe</code>, therefore granted <code>tpe</code> was the same, the token would be the same and therefore the data would be stored on the same replicas.</p>

<p>In any case, I would not recommend batching operations to update <code>user_by_username</code> and <code>user</code> together, but it'd be better in the case where the partition key was the same as less C* nodes need to be written to in the batch.</p>

<p>Since the only difference between your tables is your primary key, I think a good candidate for you if you are on a 3.0+ version would be to look into <a href=""http://www.datastax.com/dev/blog/new-in-cassandra-3-0-materialized-views"" rel=""nofollow"">materialized views</a> which were introduced in 3.0.  With this you could set up a <code>user_by_username</code> view off of the <code>user</code> table like:</p>

<pre class=""lang-sql prettyprint-override""><code>CREATE MATERIALIZED VIEW user_by_username AS 
SELECT * FROM users 
WHERE username IS NOT NULL
PRIMARY KEY ((tpe, username));
</code></pre>

<p>This way you only have to make changes to <code>user</code>, which will then be propagated to <code>user_by_username</code> for you.</p>
",['table']
38296328,38315855,2016-07-10 20:46:57,How to add a decommissioned node in cassandra?,"<p>I just did nodetool decommission and it removed the node and I am trying to add it back in so I just started cassandra(decomissioned node) again but it doesnt seem to join the cluster?</p>
",<cassandra>,"<p>The other nodes will remember that the host ID you decommissioned should no longer be part of the cluster and will refuse to talk to it.</p>

<p>So if you want that machine to rejoin the cluster, you have to make it look like a new empty node so that the remaining nodes will let it rejoin. The easiest way to do that is to clear out all the data on the decommissioned node so that it will generate a new host ID for itself. Then it should be able to rejoin.</p>

<p>To clear out the old data, do this:</p>

<p>Stop Cassandra on the node, then:</p>

<pre><code>rm -r &lt;the commitlog_directory specified in cassandra.yaml&gt;
rm -r &lt;the data_file_directories specified in cassandra.yaml&gt;
rm &lt;the contents of the saved_caches_directory specified in cassandra.yaml&gt;
rm &lt;old logfiles in /var/log/cassandra/&gt;
</code></pre>

<p>Then restart the Cassandra service</p>
","['saved_caches_directory', 'commitlog_directory', 'data_file_directories']"
38350656,38350839,2016-07-13 11:37:10,Cassandra asks for ALLOW FILTERING even though column is clustering key,"<p>Very new to Cassandra so apologies if the question is simple.</p>

<p>I created a table:</p>

<pre><code>create table ApiLog (
LogId uuid,     
DateCreated timestamp,
ClientIpAddress varchar,
primary key (LogId, DateCreated));
</code></pre>

<p>This work fine:</p>

<pre><code>select * from apilog
</code></pre>

<p>If I try to add a where clause with the DateCreated like this:</p>

<pre><code>select * from apilog where datecreated &lt;= '2016-07-14'
</code></pre>

<p>I get this:</p>

<p><code>Cannot execute this query as it might involve data filtering and thus may have unpredictable performance. If you want to execute this query despite the performance unpredictability, use ALLOW FILTERING</code></p>

<p>From other questions here on SO and from the tutorials on datastax it is my understanding that since the datecreated column is a clustering key it can be used to filter data.</p>

<p>I also tried to create an index but I get the same message back. And I tried to remove the DateCreated from the primary key and have it only as an index and I still get the same back:</p>

<pre><code>create index ApiLog_DateCreated on dotnetdemo.apilog (datecreated);
</code></pre>
",<cassandra>,"<p>The partition key LogId determines on which node each partition will be stored. So if you don't specify the partition key, then Cassandra has to filter all the partitions of this table on all the nodes to find matching data. That's why you have to say ALLOW FILTERING, since that operation is very inefficient and is discouraged.</p>

<p>If you specify a specific LogId, then Cassandra can find the partition on a single node and efficiently do a range query by the clustering key.</p>

<p>So you need to plan your schema such that you can do your range queries within a single partition and not have to do a full table scan like you're trying to do.</p>
",['table']
38359717,38387725,2016-07-13 19:00:44,"How to model Cassandra DB for Time Series, server metrics","<p>My name is Daniel, 
I'm a newcomer accountwise but a long time lurker.
I decided to learn Apache Cassandra for my next ""lets write some code while the kids are sleeping"" project.</p>

<p>What i'm writing is a neat little api that will do read and writes against a cassandra database.
I had a lot of the db layout figured out in mongodb, but for me it's time to move on and grow as a engineer :)</p>

<p>Mission:
I will collect metrics from the servers in my rack, an agent will send a payload of metrics every minute.
I have the api part pretty much figured out, will use JWT tokens signing the payloads.
The type of data i will store can be seen below.
cpuload, cpuusage, memusage, diskusage etc.</p>

<p>The part where i am confused with cassandra is how to write the actual model, i understand the storagengines sort of writes it all as a time serie
on disk for me making reads quite amazing. i know anything i would whip together now would work for my lab since it's jsut 30 machines, 
but i'm trying to understand how these things are done properly and how it could be done for a real life scenario like server density, datadog , ""insert your prefered server monitoring service"". :)</p>

<p>But how are you more experienced engineers designing a schema like this ?</p>

<blockquote>
  <p>Usage scenarios for the database:</p>
  
  <ul>
  <li>write payloads every minute through the api. (lets imagine thats atleast 100k writes per minute for the sake of learning something
  useful)</li>
  <li><p>Read the assets associated with ones userid</p>
  
  <ul>
  <li>pull most recent data (3h)</li>
  <li>pull most recent data (daily)</li>
  <li>pull most recent data (weekly)</li>
  <li>pull most recent data (monthly)</li>
  <li>etc etc</li>
  </ul></li>
  <li><p>Generate monthly pdf reports showing uptime and such.</p></li>
  </ul>
</blockquote>

<p>Should i insert the rows containing the full payload or am i better of inserting them per service basis: timeuid|cpuusage<br>
Per service row</p>

<pre><code>CREATE TABLE metrics(
    id uuid PRIMARY KEY,
    assetid int,
    serviceType text,
    metricValue int
)
</code></pre>

<p>All in one</p>

<pre><code>CREATE TABLE metrics(
    id uuid PRIMARY KEY,
    assetid int,
    cpuload int,
    cpuusage int,
    memusage int,
    diskusage int,
)
</code></pre>

<p>In mongo i would preallocate the buckets, and also keep a quick read avg inside of the document.
So in the webgui i could simply show the avg stats for pre-defined time periods.</p>

<p>Examples for dumbasses are highly appreciated.
Hope you can decipher my rather poor english.</p>

<p>Just found this url in the SO suggestions:
<a href=""https://stackoverflow.com/questions/16191410/cassandra-data-model-for-time-series"">Cassandra data model for time series</a>
i guess that is something that applies to me aswell. </p>

<p>Sincerly
Daniel Olsson</p>
",<cassandra><time-series><nosql>,"<p>For your data model, I would suggest adding  <strong>time</strong>  as a clustering column:</p>

<pre><code>CREATE TABLE metrics(
id uuid,
time timeuuid,
assetid int,
cpuload int,
cpuusage int,
memusage int,
diskusage int,
PRIMARY KEY (id, time) WITH CLUSTERING ORDER BY (time DESC))
</code></pre>

<p>Use descending order to keep the latest metrics first.  You can then query using the LIMIT clause to get the most recent hour:</p>

<pre><code>SELECT * FROM metrics WHERE id = &lt;UUID&gt; LIMIT 60
</code></pre>

<p>Or day:</p>

<pre><code>SELECT * FROM metrics WHERE id = &lt;UUID&gt; LIMIT 1440
</code></pre>

<p>Depending upon how long you plan to keep the data, you may want to add a  column for year, month, or days to the table to limit your partition size.  For example, if you wish to keep data for 3 months,  a <strong>month</strong> column can be added to partition your keys by id and month:</p>

<pre><code>CREATE TABLE metrics(
id uuid,
time timeuuid,
month text,
assetid int,
cpuload int,
cpuusage int,
memusage int,
diskusage int,
PRIMARY KEY ((id, month), time) WITH CLUSTERING ORDER BY (time DESC))
</code></pre>

<p>If you keep data for several years, use year + month or a date value.</p>

<p>Regarding your final question, about separate tables or a single table.  Cassandra supports sparse columns, so you can make multiple inserts in a common table for each metric without updating any data.  However, it's always faster to write just once per row.  </p>

<p>You may need separate tables if you have to query for different metrics by an alternative key.  For example, query for disk usage by id and disk name.  You'd need a separate table or a materialized view to support that query pattern.</p>

<p>Finally, your schema defines an <strong>assetid</strong>, but this isn't defined in your primary key so with your current schema you can't query using assetid.</p>
",['table']
38388013,38388783,2016-07-15 04:46:53,Apply TTL in column level,"<p>Want to know, how to apply <code>TTL</code> in column level. </p>

<p>below query set the <code>TTL</code> at record level </p>

<pre><code>  INSERT INTO excelsior.clicks (
  userid, url, date, name)
  VALUES 
    (
    3715e600-2eb0-11e2-81c1-0800200c9a66,
   'http://apache.org',
   '2013-10-09', 'Mary'
     )
    USING TTL 86400;
</code></pre>

<p>whereas my requirement is setting <code>TTL</code> for a particular column. Is there any way to achieve this</p>
",<cassandra><ttl>,"<p>You can do an <code>INSERT</code> with partial data:</p>

<pre><code>cqlsh&gt; create KEYSPACE test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};
cqlsh&gt; use test;
cqlsh:test&gt; create table test(userid uuid, url text, date text, name text, primary key(userid));
cqlsh:test&gt; 
cqlsh:test&gt; insert into test(userid, url, date, name) VALUES 
        ...     (
        ...     3715e600-2eb0-11e2-81c1-0800200c9a66,
        ...    'http://apache.org',
        ...    '2013-10-09', 'Mary'
        ...      )
        ...     USING TTL 86400;
cqlsh:test&gt; 
cqlsh:test&gt; select userid, url, TTL(url), date, TTL(date), name, TTL(name) from test;

 userid                               | url               | ttl(url) | date       | ttl(date) | name | ttl(name)
--------------------------------------+-------------------+----------+------------+-----------+------+-----------
 3715e600-2eb0-11e2-81c1-0800200c9a66 | http://apache.org |    86342 | 2013-10-09 |     86342 | Mary |     86342

(1 rows)    
cqlsh:test&gt; insert into test(userid, url ) VALUES (3715e600-2eb0-11e2-81c1-0800200c9a66,    'http://apache.org'   ) USING TTL 864000; 
cqlsh:test&gt; 
cqlsh:test&gt; select userid, url, TTL(url), date, TTL(date), name, TTL(name) from test;

 userid                               | url               | ttl(url) | date       | ttl(date) | name | ttl(name)
--------------------------------------+-------------------+----------+------------+-----------+------+-----------
 3715e600-2eb0-11e2-81c1-0800200c9a66 | http://apache.org |   863992 | 2013-10-09 |     86109 | Mary |     86109

(1 rows)
cqlsh:test&gt; 
</code></pre>

<p>If you do an insert statement per column, you can set a TTL on each column individually. </p>
",['table']
38476356,38476664,2016-07-20 08:40:43,Can a cassandra table be queried using only a part of the composite partition key?,"<p>Consider a table like this to store a user's contacts -</p>

<pre><code>CREATE TABLE contacts {
    user_name text,
    contact_name text,
    contact_id int, 
    contact_data blob,
    PRIMARYKEY ((user, contact_name), contact_id)
    //          ^-- Note the composite partition key
}
</code></pre>

<p>The composite partition key results in a row per contact. </p>

<p>Let's say there are a 100 million users and every user has a few hundred contacts.</p>

<p>I can look up a particular user's particular contact's data by using </p>

<pre><code>SELECT contact_data FROM contacts WHERE user_name='foo' AND contact_name='bar'
</code></pre>

<p>However, is it also possible to look up all contact names for a user using something like,</p>

<pre><code>SELECT contact_name FROM contacts WHERE user_name='foo'
</code></pre>

<p>? could the WHERE clause contain only some of all the columns that form the primary key?</p>

<p>EDIT -- I tried this and cassandra doesn't allow it. So my question now is, how would you model the data to support two queries - </p>

<ol>
<li>Get data for a specific user &amp; contact</li>
<li>Get all contact names for a user</li>
</ol>

<p>I can think of two options -</p>

<ol>
<li>Create another table containing user_name and contact_name with only user_name as the primary key. But then if a user has too many contacts, could that be a wide row issue?</li>
<li>Create an index on user_name. But given 100M users with only a few hundred contacts per user, would user_name be considered a high-cardinality value hence bad for use in index?</li>
</ol>
",<cassandra><data-modeling><cql>,"<p>In a RDBMS the query planner might be able to create an efficient query plan for that kind of query. But Cassandra can not. Cassandra would have to do a table scan. Cassandra tries hard not to allow you to make those kinds of queries. So it should reject it.</p>
",['table']
38506734,38509423,2016-07-21 14:17:47,Cassandra commit log clarification,"<p>I have read over several documents regarding the Cassandra commit log and, to me, there is conflicting information regarding this ""structure(s)"". The diagram shows that when a write occurs, Cassandra writes to the memtable and commit log. The confusing part is where this commit log resides.</p>

<p>The diagram that I've seen over-and-over shows the commit log on disk. However, if you do some more reading, they also talk about a commit log buffer in memory - and that piece of memory is flushed to disk every 10 seconds.</p>

<p>DataStax Documentation states: 
""When a write occurs, Cassandra stores the data in a memory structure called memtable, and to provide configurable durability, it also appends writes to the commit log buffer in memory. This buffer is flushed to disk every 10 seconds"". </p>

<p>Nowhere in their diagram do they show a memory structure called a commit log buffer. They only show the commit log residing on disk.</p>

<p>It also states:
""When a write occurs, Cassandra stores the data in a structure in memory, the memtable, and also appends writes to the commit log on disk.""</p>

<p>So I'm confused by the above. Is it written to the commit log memory buffer, which is eventually flushed to disk (which I would assume is also called the ""commit log""), or is it written to the memtable and commit log on disk?</p>

<p>Apache's documentation states this:
""Instead, like other modern systems, Cassandra provides durability by appending writes to a commitlog first. This means that only the commitlog needs to be fsync'd, which, if the commitlog is on its own volume, obviates the need for seeking since the commitlog is append-only. Implementation details are in ArchitectureCommitLog.</p>

<p>Cassandra's default configuration sets the commitlog_sync mode to periodic, causing the commitlog to be synced every commitlog_sync_period_in_ms milliseconds, so you can potentially lose up to that much data if all replicas crash within that window of time.""</p>

<p>What I have inferred from the Apache statement is that ONLY because of the asynchronous nature of writes (acknowledgement of a cache write) could you lose data (it even states you can lose data if all replicas crash before it is flushed/sync'd). </p>

<p>I'm not sure what I can infer from the DataStax documentation and diagram as they've mentioned two different statements regarding the commit log - one in memory, one on disk.</p>

<p>Can anyone clarify, what I consider, a poorly worded and conflicting set of documentation?</p>

<p>I'll assume there is a commit log buffer, as they both reference it (yet DataStax doesn't show it in the diagram). How and when this is managed, I think, is a key to understand.</p>
",<cassandra><datastax>,"<p>Generally when explaining the write path, the commit log is characterized as a file - and it's true the commit log is the on-disk storage mechanism that provides durability.  The confusion is introduced when going deeper and the part about buffer cache and having to issue fsyncs is introduced.  The reference to ""commit log buffer in memory"" is talking about OS buffer cache, not a memory structure in Cassandra. You can see in the <a href=""https://github.com/apache/cassandra/blob/cassandra-2.2/src/java/org/apache/cassandra/db/commitlog/CommitLog.java#L272"" rel=""noreferrer"">code</a> that there's not a separate in-memory structure for the commit log, but rather the mutation is serialized and written to a <a href=""https://github.com/apache/cassandra/blob/cassandra-2.2/src/java/org/apache/cassandra/db/commitlog/CommitLogSegment.java#L145"" rel=""noreferrer"">file-backed buffer</a>.</p>

<p>Cassandra comes with two strategies for managing fsync on the commit log.</p>

<pre><code>commitlog_sync 
    (Default: periodic) The method that Cassandra uses to acknowledge writes in milliseconds:
    periodic: (Default: 10000 milliseconds [10 seconds])
    Used with commitlog_sync_period_in_ms to control how often the commit log is synchronized to disk. Periodic syncs are acknowledged immediately.

    batch: (Default: disabled)note
    Used with commitlog_sync_batch_window_in_ms (Default: 2 ms) to control how long Cassandra waits for other writes before performing a sync. When using this method, writes are not acknowledged until fsynced to disk.
</code></pre>

<p>The <code>periodic</code> offers better performance at the cost of a small increase in the chance that data can be lost.  The <code>batch</code> setting guarantees durability at the cost of latency.</p>
",['commitlog_sync_batch_window_in_ms']
38536808,38540632,2016-07-23 00:01:02,Cassandra how can I simulate a join statement,"<p>I am new to cassandra and am coming from Postgres. I was wondering if there is a way that I can get data from 2 different tables or column family and then return the results. I have this query</p>

<pre><code>select p.fullname,p.picture s.post, s.id, s.comments, s.state, s.city FROM profiles as p INNER JOIN Chats as s ON(p.id==s.profile_id) WHERE s.latitudes&gt;=28 AND 29&gt;= s.latitudes AND s.longitudes
    ""&gt;=-21 AND -23&gt;= s.longitudes 
</code></pre>

<p>The query has 2 tables: Profiles and Chat and they both share a common field <strong>Chats.id==Proifles.profile_id</strong> it boils down to this basically return all rows where Chat ID is equal to Profiles id. I would like to keep it that way because now updating profiles are simple and would only need to update 1 row per profile update instead of de-normalizing everything and updating thousands of records. Any help or suggestions would be great </p>
",<cassandra><cassandra-2.0><nosql>,"<p>You have to design tables in way you won't need joins. Best practice is if your table matches exactly the use case it is used for.</p>

<p>Cassadra has a feature called <a href=""https://docs.datastax.com/en/cql/3.3/cql/cql_reference/refStaticCol.html"" rel=""nofollow"">shared static columns</a>; this allows you to bind values with partition part of primary key. Thus, you can create ""joined"" version of table without duplicates.</p>

<pre><code>CREATE TABLE t (
    p_id uuid,
    p_fullname text STATIC,
    p_picture text STATIC,
    s_id uuid,
    s_post text,
    s_comments text,
    s_state text,
    s_city text,
    PRIMARY KEY (p_id, s_id)
);
</code></pre>
",['table']
38591530,38640788,2016-07-26 13:41:07,Securing an individual column in Cassandra,"<p>We want our developers to be able to query our Cassandra tables in production (for trouble shooting and other analysis).  However, we don't want them seeing secure data like e-mail addresses and names of customers.</p>

<p>I've thought about creating a MATERIALIZED VIEW in Cassandra that selects then entire table except for the sensitive columns.  Then I could grant developers SELECT access on the view, but not the main table.</p>

<p>Is that the best way to secure a column in Cassandra?  </p>

<p>Someone else I know suggested just encrypting those columns using a data-encrypt feature that our application already has.  We would encrypt a field into a string of Hex codes before giving it to Cassandra to store.  That would have an added benefit of encrypting the data at-rest in the Cassandra sstable and commit log.  BUT if that field is encrypted, then Cassandra can't see the true value of that field, and that could be a big problem for ORDER BY or other CQL comparison functions.</p>

<p>How are most people securing data in Cassandra?  :)</p>
",<security><cassandra>,"<p>There is no one right answer to this today.  In addition to encryption or views, another approach would be to basically normalize the data, and store the sensitive data in a separate table and/or keyspace.</p>

<p><a href=""http://dataguise"" rel=""nofollow"">DataGuise</a>'s DgSecure is a commercial product which offers masking capabilities for NoSQL, including Cassandra.  There seems to be little information other than <a href=""http://www.marketwired.com/press-release/dataguise-announces-new-sensitive-data-monitoring-solution-that-accelerates-breach-detection-2104970.htm"" rel=""nofollow"">press releases</a> available about it.</p>
",['table']
38634874,38638377,2016-07-28 11:20:18,Cassandra: how to initialize the counter column with value?,"<p>I have to benchmark Cassandra with the Facebook Linkbench. There are two phase during the Benchmark, the load and the request phase.
in the Load Phase, Linkbench fill the cassandra tables : nodes, links and counts (for links counting) with default values(graph data). </p>

<p>the count table looks like this: </p>

<p><code>keyspace.counttable (
    link_id bigint,
    link_type bigint,
    time bigint,
    version bigint,
    count counter,
    PRIMARY KEY (link_id, link_type, time, version)
</code></p>

<p>my question is how to insert the default counter values (before incrementing and decrementing the counter in the Linkbench request phase) ?</p>

<p>If it isn't possible to do that with cassandra, how should i increment/decrement a bigint variable (instead of counter variable)</p>

<p>Any suggest and comments? Thanks a lot.</p>
",<cassandra>,"<p>The default value is zero. Given</p>

<p><code>create table counttable (
    link_id bigint,
    link_type bigint,
    time bigint,
    version bigint,
    count counter,
    PRIMARY KEY (link_id, link_type, time, version)
);
</code></p>

<p>and </p>

<p><code>update counttable set count = count + 1 where link_id = 1 and link_type = 1 and time = 1 and version = 1;
</code></p>

<p>We see that the value of count is now 1.</p>

<p><code>select * from counttable ;
 link_id | link_type | time | version | count
---------+-----------+------+---------+-------
       1 |         1 |    1 |       1 |     1
(1 rows)
</code></p>

<p>So, if we want to set it to some other value we can:</p>

<p><code>update counttable set count = count + 500 where link_id = 1 and link_type = 1 and time = 1 and version = 2;
select * from counttable ;
 link_id | link_type | time | version | count
---------+-----------+------+---------+-------
       1 |         1 |    1 |       1 |     1
       1 |         1 |    1 |       2 |   500
(2 rows)
</code></p>
",['table']
38637245,38640564,2016-07-28 13:04:08,Cassandra AssertionError: length is not > 0,"<p>Running into the following exception trying to start my 3.0.5 cassandra cluster.  Not sure what this means or how to proceed.</p>

<pre><code>INFO  14:14:05 Initializing keyspace.table
Exception (java.lang.AssertionError) encountered during startup: length is not &gt; 0: 0
java.lang.AssertionError: length is not &gt; 0: 0
    at org.apache.cassandra.utils.ByteBufferUtil.readBytes(ByteBufferUtil.java:408)
    at org.apache.cassandra.io.sstable.metadata.CompactionMetadata$CompactionMetadataSerializer.deserialize(CompactionMetadata.java:93)
    at org.apache.cassandra.io.sstable.metadata.CompactionMetadata$CompactionMetadataSerializer.deserialize(CompactionMetadata.java:73)
    at org.apache.cassandra.io.sstable.metadata.MetadataSerializer.deserialize(MetadataSerializer.java:123)
    at org.apache.cassandra.io.sstable.metadata.MetadataSerializer.deserialize(MetadataSerializer.java:94)
    at org.apache.cassandra.io.sstable.metadata.MetadataSerializer.mutateLevel(MetadataSerializer.java:133)
    at org.apache.cassandra.db.compaction.LeveledManifest.add(LeveledManifest.java:132)
    at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.addSSTable(LeveledCompactionStrategy.java:278)
    at org.apache.cassandra.db.compaction.CompactionStrategyManager.startup(CompactionStrategyManager.java:135)
    at org.apache.cassandra.db.compaction.CompactionStrategyManager.reload(CompactionStrategyManager.java:187)
    at org.apache.cassandra.db.compaction.CompactionStrategyManager.&lt;init&gt;(CompactionStrategyManager.java:75)
    at org.apache.cassandra.db.ColumnFamilyStore.&lt;init&gt;(ColumnFamilyStore.java:394)
    at org.apache.cassandra.db.ColumnFamilyStore.&lt;init&gt;(ColumnFamilyStore.java:353)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:560)
    at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:537)
    at org.apache.cassandra.db.Keyspace.initCf(Keyspace.java:368)
    at org.apache.cassandra.db.Keyspace.&lt;init&gt;(Keyspace.java:305)
    at org.apache.cassandra.db.Keyspace.open(Keyspace.java:129)
    at org.apache.cassandra.db.Keyspace.open(Keyspace.java:106)
    at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:250)
    at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:551)
    at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:679)
ERROR 10:37:43 Exception encountered during startup
</code></pre>
",<cassandra>,"<p>Your one table looks to have a corrupted metadata component. Try running <code>sstablescrub</code> (<a href=""https://docs.datastax.com/en/cassandra/2.2/cassandra/tools/toolsSSTableScrub.html"" rel=""nofollow"">https://docs.datastax.com/en/cassandra/2.2/cassandra/tools/toolsSSTableScrub.html</a>) on the sstables in the affected table and remove the offending one. You should only need to run the manifest check (<code>--manifest-check</code> option) since thats whats actually failing in your stacktrace. Once node starts up again be sure to run a repair on it to restore any lost data.</p>
",['table']
38670164,41263146,2016-07-30 03:47:44,How to update multiple tables in Spring CassandraRepository,"<p>Suppose I have a users table in cassandra called 'UserPrincipal', the repository will look something like the following</p>

<pre><code>public interface UserRepository extends CassandraRepository&lt;UserPrincipal&gt; 
{
   @Query(""SELECT * FROM UserPrincipal WHERE email = ?0"")
   UserPrincipal findByEmailAddress(String emailAddress);   
}
</code></pre>

<p>If I need to query the table with username for example, I have to denormalize the table and create a duplicate and let's call it UserPrincipalByUsername which is identical to the first one and only different with the primary key, now, can I use the following Interface as a repository? and what about saving/removing a user to/from  both tables simultaneously to maintain data consistency?</p>

<pre><code>public interface UserRepository extends CassandraRepository&lt;UserPrincipal&gt; 
{
   @Query(""SELECT * FROM UserPrincipal WHERE email = ?0"")
   UserPrincipal findByEmailAddress(String emailAddress);   

   @Query(""SELECT * FROM UserPrincipalByUsername WHERE username= ?0"")
   UserPrincipal findByUsername(String username);   
}
</code></pre>

<p>It can be noted that two separate interfaces can be used to deal with each table alone, but still, I need to have some logic to maintain the consistency at some point.</p>

<p>I am using Cassandra 2.0.11, CQL spec 3.1.1, Spring data Cassandra 1.3.2 and Spring boot 1.3.1</p>
",<java><spring><spring-boot><cassandra><spring-data-cassandra>,"<p>The only procedure I found to solve this is, as mentioned in the question, to use two separate interfaces to deal with each table alone, I have added a wrapper class to use both of them to <code>save</code> using one call, but this dosen't guarantee consistency all the time (in a case of server/system failure for example), but this is ok in my specific application.</p>
",['table']
38696316,38698339,2016-08-01 10:16:19,How to list all cassandra tables,"<p>There are many tables in cassandra database, which contain column titled user_id. The values user_id are referred to user stored in table users. As some users are deleted, I would like to delete orphan records in all tables that contain column titled user_id.</p>

<p>Is there a way to list all tables using CassandraSQLContext or any other built-in method or custom procedure in order to avoid explicitly defining the list of tables?</p>
",<scala><apache-spark><cassandra><spark-cassandra-connector>,"<p>There are system tables which can provide information about stored keyspaces, tables, columns.</p>
<p>Try run follows commands in cqlsh console:</p>
<ol>
<li><p>Get keyspaces info</p>
<p><code>SELECT * FROM system.schema_keyspaces ;</code></p>
</li>
<li><p>Get tables info</p>
<p><code>SELECT columnfamily_name FROM system.schema_columnfamilies WHERE keyspace_name = 'keyspace name';</code></p>
</li>
<li><p>Get table info</p>
<p><code>SELECT column_name, type, validator FROM system.schema_columns WHERE keyspace_name = 'keyspace name' AND columnfamily_name = 'table name';</code></p>
</li>
</ol>
<p>Since v 5.0.x
<a href=""https://docs.datastax.com/en/dse/5.1/cql/cql/cql_using/useQuerySystemTable.html"" rel=""noreferrer"">Docs</a></p>
<ol>
<li><p>Get keyspaces info</p>
<p><code>SELECT * FROM system_schema.keyspaces;</code></p>
</li>
<li><p>Get tables info</p>
<p><code>SELECT * FROM system_schema.tables WHERE keyspace_name = 'keyspace name';</code></p>
</li>
<li><p>Get table info</p>
<p><code>SELECT * FROM system_schema.columns
WHERE keyspace_name = 'keyspace_name' AND table_name = 'table_name';</code></p>
</li>
</ol>
<p>Since v 6.0
<a href=""https://docs.datastax.com/en/dse/6.0/cql/cql/cql_using/useQuerySystemTable.html"" rel=""noreferrer"">Docs</a></p>
<ol>
<li><p>Get keyspaces info</p>
<p><code>SELECT * FROM system_schema.keyspaces</code></p>
</li>
<li><p>Get tables info</p>
<p><code>SELECT * FROM system_schema.tables WHERE keyspace_name = 'keyspace name';</code></p>
</li>
<li><p>Get table info</p>
<p><code>SELECT * FROM system_schema.columns
WHERE keyspace_name = 'keyspace_name' AND table_name = 'table_name';</code></p>
</li>
</ol>
",['table']
38718623,38721676,2016-08-02 11:01:37,Connecting two desktop PC into multi node cluster Cassandra,"<p>I'm doing this for the very first time and I need help about clearing some stuffs. 
I have Ubuntu 14.04 desktop on one machine, and on the other I have Windows 8.1. also on both machines I have installed the same version of Cassandra. </p>

<p>Can someone tell me, is it possible to connect 2 desktop machines and make a cluster with 2 nodes in Cassandra and how?</p>
",<cassandra><replication><datastax><hierarchical-clustering><cassandra-2.1>,"<p>You need make sure the following settings are set correctly in the cassandra.yaml on each machine:</p>

<ol>
<li>cluster_name - this needs to be the same on both nodes</li>
<li>seed_provider.parameters.seeds - this needs to be set to the external IP address of one of the nodes and needs to be the same on both nodes.</li>
<li>listen_address - this needs to be set to the external IP address on each machine.</li>
</ol>

<p>Note: Make sure you can ping each machine from the other on the IP address you use for the listen_address and make sure that the storage_port 7000 isn't blocked by a firewall on either machine.</p>
","['storage_port', 'listen_address']"
38730024,38733805,2016-08-02 20:37:57,cannot connect to local Cassandra instance?,"<p>I am a Cassandra newbie so this is a very rudimentary question. For my project I need an older version of Cassandra, so I installed it like so: </p>

<pre><code>brew install python
brew install homebrew/versions/cassandra22
pip install cql
</code></pre>

<p>After that, I simply started it via Homebrew too, like so:</p>

<pre><code>brew services start homebrew/versions/cassandra22
</code></pre>

<p>I can see it in the list of services having been started:</p>

<pre><code>tracyxia$ brew services list
Name        Status  User     Plist
cassandra22 started tracyxia  /Users/tracyxia/Library/LaunchAgents/homebrew.mxcl.cassandra22.plist
</code></pre>

<p>Furthermore, I can also see it running as a service on my Mac:</p>

<pre><code>tracyxia$ ps -ef | grep cassandra
1425523232  9962 87919   0  4:33PM ttys000    0:00.00 grep cassandra
</code></pre>

<p>But when I try to connect to my local instance of Cassandra via DevCenter, I kept getting the ""cannot connect to host"" error. :( I am pretty sure this is an installation issue because it works perfectly fine when I installed Cassandra 3.0.7 (current default version for Homebrew cassandra). </p>

<p>Any help would be most appreciated! </p>
",<cassandra>,"<p><a href=""https://docs.datastax.com/en/cassandra/1.2/cassandra/configuration/configCassandra_yaml_r.html"" rel=""nofollow"">configure</a> listen_address and rpc_address as below in cassandra.yaml</p>

<p>listen_address : 192.168.1.15 (configure local IP)</p>

<p>rpc_address : 0.0.0.0</p>
","['rpc_address', 'listen_address']"
38738282,38857262,2016-08-03 08:26:06,YCSB low read throughput cassandra,"<p>The <a href=""http://www.datastax.com/wp-content/themes/datastax-2014-08/files/NoSQL_Benchmarks_EndPoint.pdf"" rel=""nofollow"">YCSB Endpoint benchmark</a> would have you believe that Cassandra is the golden child of Nosql databases.  However, recreating the results on our own boxes (8 cores with hyperthreading, 60 GB memory, 2 500 GB SSD), we are having dismal read throughput for workload b (read mostly, aka 95% read, 5% update).</p>

<p>The cassandra.yaml settings are exactly the same as the Endpoint settings, barring the different ip addresses, and our disk configuration (1 SSD for data, 1 for a commit log).  While their throughput is ~38,000 operations per second, ours is ~16,000 regardless (relatively) of the threads/number of client nodes.  I.e. one worker node with 256 threads will report ~16,000 ops/sec, while 4 nodes will each report ~4,000 ops/sec</p>

<p>I've set the readahead value to 8KB for the SSD data drive.  I'll put the custom workload file below.</p>

<p>When analyzing disk io &amp; cpu usage with iostat, it seems that the reading throughput is consistently ~200,000 KB/s, which seems to suggest that the ycsb cluster throughput should be higher (records are 100 bytes).  ~25-30% of cpu seems to be under %iowait, 10-25% in use by the user.</p>

<p>top and nload stats are not ostensibly bottlenecked (&lt;50% memory usage, and 10-50 Mbits/sec for a 10 Gb/s link).</p>

<pre><code># The name of the workload class to use
workload=com.yahoo.ycsb.workloads.CoreWorkload

# There is no default setting for recordcount but it is
# required to be set.
# The number of records in the table to be inserted in
# the load phase or the number of records already in the
# table before the run phase.
recordcount=2000000000

# There is no default setting for operationcount but it is
# required to be set.
# The number of operations to use during the run phase.
operationcount=9000000

# The offset of the first insertion
insertstart=0
insertcount=500000000

core_workload_insertion_retry_limit = 10
core_workload_insertion_retry_interval = 1

# The number of fields in a record
fieldcount=10

# The size of each field (in bytes)
fieldlength=10

# Should read all fields
readallfields=true

# Should write all fields on update
writeallfields=false

fieldlengthdistribution=constant

readproportion=0.95

updateproportion=0.05

insertproportion=0

readmodifywriteproportion=0

scanproportion=0

maxscanlength=1000

scanlengthdistribution=uniform

insertorder=hashed

requestdistribution=zipfian
hotspotdatafraction=0.2

hotspotopnfraction=0.8
table=usertable

measurementtype=histogram

histogram.buckets=1000
timeseries.granularity=1000
</code></pre>
",<cassandra><benchmarking><ycsb>,"<p>The key was increasing native_transport_max_threads in the casssandra.yaml file.</p>

<p>Along with the increased settings in the comment (increasing connections in ycsb client as well as concurrent read/writes in cassandra), Cassandra jumped to ~80,000 ops/sec.</p>
",['native_transport_max_threads']
38774693,38775127,2016-08-04 18:20:18,How to create unique key using cassandra database,"<p>I am beginner of cassandra DB I want to create unique key like oracle in cassandra.</p>

<p>I searched a lot site but not able to get relevant answer.</p>

<p>is it possible to create unique key using cassandra ?</p>
",<database><cassandra><cassandra-2.1>,"<p>In Cassandra, the <code>PRIMARY KEY</code> definition of your table is used for uniqueness. For example:</p>

<pre><code>CREATE TABLE users (
  userid uuid,
  firstname text,
  lastname text,
  email text,
  created_date timestamp,
  PRIMARY KEY (userid)
);
</code></pre>

<p>Here, the <code>userid</code> column is the unique identifier. You can, of course, have multiple columns as part of your <code>PRIMARY KEY</code> definition as well. But a few things to keep in mind:</p>

<ul>
<li>Primary key columns have other implications in Cassandra as well (beyond uniqueness). You'll want to read up on Partition Keys and Clustering Columns and how Cassandra uses it to organize data around the cluster and on disk.</li>
<li>Cassandra doesn't have or enforce constraints (for example, no foreign keys)</li>
<li>Cassandra doesn't do a read before a write (unless you're using the Lightweight Transactions feature), and so doing an <code>INSERT</code> or an <code>UPDATE</code> are functionally equivalent (i.e. an ""upsert"") and will overwrite data that already exists</li>
</ul>

<p>If you're looking for a feature like a ""unique constraint"" or ""unique index"" in Oracle, you won't find it in Cassandra. There's a simple data modeling example available in <a href=""https://docs.datastax.com/en/cql/3.1/cql/ddl/ddlCQLDataModelingTOC.html"" rel=""nofollow"">the CQL docs</a> and I also recommend checking out the data modeling course it links to if you're just getting started with Cassandra. Good luck!</p>
",['table']
38823967,38835436,2016-08-08 07:57:55,Confusion over data model in cassandra,"<p>Hello we have a table in Cassandra whose structure is as below</p>

<pre><code>CREATE TABLE dmp.user_profiles_6 (
    vuid text PRIMARY KEY,
    brand_model text,
    first_seen timestamp,
    last_seen timestamp,
    total_day_count int,
    total_usage_count int,
    user_type text
) WITH bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.LeveledCompactionStrategy'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.1
    AND speculative_retry = '99PERCENTILE';
</code></pre>

<p>I read a few articles about data modeling in Cassandra from datastax. In in it said that primary key consists of partition key and clustering key. </p>

<p>Now in above case we have a vuid column which is an identifier for every unique user. It is primary key. We have 400M unique users. So now does it mean that Cassandra is making 400M partitions? Then this must degrade the performance. In one datastax article about data modeling an example table shows primary key on a uuid column which is unique and having a very high cardinality. I am totally confused, can anyone help me identify which column can be set as partition key and which as cluster key?</p>

<p>Queries can be as below:
1. Select record directly on basis of vuid
2. Select vuids on basis of range of last seen or first seen</p>
",<cassandra>,"<ol>
<li>Select record directly on basis of vuid  >>
Your table does that. It already has vuid as a primary key.  </li>
<li>Select vuids on basis of range of last seen or first seen >><br>
There are two options here:
Either add last_seen or first_seen in clustering columns (you can do range selection on <code>clustering columns</code> only)<br>
In this case you need to provide vuid along with last_seen and first_seen on the query. I don't think you want that.<br>
OR<br>
Create another table which has the same data(Yes,in C* we create another table for different query with same data and change the keys as per query. Welcome to data duplication). In this table you have to have to add a dummy column as primary key and make the last_seen and first_seen as clustering keys.You pass these <code>seen</code> dates in query to fetch vuid.  </li>
</ol>

<p>Hope this is clear.</p>
",['table']
38931909,38974587,2016-08-13 10:49:10,Cassandra : Batch write optimisation,"<p>I get bulk write request for let say some 20 keys from client.
I can either write them to C* in one batch or write them individually in async way and wait on future to get them completed.</p>

<p>Writing in batch does not seem to be a goo option as per documentation as my insertion rate will be high and if keys belong to different partitions co-ordinators will have to do extra work.</p>

<blockquote>
  <p>Is there a way in datastax java driver with which I can group keys
  which could belong to same partition and then club them into small
  batches and then do invidual unlogged batch write in async. IN that
  way i make less rpc calls to server at the same time coordinator will
  have to write locally. I will be using token aware policy.</p>
</blockquote>
",<cassandra><datastax><datastax-java-driver><cassandra-3.0>,"<p>Your idea is right, but there is no built-in way, you usually do that manually. </p>

<p>Main rule here is to use <code>TokenAwarePolicy</code>, so some coordination would happen on driver side.
Then, you could group your requests by equality of partition key, that would probably be enough, depending on your workload. </p>

<p>What I mean by 'grouping by equality of partition key` is e.g. you have some data that looks like </p>

<pre><code>MyData { partitioningKey, clusteringKey, otherValue, andAnotherOne }
</code></pre>

<p>Then when inserting several such objects, you group them by <code>MyData.partitioningKey</code>. It is, for all existsing <code>paritioningKey</code> values, you take all objects with same <code>partitioningKey</code>, and wrap them in <code>BatchStatement</code>. Now you have several <code>BatchStatements</code>, so just execute them.</p>

<p>If you wish to go further and mimic cassandra hashing, then you should look at cluster metadata via <code>getMetadata</code> method in <code>com.datastax.driver.core.Cluster</code> class, there is method <code>getTokenRanges</code> and compare them to result of <code>Murmur3Partitioner.getToken</code> or any other partitioner you configured in <code>cassandra.yaml</code>. I've never tried that myself though.</p>

<p>So, I would recommend to implement first approach, and then benchmark your application. I'm using that approach myself, and on my workload it works far better than without batches, let alone batches without grouping.</p>
",['partitioner']
38966660,39195778,2016-08-16 04:28:51,How to define keyspaces for a timeseries data in Cassandra?,"<p>There are 100s of data points, each data point has its own seperate table with schema and queries as mentioned below:</p>
<h2>Current Schema in SQLite</h2>
<ul>
<li>Table Name: Name of Data Point e.g. Tempearature</li>
<li>Column-1: Name: Timestamp Type: TEXT (yyyy-MM-dd HH:mm:ss.ttt format) PRIMARY KEY</li>
<li>Column-2: Name: Value Type: FLOAT</li>
<li>Column-3: Name: Quality Type: TEXT (&quot;GOOD&quot;, &quot;BAD&quot;)</li>
</ul>
<h2>Queries for SQLite</h2>
<ul>
<li>SELECT * FROM <em>data-point-name</em>;</li>
<li>SELECT * FROM <em>data-point-name</em> WHERE Timestamp BETWEEN <em>timesamp-1</em> AND <em>timestamp-2</em>;</li>
<li>INSERT INTO <em>data-point-name</em> (Timestamp, Value, Quality) VALUES (&quot;2016-01-01 00:00:05.254&quot;, 123.25454, &quot;GOOD&quot;); (this is an example)</li>
</ul>
<p>Currently I have SQLite db where I have a table per data-point with above schema, essentially I have 100s of tables. This way reads/writes are not disturbing queries running on different data-points.</p>
<p>How to translate this schema to be used in Cassandra?</p>
",<cassandra>,"<p>In your case, you can store all your data points in a single table :</p>

<pre><code>CREATE TABLE datapoints (
    datatype varchar(30),
    time timestamp,
    value float,
    quality varchar(4),
    PRIMARY KEY (datatype, time)
);
</code></pre>

<p>With this structure, you can run queries like : </p>

<pre><code>SELECT * 
FROM datapoints 
WHERE datatype = 'data-point-name';

SELECT * 
FROM datapoints 
WHERE datatype = 'data-point-name'
   AND time &gt;= '2016-01-01 00:00:00' 
   AND time &lt;= '2016-01-02 00:00:00';
</code></pre>

<p>But with this structure, cassandra will partition data by datapoint name,
if you have many points, your partition will be huge and you can have query performence issues.</p>

<p>You can also refine the partitionning by decompose the time : </p>

<pre><code>CREATE TABLE datapoints (
   datatype varchar(30),
   year int,
   month int,
   day int,
   milisecondsinday int,
   value float,
   quality varchar(4),
   PRIMARY KEY ((datatype, year, month, day), milisecondsinday)
) WITH CLUSTERING ORDER BY (milisecondsinday ASC);
</code></pre>

<p>In this case, this structure allow cassandra to store datas in more small partition than the first exemple and it's more powerfull if you query you data by day :</p>

<pre><code>SELECT *
FROM datapoints
WHERE datatype = 'data-point-type'
   AND year = 2016
   AND month = 1
   AND day = 1;
</code></pre>

<p>get all points for 'data-points-type'
for the <code>2016-01-01</code>
between <code>00:00 AM</code> and <code>01:00 AM</code></p>

<pre><code>SELECT *
FROM datapoints
WHERE datatype = 'data-point-type'
   AND year = 2016
   AND month = 1
   AND day = 1
   AND milisecondsinday &gt;= 0
   AND milisecondsinday &lt;= 3600000;
</code></pre>

<p>Of course, you can partition by day (like exemple) or others time scale (hours, minutes, seconds and miliseconds). If you can, small partition will be good for performence.</p>

<p>Hope this can help you.</p>
",['table']
38973463,38976606,2016-08-16 11:15:02,How to update configuration of a Cassandra cluster,"<p>I have a 3 node Cassandra cluster and I want to make some adjustments to the cassandra.yaml</p>

<p>My question is, how should I perform this? One node at a time or is there a way to make it happen without shutting down nodes?</p>

<p>Btw, I am using Cassandra 2.2 and this is a production cluster.</p>
",<cassandra><devops>,"<p>There are multiple approaches here:</p>

<ol>
<li><p>If you edit the cassandra.yaml file, you need to restart cassandra to re-read the contents of that file. If you restart all nodes at once, your cluster will be unavailable. Restarting one node at a time is almost always safe (provided you have sane replication-factors and consistency-levels). If your cluster is configured to survive a rack or datacenter outage, then you can safely restart more nodes concurrently.</p></li>
<li><p>Many settings can be changed without a restart via JMX, though I don't have a documentation link handy. Changing via JMX WON'T change cassandra.yml though, so you'll need to update that also or your config will revert back to what's in the file when the node restarts.</p></li>
<li><p>If you're using DSE, OpsCenter's Lifecycle Manager feature makes updating configs a simple point-and-click affair (disclaimer, I'm biased as I'm an LCM dev).</p></li>
</ol>
",['rack']
39011064,39023613,2016-08-18 06:14:13,Regular updates on particular set of rows has degraded performance of cassandra,"<p>In one of my tables, there are around 20 million rows(can grow more in future) which need to be updated daily.</p>

<p>Earlier this updation process was quite smooth (<strong>throughput around 100K updates/min, without any increase in load avg. on machines</strong>). But after 1 month of regular updates, performance has degraded a lot. Now, even if I try to do updates at low throughput i.e. around 30K rpm, load average on machines gets high and other queries also gets affected.</p>

<p><em>Cassandra version</em>--> 2.0.14</p>

<p><em>Machines config</em>--> (RAID-1, 1TB, 32core, 64 GB RAM), similar 4 machines with replication factor of 3.</p>

<p><em>Compaction Strategy of given table</em> --> SizeTiered</p>

<p>What changes should I try to scale it?</p>
",<cassandra><cassandra-2.0>,"<p>Here are some ideas for you:</p>

<ul>
<li><p>Cassandra prefers more small machines instead of a few large ones. It is recommended to keep heap 8GB max. large heap = long GC = pauses and lower performance (this could be your case, but you have to monitor your environment, check if it's a GC taking CPU or what). Also do not store too much data on single node, repair &amp; compaction might take too long and take cpu.</p></li>
<li><p>you could try to tune your cassandra (see e.g. <a href=""https://tobert.github.io/pages/als-cassandra-21-tuning-guide.html"" rel=""nofollow"">this guide</a> - unfortunatelly for Cassandra 2.1)</p></li>
<li>you may consider also cassandra upgrade</li>
<li>if you update whole table daily - then this might be an anti-pattern for cassandra - large number of updates = longer compactions. if you want to keeps current design I would recommend too have more nodes with less data per node and check how much time&amp;resources takes the compaction process</li>
</ul>
",['table']
39105490,39106284,2016-08-23 15:41:22,"Insert data in map<text,text> in cassandra db","<p>i have a column in cassandra database as <code>map&lt;text,text&gt;</code></p>

<p>I insert the data in this table as :</p>

<pre><code>INSERT INTO ""Table1"" (col1) VALUES ({'abc':'abc','hello':'world','flag':'true'});
</code></pre>

<p>So, in my code i can get the data as :</p>

<pre><code>{
    ""abc"":""abc"",
    ""hello"":""world"",
    ""flag"":""true""
}
</code></pre>

<p>But, now i want this like :</p>

<pre><code>{
    ""abc"":""abc"",
    ""hello"":""world"",
    ""flag"":{
    ""data"":{ ""hi"":""cassandra""},
    ""working"":""no""
    }
}
</code></pre>

<p>For this, when I try the insert query, it says that it does not match the type <code>map&lt;text,text&gt;</code></p>

<p>How can I make this work ?</p>
",<cassandra><cql>,"<p>The problem here (in your second example) is that the type of <code>col1</code> is a <code>map&lt;text,text&gt;</code> but <code>flag</code> is a complex type and no longer matches that definition.  One way to solve this would be to create individual <code>TEXT</code> columns for each property, as well as a user defined type for <code>flag</code> and the data it contains:</p>

<pre><code>&gt; CREATE TYPE flagtype (data map&lt;text,text&gt;,working text);
&gt; CREATE TABLE table1 (abc text,
                     hello text,
                      flag frozen&lt;flagtype&gt;
                     PRIMARY KEY (abc));
</code></pre>

<p>Then INSERTing the JSON text from your second example works.</p>

<pre><code>&gt; INSERT INTO table1 JSON '{""abc"":""abc"",
                          ""hello"":""world"",
                           ""flag"":{""data"":{""hi"":""cassandra""},
                                      ""working"":""no""}}';

&gt; SELECT * FROM table1;

 abc | flag                                       | hello
-----+--------------------------------------------+-------
 abc | {data: {'hi': 'cassandra'}, working: 'no'} | world

(1 rows)
</code></pre>

<p>If you are stuck on using the <code>map&lt;text,text&gt;</code> type, and want the value JSON sub properties to be treated a large <code>text</code> string, you could try a simple table like this:</p>

<pre><code>CREATE TABLE stackoverflow.table2 (
  key1 text PRIMARY KEY,
  col1 map&lt;text, text&gt;);
</code></pre>

<p>And on your <code>INSERT</code>s just escape out the inner quotes:</p>

<pre><code>&gt; INSERT INTO table2 JSON '{""key1"":""1"",""col1"":{""abc"":""abc"",""hello"":""world""}}';
&gt; INSERT INTO table2 JSON '{""key1"":""2"",""col1"":{""abc"":""abc"",""hello"":""world"",
                  ""flag"":""{\""data\"":{\""hi\"":\""cassandra\""},\""working\"":\""no\""}""}}';

&gt; SELECT * FROm table2;

 key1 | col1
------+----------------------------------------------------------------------------------------
    2 | {'abc': 'abc', 'flag': '{""data"":{""hi"":""cassandra""},""working"":""no""}', 'hello': 'world'}
    1 |                                                       {'abc': 'abc', 'hello': 'world'}

(2 rows)
</code></pre>

<p>That's a little hacky and will probably require some additional parsing on your application side.  But it gets you around the problem of having to define each column.</p>
",['table']
39111152,39112281,2016-08-23 21:43:30,How to migrate cassandra cluster column change,"<p>We have a use case to change cassandra table column (change the type from Int to Long), since it not supported changing from Int to varInt is supported and we are fine with that.</p>

<p>But in some of the tables this column is a cluster column and we have no way of changing this.</p>

<p>I am curious what is the best way to handle this case.</p>
",<cassandra><spark-cassandra-connector>,"<p>You can not alter a clustering column in Cassandra - you'll need to make a new table and load the data into that table using a third party application (cqlsh <code>COPY</code> being the simplest, or something like Spark). If you're unable to tolerate a change in the table's name, you'll need to backup your data, drop the old table, and recreate it with the proper types.</p>
",['table']
39144713,39151096,2016-08-25 11:58:51,Cassandra compaction takes too much RAM,"<p>Running an 8-node Cassandra 2.2.5 cluster with a CF about 1TB big. </p>

<p>Switching to LeveledCompactionStrategy for this CF causes thousands of compaction jobs, which doesn't seem to be problem by itself. But Cassandra starts using constantly increasing amount of RAM, eventually getting killed by the kernel.</p>

<p>What might be the reason C* uses 100G of RAM to merge some sorted files?</p>
",<cassandra><cassandra-2.0><nosql>,"<p>The initial switch to LCS will cause a massive recompaction of all the data. If you've got a TB, that's a lot of SSTables, and a lot of compactions. When Cassandra does a compaction, it's not as simple as ""merging some sorted files"" as it actually has to merge updates and tombstones across the SSTables, requiring more processing than just a simple comparison.</p>

<p>Compactions will use RAM, however unless you've configured differently the defaults should have a limit on heap size and also on number of concurrent compactions. Having said that Cassandra will utilise cached memory as best it can, however this shouldn't cause issues.</p>

<p>If you have very wide partitions Cassandra will also use more off-heap memory during compactions, however 100GB is excessive. I suggest you configure your heap size, compaction throughput, and concurrent_compactors to be lower to hopefully avoid the oom-killer.  </p>

<p>You should be switching to LCS one node/rack at a time using JMX if the cluster is under load, have a look at this guide for the info <a href=""http://blog.alteroot.org/articles/2015-04-20/change-cassandra-compaction-strategy-on-production-cluster.html"" rel=""nofollow"">http://blog.alteroot.org/articles/2015-04-20/change-cassandra-compaction-strategy-on-production-cluster.html</a></p>

<p>If there is no load on the cluster then you could also try starting cassandra with the disable_stcs_in_l0 parameter in cassandra-env.sh to see if that helps. This will disable sizetiered compaction in L0 which should reduce the total number of compactions to recompact all the data into LCS (however the recompaction will still take a long time with 1TB of data).
<code>-Dcassandra.disable_stcs_in_l0=true</code></p>
",['concurrent_compactors']
39145246,39180885,2016-08-25 12:23:36,Spark 2.0 Cassandra Scala Shell Error: java.lang.NoClassDefFoundError: scala/collection/GenTraversableOnce$class,"<p>I have configured spark 2.0 shell to run with datastax cassandra connector.</p>

<pre><code>spark-shell --packages datastax:spark-cassandra-connector:2.0.0-M1-35-s_2.11
</code></pre>

<p>When running this snippet in the shell</p>

<pre><code>sc.stop
import org.apache.spark
import org.apache.spark._
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.cassandra
import org.apache.spark.sql.cassandra._
import com.datastax.spark
import com.datastax.spark._
import com.datastax.spark.connector
import com.datastax.spark.connector._
import com.datastax.spark.connector.cql
import com.datastax.spark.connector.cql._
import com.datastax.spark.connector.cql.CassandraConnector
import com.datastax.spark.connector.cql.CassandraConnector._

val conf = new SparkConf(true).set(""spark.cassandra.connection.host"", ""dbserver"")
conf.set(""spark.cores.max"", ""1"")

val sc = new SparkContext(""spark://localhost:7077"", ""test"", conf)
val table = sc.cassandraTable(""blackwell"", ""users"")
println(table.count)
</code></pre>

<p>On this line</p>

<pre><code>println(table.count)
</code></pre>

<p>Getting this error
    java.lang.NoClassDefFoundError: scala/collection/GenTraversableOnce$class</p>

<pre><code>[Stage 0:&gt;                                                          (0 + 2) / 6]
16/08/25 11:59:38 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, 0.0.0.0): 
java.lang.NoClassDefFoundError: scala/collection/GenTraversableOnce$class
at com.datastax.spark.connector.util.CountingIterator.&lt;init&gt;(CountingIterator.scala:4)
at com.datastax.spark.connector.rdd.CassandraTableScanRDD.compute(CassandraTableScanRDD.scala:336)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
at org.apache.spark.scheduler.Task.run(Task.scala:85)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ClassNotFoundException: scala.collection.GenTraversableOnce$class
at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
</code></pre>

<p>Has anyone seen this issue?</p>
",<apache-spark><cassandra><spark-cassandra-connector>,"<p>I finally got this working. 
I've added a gist for reference.</p>

<p><a href=""https://gist.github.com/ghafran/19d0067d88dc074413422d4cae4cc344"" rel=""nofollow"">https://gist.github.com/ghafran/19d0067d88dc074413422d4cae4cc344</a></p>

<p>Here is the entire script:</p>

<pre><code># install java
sudo apt-get update -y
sudo apt-get install software-properties-common -y
sudo add-apt-repository -y ppa:openjdk-r/ppa
sudo apt-get install wget -y
sudo apt-get install openjdk-8-jdk -y
sudo apt-get update -y

# make serve directory
sudo mkdir -p /srv
cd /srv

# install scala 2.11
sudo wget http://downloads.lightbend.com/scala/2.11.7/scala-2.11.7.deb
sudo dpkg -i scala-2.11.7.deb

# get spark 2.0
sudo wget http://d3kbcqa49mib13.cloudfront.net/spark-2.0.0-bin-hadoop2.7.tgz
sudo tar -zxf spark-2.0.0-bin-hadoop2.7.tgz
sudo mv spark-2.0.0-bin-hadoop2.7 spark

# build spark cassandra connector
echo ""deb https://dl.bintray.com/sbt/debian /"" | sudo tee -a /etc/apt/sources.list.d/sbt.list
sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 642AC823
sudo apt-get install apt-transport-https -y
sudo apt-get update -y
sudo apt-get install sbt -y
git clone https://github.com/datastax/spark-cassandra-connector.git
cd spark-cassandra-connector
git checkout v2.0.0-M2
sudo sbt assembly -Dscala-2.11=true

# move spark cassandra connector to spark jar directory
find . -iname ""*.jar"" -type f -exec /bin/cp {} /srv/spark/jars/ \;

# start master
/srv/spark/sbin/start-master.sh --host 0.0.0.0

# start slave
/srv/spark/sbin/start-slave.sh --host 0.0.0.0 spark://localhost:7077

# start shell
/srv/spark/sbin/spark-shell --driver-class-path $(echo /srv/spark/jars/*.jar |sed 's/ /:/g')

# test
sc.stop
import org.apache.spark
import org.apache.spark._
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.cassandra
import org.apache.spark.sql.cassandra._
import com.datastax.spark
import com.datastax.spark._
import com.datastax.spark.connector
import com.datastax.spark.connector._
import com.datastax.spark.connector.cql
import com.datastax.spark.connector.cql._
import com.datastax.spark.connector.cql.CassandraConnector
import com.datastax.spark.connector.cql.CassandraConnector._

val conf = new SparkConf(true).set(""spark.cassandra.connection.host"", ""cassandraserver"")
val sc = new SparkContext(""spark://localhost:7077"", ""test"", conf)
val table = sc.cassandraTable(""keyspace"", ""users"")
println(table.count)
</code></pre>
",['table']
39167776,39634219,2016-08-26 13:40:53,"How to fetch offset id while consuming Kafka from Spark, save it in Cassandra and use it to restart Kafka?","<p>I am using Spark to consume data from Kafka and save it in Cassandra. My program is written in Java. I am using the <code>spark-streaming-kafka_2.10:1.6.2</code> lib to accomplish this. My code is:</p>

<pre><code>SparkConf sparkConf = new SparkConf().setAppName(""name"");
JavaStreamingContext jssc = new JavaStreamingContext(sparkConf, new Duration(2000));
Map&lt;String,String&gt; kafkaParams = new HashMap&lt;&gt;();
kafkaParams.put(""zookeeper.connect"", ""127.0.0.1"");
kafkaParams.put(""group.id"", App.GROUP);
JavaPairReceiverInputDStream&lt;String, EventLog&gt; messages =
  KafkaUtils.createStream(jssc, String.class, EventLog.class, StringDecoder.class, EventLogDecoder.class,
    kafkaParams, topicMap, StorageLevel.MEMORY_AND_DISK_SER_2());
JavaDStream&lt;EventLog&gt; lines = messages.map(new Function&lt;Tuple2&lt;String, EventLog&gt;, EventLog&gt;() {
    @Override
    public EventLog call(Tuple2&lt;String, EventLog&gt; tuple2) {
        return tuple2._2();
    }
});
lines.foreachRDD(rdd -&gt; {
    javaFunctions(rdd).writerBuilder(""test"", ""event_log"", mapToRow(EventLog.class)).saveToCassandra();
});
jssc.start();
</code></pre>

<p>In my Cassandra table <code>event_log</code>, there is a column named <code>offsetid</code> to store the offset ID of the stream. How do I get the offset id till where this stream has read the Kafka stream and store it in Cassandra?</p>

<p>After saving it in Cassandra, I want to use the latest offset id to be used when Spark is started again. How do I do that?</p>
",<java><apache-spark><cassandra><apache-kafka>,"<p>Below is the code for reference you may need to change the things as per your requirement. What I have done with the code and approach is that maintain Kafka partition wise offset for each topic in Cassandra(This can be done in zookeeper also as a suggestion using its java api). Store or update the the latest offset range for the topic with each string message received, in EventLog table. So always retrieve from table and see if present, then create direct stream from that offset, otherwise fresh direct stream.</p>

<pre><code>package com.spark;

import static com.datastax.spark.connector.japi.CassandraJavaUtil.javaFunctions;
import static com.datastax.spark.connector.japi.CassandraJavaUtil.mapRowTo;

import java.util.Arrays;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map;

import kafka.common.TopicAndPartition;
import kafka.message.MessageAndMetadata;
import kafka.serializer.StringDecoder;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.streaming.Duration;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import org.apache.spark.streaming.kafka.HasOffsetRanges;
import org.apache.spark.streaming.kafka.KafkaUtils;
import org.apache.spark.streaming.kafka.OffsetRange;

import scala.Tuple2;

public class KafkaChannelFetchOffset {
    public static void main(String[] args) {
        String topicName = ""topicName"";
        SparkConf sparkConf = new SparkConf().setAppName(""name"");
        JavaStreamingContext jssc = new JavaStreamingContext(sparkConf, new Duration(2000));
        HashSet&lt;String&gt; topicsSet = new HashSet&lt;String&gt;(Arrays.asList(topicName));
        HashMap&lt;TopicAndPartition, Long&gt; kafkaTopicPartition = new HashMap&lt;TopicAndPartition, Long&gt;();
        Map&lt;String, String&gt; kafkaParams = new HashMap&lt;&gt;();
        kafkaParams.put(""zookeeper.connect"", ""127.0.0.1"");
        kafkaParams.put(""group.id"", ""GROUP"");
        kafkaParams.put(""metadata.broker.list"", ""127.0.0.1"");
        List&lt;EventLog&gt; eventLogList = javaFunctions(jssc).cassandraTable(""test"", ""event_log"", mapRowTo(EventLog.class))
                .select(""topicName"", ""partion"", ""fromOffset"", ""untilOffset"").where(""topicName=?"", topicName).collect();
        JavaDStream&lt;String&gt; kafkaOutStream = null;
        if (eventLogList == null || eventLogList.isEmpty()) {
            kafkaOutStream = KafkaUtils.createDirectStream(jssc, String.class, String.class, StringDecoder.class, StringDecoder.class, kafkaParams,
                    topicsSet).transform(new Function&lt;JavaPairRDD&lt;String, String&gt;, JavaRDD&lt;String&gt;&gt;() {
                @Override
                public JavaRDD&lt;String&gt; call(JavaPairRDD&lt;String, String&gt; pairRdd) throws Exception {
                    JavaRDD&lt;String&gt; rdd = pairRdd.map(new Function&lt;Tuple2&lt;String, String&gt;, String&gt;() {
                        @Override
                        public String call(Tuple2&lt;String, String&gt; arg0) throws Exception {
                            return arg0._2;
                        }
                    });
                    writeOffset(rdd, ((HasOffsetRanges) rdd.rdd()).offsetRanges());
                    return rdd;
                }
            });
        } else {
            for (EventLog eventLog : eventLogList) {
                kafkaTopicPartition.put(new TopicAndPartition(topicName, Integer.parseInt(eventLog.getPartition())),
                        Long.parseLong(eventLog.getUntilOffset()));
            }
            kafkaOutStream = KafkaUtils.createDirectStream(jssc, String.class, String.class, StringDecoder.class, StringDecoder.class, String.class,
                    kafkaParams, kafkaTopicPartition, new Function&lt;MessageAndMetadata&lt;String, String&gt;, String&gt;() {
                        @Override
                        public String call(MessageAndMetadata&lt;String, String&gt; arg0) throws Exception {
                            return arg0.message();
                        }
                    }).transform(new Function&lt;JavaRDD&lt;String&gt;, JavaRDD&lt;String&gt;&gt;() {

                @Override
                public JavaRDD&lt;String&gt; call(JavaRDD&lt;String&gt; rdd) throws Exception {
                    writeOffset(rdd, ((HasOffsetRanges) rdd.rdd()).offsetRanges());
                    return rdd;
                }
            });
        }
        // Use kafkaOutStream for further processing.
        jssc.start();
    }

    private static void writeOffset(JavaRDD&lt;String&gt; rdd, final OffsetRange[] offsets) {
        for (OffsetRange offsetRange : offsets) {
            EventLog eventLog = new EventLog();
            eventLog.setTopicName(String.valueOf(offsetRange.topic()));
            eventLog.setPartition(String.valueOf(offsetRange.partition()));
            eventLog.setFromOffset(String.valueOf(offsetRange.fromOffset()));
            eventLog.setUntilOffset(String.valueOf(offsetRange.untilOffset()));
            javaFunctions(rdd).writerBuilder(""test"", ""event_log"", null).saveToCassandra();
        }
    }
}
</code></pre>

<p>Hope this helps and resolve your problem...</p>
",['table']
39172412,39173864,2016-08-26 18:15:08,SQL vs Cassandra Data type mappings,"<p>I am mapping some data types from SQL server to cassandra, such as int to bigint, real to float, varchar to text. Where can I get the mappings from SQL server to cassandra?</p>
",<sql-server><cassandra>,"<p>Looking at <a href=""https://docs.datastax.com/en/cql/3.0/cql/cql_reference/cql_data_types_c.html"" rel=""nofollow"">CQL Data Types</a> descriptions compared to <a href=""https://msdn.microsoft.com/en-us/library/ms187752.aspx"" rel=""nofollow"">SQL Server Data Types</a>, here are some mappings, but there's no guarantees (not overly confident considering the typos in the CQL Data Types reference) they are accurate. </p>

<p>The comparison doesn't consider settings on SQL Server that alter data type representation such as collation sets with character data types or how you are converting and passing this data to SQL Server.</p>

<p>I'm making the comparison based on the <em>values</em> that can be represented by both types. Pay close attention to the comments. </p>

<pre>
CQL Data Type |   Match?  | SQL Server Data Type | Comment
-------------------------------------------------------------------------------------
list               N        none                   A collection; no native SQL equivalent. Perhaps sql_variant or XML could be used but operations on list in CQL wouldn't apply in SQL Server. Custom data types and CLR integrations would most likely be required
map                N        none                   Similar to above except as of SQL Server 2016, [JSON Data](https://msdn.microsoft.com/en-gb/library/dn921897.aspx) handling has been introduced so it's possible it could parse CQL maps
set                N        none                   ""

int                Y        int                    Both represent 32-bit signed integers
bigint             Y        bigint                 Both represent 64-bit signed integers
varint             ?        smallint               Not clear if varint storage size will change, so if precision was -32768 to 32767, would it take 2 bytes? Also, if varint has values outside of smallint range, you may run into overflow errors. From smallint to varint, there's no indication in the above links
varint             ?        tinyint                Similar to above except if precision was 0 to 255, would it take 1 bytes?

float              Y        float

decimal            ?        decimal                Not clear of the precision and scaling limits of CQL decimal

ascii              ?        char, varchar          Not clear this mapping is accurate, more an assumption. Limits and conversion behaviour are not known
text               ?        ntext                  Based on UTF-8 encoding and that CQL seems to have varchar/text as does SQL. So it's likely text represents larger length text strings
varchar            ?        nchar, nvarchar        Based on UTF-8 encoding supported by both. Not clear what varchar limits are or the conversion behaviour

timestamp          ?        datetime               Not clear what timestamp limits are or the conversion behaviour

boolean            ?        bit                    Not clear on conversion behaviour

blob               ?        binary, varbinary      Not clear what the limits are on length of a CQL blob

uuid               ?        uniqueidentifier       uuid follows standard UUID format, most likely 128 bits (16 bytes) which is the same storage size as uniqueidentifier. Not clear on the conversion behaviour
</pre>  
",['precision']
39203542,39205220,2016-08-29 10:03:16,Change the type of a column in Cassandra,"<p>I have created a table <code>my_table</code> with a column <code>phone</code>, which has been declared as of <code>type varint.</code> After entering some data, I realized that it would have been better if I had declared this column as <code>list&lt;int&gt;.</code> </p>

<p>I tried to:</p>

<pre><code>ALTER TABLE my_table
ALTER phone TYPE list&lt;int&gt;
</code></pre>

<p>but unfortunately I am not allowed to do so. Hopefully, there is a way to make this change.</p>

<p><strong>UPDATE:</strong> Assume that I make a new column <code>phonelist</code> of <code>type list&lt;int&gt;</code>.  Is there any efficient way to move the data in the <code>phone</code> column into the <code>phonelist</code> column? </p>
",<cassandra><cql>,"<p>You cannot change the type of an existing column to a map or collection.</p>

<p>The table shows the allowed alterations for data types</p>

<p><a href=""https://i.stack.imgur.com/E3Gzw.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/E3Gzw.png"" alt=""enter image description here""></a></p>
",['table']
39216146,39218289,2016-08-29 22:29:43,How can I use CQL in Cassandra 3 to determine if a table uses compact storage?,"<p>If I use the <code>cqlsh</code> tool that comes with Cassandra 3, it can tell me if a table was created <code>WITH COMPACT STORAGE</code>. All I have to do is <code>describe table_name;</code> and it shows me the CQL used to create the table.</p>

<p>The <code>describe</code> functionality is a feature of <code>cqlsh</code>, not of the CQL language. I need to determine if a table uses compact storage using just CQL. What do I need to query in the <code>system_schema</code> to determine if a table is using compact storage?</p>
",<cassandra><cql3>,"<p>From the definition of the <code>TableMetadataV3</code> class in the cassandra driver for python, the logic for determining compact storage is as follows</p>

<pre><code> flags = row.get('flags', set())
            if flags:
                compact_static = False
                table_meta.is_compact_storage = 'dense' in flags or 'super' in flags or 'compound' not in flags
                is_dense = 'dense' in flags
            else:
                compact_static = True
                table_meta.is_compact_storage = True
                is_dense = False
</code></pre>

<p>The <code>row</code> object is a dictionary that is the result of the query <code>""SELECT * FROM system_schema.tables""</code></p>

<p>So to determine if a table uses compact storage, the following steps are necessary.</p>

<ol>
<li>Use CQL to query <code>Select flags from system_schema.tables where keyspace_name=? and table_name=?</code>. Substitute the keyspace and table in as parameters</li>
<li>If flags is empty, then the table uses compact storage.</li>
<li>If flags is present with 'dense' or 'super' as members of the set then the table uses compact storage.</li>
<li>If 'compound' is not in the set then the table uses compact storage.</li>
<li>Otherwise the table does not use compact storage.</li>
</ol>
",['table']
39248090,39253533,2016-08-31 11:13:31,Spark SQL to insert data into Cassandra,"<p>I am a beginner with Scala and Apache Spark and I am facing the below problem.</p>

<p>I am trying to insert data into a Cassandra table..user (name,favorite_food) using spark SQL.</p>

<p>The code snippet looks like this</p>

<pre><code>val conf = new SparkConf(true)
  .set(""spark.cassandra.connection.host"", ""127.0.0.1"")

val sc = new SparkContext(""local"", ""test"", conf)
val sqlC = new CassandraSQLContext(sc)
sqlC.setKeyspace(""myKeySpace"")
sqlC.sql(""INSERT into user (name,favorite_food) values ('John Doe','brownies')"")
</code></pre>

<p>However I face the issue
Exception in thread ""main"" java.lang.RuntimeException: [1.13] failure: ``table'' expected but identifier user found </p>

<p>I am running a local instance of Cassandra DB</p>

<p>My Maven POM looks like</p>

<pre><code>&lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-core_2.10&lt;/artifactId&gt;
            &lt;version&gt;1.6.0&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
            &lt;artifactId&gt;spark-sql_2.10&lt;/artifactId&gt;
            &lt;version&gt;1.6.2&lt;/version&gt;
        &lt;/dependency&gt;
&lt;/dependencies&gt;
</code></pre>

<p>My question is;
Please can you let me know why I face the below error (The insert statement works perfectly fine on the cassandra shell)
Exception in thread ""main"" java.lang.RuntimeException: [1.13] failure: ``table'' expected but identifier user found</p>

<p>PS : I do know I can use the spark connector provided by datastax to save data to Cassandra, however I want to use Spark SQL...is that possible?</p>
",<scala><apache-spark><cassandra><apache-spark-sql>,"<p>We can't insert a data to table using Cassandra Context. Spark doesn't provide that option.</p>

<p>You will try it this it will definitely works,</p>

<pre><code>import org.apache.spark.SparkContext
import org.apache.spark.SparkConf
import com.datastax.spark.connector._
import com.datastax.spark.connector.UDTValue 

//we need define a class
//case class name(column name: datatype,column name: datatype)

case class model(name: String, favorite_food: String)

// define sequence to insert a data 
// val coll = sc.parallelize(Seq(classname(data, data), model(data, data)))

val collection = sc.parallelize(Seq(model(""John Doe"", ""brownies"")))

 //then save to cassandra
 //collection.saveToCassandra(""keyspace_name"", ""table_name"", SomeColumns(""col name"", ""col name""))

collection.saveToCassandra(""myKeySpace"", ""user"", SomeColumns(""name"", ""favorite_food""))
</code></pre>

<p>Thanks,
Aravinth </p>
",['table']
39270094,39276897,2016-09-01 11:17:12,Custom TypeConverters using spark cassandra connector,"<p>I wrote an app using the spark cassandra connector . Now , when spark-submit the job i get the error <strong>java.lang.IllegalArgumentException: requirement failed: No mappable properties found in class: MailBox</strong> , even though i defined a type converter as specified in  <a href=""https://github.com/datastax/spark-cassandra-connector/blob/master/doc/6_advanced_mapper.md"" rel=""nofollow"">https://github.com/datastax/spark-cassandra-connector/blob/master/doc/6_advanced_mapper.md</a> , my thoughts are i need a companion object for MailBox where i define a mapper , but i can't find an example for it in the doc. Does anyone know how to solve this ? Thanks </p>

<p><strong>The code</strong>:</p>

<pre><code>object Test {
  case class Size(size: Long) {
    if (size &lt; 0) throw new IllegalArgumentException
    def +(s: Size): Size = Size(size + s.size)
  }

  object LongToSizeConverter extends TypeConverter[Size] {
    def targetTypeTag = typeTag[Size]
    def convertPF = { case long: Long =&gt; Size(long)  }
  }
  object SizeToLongConverter extends TypeConverter[Long] {
    def targetTypeTag = typeTag[Long]
    def convertPF = { case Size(long) =&gt; long.toLong }
  }
  case class MailBox(id: String,totalsize: Size)
  case class Id(mailboxid:String) 
  object StringToIdConverter extends TypeConverter[Id] {
    def targetTypeTag = typeTag[Id]
    def convertPF = { case str: String =&gt; Id(str) 
                      case str: UUID =&gt; Id(str.toString)  }
  }
  object IdToStringConverter extends TypeConverter[String] {
    def targetTypeTag = typeTag[String]
    def convertPF = { case Id(str) =&gt; str.toString }
  }

  def main(args: Array[String]) {
    val sc = new SparkContext();
    TypeConverter.registerConverter(StringToIdConverter)
    TypeConverter.registerConverter(IdToStringConverter)
    TypeConverter.registerConverter(LongToSizeConverter)
    TypeConverter.registerConverter(SizeToLongConverter)
    val test= sc.parallelize(Array(MailBox(Id(""1""),Size(10))))
    test.saveAsCassandraTable(""test"",""Mailbox"")
  }
}
</code></pre>
",<scala><apache-spark><cassandra><spark-cassandra-connector>,"<p>First let me post a quick working example, then I'll walk through what is going wrong</p>

<pre><code>package com.datastax.spark.example

import com.datastax.spark.connector._
import org.apache.spark.{SparkConf, SparkContext}
import com.datastax.spark.connector.types._
import scala.reflect.runtime.universe._
import java.util.UUID

import org.apache.spark.sql.catalyst.ReflectionLock.SparkReflectionLock

case class Size(size: Long) {
    if (size &lt; 0) throw new IllegalArgumentException
    def +(s: Size): Size = Size(size + s.size)
}
case class MailBox(id: Id,totalsize: Size)
case class Id(mailboxid:String)



object Test {

    val LongTypeTag = SparkReflectionLock.synchronized {
            implicitly[TypeTag[java.lang.Long]]
    }
    val SizeTypeTag = SparkReflectionLock.synchronized {
            typeTag[Size]
    }
    val IdTypeTag = SparkReflectionLock.synchronized {
            typeTag[Id]
    }
    val StringTypeTag = SparkReflectionLock.synchronized {
            implicitly[TypeTag[String]]
    }

    object LongToSizeConverter extends TypeConverter[Size] {
        def targetTypeTag = SizeTypeTag
        def convertPF = { case long: Long =&gt; Size(long)  }
    }
    object LongToSizeConverter extends TypeConverter[Size] {
        def targetTypeTag = SizeTypeTag
        def convertPF = { case long: Long =&gt; Size(long)  }
    }

    object SizeToLongConverter extends TypeConverter[java.lang.Long] {
        def targetTypeTag = LongTypeTag
        def convertPF = { case Size(long) =&gt; long.toLong }
    }

    object StringToIdConverter extends TypeConverter[Id] {
        def targetTypeTag = IdTypeTag
        def convertPF = { 
            case str: String =&gt; Id(str)
            case str: UUID =&gt; Id(str.toString)
        }
    }

    object IdToStringConverter extends TypeConverter[String] {
        def targetTypeTag = StringTypeTag
        def convertPF = { case Id(str) =&gt; str.toString }
    }

    TypeConverter.registerConverter(StringToIdConverter)
    TypeConverter.registerConverter(IdToStringConverter)
    TypeConverter.registerConverter(LongToSizeConverter)
    TypeConverter.registerConverter(SizeToLongConverter)


    def main(args: Array[String]) {
        val sc = new SparkContext();
        val test = sc.parallelize(Array(MailBox(Id(""1""),Size(10))))
        test.saveToCassandra(""ks"",""mailbox"")
    }
}
</code></pre>

<hr>

<h2>saveAsCassandraTable doesn't work with Custom Types</h2>

<p><code>saveAsCassandraTable</code> uses the fromType method which requires known types (not Custom ones). This is because saveAsCassandraTable creates a Cassandra column based on a known field type. With the a custom type converter you don't explicitly state the (1 to 1) mapping between your type and a Cassandra Column so it can't be looked up. Since saveAsCassandraTable creates the Cassandra table before inserting to it, it gets stuck since it doesn't know how to make the table.</p>

<p>To fix this we change the line</p>

<pre><code>test.saveAsCassandraTable(""test"",""Mailbox"")
</code></pre>

<p>to</p>

<pre><code>test.saveToCassandraTable(""test"",""Mailbox"")
</code></pre>

<p>Where we have pre-made the the table in CQLSH but you could also do this using the Java Driver in your application.</p>

<h2>We need to convert to Java Types</h2>

<p>TypeConverter chaining doesn't work with custom type converters. This means that we need to provide converters from Custom types to Java types. For this I changed the SizeToLong Converter</p>

<pre><code>object SizeToLongConverter extends TypeConverter[java.lang.Long] {
</code></pre>

<h2>We should defend against Scala Reflection Lack of Thread Safety</h2>

<p>I've added synchronized blocks (using the SparkReflectionLock) to make sure
we don't end up with any issues there.</p>

<p>See</p>

<pre><code>SparkReflectionLock.synchronized
</code></pre>

<h2>We need to do the registration at an Object Level</h2>

<p>To make sure our registrations happen on the executor JVMs I moved them out of the ""main"" scope. I'm not sure how important this is but it's best to reflect that this should be happening wherever the code is shipped to and not just during the main method.</p>
",['table']
39308666,39381054,2016-09-03 15:44:27,Cassandra error 'NoneType' object has no attribute 'datacenter' while importing csv,"<p>I have set up a cassandra cluster with 3 nodes.</p>

<p>I am trying to do a simple export/ import using copy command, but it fails with the following error:</p>

<pre><code>cqlsh:walmart&gt; select * from test;

 store | date       | isholiday | dept
-------+------------+-----------+------
     1 | 22/04/1993 |     False |    1


cqlsh&gt; use walmart;
cqlsh:walmart&gt; copy test to 'test.csv';
'NoneType' object has no attribute 'datacenter'
</code></pre>

<p>I researched the error and every related link seems to point out to python problems.</p>

<p>I also installed python driver pip cassandra-driver. Inserting data manually works, but not through export/ import.</p>

<pre><code>cassandra@cassandra-srv01:~$ python -c 'import cassandra; print cassandra.__version__'
3.6.0
</code></pre>

<p>Ubuntu 16.04 64bit.</p>

<p>how can I fix this error?</p>

<p>the logs inside <code>$CASSANDRA_HOME/logs</code> don't have any entries regarding the error.</p>

<p>Traceback:</p>

<pre><code>Traceback (most recent call last):
  File ""/usr/local/Cellar/cassandra/3.7/libexec/bin/cqlsh.py"", line 1152, in onecmd
    self.handle_statement(st, statementtext)
  File ""/usr/local/Cellar/cassandra/3.7/libexec/bin/cqlsh.py"", line 1189, in handle_statement
    return custom_handler(parsed)
  File ""/usr/local/Cellar/cassandra/3.7/libexec/bin/cqlsh.py"", line 1907, in do_copy
    task = ImportTask(self, ks, table, columns, fname, opts, DEFAULT_PROTOCOL_VERSION, CONFIG_FILE)
  File ""/usr/local/Cellar/cassandra/3.7/libexec/bin/../pylib/cqlshlib/copyutil.py"", line 1061, in __init__
    CopyTask.__init__(self, shell, ks, table, columns, fname, opts, protocol_version, config_file, 'from')
  File ""/usr/local/Cellar/cassandra/3.7/libexec/bin/../pylib/cqlshlib/copyutil.py"", line 207, in __init__
    self.local_dc = shell.conn.metadata.get_host(shell.hostname).datacenter
AttributeError: 'NoneType' object has no attribute 'datacenter
</code></pre>
",<python><csv><cassandra><copy><nonetype>,"<p>it is not so good, but i will try to contribute to the problem. i'm new in cassandra and had exactly the same problem while trying to import data into a cassandra table via the copy function. i connect to the server on which cassandra is installed through cqlsh installed on a virtual machine. so i have to specify the server ip address and the port while running the cqlsh command: # cqlsh ip_address port
i connected with the servername like that: # cqlsh myserver.example.com 9040 and i was connected and the copy fonction didn't work.</p>

<p>But connecting with the numerical ip address of the server (for example:</p>

<h1>cqlsh 127.0.0.1 9040) it has worked.</h1>

<p>it was by pure chance, i have simply tested and it has worked for me.</p>

<p>when someone here can explain this fact it would be great!</p>
",['table']
39336176,39371189,2016-09-05 18:47:29,Query min partition key based on date range (clustering key),"<p>I have a table Foo in cassandra with 4 columns foo_id bigint, date datetime, ref_id bigint, type int</p>

<p>here the partitioning key is foo_id. the clustering keys are date desc, ref_id and type</p>

<p>I want to write a CSQL query which is the equivalent of the SQL below</p>

<pre><code>select min(foo_id) from foo where date &gt;= '2016-04-01 00:00:00+0000'
</code></pre>

<p>I wrote the following CSQL</p>

<pre><code>select foo_id from foo where 
foo_id IN (-9223372036854775808, 9223372036854775807) 
and date &gt;= '2016-04-01 00:00:00+0000';
</code></pre>

<p>but this returns empty results. </p>

<p>Then I tried </p>

<pre><code>select foo_id from foo where 
  token(foo_id) &gt; -9223372036854775808 
  and token(foo_id) &lt; 9223372036854775807 
  and date &gt;= '2016-04-01 00:00:00+0000';
</code></pre>

<p>but this results in error</p>

<pre><code>Unable to execute CSQL Script on 'Cassandra'. Cannot execute this query
as it might involve data filtering and thus may have unpredictable 
performance. If you want to execute this query despite performance   
unpredictability, use ALLOW FILTERING.
</code></pre>

<p>I don't want to use ALLOW FILTERING. but I want the minimum of foo_id at the start of the specified date.</p>
",<cassandra>,"<p>You should probably denormalize your data and create a new table for the purpose. I propose something like:</p>

<pre><code>CREATE TABLE foo_reverse (
    year int,
    month int,
    day int,

    foo_id bigint, 
    date datetime,
    ref_id bigint, 
    type int,
    PRIMARY KEY ((year, month, day), foo_id)
)
</code></pre>

<p>To get the minimum foo_id you would query that table by something like:</p>

<pre><code>SELECT * FROM foo_reverse WHERE year = 2016 AND month = 4 AND day = 1 LIMIT 1;
</code></pre>

<p>That table would allow you to query on a ""per day"" basis. You can change the partition key to better reflect your needs. Beware of the potential hot spots you (and I) could create by selecting an appropriate time range.</p>
",['table']
39354761,39355450,2016-09-06 17:31:25,Cassandra data modeling - Do I choose hotspots to make the query easier?,"<p><strong>Is it ever okay to build a data model that makes the fetch query easier even though it will likely created hotspots within the cluster?</strong></p>

<p>While reading, please keep in mind I am not working with Solr right now and given the frequency this data will be accessed I didn’t think using spark-sql would be appropriate.  I would like to keep this as pure Cassandra.</p>

<p>We have transactions, which are modeled using a UUID as the partition key so that the data is evenly distributed around the cluster.  One of our access patterns requires that a UI get all records for a given user and date range, query like so:</p>

<pre><code>select * from transactions_by_user_and_day where user_id = ? and created_date_time &gt; ?;
</code></pre>

<p>The first model I built uses the user_id and created_date (day the transaction was created, always set to midnight) as the primary key:</p>

<pre><code>CREATE transactions_by_user_and_day (
    user_ id int,
    created_date timestamp,
    created_date_time timestamp,
    transaction_id uuid,
    PRIMARY KEY ((user_id, created_date), created_date_time)
) WITH CLUSTERING ORDER BY (created_date_time DESC);
</code></pre>

<p>This table seems to perform well.  Using the created_date as part of the PK allows users to be spread around the cluster more evenly to prevent hotspots.  However, from an access perspective it makes the data access layer do a bit more work that we would like.  It ends up having to create an IN statement with all days in the provided range instead of giving a date and greater than operator:</p>

<pre><code>select * from transactions_by_user_and_day where user_id = ? and created_date in (?, ?, …) and created_date_time &gt; ?;
</code></pre>

<p>To simplify the work to be done at the data access layer, I have considered modeling the data like so:</p>

<pre><code>CREATE transactions_by_user_and_day (
    user_id int,
    created_date_time timestamp,
    transaction_id uuid,
    PRIMARY KEY ((user_global_id), created_date_time)
) WITH CLUSTERING ORDER BY (created_date_time DESC);
</code></pre>

<p>With the above model, the data access layer can fetch the transaction_id’s for the user and filter on a specific date range within Cassandra.  However, this causes a chance of hotspots within the cluster.  Users with longevity and/or high volume will create quite a few more columns in the row.  We intend on supplying a TTL on the data so anything older than 60 days drops off.  Additionally, I’ve analyzed the size of the data and 60 days’ worth of data for our most high volume user is under 2 MB.  Doing the math, if we assume that all 40,000 users (this number wont grow significantly) are spread evenly over a 3 node cluster and 2 MB of data per user you end up with a max of just over 26 GB per node ((13333.33*2)/1024).  In reality, you aren’t going to end up with 1/3 of your users doing that much volume and you’d have to get really unlucky to have Cassandra, using V-Nodes, put all of those users on a single node.  From a resources perspective, I don’t think 26 GB is going to make or break anything either.</p>

<p>Thanks for your thoughts.</p>
",<cassandra><data-modeling>,"<p>Date Model 1:Something else you could do would be to change your data access layer to do a query for each ID individually, instead of using the IN clause. Check out this page to understand why that would be better.</p>

<p><a href=""https://lostechies.com/ryansvihla/2014/09/22/cassandra-query-patterns-not-using-the-in-query-for-multiple-partitions/"" rel=""nofollow"">https://lostechies.com/ryansvihla/2014/09/22/cassandra-query-patterns-not-using-the-in-query-for-multiple-partitions/</a></p>

<p>Data model 2: 26GB of data per node doesn't seem like much, but a 2MB fetch seems a bit large.  Of course if this is an outlier, then I don't see a problem with it.  You might try setting up a cassandra-stress job to test the model. As long as the majority of your partitions are smaller than 2MB, that should be fine.</p>

<p>One other solution would be to use Data Model 2 with Bucketing.  This would give you more overhead on writes as you'd have to maintain a bucket lookup table as well though.  Let me know if need me to elaborate more on this approach.</p>
",['table']
39372012,39374411,2016-09-07 14:01:19,One to many mapping in Cassandra,"<p>I am new to Cassandra and would like to do One to many mapping of User and its vehicle. One user may have multiple Vehicles. My User table will contain User details like name, surname, etc. And Vehicle table will have Vehicle details.</p>

<p>My select query will fetch all Vehicle details for particular User.</p>

<p>How should I design this in Cassandra?</p>
",<cassandra><cassandra-2.0>,"<p>You can easily model this in a single table:</p>

<pre><code>CREATE TABLE userVehicles (
  userid text,
  vehicleid text,
  name text static,
  surname text static,
  vehicleMake text,
  vehicleModel text,
  vehicleYear text,
  PRIMARY KEY (userid,vehicleid)
);
</code></pre>

<p>This way you can query vehicles for a single user in one shot, and your user data can be <code>static</code> so that it is stored at the partition key level.  As long as the cardinality of user to vehicle isn't too big (as-in, like a user has 1000 vehicles) this should work just fine.</p>

<blockquote>
  <p>The case I have considered above is very simple. But what if my User has lot of details around 20 to 30 fields and same for Vehicle. Still you would suggest to have a single table and copying User data for all vehicle?</p>
</blockquote>

<p>It depends.  Would your use case require returning all of them?  If so, then ""yes"" I would still recommend this approach.  The way to get the best query performance out of Cassandra, is to model your tables to fit your queries.  Cassandra works best when it can read a single row by a specific key, or a range of rows (stored sequentially).  You want to avoid performing multiple queries or writing queries that force Cassandra to perform random reads.</p>

<blockquote>
  <p>What are the consequences of having 2 different tables like User and Vehicle and Vehicle table will have primary key as User_Id and Vehicle_Id?</p>
</blockquote>

<p>In a distributed system network time is the enemy.  By having two tables, you are now making two queries...assuming a 1 to 1 ratio of users to vehicles.  But if your user has 8 vehicles, you now need 9 queries to achieve your result.  With the design above you can build your result set in 1 query (minimizing network time).  Also with <code>userid</code> as a partition key, that query is guaranteed to be served by one node, as opposed to additional queries for vehicle data which will most likely require contacting multiple nodes.</p>
",['table']
39386247,39432797,2016-09-08 08:36:19,Why Cassandra doesn't let to query clustering key by IN restriction?,"<p>Does anyone know why I cannot use query where I restrict clustering column by <code>IN</code> and I <code>select</code> collection column?</p>

<p>Let me elaborate on that. Let's say I have data model similar to the following:</p>

<pre><code>create table inventory (
                sku text,
                class text,
                unit text,
                node text,
                supply map&lt;text, frozen&lt;delta_and_time&gt;&gt;,
                supply_compacted int,
                primary key ((sku, class, unit), node));
</code></pre>

<p>when I try to use following select statement:</p>

<pre><code>select sku, class, unit, node, supply_compacted where sku = '0' 
           and class = 'good' and unit = 'each' and node in ('1', '2', '3')
</code></pre>

<p>everything is fine. But when I try to <code>select *</code> with the same restrictions I get follow
ing error:</p>

<pre><code>Cannot restrict clustering columns by IN relations when a collection is selected by the query
</code></pre>

<p>I tried to find out why there is such restriction in C* but I couldn't find anything. Also I looked into the code but there is no information why such check is performed. </p>

<p>Does anyone know what is the reason of such restriction?</p>
",<select><cassandra><cql><nosql>,"<p>This is a limitation based on how Collection Data types were ""hacked"" into the existing storage engine. A Map collection is implemented by storing each key as a unique column name in the same area as the clustering column. Thus making it difficult for Cassandra to efficiently do an ""IN"" operation especially for large collection sizes for each ""row"".</p>

<p>However I do feel that you could re-work your data model to get around this limitation and not even use a Collection type (since they have many issues associated with them if your are not careful). It looks like ""supply_compacted"" could be a roll up of the total inventory within the supply map? If so you could do the following:</p>

<pre><code>create table inventory (
  sku text,
  class text,
  unit text,
  node text,
  supply_compacted int static, -- stored once, total amount of inventory across all nodes
  supply frozen&lt;delta_and_time&gt;,
  primary key ((sku, class, unit), node)
);
</code></pre>
",['table']
39395412,39396194,2016-09-08 15:52:59,Is there a feature in PostgreSQL similar to Cassandra's keyspace?,"<p>I'm weighing the options of moving a database from Cassandra to PostgreSQL. One significant hurdle is that our current, multi-tenant, Cassandra database utilizes keyspaces to segregate client data. Each client has an identical schema within their own dedicated keyspace. I'm not sure how, or if it is possible, to organize our data similarly in PostgreSQL?</p>
",<postgresql><cassandra><multi-tenant>,"<pre><code>CREATE DATABASE keyspace_name
</code></pre>

<p>Key space is similar to database or table space in PostGres/Oracle/MySQL. </p>
",['table']
39498313,39499421,2016-09-14 19:34:01,How to force Cassandra not to use the same node for replication in a schema with vnodes,"<p>Installing Cassandra in a single node to run some tests, we noticed that we were using a RF of 3 and everything was working correctly.</p>

<p>This is of course because that node has 256 vnodes (by default) so the same data can be replicated in the same node in different vnodes.</p>

<p>This is worrying because if one node were to fail, you'd lose all your data even though you <em>thought</em> the data was replicated in different nodes.</p>

<p>How can I be sure that in a standard installation (with a ring with several nodes) the same data will not be replicated in the same ""physical"" node? Is there a setting to avoid Cassandra from using the same node for replicating data?</p>
",<cassandra>,"<p>Replication strategy is <strong>schema</strong> dependent. You probably used the <strong>SimpleStrategy</strong> with <strong>RF=3</strong> in your schema. That means that each piece of data will be placed on the node determined by the partition key, and successive replicas will be placed on the successive nodes. In your case, the successive node is the <em>same physical</em> node, hence you get 3 copies of your data there. </p>

<p>Increasing the number of nodes solves your problem. In general, your data will be placed in different physical nodes when your replication factor <strong>RF</strong> is less than/equal to your number of nodes <strong>N</strong>.</p>

<p>The other solution is to switch replication strategy and use the <strong>NetworkTopologyStrategy</strong>, usually used in multi datacenter clusters, and where you can specify how many replicas you want in each data center. This strategy</p>

<blockquote>
  <p>places replicas in the same data center by walking the ring clockwise
  until reaching the first node in another rack. <strong>NetworkTopologyStrategy</strong>
  attempts to place replicas on distinct racks because nodes in the same
  rack (or similar physical grouping) often fail at the same time due to
  power, cooling, or network issues.</p>
</blockquote>

<p>Look at <a href=""https://docs.datastax.com/en/cassandra/2.0/cassandra/architecture/architectureDataDistributeReplication_c.html"" rel=""nofollow"">DataStax documentation</a> for more information.</p>
",['rack']
39498322,39498323,2016-09-14 19:34:33,Datastax Object Mapper: change table on the fly,"<p>I have a need to change what table my mapping annoatation points to on  the fly.  Consider the below:</p>

<pre><code>@Table(name=""measurementtable_one"", keyspace=""mykeyspace"")
public class Measurement {/*...*/}
</code></pre>

<p>I have multiple tables with the naming pattern 'measurementtable_*' whose names are not necessarily known at compile-time, and the one I need to work with is selected by the input to my program.  Since all of these tables are identical, I have no desire to create a new class for each table; and I have no desire to recompile my program for each input.</p>

<p>Is there a way to retain Object Mapping functionality without having to dictate my exact table name in the annotation?</p>
",<java><cassandra><datastax>,"<p>By conventional means, no.  Since annotations are effectively constants, you cannot change them by conventional means.  Since the Datastax object mapper does not expose any way to switch up tables for mapped object on the fly, darker arts must be employed: bytecode manipulation.</p>

<p>While one could have manipulated Measurement's annotation directly, I'm not a fan of changing what <em>should</em> be constant.  As a result, the <code>Measurement</code> class should lose its annotation and be made abstract:</p>

<pre><code>public class Measurement { /*...*/ }
</code></pre>

<p>Then, once the real table name is known, one can use javassist to generate a subclass with the correct annotation:</p>

<pre><code>String modelname = getNameFromExternalSource(); //Replace with real external source.
String modelcleanname = modeldir.getName().replaceAll(""\\W"", """");
ClassPool pool = ClassPool.getDefault();
String measurementclassname = ""measurementtable_"" + modelcleanname;
CtClass stagingmeasurementclass = pool.makeClass(measurementclassname);
stagingmeasurementclass.setSuperclass(pool.get(StagingMeasurementRecord.class.getName()));
stagingmeasurementclass.setModifiers(Modifier.PUBLIC);
ClassFile stagingmeasurementclassfile = stagingmeasurementclass.getClassFile();
ConstPool constpool = stagingmeasurementclassfile.getConstPool();
AnnotationsAttribute attribute = new AnnotationsAttribute(constpool,
        AnnotationsAttribute.visibleTag);
Annotation tableannotation = new Annotation(constpool, pool.get(Table.class.getName()));
tableannotation.addMemberValue(""name"", new StringMemberValue(measurementclassname, constpool));
tableannotation.addMemberValue(""keyspace"", new StringMemberValue(""mykeyspace"", constpool));
attribute.addAnnotation(tableannotation);
stagingmeasurementclassfile.addAttribute(attribute);
stagingmeasurementclass.addConstructor(
        CtNewConstructor.make(new CtClass[0], new CtClass[0], stagingmeasurementclass));
Class&lt;? super StagingMeasurementRecord&gt; myoutputclass = stagingmeasurementclass.toClass();
LOGGER.info(""Created custom measurementtable class with the name "" + myoutputclass.getName());
</code></pre>

<p>You may then feed the <code>myoutputclass</code> instance to a <code>MappingManagerInstance.mapper(...)</code> call to produce an object mapper that points to your desired table.</p>

<p>It's not the prettiest solution out there, given that bytecode manipulation is necessary, but it does what it needs to do while avoiding recompiling for each input or making a million identical classes for your objects.</p>
",['table']
39530568,39533321,2016-09-16 11:37:42,Cassandra - Primary key with column which I want to update,"<p>I need to build table in Cassandra to store operation statuses.
My model looks like this:</p>

<pre><code>import 1.. * import_statuses
</code></pre>

<p>table import:</p>

<pre><code>id - 1
date - 2016-08-09
</code></pre>

<p>table import_statuses:</p>

<pre><code>id - 232
import_id - 1
status - IMPORT
</code></pre>

<p>And now I have to search in import and on the status in second table. But I need only last status from second table.</p>

<p>Denormalized data in these two tables:</p>

<pre><code>1, 2016-08-09, 232, 1 IMPORT
1, 2016-08-09, 233, 1 SENDING
1, 2016-08-09, 234, 1 SENT
2, 2016-08-11, 235, 2 IMPORT
2, 2016-08-11, 236, 2 SENDING
</code></pre>

<p>And I need to get only rows third and fifth:</p>

<pre><code>1, 2016-08-09, 234, 1 SENT
2, 2016-08-11, 236, 2 SENDING
</code></pre>
",<cassandra><one-to-many><data-modeling>,"<p>I would create the following two tables, and would update both:</p>

<pre><code>CREATE TABLE import (
  id int,
  status_id int,
  date date static,
  status text,
  PRIMARY KEY (id, status_id)
) WITH CLUSTERING ORDER BY (status_id DESC);

CREATE TABLE last_import_status (
  id int PRIMARY KEY,
  date date, 
  status_id int,
  status text
);
</code></pre>

<p>The first one contains the denormalized data, but as the date field is static, it is stored only once for every import. The status records are stored in descending order - I understood that this is an increasing number. If status_id is not an ever-increasing number, you could add a timeuuid field, and use that as clustering key (second field in the PRIMARY KEY).</p>

<p>The last_import_status table will contain a record for each import, and the status_id and status fields will always contain the last value.</p>
",['table']
39535034,39535392,2016-09-16 15:21:47,c# cassandra DataStax driver version compatibility,"<p>Is it possible to use Cassandra version 3.6.0 using DataStax c# driver version 3.0.8 ? If so, why do I get the error unconfigured table ""table name"" error. What other alternatives do I have?</p>

<p>Edit:</p>

<p>I think the issue is I created the table as TableName , however the driver searches for the table tablename (LOWER CASE FOR T and N) and my query is INSERT INTO TableName(Column1) Values(value1)</p>
",<c#><cassandra><datastax>,"<p>Yes, you can use that version of the C# driver against Cassandra 3.6 (or really, any 3.x version). I suspect you're getting the ""unconfigured table"" error because either:</p>

<ol>
<li>You're not connecting the driver to the keyspace where ""table name"" has been created.</li>
<li>You haven't created the table yet in the keyspace the driver is connecting to.</li>
</ol>

<p><strong>Update:</strong> Casing</p>

<p>Sounds like from the comments above you used double quotes when creating tables and the keyspace. My recommendation (if it's feasible) is to just drop and recreate them without the quotes so you don't have to remember to quote everything and use proper case when doing queries. It just tends to be a lot simpler that way. If you want more information on how casing works in CQL, check out the <a href=""http://docs.datastax.com/en/cql/3.3/cql/cql_reference/ucase-lcase_r.html"" rel=""nofollow"">Uppercase and lowercase section</a> of the CQL docs.</p>
",['table']
39632527,39634738,2016-09-22 07:13:11,Spark-Cassandra Connector throws InvalidQueryException,"<p>I am facing this error : </p>

<p>Stack Trace:</p>

<pre><code>16/09/22 12:35:01 ERROR QueryExecutor: Failed to execute: com.datastax.spark.connector.writer.RichBoundStatement@58aafaf
com.datastax.driver.core.exceptions.InvalidQueryException: Key length of 105500 is longer than maximum of 65535
    at com.datastax.driver.core.Responses$Error.asException(Responses.java:136)
    at com.datastax.driver.core.DefaultResultSetFuture.onSet(DefaultResultSetFuture.java:179)
    at com.datastax.driver.core.RequestHandler.setFinalResult(RequestHandler.java:184)
    at com.datastax.driver.core.RequestHandler.access$2500(RequestHandler.java:43)
    at com.datastax.driver.core.RequestHandler$SpeculativeExecution.setFinalResult(RequestHandler.java:798)
    at com.datastax.driver.core.RequestHandler$SpeculativeExecution.onSet(RequestHandler.java:617)
    at com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:1005)
    at com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:928)
    at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:318)
    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:304)
    at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:318)
    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:304)
    at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:318)
    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:304)
    at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:276)
    at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:263)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:318)
    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:304)
    at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)
    at io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:823)
    at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:339)
    at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:255)
    at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:112)
    at java.lang.Thread.run(Thread.java:745)
16/09/22 12:35:01 ERROR QueryExecutor: Failed to execute: com.datastax.spark.connector.writer.RichBoundStatement@3bdd2570
com.datastax.driver.core.exceptions.InvalidQueryException: Key length of 108452 is longer than maximum of 65535
    at com.datastax.driver.core.Responses$Error.asException(Responses.java:136)
    at com.datastax.driver.core.DefaultResultSetFuture.onSet(DefaultResultSetFuture.java:179)
    at com.datastax.driver.core.RequestHandler.setFinalResult(RequestHandler.java:184)
    at com.datastax.driver.core.RequestHandler.access$2500(RequestHandler.java:43)
    at com.datastax.driver.core.RequestHandler$SpeculativeExecution.setFinalResult(RequestHandler.java:798)
    at com.datastax.driver.core.RequestHandler$SpeculativeExecution.onSet(RequestHandler.java:617)
    at com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:1005)
    at com.datastax.driver.core.Connection$Dispatcher.channelRead0(Connection.java:928)
    at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:318)
    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:304)
    at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:318)
    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:304)
    at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:318)
    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:304)
    at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:276)
    at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:263)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:318)
    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:304)
    at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)
    at io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:823)
    at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:339)
    at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:255)
    at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:112)
    at java.lang.Thread.run(Thread.java:745)
</code></pre>

<p>While trying to save a large string into the cassandra !
Could anyone please help me out in figuring this :</p>

<p>What is causing this error and how can I remove it , My use case is to save a very Large String in cassandra!</p>
",<apache-spark><cassandra><spark-cassandra-connector>,"<p>There is a limit on the size of the key (partition key) in the CQL that you can check at <a href=""https://docs.datastax.com/en/cql/3.1/cql/cql_reference/refLimits.html"" rel=""nofollow"">https://docs.datastax.com/en/cql/3.1/cql/cql_reference/refLimits.html</a></p>

<p>The column in which you are trying to store string must be your partition key and it is crossing the limit.</p>

<p>you should change your table structure and remove that column for partition key and clustering key</p>

<p>May this will help</p>
",['table']
39654992,39875069,2016-09-23 07:24:08,sstableloader not loading data into DSE 4.5.1 Cassandra cluster,"<p>I have SSTables copied from a source placed here : 
<code>/dev/shm/datafiles/node1/ppr/online_inv</code></p>

<p>When I run SSTable loader to load the data , it runs and terminates within a second without loading the data .</p>

<pre><code>[cassandra@nmd bin]$ ./sstableloader --debug  -d 10.241.17.107 /dev/shm/datafiles/node1/ppr/online_inv
</code></pre>

<p>Established connection to initial hosts
Opening sstables and calculating sections to stream
Streaming relevant part of <code>/dev/shm/datafiles/node1/ppr/online_inv/ppr-online_inv-jb-546-Data.db</code> to []
Streaming session ID: <code>86e92720-815c-11e6-b0c0-45a6be0356e5</code></p>

<p>This is a new DSE 4.5.1 cluster and we just began loading data through prod sstables . Schema is already created just like prod cluster from where we have copied sstables. This is happening for all keyspaces/tables.</p>

<p>We also tried jmxsh method but this is also getting terminated in the same way as sstable loader :</p>

<p><code>INFO [RMI TCP Connection(14)-10.241.17.107] 2016-09-23 07:02:01,482 OutputHandler.java (line 42) Streaming relevant part of /dev/shm/datafiles/node3/look/look_details/look-look_details-jb-1240-Data.db /dev/shm/datafiles/node3/look/look_details/look-look_details-jb-1239-Data.db /dev/shm/datafiles/node3/look/look_details/look-look_details-jb-1237-Data.db /dev/shm/datafiles/node3/look/look_details/look-look_details-jb-1236-Data.db /dev/shm/datafiles/node3/look/look_details/look-look_details-jb-1235-Data.db to []
</code></p>
",<cassandra><datastax><cassandra-2.0><datastax-enterprise><cassandra-2.1>,"<p>The issue was related to snitch settings which picked up default names of DC and rack as 'Cassandra' &amp; 'rack1' . Altered the schema definition and problem was resolved .</p>
",['rack']
39770299,39771551,2016-09-29 12:25:31,Cassandra - one big table vs many tables,"<p>I'm currently looking trying out Cassandra database.
I'm using DataStax Dev center and DataStax C# driver.</p>

<p>My Current model is quite simple and consists of only:</p>

<ul>
<li>ParameterId (int) - would serve as the id of the table.</li>
<li>Value (bigint)</li>
<li>MeasureTime (timestamp)</li>
</ul>

<p>I will be having 1000 (no more, no less) parameters, from 1 - 1000. And will be getting an entry for each parameter once pr. second and will be running for years.</p>

<p>My question is regarding whether it is better practice to create a table as:</p>

<pre><code>CREATE TABLE keyspace.measurement (
    parameterId int,
    value bigint,
    measureTime timestamp,
    PRIMARY KEY(parameterId, measureTime)
) WITH CLUSTERING ORDER BY (measureTime DESC)
</code></pre>

<p>Or it would be better to create 1000 tables consisting only of a value and measureTime, and if so would I be able to range query on my MeasureTime?</p>
",<database-design><cassandra><datastax>,"<p>You are going to hit very wide rows with this. I would advise against your table format, and I'd go with something that allows you to control the wideness of the rows. </p>

<p>Depending on your query requirements, I'll write you down a more suitable schema (IMHO):</p>

<pre><code>CREATE TABLE keyspace.measurement (
    parameterId int,
    granularity timestamp,
    value bigint,
    measureTime timestamp,
    PRIMARY KEY((parameterId, granularity), measureTime)
) WITH CLUSTERING ORDER BY (measureTime DESC)
</code></pre>

<p>This is very similar to yours, however it has a major advantage: you can configure the wideness of your rows, and you don't have any hotspots. The idea is dead simple: both <code>parameterId</code> and <code>granularity</code> fields make the <em>partition key</em>, so they tell where your data will go, while <code>measureTime</code> will keep your data ordered.  Supposing you want to query on a day-by-day basis, you'd store into <code>granularity</code> the value <code>yyyy-mm-dd</code> of your <code>measureTime</code>, grouping together all the measures of the same day. </p>

<p>This allows you to retrieve all the values lying on the same partition (so per given <code>parameterId</code> and <code>granularity</code> fields pair) with an efficient range query. In a day-by-day configuration, you'd end up with 86400 records per partition. This number could be still high (the suggested limit is 10k IIRC), and you can lower tht value by going on hour-by-hour grouping with <code>yyyy-mm-dd HH:00</code> value instead.</p>

<p>The drawback of that approach is that if you need data from multiple partitions (eg you are grouping on day-by-day basis, but you need data for two consecutive days, eg the last 6 hours of the Jan 19th, and the first 6 hours of Jan 20th), then you'll need to perform multiple queries.</p>
",['table']
39791509,39792601,2016-09-30 12:25:48,"How to add a user's 'firstname' to cassandra, getting unknown identifier firstname when trying to add the data","<p>This is the error that is displayed when trying to add to cassandra. Trying to allow a user to enter a username, password and firstname, this data is then stored in cassandra.</p>

<pre><code>HTTP Status 500 - Unknown identifier firstname
</code></pre>

<p>type Exception report</p>

<pre><code>message Unknown identifier firstname

description The server encountered an internal error that prevented it from fulfilling this request.
</code></pre>

<p>exception</p>

<pre><code>com.datastax.driver.core.exceptions.InvalidQueryException: Unknown identifier firstname
com.datastax.driver.core.exceptions.InvalidQueryException.copy(InvalidQueryException.java:35)
com.datastax.driver.core.DefaultResultSetFuture.extractCauseFromExecutionException(DefaultResultSetFuture.java:258)
com.datastax.driver.core.AbstractSession.prepare(AbstractSession.java:79)
uk.ac.dundee.computing.aec.instagrim.models.User.RegisterUser(User.java:40)
uk.ac.dundee.computing.aec.instagrim.servlets.Register.doPost(Register.java:56)
javax.servlet.http.HttpServlet.service(HttpServlet.java:648)
javax.servlet.http.HttpServlet.service(HttpServlet.java:729)
org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52)
org.netbeans.modules.web.monitor.server.MonitorFilter.doFilter(MonitorFilter.java:393)
</code></pre>

<p>root cause</p>

<pre><code>com.datastax.driver.core.exceptions.InvalidQueryException: Unknown identifier firstname
com.datastax.driver.core.Responses$Error.asException(Responses.java:97)
com.datastax.driver.core.SessionManager$1.apply(SessionManager.java:154)
com.datastax.driver.core.SessionManager$1.apply(SessionManager.java:129)
com.google.common.util.concurrent.Futures$1.apply(Futures.java:713)
com.google.common.util.concurrent.Futures$ChainingListenableFuture.run(Futures.java:861)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
java.lang.Thread.run(Thread.java:724)
</code></pre>

<p>Servlet for registering a user</p>

<pre><code>@WebServlet(name = ""Register"", urlPatterns = {""/Register""})
public class Register extends HttpServlet {
Cluster cluster=null;
public void init(ServletConfig config) throws ServletException {
    // TODO Auto-generated method stub
    cluster = CassandraHosts.getCluster();
}

/**
 * Handles the HTTP &lt;code&gt;POST&lt;/code&gt; method.
 *
 * @param request servlet request
 * @param response servlet response
 * @throws ServletException if a servlet-specific error occurs
 * @throws IOException if an I/O error occurs
 */
@Override
protected void doPost(HttpServletRequest request, HttpServletResponse response)
        throws ServletException, IOException {
    String username=request.getParameter(""username"");
    String password=request.getParameter(""password"");
    String firstName=request.getParameter(""firstname"");

    User us=new User();
    us.setCluster(cluster);
    us.RegisterUser(username, password, firstName);

response.sendRedirect(""/Instagrim"");

}

/**
 * Returns a short description of the servlet.
 *
 * @return a String containing servlet description
 */
@Override
public String getServletInfo() {
    return ""Short description"";
}// &lt;/editor-fold&gt;
</code></pre>

<p>}</p>

<p>Method that tries to insert into the database.</p>

<pre><code> public boolean RegisterUser(String username, String Password, String firstName){
    AeSimpleSHA1 sha1handler= new AeSimpleSHA1();
    String EncodedPassword=null;
    try {
        EncodedPassword= sha1handler.SHA1(Password);
    }catch (UnsupportedEncodingException | NoSuchAlgorithmException et){
        System.out.println(""Can't check your password"");
        return false;
    }
    Session session = cluster.connect(""instagrim"");
    PreparedStatement ps = session.prepare(""insert into userprofiles (login,password,firstname) Values(?,?,?)"");

    BoundStatement boundStatement = new BoundStatement(ps);
    session.execute( // this is where the query is executed
            boundStatement.bind( // here you are binding the 'boundStatement'
                   username,EncodedPassword,firstName));
    //We are assuming this always works. Also a transaction would be good here !

    return true;
}
</code></pre>

<p>Java Bean for user data.</p>

<pre><code>public class ProfileBean {
private String login = null;
private String first_name = null;
private String last_name = null;
private String Email = null;
private ByteBuffer pImage = null;
private int length;
private String type;
private java.util.UUID UUID = null;

public void ProfileBean(){

}
public void setLogin(String login){
    this.login = login;
}
public String getLogin(){
    return login;
}

public void setFirstname(String firstName){
    this.first_name = firstName;
}
public String getFirstname(){
    return first_name;
}
public void setLastname(String lastName){
    this.last_name = lastName;
}
public String getLastname(){
    return last_name;
}
public void setEmail(String Email){
    this.Email = Email;
}
public String getEmail(){
    return Email;
}
public void setUUID(java.util.UUID UUID){
    this.UUID = UUID;
}
public String getUUID(){
    return UUID.toString();
}
public void setProfilePic(ByteBuffer pImage, int length, String type)
{
    this.pImage = pImage;
    this.length = length;
    this.type = type;
}
public ByteBuffer getBuffer(){
    return pImage;
}
public int getLength(){
    return length;
}
public String getType(){
    return type;
}
public byte[] getBytes(){
    byte image[] = Bytes.getArray(pImage);
    return image;
}
}
</code></pre>

<p>JSP code that gets the users information.</p>

<pre><code>  &lt;h3&gt;Register as user&lt;/h3&gt;
        &lt;form method=""POST""  action=""Register""&gt;
            &lt;ul&gt;
                &lt;li&gt;User Name &lt;input type=""text"" name=""username""&gt;&lt;/li&gt;
                &lt;li&gt;Password &lt;input type=""password"" name=""password""&gt;&lt;/li&gt;
                &lt;li&gt;First Name&lt;input type=""text"" name=""firstname""&gt;&lt;/li&gt;
            &lt;/ul&gt;
            &lt;br/&gt;
            &lt;input type=""submit"" value=""Register""&gt;
        &lt;/form&gt;
</code></pre>

<p>Code that creates the Cassandra table.</p>

<pre><code>String CreateUserProfile = ""CREATE TABLE if not exists instagrim.userprofiles (\n""
                + ""      login text PRIMARY KEY,\n""
                + ""      password text,\n""
                + ""      firstname text,\n""
                + ""      lastname text,\n""
                + ""      email text,\n""
                +""       picid uuid, \n""
                + ""      addresses  map&lt;text, frozen &lt;address&gt;&gt;\n""
                + ""  );"";
</code></pre>
",<java><html><jsp><servlets><cassandra>,"<p>First, make sure that your <code>address</code> UDT exists before you try and run that <code>CREATE</code> statement.</p>

<p>Secondly, this is failing silently within the driver.  Not sure why, but I could not get the <code>CREATE</code> to actually give me an exception back.</p>

<p>Third, get rid of the <code>\n</code>'s as they are getting sent to Cassandra as a part of your table definition.  Cassandra doesn't need them, and in fact, they cause the <code>CREATE</code> to fail.  I was able to get this to succeed:</p>

<pre><code>String createUserProfile = ""CREATE TABLE if not exists aaron.userprofiles (""
        + ""      login text PRIMARY KEY,""
        + ""      password text,""
        + ""      firstname text,""
        + ""      lastname text,""
        + ""      email text,""
        + ""      picid uuid);"";
</code></pre>

<p>And fourth, don't do that.  Your application will have much better chances for success if you <code>CREATE</code> your tables via cqlsh (or even DevCenter) ahead of time.</p>
",['table']
39839133,39840707,2016-10-03 19:36:41,Understanding the Token Function in Cassandra,"<p>Hello I was reading the Cassandra documentation on Token Function,</p>

<p><a href=""https://i.stack.imgur.com/PoeKh.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/PoeKh.png"" alt=""Screenshot of documentation, https://docs.datastax.com/en/cql/3.1/cql/cql_using/paging_c.html""></a></p>

<p>I am trying to achieve pagination for a Cassandra table, I am unable to understand the lines highlighted. The document speaks about the difference between k > 42 and TOKEN(k) > TOKEN(42), but I am not able to understand the ""token based comparison""</p>

<p>Looking forward for a detailed explanation of what token function does when part of a WHERE clause.</p>
",<cassandra><pagination><cql><cql3>,"<p>In order to understand in which <em>partition</em> it should put your data, C* makes some calculations on the <code>PARTITION KEY</code>s of every row. Specifically, on each node, rows are sorted by the token generated by the partitioner, (and each partition have data sorted by the cluster key). Different <a href=""https://docs.datastax.com/en/cassandra/3.x/cassandra/architecture/archPartitionerAbout.html"" rel=""noreferrer""><em>partitioners</em></a> perform different types of calculations. </p>

<p>While the <a href=""https://docs.datastax.com/en/cassandra/3.x/cassandra/architecture/archPartitionerM3P.html"" rel=""noreferrer"">Murmur3Partitioner</a> calculates the <a href=""https://en.wikipedia.org/wiki/MurmurHash"" rel=""noreferrer"">MurmurHash</a> of the partion key, the <a href=""https://docs.datastax.com/en/cassandra/3.x/cassandra/architecture/archPartitionerBOP.html"" rel=""noreferrer"">ByteOrderedPartitioner</a> uses the raw data bytes of the partition key itself: when you use the Murmur3Partitioner, your rows are sorted by their <em>hashes</em>, while when you use the <em>ByteOrderedPartitioner</em>, your rows are sorted <strong>directly</strong> by their <em>raw values</em>. </p>

<p>As an example, assume you have a table like this:</p>

<pre><code>CREATE TABLE test (
    username text,
    ...
    PRIMARY KEY (username)
);
</code></pre>

<p>And assume you're trying to locate where the rows corresponding to the usernames <code>abcd</code> and <code>abce</code> and <code>abcf</code> are stored. The hex representation of these strings are <code>0x61626364</code> and <code>0x61626365</code> and <code>0x61626366</code> respectively. Assuming we apply this <a href=""http://murmurhash.shorelabs.com"" rel=""noreferrer"">MH3</a> implementation (x86, 32-bit for simplicity, no optional seed) on both strings we get <code>‭0x‭43ED676A‬‬</code> and <code>0x‭‭E297E8AA‬‬</code> and <code>0x‭‭87E62668‬‬</code> respectively. So, in the case of MH3, the tokens of the strings will be these 3 values, while in the case of the BOP the tokens will be the raw data values themselves: <code>0x61626364</code>, <code>0x61626365</code> and <code>0x61626366</code>. </p>

<p>Now you can see that storing data sorted by <em>token</em> produces different results when different partitioners are used. A <code>SELECT * FROM test;</code> query would return rows in different order. This <em>can</em> (but <em>should not</em>) be a problem if you have data already sorted by their raw values <strong>and</strong> you need to retrieve that in the same order because when you use MH3 the order is complelety unrelated to your data. </p>

<p>Back to the question, the <code>TOKEN</code> function allows you to filter directly by the <em>tokens of your data</em> instead of <em>your data</em>. The <a href=""https://docs.datastax.com/en/cql/3.3/cql/cql_using/useToken.html"" rel=""noreferrer"">documentation</a> says:</p>

<blockquote>
  <p>ordering with the TOKEN function does not always provide the expected
  results. Use the TOKEN function to express a conditional relation on a
  partition key column. In this case, the query returns rows based on
  the token of the partition key rather than on the value.</p>
</blockquote>

<p>As an example, you could issue:</p>

<pre><code>SELECT * FROM test WHERE TOKEN(username) &lt;= TOKEN('abcf');
</code></pre>

<p>and you'd get figure what? <code>abcd</code> and <code>acbf</code> rows!!! This is because order <em>sometimes</em> matters... Like in the case of the pagination you're trying to do, which <strong>will be handled flawlessy for you by any available C* driver</strong> (eg the <a href=""https://datastax.github.io/java-driver/manual/paging/"" rel=""noreferrer"">Java driver</a>).</p>

<p>That said, the recommended partitioner for new clusters is <a href=""https://docs.datastax.com/en/cassandra/3.x/cassandra/architecture/archPartitionerM3P.html"" rel=""noreferrer""><em>Murmur3Partitioner</em></a>, you can check the <a href=""https://docs.datastax.com/en/cassandra/3.x/cassandra/architecture/archPartitionerAbout.html"" rel=""noreferrer"">documentation</a> for both pros and cons of each partitioner. Please note that the partitioner is a <strong>cluster-wide</strong> settings, and once set you cannot change it without pushing all of your data into another cluster. </p>

<p>Make your choice carefully.</p>
","['partitioner', 'table']"
39868596,39894604,2016-10-05 08:08:42,Cassandra LeveledCompactionStrategy and high SSTable number per read,"<p>We are using cassandra 2.0.17 and we have a table with 50% selects, 40% of updates and 10% of inserts (no deletes). </p>

<p>To have high read performance for such table we found that it is suggested to use LeveledCompactionStrategy (it is supposed to guarantee that 99% of reads will be fulfilled from single SSTable). Every day when I run <code>nodetool cfhistograms</code> i see more and more SSTtables per read. First day we had 1, than we had 1,2,3 ...<br>
and this morning I am seeing this:</p>

<pre><code>ubuntu@ip:~$ nodetool cfhistograms prodb groups | head -n 20                                                                                                                                
prodb/groups histograms

SSTables per Read
1 sstables: 27007
2 sstables: 97694
3 sstables: 95239
4 sstables: 3928
5 sstables: 14
6 sstables: 0
7 sstables: 19
</code></pre>

<p>The describe groups returns this:</p>

<pre><code>CREATE TABLE groups (
  ...
) WITH
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.100000 AND
  gc_grace_seconds=172800 AND
  index_interval=128 AND
  read_repair_chance=0.000000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  default_time_to_live=0 AND
  speculative_retry='99.0PERCENTILE' AND
  memtable_flush_period_in_ms=0 AND
  compaction={'class': 'LeveledCompactionStrategy'} AND
  compression={'sstable_compression': 'LZ4Compressor'};
</code></pre>

<p>Is it normal? In such case we loose the advantage of using LeveledCompaction which as described in the documentation should guarantee 99% of reads from single sstable.</p>
",<cassandra><cassandra-2.0>,"<p>It does depend on the usecase - but as a rule of thumb I normally look at LCS for 90% read to 10% write ratio.  From your description you're looking at 50/50 at best. </p>

<p>The additional compaction demands placed by LCS makes it pretty io hungry.  It's highly likely that compaction is backed up and your levels are not balanced.  The easiest way to tell is to run nodetool cfstats for the table in question.</p>

<p>You're looking for the line:</p>

<p>SSTables in each level: [2042/4, 10, 119/100, 232, 0, 0, 0, 0, 0]</p>

<p>The numbers in the square brackets shows how many sstables are in each level.  [L0, L1, L2 ...].  The number after the slash is the ideal level.  As a rule of thumb L1 should be 10, L2 100, L3 1000 etc.</p>

<p>New sstables go in at L0 and then gradually move up.  You can see the above example is in a really bad state.  We've still got 2000 sstables to process more than exists in all other levels.  The performance here will be massively worse than if I'd just used STCS.  </p>

<p>Nodetool cfstats makes it pretty easy to measure if LCS is keeping up with your usecase.  Just dump out the above every 15 minutes throughout the day.  Any time your levels are unbalanced the read performance will suffer.  If it's constantly behind you probably want to switch to STCS.  If it spikes for say 10 minutes when you data load but the rest of the day is fine - then you may decide to live with it.  If it never goes out of balance - stick with LCS - it's totally working for you.</p>

<p>As a side note - 2.1 allows L0 to carry out STCS style merging which will help in the situation where you have a temporary spike.  If you're in the ten minute scenario above - it's almost certainly worth an upgrade.</p>
",['table']
39887323,39890996,2016-10-06 04:05:02,"Spark and Cassandra Java application: Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/spark/sql/Dataset","<p>I got an amazingly siplme java application which I almost copied from this one example: <a href=""http://markmail.org/download.xqy?id=zua6upabiylzeetp&amp;number=2"" rel=""nofollow"">http://markmail.org/download.xqy?id=zua6upabiylzeetp&amp;number=2</a></p>

<p>All I wanted to do is to read the table data and display in the Eclipse console.</p>

<p>My pom.xml:</p>

<pre><code>        &lt;project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd""&gt;
  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
  &lt;groupId&gt;chat_connaction_test&lt;/groupId&gt;
  &lt;artifactId&gt;ChatSparkConnectionTest&lt;/artifactId&gt;
  &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;
 &lt;dependencies&gt; 
    &lt;dependency&gt;
    &lt;groupId&gt;com.datastax.cassandra&lt;/groupId&gt;
    &lt;artifactId&gt;cassandra-driver-core&lt;/artifactId&gt;
    &lt;version&gt;3.1.0&lt;/version&gt;
    &lt;/dependency&gt;

    &lt;dependency&gt;
    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-core_2.10&lt;/artifactId&gt;
    &lt;version&gt;2.0.0&lt;/version&gt;
    &lt;/dependency&gt;

    &lt;!-- https://mvnrepository.com/artifact/com.datastax.spark/spark-cassandra-connector_2.10 --&gt;
    &lt;dependency&gt;
    &lt;groupId&gt;com.datastax.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-cassandra-connector_2.10&lt;/artifactId&gt;
    &lt;version&gt;2.0.0-M3&lt;/version&gt;
    &lt;/dependency&gt;

    &lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-streaming_2.10 --&gt;
    &lt;dependency&gt;
    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-streaming_2.10&lt;/artifactId&gt;
    &lt;version&gt;2.0.0&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;!--
    &lt;dependency&gt; 
    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; 
    &lt;artifactId&gt;spark-hive_2.10&lt;/artifactId&gt; 
    &lt;version&gt;1.5.2&lt;/version&gt; 
    &lt;/dependency&gt;
    --&gt;
  &lt;/dependencies&gt;
&lt;/project&gt;
</code></pre>

<p>And my java code:</p>

<pre><code>    package com.chatSparkConnactionTest;

import static com.datastax.spark.connector.japi.CassandraJavaUtil.javaFunctions;
import java.io.Serializable;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.api.java.function.Function;
import com.datastax.spark.connector.japi.CassandraRow;

public class JavaDemo implements Serializable {
    private static final long serialVersionUID = 1L;
    public static void main(String[] args) {

        SparkConf conf = new SparkConf().
            setAppName(""chat"").
            setMaster(""local"").
            set(""spark.executor.memory"",""1g"").
            set(""spark.cassandra.connection.host"", ""127.0.0.1"");
        JavaSparkContext sc = new JavaSparkContext(conf);

        JavaRDD&lt;String&gt; cassandraRowsRDD = javaFunctions(sc).cassandraTable(
            ""chat"", ""dictionary"")

            .map(new Function&lt;CassandraRow, String&gt;() {
                @Override
                public String call(CassandraRow cassandraRow) throws Exception {
                    String tempResult = cassandraRow.toString();
                    System.out.println(tempResult);
                    return tempResult;
                    }
                }
            );
        System.out.println(""Data as CassandraRows: \n"" + 
        cassandraRowsRDD.collect().size()); // THIS IS A LINE WITH ERROR
    } 
}
</code></pre>

<p>And here is my error:</p>

<blockquote>
  <p>16/10/05 20:49:18 INFO CassandraConnector: Connected to Cassandra
  cluster: Test Cluster Exception in thread ""main""
  java.lang.NoClassDefFoundError: org/apache/spark/sql/Dataset  at
  java.lang.Class.getDeclaredMethods0(Native Method)    at
  java.lang.Class.privateGetDeclaredMethods(Unknown Source)     at
  java.lang.Class.getDeclaredMethod(Unknown Source)     at
  java.io.ObjectStreamClass.getPrivateMethod(Unknown Source)    at
  java.io.ObjectStreamClass.access$1700(Unknown Source)     at
  java.io.ObjectStreamClass$2.run(Unknown Source)   at
  java.io.ObjectStreamClass$2.run(Unknown Source)   at
  java.security.AccessController.doPrivileged(Native Method)    at
  java.io.ObjectStreamClass.(Unknown Source)  at
  java.io.ObjectStreamClass.lookup(Unknown Source)  at
  java.io.ObjectOutputStream.writeObject0(Unknown Source)   at
  java.io.ObjectOutputStream.defaultWriteFields(Unknown Source)     at
  java.io.ObjectOutputStream.writeSerialData(Unknown Source)    at
  java.io.ObjectOutputStream.writeOrdinaryObject(Unknown Source)    at
  java.io.ObjectOutputStream.writeObject0(Unknown Source)   at
  java.io.ObjectOutputStream.defaultWriteFields(Unknown Source)     at
  java.io.ObjectOutputStream.writeSerialData(Unknown Source)    at
  java.io.ObjectOutputStream.writeOrdinaryObject(Unknown Source)    at
  java.io.ObjectOutputStream.writeObject0(Unknown Source)   at
  java.io.ObjectOutputStream.writeObject(Unknown Source)    at
  scala.collection.immutable.$colon$colon.writeObject(List.scala:379)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at
  sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)   at
  sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)   at
  java.lang.reflect.Method.invoke(Unknown Source)   at
  java.io.ObjectStreamClass.invokeWriteObject(Unknown Source)   at
  java.io.ObjectOutputStream.writeSerialData(Unknown Source)    at
  java.io.ObjectOutputStream.writeOrdinaryObject(Unknown Source)    at
  java.io.ObjectOutputStream.writeObject0(Unknown Source)   at
  java.io.ObjectOutputStream.defaultWriteFields(Unknown Source)     at
  java.io.ObjectOutputStream.writeSerialData(Unknown Source)    at
  java.io.ObjectOutputStream.writeOrdinaryObject(Unknown Source)    at
  java.io.ObjectOutputStream.writeObject0(Unknown Source)   at
  java.io.ObjectOutputStream.defaultWriteFields(Unknown Source)     at
  java.io.ObjectOutputStream.writeSerialData(Unknown Source)    at
  java.io.ObjectOutputStream.writeOrdinaryObject(Unknown Source)    at
  java.io.ObjectOutputStream.writeObject0(Unknown Source)   at
  java.io.ObjectOutputStream.defaultWriteFields(Unknown Source)     at
  java.io.ObjectOutputStream.writeSerialData(Unknown Source)    at
  java.io.ObjectOutputStream.writeOrdinaryObject(Unknown Source)    at
  java.io.ObjectOutputStream.writeObject0(Unknown Source)   at
  java.io.ObjectOutputStream.writeObject(Unknown Source)    at
  org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)
    at
  org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
    at
  org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:295)
    at
  org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:288)
    at
  org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:108)
    at org.apache.spark.SparkContext.clean(SparkContext.scala:2037)     at
  org.apache.spark.SparkContext.runJob(SparkContext.scala:1896)     at
  org.apache.spark.SparkContext.runJob(SparkContext.scala:1911)     at
  org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:893)  at
  org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
    at
  org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
    at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)    at
  org.apache.spark.rdd.RDD.collect(RDD.scala:892)   at
  org.apache.spark.api.java.JavaRDDLike$class.collect(JavaRDDLike.scala:360)
    at
  org.apache.spark.api.java.AbstractJavaRDDLike.collect(JavaRDDLike.scala:45)
    at com.chatSparkConnactionTest.JavaDemo.main(JavaDemo.java:37) Caused
  by: java.lang.ClassNotFoundException: org.apache.spark.sql.Dataset    at
  java.net.URLClassLoader.findClass(Unknown Source)     at
  java.lang.ClassLoader.loadClass(Unknown Source)   at
  sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source)    at
  java.lang.ClassLoader.loadClass(Unknown Source)   ... 58 more</p>
</blockquote>

<p>I made my pom.xml updated but that didn't resolve the error. Could anybody help me to resolve this problem?</p>

<p>Thank you!</p>

<p>Update 1:
here is my build path screenshot:
<a href=""http://i.stack.imgur.com/uQhp9.png"" rel=""nofollow"">Link to my screenshot</a></p>
",<java><apache-spark><cassandra><datastax>,"<p>You are getting ""java.lang.NoClassDefFoundError: org/apache/spark/sql/Dataset"" error because ""spark-sql"" dependency is missing from your pom.xml file. </p>

<p>If you want to read Cassandra table with Spark 2.0.0 then you need below minimum dependencies.</p>

<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;
    &lt;version&gt;2.0.0&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt;
    &lt;version&gt;2.0.0&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;com.datastax.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-cassandra-connector_2.11&lt;/artifactId&gt;
    &lt;version&gt;2.0.0-M3&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<p>Spark 2.0.0 provides SparkSession and Dataset API. Below is the sample program for reading Cassandra table and printing the records.</p>

<pre><code> public class SparkCassandraDatasetApplication {
 public static void main(String[] args) {
 SparkSession spark = SparkSession
          .builder()
          .appName(""SparkCassandraDatasetApplication"")
          .config(""spark.sql.warehouse.dir"", ""/file:C:/temp"")
          .config(""spark.cassandra.connection.host"", ""127.0.0.1"")
          .config(""spark.cassandra.connection.port"", ""9042"")
          .master(""local[2]"")
          .getOrCreate();

 //Read data
 Dataset&lt;Row&gt; dataset = spark.read().format(""org.apache.spark.sql.cassandra"")
        .options(new HashMap&lt;String, String&gt;() {
            {
                put(""keyspace"", ""mykeyspace"");
                put(""table"", ""mytable"");
            }
        }).load();

   //Print data
   dataset.show();       
   spark.stop();
   }        
}
</code></pre>

<p>If you still want to use RDD then use below sample program.</p>

<pre><code>public class SparkCassandraRDDApplication {
public static void main(String[] args) {
    SparkConf conf = new SparkConf()
            .setAppName(""SparkCassandraRDDApplication"")
            .setMaster(""local[2]"")
            .set(""spark.cassandra.connection.host"", ""127.0.0.1"")
            .set(""spark.cassandra.connection.port"", ""9042"");

    JavaSparkContext sc = new JavaSparkContext(conf);

    //Read
    JavaRDD&lt;UserData&gt; resultsRDD = javaFunctions(sc).cassandraTable(""mykeyspace"", ""mytable"",CassandraJavaUtil.mapRowTo(UserData.class));

    //Print
    resultsRDD.foreach(data -&gt; {
        System.out.println(data.id);
        System.out.println(data.username);
    });

    sc.stop();
  }
}
</code></pre>

<p>Javabean (UserData) used in above program is like below.</p>

<pre><code>public class UserData implements Serializable{  
  String id;
  String username;     
  public String getId() {
      return id;
  }
  public void setId(String id) {
      this.id = id;
  }
  public String getUsername() {
     return username;
  }
  public void setUsername(String username) {
     this.username = username;
   }    
}
</code></pre>
",['table']
39893193,41800447,2016-10-06 10:08:51,cassandra cql shell window got disappears after installation in windows,"<p>cassandra cql shell window got disappears after installation in windows?
this was installed using MSI installer availalbe in planet cassandra.</p>

<p>Why this happens ? please help me..</p>

<p>Thanks in advance.</p>
",<cassandra><datastax><datastax-enterprise><cqlsh>,"<p>I had the same issue with DataStax 3.9. This is how I sorted this:</p>

<p>Step 1: Open file: DataStax-DDC\apache-cassandra\conf\cassandra.yaml</p>

<p>Step 2: Uncomment the cdc_raw_directory and set new value to (for windows)</p>

<p>cdc_raw_directory: ""C:/Program Files/DataStax-DDC/data/cdc_raw"" </p>

<p>Step 3: Goto Windows Services and Start the DataStax DDC Server 3.9.0 Service</p>
",['cdc_raw_directory']
39898410,40001752,2016-10-06 14:15:10,Cassandra Query fails after data load of 5m rows - Cassandra failure during read query at consistency LOCAL_ONE,"<p><strong>The Problem</strong></p>

<p>Simple CQL select failing when I have a large data load.</p>

<p><strong>Setup</strong></p>

<p>I am using the following Cassandra schema: </p>

<pre><code>CREATE KEYSPACE fv WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };


create table entity_by_identifier (
    identifier text,
    state entity_state,
    PRIMARY KEY(identifier)
);

CREATE TYPE entity_state,(
    identifier text,
    number1 int,
    number2 double,
    entity_type text,
    string1 text,
    string2 text
);
</code></pre>

<p>The query I am trying to execute:</p>

<pre><code>SELECT * FROM fv.entity_by_identifier WHERE identifier=:identifier;
</code></pre>

<p><strong>The Issue</strong></p>

<p>This query works fine in a small dataset (tried with 500 rows).
However, with a large data load test, I am creating over 5million rows in this table before proceeding to execute this query multiple times (10 threads continuously performing this query for 1 hour).</p>

<p>Once the data load has completed, the queries begin but immediately fail with the following error: </p>

<pre><code>com.datastax.driver.core.exceptions.ReadFailureException: Cassandra failure during read query at consistency LOCAL_ONE (1 responses were required but only 0 replica responded, 1 failed)
    at com.datastax.driver.core.exceptions.ReadFailureException.copy(ReadFailureException.java:85)
    at com.datastax.driver.core.exceptions.ReadFailureException.copy(ReadFailureException.java:27)
    at com.datastax.driver.core.DriverThrowables.propagateCause(DriverThrowables.java:37)
    at com.datastax.driver.core.DefaultResultSetFuture.getUninterruptibly(DefaultResultSetFuture.java:245)
    at com.datastax.driver.core.AbstractSession.execute(AbstractSession.java:64)
...my calling classes...
</code></pre>

<p>I've checked the Cassandra log and found only this exception:</p>

<pre><code>java.lang.AssertionError: null
    at org.apache.cassandra.db.rows.BTreeRow.getCell(BTreeRow.java:212) ~[apache-cassandra-3.7.jar:3.7]
    at org.apache.cassandra.db.SinglePartitionReadCommand.canRemoveRow(SinglePartitionReadCommand.java:899) [apache-cassandra-3.7.jar:3.7]
    at org.apache.cassandra.db.SinglePartitionReadCommand.reduceFilter(SinglePartitionReadCommand.java:863) [apache-cassandra-3.7.jar:3.7]
    at org.apache.cassandra.db.SinglePartitionReadCommand.queryMemtableAndSSTablesInTimestampOrder(SinglePartitionReadCommand.java:748) [apache-cassandra-3.7.jar:3.7]
    at org.apache.cassandra.db.SinglePartitionReadCommand.queryMemtableAndDiskInternal(SinglePartitionReadCommand.java:519) [apache-cassandra-3.7.jar:3.7]
    at org.apache.cassandra.db.SinglePartitionReadCommand.queryMemtableAndDisk(SinglePartitionReadCommand.java:496) [apache-cassandra-3.7.jar:3.7]
    at org.apache.cassandra.db.SinglePartitionReadCommand.queryStorage(SinglePartitionReadCommand.java:358) [apache-cassandra-3.7.jar:3.7]
    at org.apache.cassandra.db.ReadCommand.executeLocally(ReadCommand.java:366) ~[apache-cassandra-3.7.jar:3.7]
    at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:1797) ~[apache-cassandra-3.7.jar:3.7]
    at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2466) ~[apache-cassandra-3.7.jar:3.7]
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_101]
    at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) [apache-cassandra-3.7.jar:3.7]
    at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:136) [apache-cassandra-3.7.jar:3.7]
    at org.apache.cassandra.concurrent.SEPExecutor.maybeExecuteImmediately(SEPExecutor.java:192) [apache-cassandra-3.7.jar:3.7]
    at org.apache.cassandra.service.AbstractReadExecutor.makeRequests(AbstractReadExecutor.java:117) [apache-cassandra-3.7.jar:3.7]
    at org.apache.cassandra.service.AbstractReadExecutor.makeDataRequests(AbstractReadExecutor.java:85) [apache-cassandra-3.7.jar:3.7]
    at org.apache.cassandra.service.AbstractReadExecutor$NeverSpeculatingReadExecutor.executeAsync(AbstractReadExecutor.java:214) [apache-cassandra-3.7.jar:3.7]
    at org.apache.cassandra.service.StorageProxy$SinglePartitionReadLifecycle.doInitialQueries(StorageProxy.java:1702) [apache-cassandra-3.7.jar:3.7]
    at org.apache.cassandra.service.StorageProxy.fetchRows(StorageProxy.java:1657) [apache-cassandra-3.7.jar:3.7]
    at org.apache.cassandra.service.StorageProxy.readRegular(StorageProxy.java:1604) [apache-cassandra-3.7.jar:3.7]
    at org.apache.cassandra.service.StorageProxy.read(StorageProxy.java:1523) [apache-cassandra-3.7.jar:3.7]
    at org.apache.cassandra.db.SinglePartitionReadCommand.execute(SinglePartitionReadCommand.java:335) [apache-cassandra-3.7.jar:3.7]
    at org.apache.cassandra.service.pager.AbstractQueryPager.fetchPage(AbstractQueryPager.java:67) [apache-cassandra-3.7.jar:3.7]
    at org.apache.cassandra.service.pager.SinglePartitionPager.fetchPage(SinglePartitionPager.java:34) [apache-cassandra-3.7.jar:3.7]
    at org.apache.cassandra.cql3.statements.SelectStatement$Pager$NormalPager.fetchPage(SelectStatement.java:325) [apache-cassandra-3.7.jar:3.7]
    at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:361) [apache-cassandra-3.7.jar:3.7]
    at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:237) [apache-cassandra-3.7.jar:3.7]
    at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:78) [apache-cassandra-3.7.jar:3.7]
    at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:208) [apache-cassandra-3.7.jar:3.7]
    at org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:486) [apache-cassandra-3.7.jar:3.7]
    at org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:463) [apache-cassandra-3.7.jar:3.7]
    at org.apache.cassandra.transport.messages.ExecuteMessage.execute(ExecuteMessage.java:130) [apache-cassandra-3.7.jar:3.7]
    at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:507) [apache-cassandra-3.7.jar:3.7]
    at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:401) [apache-cassandra-3.7.jar:3.7]
    at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.36.Final.jar:4.0.36.Final]
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:292) [netty-all-4.0.36.Final.jar:4.0.36.Final]
    at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:32) [netty-all-4.0.36.Final.jar:4.0.36.Final]
    at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:283) [netty-all-4.0.36.Final.jar:4.0.36.Final]
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_101]
    at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) [apache-cassandra-3.7.jar:3.7]
    at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [apache-cassandra-3.7.jar:3.7]
    at java.lang.Thread.run(Thread.java:745) [na:1.8.0_101]
</code></pre>

<p>As you can see I am using Cassandra 3.7. 
The Datastax driver in use is version 3.1.0.</p>

<p>Any ideas why the larger data set could cause this error? </p>
",<cassandra><cql><datastax-java-driver>,"<p>Found a solution to the problem.</p>

<p>When using a User Defined Type the ""frozen"" keyword needs to be used.</p>

<pre><code>create table entity_by_identifier (
identifier text,
state entity_state,
PRIMARY KEY(identifier)
);
</code></pre>

<p>becomes:</p>

<pre><code>create table entity_by_identifier (
identifier text,
state frozen &lt;entity_state&gt;,
PRIMARY KEY(identifier)
);
</code></pre>

<p>You can find information about this Frozen keyword at:
<a href=""http://docs.datastax.com/en/cql/3.1/cql/cql_using/cqlUseUDT.html"" rel=""nofollow"">http://docs.datastax.com/en/cql/3.1/cql/cql_using/cqlUseUDT.html</a>
and 
<a href=""http://docs.datastax.com/en/cql/3.1/cql/cql_reference/create_table_r.html#reference_ds_v3f_vfk_xj__tuple-udt-columns"" rel=""nofollow"">http://docs.datastax.com/en/cql/3.1/cql/cql_reference/create_table_r.html#reference_ds_v3f_vfk_xj__tuple-udt-columns</a></p>

<p>Although, it is still not clear to me why the lack of this ""Frozen"" keyword led to the error I was seeing.</p>
",['table']
39932666,39937137,2016-10-08 13:00:56,Error NoHostAvailableException: All host(s) tried for query failed (tried: /127.0.0.1:9042,"<p>I Recently started learning cassandra and going through online tutorials for cassandra with DataStax Java Drivers.  I am simply trying to connect a localhost node on my laptop.</p>

<p>Setup Details -</p>

<p>OS - Windows 7</p>

<p>Cassandra Version - Cassandra version: 2.1-SNAPSHOT</p>

<p>DataStax java driver version - 3.1.0</p>

<p>I could able to connect to local node by using CQLSH and cassandra-cli clients.
I can also see the default keyspace system and system_traces.
Below is the cassandra server log</p>

<pre><code>INFO  12:12:51 Node localhost/127.0.0.1 state jump to normal
INFO  12:12:51 Startup completed! Now serving reads.
INFO  12:12:51 Starting listening for CQL clients on /0.0.0.0:9042...
INFO  12:12:51 Binding thrift service to /0.0.0.0:9160
INFO  12:12:51 Using TFramedTransport with a max frame size of 15728640 bytes.
INFO  12:12:51 Using synchronous/threadpool thrift server on 0.0.0.0 : 9160
INFO  12:12:51 Listening for thrift clients...
</code></pre>

<p>I am trying below simple code -</p>

<pre><code>        Cluster cluster = Cluster.builder().addContactPoint(""127.0.0.1"").build();
        Metadata metadata = cluster.getMetadata();
</code></pre>

<p>This code throws below exception - Part of the Trace</p>

<pre><code>com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: /127.0.0.1:9042 (com.datastax.driver.core.exceptions.InvalidQueryException: unconfigured columnfamily schema_usertypes))
at com.datastax.driver.core.ControlConnection.reconnectInternal(ControlConnection.java:233)
</code></pre>

<p>I have been through all the previously asked question. Most of answers suggests changing the configuration in cassandra.yaml file. </p>

<p>My cassandra.yaml configuration is - </p>

<pre><code>start_native_transport: true
native_transport_port: 9042
listen_address: localhost
rpc_address: 0.0.0.0
rpc_port: 9160
</code></pre>

<p>Most of the answers suggests to use actual IP address of machine at rpc_address, which I tried but did not worked.</p>

<p>Here are the Questions I been through -
<a href=""https://stackoverflow.com/questions/18146781/why-cant-i-connect-to-cassandra"">Question One</a>, <a href=""https://stackoverflow.com/questions/35254443/error-on-127-0-0-1-connection-com-datastax-driver-core-transportexception-1"">Question two</a> ,<a href=""https://stackoverflow.com/questions/16783725/error-while-connecting-to-cassandra-using-java-driver-for-apache-cassandra-1-0-f"">Question three</a>, <a href=""https://groups.google.com/a/lists.datastax.com/forum/#!topic/java-driver-user/8m1hPFjcaFc"" rel=""nofollow noreferrer"">Topic</a> ,<a href=""https://github.com/datastax/java-driver/wiki/Connection-requirements"" rel=""nofollow noreferrer"">Connection requirement</a>, <a href=""https://stackoverflow.com/questions/18724334/cant-connect-to-cassandra-nohostavailableexception"">Question four</a>.</p>

<p><a href=""http://www.datastax.com/dev/blog/java-driver-2-1-2-native-protocol-v3"" rel=""nofollow noreferrer"">This page</a>  lists compatibility of Java DataStax drivers with cassandra versions, so I changed the driver version to 2.1.1 (As I am using cassandra 2.1), but it did not worked. </p>

<p>Please suggest what could be wrong.</p>
",<java><cassandra><connection><datastax>,"<p>The error with <code>schema_usertypes</code> seems like the driver is trying to look for a table that isn't there maybe related to this <a href=""https://datastax-oss.atlassian.net/browse/JAVA-1197"" rel=""nofollow"">Jira</a>. </p>

<p>You say you are running a 2.1-SNAPSHOT of Cassandra? Try Cassandra 2.1.15. Something seems off on your Cassandra node, the driver is able to talk to your cluster since it trys to look up data inside <code>schema_usertypes</code>.</p>
",['table']
40005801,40430502,2016-10-12 18:25:21,Kafka Streams Cassandra Connector,"<p>I had a couple questions regarding the Cassandra connector written by Data Mountaineer.  Any help is greatly appreciated as we're trying to figure out the best way to scale our architecture.</p>

<ol>
<li><p>Do we have to create a Connector config for each Cassandra table we want to update?  For instance, let's say I have a 1000 tables.  Each table is dedicated to a different type of widget.  Each widget has similar characteristics, but slightly different data.  Do we need to create a connector for each table? If so, how is this managed and how does this scale?</p></li>
<li><p>In Cassandra, we often need to model column families based on the business need.  We may have 3 tables representing user information.  1 by username, 1 by email and 1 by last name.  Would we need 3 connector configs and deploy 3 separate Sink tasks to push data to each table?</p></li>
</ol>
",<cassandra><stream><apache-kafka><connector>,"<p>I think both questions are similar, can the sink handle multiple topics?</p>

<p>The sink can handle multiple tables in one sink so one configuration. This is set in the kcql statement <code>connect.cassandra.export.route.query=INSERT INTO orders SELECT * FROM orders-topic;INSERT INTO positions SELECT * FROM positions</code> but at present they need to be in the same Cassandra keyspace. This would route events from the trades topic to a Cassandra table called trades and events from positions. You can also select specific columns and rename like select columnA as columnB.</p>

<p>You may want more than one sink instance for separation of concerns, i.e. isolating the write of a group of topics from other unrelated topics.</p>

<p>You can scale with the number of tasks the connector is allowed to run, each task starts a Writer for all the target tables.</p>

<p>We have a support channel of our own for more direct communication. <a href=""https://datamountaineer.com/contact/"" rel=""nofollow noreferrer"">https://datamountaineer.com/contact/</a></p>
",['table']
40097118,40110668,2016-10-17 23:17:17,Cassandra: insert value and update average/min/max,"<p>I am thinking of using Cassandra for time series data in a Java application. I also need the average (and min/max) for the last <em>n</em> minutes.</p>

<p>The simple approach is to make three calls from the client:</p>

<ol>
<li>insert the new value</li>
<li>select the average, min and max over the recent <em>n</em> minutes</li>
<li>update the average</li>
</ol>

<p>Is there a more efficient way to do this?</p>

<p>The first and second step use the same partition, and thus will run on the same node. Thus a round-trip could be saved if both statement could be executed in the same request. But BATCH does not support select (as far as I understand).</p>

<p>The third request involves a different partition (and thus likely a different node). The benefit of using BATCH would be that both tables remain in sync. And I think it would also save a client -> coordinator round trip. But BATCH does not support passing the results from a select to an update (as far as I know).</p>

<pre><code>create table metrics (
  resource_name text,
  metric_name text,
  recorded_at timestamp,
  value double,
  primary key ((resource_name, metric_name), recorded_at)
) with clustering order by (recorded_at desc);

create table last_30m (
  metric_name text,
  resource_name text,
  avg_value double,
  min_value double,
  max_value double,
  primary key (metric_name, resource_name)
) with clustering order by (resource_name asc);
</code></pre>
",<cassandra><cql>,"<p>There is a <code>avg</code>, <code>min</code>, and <code>max</code> aggregate functions (as of 2.2). So you dont really need a table for this, you can just query for it. The data will all be sequential on disk or in memtables since sorted by <code>recorded_at</code> </p>

<p><code>SELECT avg(value), min(value), max(value) FROM metrics WHERE resource_name = 'blarg' AND metric_name = 'cpu' AND recorded_at &gt; {half hour ago}</code></p>

<p>In the future there will be a <code>now()-30m</code> (CASSANDRA-11936) but for now you have to manually compute and put the ""half hour ago"" value in.</p>

<p>I would strongly recommend avoiding BATCH and updating a 2nd table for this and just make the query to read the value when you need it. If you are concerned about the performance of the above query, test it before making an optimization that will in all likelihood be more expensive. If you need things like ""average for past day"" it may be worth it, but I would not do it as part of your updates but more of a ""every minute update"" kinda thing (also consider spark streaming)</p>
",['table']
40140126,40143996,2016-10-19 19:37:35,Query regarding Cassandra java BoundStatement with multiple attributes,"<p>I'm trying to fetch records from Cassandra using BoundStatement's bind method and then doing session.execute on that.
Suppose the json looks like this as shown below</p>

<pre><code>{
id:1,
name:abc,
.
.
.
}
</code></pre>

<p>Here,if I want to query using multiple columns (id=1 and name=abc),I'm able to fetch the records successfully.</p>

<p>However, my json looks something like this:</p>

<pre><code>{
id:1
name:[""abc"",""xyz""]
.
.
.
}
</code></pre>

<p>In the database, it gets saved in two different rows(as there are list of values associated with the name attribute)</p>

<p>The problem is how can I fetch the records from the database?</p>

<p>P.S.:id and name are compound partition key</p>
",<java><rest><cassandra>,"<p>Since <code>id</code> and <code>name</code> are part of a compound partition key you will always need to search by both of them. So you can take your JSON and fan out the name list into a few select queries or use an <code>IN</code>.</p>

<p><strong>Recommended Solution</strong></p>

<p>Use two different SELECT queries at the same time then aggregate the results this allows each SELECT to hit the best node assuming you are using TokenAware.</p>

<p>I use RxJava to spread out to many requests then merge them back to a list of rows.</p>

<pre><code>public class CassandraService {
  //...

  public void run() {

    String id = ""1"";
    List&lt;String&gt; names = ['abc','xyz'];


    PreparedStatement selectStmt = session.prepare(""SELECT id, name, value FROM table WHERE id=? AND name=?;"");
    List statements = new ArrayList&lt;Statement&gt;();

    for(String name : names) {
      statements.add(selectStmt.bind(id,name));
    }

    Observable&lt;Row&gt; rows = execute(statements);

    //Do work with rows.
  }


  public Observable&lt;ResultSet&gt; executeAndReturnResultSet(Statement statement) {
    return Observable.from(session.executeAsync(statement));
  }


  public Observable&lt;Row&gt; execute(List&lt;Statement&gt; statements) {
    List&lt;Observable&lt;ResultSet&gt;&gt; resultSets = Lists.transform(statements, this::executeAndReturnResultSet);
    return Observable.merge(resultSets).flatMap(Observable::from);
  }
}
</code></pre>

<p><strong>Alternative</strong></p>

<p>Otherwise you could use <code>IN</code> like the following CQL:</p>

<pre><code>SELECT * FROM multi_partition_key WHERE id='1' AND name IN ('abc','xyz');
</code></pre>

<p>For binding to a prepared statement in the Java Driver.</p>

<pre><code>String id = ""1"";
List&lt;String&gt; names = ['abc','xyz'];

PreparedStatement sel = session.prepare(""SELECT id, name, value FROM table WHERE id=? AND name IN ?;"");
session.execute(sel.bind(id,name));
</code></pre>
",['table']
40251630,40256765,2016-10-26 00:02:11,"Is a UUID in Cassandra really ""necessary?""","<p>I recently started using Cassandra - I come from a traditional relational database background, so it's definitely a bit different. One thing I'm used to always doing is generating a unique ID for each row (OID, etc.). So, for my tables that I've been creating in Cassandra I've been putting a UUID column on each of them and generating a UUID. My question is...is this really ""necessary""? I'm not using the UUID as part of my partition key, so I'm not really using it for anything at the moment, but it's a tough habit to break. Some advice would be great!</p>
",<cassandra>,"<p>Exactly it's not necessary. But introducing a UUID in a table may be useful in certain cases. 
For example imagine you have a table like : </p>

<pre><code>CREATE TABLE user (
    id uuid,
    name text,
    login text,
    day_of_birth date
) PRIMARY KEY (login);
</code></pre>

<p>This table allows you to query users by login.
Now imagine you also want to query users by name. 
Of course if this kind of query will be run just a few time, you can create a <code>SECONDARY INDEX</code>. 
But if you want to have good read performance, you can denormalize your data by having a table structure like : </p>

<pre><code>CREATE TABLE user (
    id uuid,
    name text,
    login text,
    day_of_birth date
) PRIMARY KEY (id);

CREATE TABLE user_by_name (
    id uuid,
    name text
) PRIMARY KEY (name);

CREATE TABLE user_by_login (
    id uuid,
    login text
) PRIMARY KEY (login);
</code></pre>

<p>But with this structure, you have to insert and update in all 3 tables to maintain data. Instead of creating two other tables you can use <code>MATERIALIZED VIEW</code> to maintain only one table an let cassandra maintain view: </p>

<pre><code>CREATE TABLE user (
    id uuid,
    name text,
    login text,
    day_of_birth date
) PRIMARY KEY (id);

CREATE MATERIALIZED VIEW user_by_name
AS 
SELECT *
FROM user
WHERE id IS NOT NULL
    AND name IS NOT NULL
PRIMARY KEY ((name), id);

CREATE MATERIALIZED VIEW user_by_login
AS
SELECT *
FROM user
WHERE id IS NOT NULL
    AND login IS NOT NULL
PRIMARY KEY ((login), id);
</code></pre>
",['table']
40340122,40396642,2016-10-31 10:11:08,How do I group data by a field in Spark?,"<p>I want to read from a database two columns, group them by the first column and insert the result into another table using Spark. My program is written in Java. I tried the following:</p>

<pre><code>public static void aggregateSessionEvents(org.apache.spark.SparkContext sparkContext) {
    com.datastax.spark.connector.japi.rdd.CassandraJavaPairRDD&lt;String, String&gt; logs = javaFunctions(sparkContext)
            .cassandraTable(""dove"", ""event_log"", mapColumnTo(String.class), mapColumnTo(String.class))
            .select(""session_id"", ""event"");
    logs.groupByKey();
    com.datastax.spark.connector.japi.CassandraJavaUtil.javaFunctions(logs).writerBuilder(""dove"", ""event_aggregation"", null).saveToCassandra();
    sparkContext.stop();
}
</code></pre>

<p>This is giving me the error: </p>

<pre><code>The method cassandraTable(String, String, RowReaderFactory&lt;T&gt;) in the type SparkContextJavaFunctions is not applicable for the arguments (String, String, RowReaderFactory&lt;String&gt;, mapColumnTo(String.class))
</code></pre>

<p>My dependencies are:</p>

<pre><code>&lt;dependencies&gt;
    &lt;dependency&gt;
    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-core_2.10&lt;/artifactId&gt;
    &lt;version&gt;2.0.1&lt;/version&gt;
    &lt;scope&gt;provided&lt;/scope&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-streaming_2.10&lt;/artifactId&gt;
    &lt;version&gt;2.0.1&lt;/version&gt;
    &lt;scope&gt;provided&lt;/scope&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;com.datastax.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-cassandra-connector_2.10&lt;/artifactId&gt;
    &lt;version&gt;1.6.2&lt;/version&gt;
&lt;/dependency&gt;
&lt;/dependencies&gt;
</code></pre>

<p>How do I solve this?</p>
",<java><apache-spark><cassandra>,"<p>To group data by a field, do the following steps:</p>

<ol>
<li>The data must be retrieved into a JavaRDD of that table.</li>
<li>The required columns must be extracted into a pair with the key as the first and the rest of the data as the second.</li>
<li>Use reduceByKey to aggregate the values according to the requirement.</li>
</ol>

<p>After that, the data can be inserted in another table or used for some further processing.</p>

<pre><code>public static void aggregateSessionEvents(SparkContext sparkContext) {
    JavaRDD&lt;Data&gt; datas = javaFunctions(sparkContext).cassandraTable(""test"", ""data"",
            mapRowTo(Data.class));
    JavaPairRDD&lt;String, String&gt; pairDatas = datas
            .mapToPair(data -&gt; new Tuple2&lt;&gt;(data.getKey(), data.getValue()));
    pairDatas.reduceByKey((value1, value2) -&gt; value1 + "","" + value2);
    sparkContext.stop();
}
</code></pre>
",['table']
40348832,40365028,2016-10-31 19:02:48,Cassandra: How to reference a field in user defined type in a user defined function (Java),"<p>How does one reference fields within a user defined type when using Java user defined functions. I have found examples for map, set and tuple, but not for user defined types with multiple fields.</p>

<p>I have the following type defined:</p>

<pre><code>create type avg_type_1 (
  accum tuple&lt;text,int,double&gt;,   // source, count, sum
  avg_map map&lt;text,double&gt;        // source, average
);
</code></pre>

<p>The following code:</p>

<pre><code>CREATE FUNCTION average_by_source_1( state avg_type_1, source text, value double)
    CALLED ON NULL INPUT
    RETURNS avg_type_1
    LANGUAGE java
    AS $$

        // when no source yet, save the first source, set the count to 1, and set the value
        if (state.accum.getString(0) == null) {
            state.accum.setString(0, source);
            state.accum.setInt(1, 1);
            state.accum.setDouble(2, value);
        }
        ...
</code></pre>

<p>returns the error:</p>

<pre><code>InvalidRequest: Error from server: code=2200 [Invalid query] message=""Java source compilation failed:
Line 4: accum cannot be resolved or is not a field
</code></pre>
",<cassandra><user-defined-functions><cql><user-defined-types>,"<p>In Java the UDT variable is represented by the class com.datastax.driver.core.<strong>UDTValue</strong>. This class has get and set methods. There are methods using an index (0 ...) to identify the fields (in the order they are defined in the UDT), and methods that use the field name.</p>

<p>See <a href=""http://docs.datastax.com/en/drivers/java/3.1/"" rel=""nofollow noreferrer"">API Doc</a>.</p>

<p>Here are some examples, using the type defined in the question:</p>

<pre><code>TupleValue accumState = state.getTupleValue( ""accum"");
String prevSource = accumState.getString( 0);
Map&lt;String,Double&gt; avgMap = state.getMap( ""avg_map"", String.class, Double.class);
</code></pre>

<p>The first line gets the <em>accum</em> field from the function's state. Instead of the name, the index 0 (zero, it is the first field) could be used.</p>

<p>The second line gets the first element of the tuple. Only the index version can be used, as the elements of a tuple are not named.</p>

<p>The third line gets the avg_map field. </p>

<pre><code>accumState.setDouble( 2, value);
state.setTupleValue( ""accum"", accumState);
</code></pre>

<p>The above example sets the third element in the tuple, and then puts the tuple back into the function's state variable. Note that you have to put the tuple back into the state variable. The following does not work.</p>

<pre><code>// does not work
state.getTupleValue( ""accum"").setDouble( 2, value);
</code></pre>

<p>Below is the full example UDF.</p>

<pre><code>// sums up until the source changes, then adds the avg to the map
// IMPORTANT: table must be ordered by source
CREATE OR REPLACE FUNCTION average_by_source_1( state avg_type_1, source text, value double)
    CALLED ON NULL INPUT
    RETURNS avg_type_1
    LANGUAGE java
    AS $$

        TupleValue accumState = state.getTupleValue( ""accum"");
        String prevSource = accumState.getString( 0);

        // when no source yet, save the first source, set the count to 1, and set the value
        if (prevSource == null) {
            accumState.setString( 0, source);
            accumState.setInt( 1, 1);
            accumState.setDouble( 2, value);
            state.setTupleValue( ""accum"", accumState);
        }

        // when same source, increment the count and add the value
        else if (prevSource.equals( source)) {
            accumState.setInt( 1, accumState.getInt( 1) + 1);
            accumState.setDouble( 2, accumState.getDouble( 2) + value);
            state.setTupleValue( ""accum"", accumState);
        }

        // when different source, calc average and copy to map, then re-init accumulation
        else if (accumState.getInt( 1) &gt; 0) {
            double avgVal = accumState.getDouble( 2) / accumState.getInt( 1);
            Map&lt;String,Double&gt; mapState = state.getMap( ""avg_map"", String.class, Double.class);
            mapState.put( prevSource, avgVal);
            state.setMap( ""avg_map"", mapState, String.class, Double.class);
            accumState.setString( 0, source);
            accumState.setInt( 1, 1);
            accumState.setDouble( 2, value);
            state.setTupleValue( ""accum"", accumState);
        }

        // should not happen - prev case uses ""if"" to avoid division by zero
        else {
            Map&lt;String,Double&gt; mapState = state.getMap( ""avg_map"", String.class, Double.class);
            mapState.put( ""ERROR: div by zero"", null);
            accumState.setString( 0, source);
            accumState.setInt( 1, 1);
            accumState.setDouble( 2, value);
            state.setTupleValue( ""accum"", accumState);
        }

        // IMPROTANT: final function must calculate the average for the last source and
        //            add it to the map.

        return state;

    $$
;
</code></pre>
",['table']
40390226,40395071,2016-11-02 21:35:16,Delete lots of rows from a very large Cassandra Table,"<p>I have a table Foo with 4 columns A, B, C, D. The partitioning key is A. The clustering key is B, C, D.</p>

<p>I want to scan the entire table and find all rows where D is in set (X, Y, Z).</p>

<p>Then I want to delete these rows but I don't want to ""kill"" Cassandra (because of compactions), I'd like these rows deleted with minimal disruption or risk.</p>

<p>How can I do this?</p>
",<cassandra>,"<p>You have a big problem here. Indeed, you really can't find the rows without actually scanning all of your partitions. The problem real problem is that C* will allow you to restrict your queries with a partition key, and then by your cluster keys in the order in which they appear in your <code>PRIMARY KEY</code> table declaration. So if your PK is like this:</p>

<pre><code>PRIMARY KEY (A, B, C, D)
</code></pre>

<p>then you'd need to filter by A first, then by B, C, and only at the end by D. </p>

<p>That being said, for the part of <strong>finding your rows</strong>, if this is something you have to run only once, you </p>

<ol>
<li>Could scan all your table and do comparisons of D in your App logic.</li>
<li>If you know the values of A you could query every partition in parallel and then compare D in your application</li>
<li>You could attach a secondary index and try to exploit speed from there.</li>
</ol>

<p>Please note that depending on how many nodes do you have 3 is really not an option, secondary indexes don't scale)</p>

<p>If you need to perform such tasks multiple times, I'd suggest you to create another table that would satisfy this query, something like <code>PRIMARY KEY (D)</code>, you'd then just scan three partitions and that would be very fast.</p>

<p>About <strong>deleting your rows</strong>, I think there's no way to do it without triggering compactions, they are part of C* and you have to live with them. If you really can't tolerate tombstone creation and/or compactions, the only alternative is to <strong>not</strong> delete rows from a C* cluster, and that often means thinking about a new data model that won't need deletes. </p>
",['table']
40391249,40397956,2016-11-02 23:04:50,merge spark dStream with variable to saveToCassandra(),"<p>I have a <code>DStream[String, Int</code>] with pairs of word counts, e.g. <code>(""hello"" -&gt; 10)</code>. I want to write these counts to cassandra with a step index. The index is initialized as <code>var step = 1</code> and is incremented with each microbatch processed.</p>

<p>The cassandra table created as:</p>

<pre><code>CREATE TABLE wordcounts (
    step int,
    word text,
    count int,
primary key (step, word)
);
</code></pre>

<p>When trying to write the stream to the table...</p>

<pre><code>stream.saveToCassandra(""keyspace"", ""wordcounts"", SomeColumns(""word"", ""count""))
</code></pre>

<p>... I get <code>java.lang.IllegalArgumentException: Some primary key columns are missing in RDD or have not been selected: step</code>.</p>

<p>How can I prepend the <code>step</code> index to the stream in order to write the three columns together?</p>

<p>I'm using spark 2.0.0, scala 2.11.8, cassandra 3.4.0 and spark-cassandra-connector 2.0.0-M3.</p>
",<apache-spark><cassandra><spark-streaming><spark-cassandra-connector><dstream>,"<p>As noted, while the Cassandra table expects something of the form <code>(Int, String, Int)</code>, the wordCount DStream is of type <code>DStream[(String, Int)]</code>, so for the call to <code>saveToCassandra(...)</code> to work, we need a <code>DStream</code> of type <code>DStream[(Int, String, Int)]</code>.</p>

<p>The tricky part in this question is how to bring a local counter, that is by definition only known in the driver, up to the level of the DStream.</p>

<p>To do that, we need to do two things: ""lift"" the counter to a distributed level (in Spark, we mean ""RDD"" or ""DataFrame"") and join that value with the existing <code>DStream</code> data.</p>

<p>Departing from the classic Streaming word count example:</p>

<pre><code>// Split each line into words
val words = lines.flatMap(_.split("" ""))

// Count each word in each batch
val pairs = words.map(word =&gt; (word, 1))
val wordCounts = pairs.reduceByKey(_ + _)
</code></pre>

<p>We add a local var to hold the count of the microbatches:</p>

<pre><code>@transient var batchCount = 0
</code></pre>

<p>It's declared transient, so that Spark doesn't try to close over its value when we declare transformations that use it.</p>

<p>Now the tricky bit: Within the context of a DStream <code>transform</code>ation, we make an RDD out of that single <code>var</code>iable and join it with underlying RDD of the DStream using cartesian product:</p>

<pre><code>val batchWordCounts = wordCounts.transform{ rdd =&gt; 
  batchCount = batchCount + 1

  val localCount = sparkContext.parallelize(Seq(batchCount))
  rdd.cartesian(localCount).map{case ((word, count), batch) =&gt; (batch, word, count)}
}
</code></pre>

<p>(Note that a simple <code>map</code> function would not work, as only the initial value of the <code>var</code>iable would be captured and serialized. Therefore, it would look like the counter never increased when looking at the DStream data.</p>

<p>Finally, now that the data is in the right shape, save it to Cassandra:</p>

<pre><code>batchWordCounts.saveToCassandra(""keyspace"", ""wordcounts"")
</code></pre>
",['table']
40438429,40446037,2016-11-05 12:48:23,What are the correct Cassandra collection limits?,"<p>I'm testing an application where the size of the collections is bound to grow in the future and 64k is a limit that may be reached in some cases. </p>

<p>This question is about the collection size limit as there seems to be a contradiction in the official documentation.</p>

<p>As per <a href=""https://docs.datastax.com/en/cql/3.1/cql/cql_using/use_collections_c.html"" rel=""nofollow noreferrer"">this</a> document: </p>

<blockquote>
  <p>If you insert more than 64K items into a collection, only 64K of them
  will be queryable, resulting in data loss.</p>
</blockquote>

<p>But if you click through to the <a href=""https://docs.datastax.com/en/cql/3.1/cql/cql_reference/refLimits.html"" rel=""nofollow noreferrer"">CQL Limits link</a> on that very page you see this:</p>

<blockquote>
  <ul>
  <li><p>Collection (List): <strong>collection size: 2B</strong> (2^31); values size: 65535 (2^16-1) (Cassandra 2.1 and later, using native protocol v3)</p></li>
  <li><p>Collection (Set): <strong>collection size: 2B</strong> (2^31); values size: 65535 (2^16-1) (Cassandra 2.1 and later, using native protocol v3)</p></li>
  <li><p>Collection (Map): <strong>collection size: 2B</strong> (2^31); number of keys: 65535 (2^16-1); values size: 65535 (2^16-1) (Cassandra 2.1 and later, using
  native protocol v3)</p></li>
  </ul>
</blockquote>

<p>So which one is it? 64k items per collection, or 2 billion items per collection? Or are 2 billion <em>writeable</em> but not readable beyond 64k? </p>

<p>Thanks in advance.</p>
",<cassandra>,"<p>which version of cassandra you are using ?</p>

<p>that documentaion is for 2.0 and 2.1 . and in that case there is a limitation of how many elements you can put in a collection. which is 64k. however each element can have a size of 2b if you are using native protocol v3.
check this <a href=""https://issues.apache.org/jira/browse/CASSANDRA-5428"" rel=""nofollow noreferrer"">https://issues.apache.org/jira/browse/CASSANDRA-5428</a></p>

<p>but if you are using cassandra 2.2 and later you can insert 2billion items into collection.
here is the link. <a href=""http://docs.datastax.com/en/cql/3.3/cql/cql_using/useCollections.html"" rel=""nofollow noreferrer"">http://docs.datastax.com/en/cql/3.3/cql/cql_using/useCollections.html</a></p>

<p>having said that <strong>you should not insert that many items into the collection. you will hit performance issues way before you hit the max elements insertion limit</strong>.</p>

<p><code>Collections cannot be ""sliced""; Cassandra reads a collection in its entirety, impacting performance. Thus, collections should be much smaller than the maximum limits listed. The collection is not paged internally.</code></p>

<p>If you have to have that much of item then in that case collections are not appropriate anymore and a specific table (with clustering columns) should be used. </p>

<p>I hope this helps.</p>
",['table']
40475154,40753894,2016-11-07 21:40:38,How to does phantom handle errors,"<p>When inserting/updating into Cassandra, or when say the table defintion in the application does not match the table definition in the C* server, how does phantom handle these types of errors?</p>

<p>I see a result set that has wasApplied etc. but nothing about exceptions or errors.</p>
",<cassandra><phantom-dsl>,"<p>If the error is catastrophic and the operation couldn't execute you will get back a failed future. One such example is a <code>scala.concurrent.Future</code> wrapping an <code>InvalidQueryException</code> for instance.</p>

<p>Now there is a great degree of variety to what failure could mean. If you insert partial data that's valid in Cassandra, columns automatically set to <code>null</code> even if business wise this doesn't make sense in your application.</p>

<p>The table definition should always match the table definition in the database if you use schema auto-generation via <code>Database.create</code> method, so phantom already provides a mechanism for you to do that automatically.</p>
",['table']
40494304,40496181,2016-11-08 18:47:24,Cassandra: how to move whole tables to another keyspace,"<p>Version info of my Cassandra:</p>

<blockquote>
  <p>[cqlsh 5.0.1 | Cassandra 2.2.5 | CQL spec 3.3.1 | Native protocol v4]</p>
</blockquote>

<p>I am trying to move some huge tables (several million rows) to another keyspace. Besides ""COPY to csv, and COPY from csv"", is there any better solution?</p>
",<cassandra>,"<p>Ok, I managed to get this to work on a single-node cluster running 2.2.8.  </p>

<p>I experimented by moving my <code>holidays</code> table from my <code>presentation</code> keyspace over to my <code>stackoverflow</code> keyspace.</p>

<p>Here are the steps I took:</p>

<p><strong>Create the table inside the new keyspace.</strong></p>

<p>This step is important, because each table has a UUID for a unique identifier, stored in the <code>system.schema_columnfamilies</code> table in the <code>cf_id</code> column.  This id is attached to the directory names that hold the data.  By copy/pasting the schema from one keyspace to another, you'll ensure that the same column names are used, but that a new unique identifier is generated.</p>

<p>Note: In 3.x, the identifier is stored in the <code>system_schema.tables</code> table.</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; CREATE TABLE holidays (
 type text,
 eventtime timestamp,
 beginend text,
 name text,
 PRIMARY KEY (type, eventtime, beginend)
) WITH CLUSTERING ORDER BY (eventtime DESC, beginend DESC);

aploetz@cqlsh:stackoverflow&gt; SELECT * FROM stackoverflow.holidays ;

 type | eventtime | beginend | name
------+-----------+----------+------

(0 rows)
</code></pre>

<p><strong>Stop your node(s) properly (DISABLEGOSSIP, DRAIN, kill/stop, etc...).</strong></p>

<p><strong>Now, find the location of the old and new table on disk, and copy/move the files to the new location</strong> (from the old location):</p>

<pre><code>$ ls -al /var/lib/cassandra/data22/stackoverflow/holidays-77a767e0a5f111e6a2bebd9d201c4c8f/
total 12
drwxrwxr-x  3 aploetz aploetz 4096 Nov  8 14:25 .
drwxrwxr-x 17 aploetz aploetz 4096 Nov  8 14:25 ..
drwxrwxr-x  2 aploetz aploetz 4096 Nov  8 14:25 backups
$ cp /var/lib/cassandra/data22/presentation/holidays-74bcfde0139011e6a67c2575e6398503/la* /var/lib/cassandra/data22/stackoverflow/holidays-77a767e0a5f111e6a2bebd9d201c4c8f/

$ ls -al /var/lib/cassandra/data22/stackoverflow/holidays-77a767e0a5f111e6a2bebd9d201c4c8f/
drwxrwxr-x  3 aploetz aploetz 4096 Nov  8 14:26 .
drwxrwxr-x 17 aploetz aploetz 4096 Nov  8 14:25 ..
drwxrwxr-x  2 aploetz aploetz 4096 Nov  8 14:25 backups
-rw-rw-r--  1 aploetz aploetz   43 Nov  8 14:26 la-1-big-CompressionInfo.db
-rw-rw-r--  1 aploetz aploetz  628 Nov  8 14:26 la-1-big-Data.db
-rw-rw-r--  1 aploetz aploetz    9 Nov  8 14:26 la-1-big-Digest.adler32
-rw-rw-r--  1 aploetz aploetz   16 Nov  8 14:26 la-1-big-Filter.db
-rw-rw-r--  1 aploetz aploetz   57 Nov  8 14:26 la-1-big-Index.db
-rw-rw-r--  1 aploetz aploetz 4468 Nov  8 14:26 la-1-big-Statistics.db
-rw-rw-r--  1 aploetz aploetz   94 Nov  8 14:26 la-1-big-Summary.db
-rw-rw-r--  1 aploetz aploetz   94 Nov  8 14:26 la-1-big-TOC.txt
-rw-rw-r--  1 aploetz aploetz   43 Nov  8 14:26 la-2-big-CompressionInfo.db
-rw-rw-r--  1 aploetz aploetz  164 Nov  8 14:26 la-2-big-Data.db
-rw-rw-r--  1 aploetz aploetz   10 Nov  8 14:26 la-2-big-Digest.adler32
-rw-rw-r--  1 aploetz aploetz   16 Nov  8 14:26 la-2-big-Filter.db
-rw-rw-r--  1 aploetz aploetz   26 Nov  8 14:26 la-2-big-Index.db
-rw-rw-r--  1 aploetz aploetz 4460 Nov  8 14:26 la-2-big-Statistics.db
-rw-rw-r--  1 aploetz aploetz  108 Nov  8 14:26 la-2-big-Summary.db
-rw-rw-r--  1 aploetz aploetz   94 Nov  8 14:26 la-2-big-TOC.txt
</code></pre>

<p><strong>Restart your node(s).</strong></p>

<p><strong>Query via cqlsh:</strong></p>

<pre><code>Connected to SnakesAndArrows at 127.0.0.1:9042.
[cqlsh 5.0.1 | Cassandra 2.2.8 | CQL spec 3.3.1 | Native protocol v4]
Use HELP for help.
aploetz@cqlsh&gt; SELECT * FROM stackoverflow.holidays ;

 type         | eventtime                | beginend | name
--------------+--------------------------+----------+------------------------
    Religious | 2016-12-26 05:59:59+0000 |        E |              Christmas
    Religious | 2016-12-25 06:00:00+0000 |        B |              Christmas
    Religious | 2016-03-28 04:59:59+0000 |        E |                 Easter
    Religious | 2016-03-27 05:00:00+0000 |        B |                 Easter
 presentation | 2016-05-06 20:40:08+0000 |        B |        my presentation
 presentation | 2016-05-06 20:40:03+0000 |        B |        my presentation
 presentation | 2016-05-06 20:39:15+0000 |        B |        my presentation
 presentation | 2016-05-06 20:38:10+0000 |        B |        my presentation
           US | 2016-07-05 04:59:59+0000 |        E |            4th of July
           US | 2016-07-04 05:00:00+0000 |        B |            4th of July
           US | 2016-05-09 04:59:59+0000 |        E |            Mothers Day
           US | 2016-05-08 05:00:00+0000 |        B |            Mothers Day
         Nerd | 2016-12-22 05:59:59+0000 |        E |               2112 Day
         Nerd | 2016-12-21 06:00:00+0000 |        B |               2112 Day
         Nerd | 2016-09-26 04:59:59+0000 |        E |             Hobbit Day
         Nerd | 2016-09-25 05:00:00+0000 |        B |             Hobbit Day
         Nerd | 2016-09-20 04:59:59+0000 |        E | Talk Like a Pirate Day
         Nerd | 2016-09-19 05:00:00+0000 |        B | Talk Like a Pirate Day
         Nerd | 2016-05-07 04:59:59+0000 |        E |         Star Wars Week
         Nerd | 2016-05-04 05:00:00+0000 |        B |         Star Wars Week
         Nerd | 2016-03-14 05:00:00+0000 |        E |                 Pi Day
         Nerd | 2016-03-14 05:00:00+0000 |        B |                 Pi Day

(22 rows)
</code></pre>

<p>The problem with this approach, is that you will need to stop the cluster, and move files around on each node.  Whereas cqlsh <code>COPY</code> would allow you to import and export on a single node, with the cluster still running.</p>

<p>And I know that <code>COPY</code> has this reputation that limits it to smaller datasets.  But 2.2.x has options that help throttle COPY to keep it from timing out on large datasets.  I recently got it export/import 370 million rows without a timeout.</p>
",['table']
40522347,40535809,2016-11-10 07:46:19,Can a Select Count(*) Affect Writes in Cassandra,"<p>I experienced a scenario where a select count(*) on a table every minute (yes, this should definitely be avoided) caused a huge increase in Cassandra writes to around 150K writes per second. </p>

<p>Can anyone explain this weird behavior? Why would a Select query significantly increase write count in Cassandra?</p>

<p>Thanks!</p>
",<count><cassandra><cql>,"<p>If you check </p>

<p><code>org.apache.cassandra.metrics:type=ReadRepair,name=RepairedBackground</code></p>

<p>and</p>

<p><code>org.apache.cassandra.metrics:type=ReadRepair,name=RepairedBlocking</code></p>

<p>metrics you can see if its read repairs sending mutations. Perhaps reading all the data to service the count(*) is causing a lot of read repairs if your data is inconsistent. If thats the case lowering the <code>read_repair_chance</code> and <code>dclocal_read_repair_chance</code> on the table (<code>ALTER TABLE</code>) could reduce load.</p>

<p>Other likely possibilities are:</p>

<ul>
<li>You have tracing enabled (either globally or on the table) as some %.</li>
<li>Or if you use DSE and you have slow query's enabled.</li>
</ul>
",['table']
40635602,40663530,2016-11-16 15:11:25,Cassandra dropped keyspaces still on HDD,"<p>I noticed an increase in the number of open files on my cassandra cluster and went to check the health of it.  Nodetool status reported only 300gb in use per node of the 3TB each has allocated.  </p>

<p>Shortly there after i began to see HEAP OOM errors showing up in the cassandra logs.</p>

<p>These nodes had been running for 3-4 months no issue, but had a series of test data populate and then dropped from them.</p>

<p>After checking the harddrives via the <code>df</code> command i was able to determine they were all between 90-100% filled in a jboded scenario.  </p>

<p>edit: further investigation shows that the remaining files are in the 'snapshot' subfolder and the data subfolder itself has no db tables.  </p>

<p><strong>My question</strong> is, has anyone seen this?  Why did compaction not free these tombstones?  Is this a bug?</p>
",<cassandra><cassandra-3.0>,"<p>Snapshots occur over the lifetime of the cassandra cluster.  These snapshots are not captured in a <code>nodetool status</code> but still occupy space.  In this case the snapshots consuming all the space were created when a table was dropped.   </p>

<p>To retrieve a list of current snapshots use the command <code>nodetool listsnapshots</code></p>

<p>This feature can be disabled through editing <code>/etc/cassandra/cassandra-env.sh</code> and setting <code>auto_snapshot</code> to false.  Alternatively these snapshots can be purged via the command <code>nodetool clearsnapshot &lt;name&gt;</code>.  </p>
",['table']
40645375,40650822,2016-11-17 01:42:47,"Group By using date, name and amount in Cassandra","<p>I'm new in using Cassandra and I can't use the Group By, is there a way that I can use the GROUP BY in Cassandra like in SQL? I want to group my data by date and also by the name of the user, and I want to sum all the amount in a specific date. I still don't have a code for this because I don't know how to start and I also aware that the group by is not supported by cassandra</p>
",<cassandra><group-by><nosql>,"<p>You can't use group by without materialized view<br>
But if you want to find the sum of amount for a specific date and name you can get easily. </p>

<p><strong>Using Apache Cassandra 3.x</strong>   </p>

<p>1.Create a table   </p>

<pre><code>CREATE TABLE data (
    date bigint,
    name text,
    amount double,
    PRIMARY KEY (date, name, amount)
);
</code></pre>

<p>2.Insert dummy Some data</p>

<pre><code>INSERT INTO data (date , name , amount) VALUES ( 1, 'a1', 10);
INSERT INTO data (date , name , amount) VALUES ( 1, 'a1', 20);
INSERT INTO data (date , name , amount) VALUES ( 1, 'a1', 30);
INSERT INTO data (date , name , amount) VALUES ( 1, 'a1', 40);
INSERT INTO data (date , name , amount) VALUES ( 1, 'a2', 50);
INSERT INTO data (date , name , amount) VALUES ( 1, 'a2', 60);
</code></pre>

<p>3.Now you can find the sum of amount in a specific date and name </p>

<pre><code>SELECT sum(amount) FROM data WHERE date = 1 AND name = 'a1' ;
</code></pre>
",['table']
40659688,40660391,2016-11-17 16:08:16,how to handle search by unique id in Cassandra,"<p>I have a table with a composite primary key. <strong>name,description, ID</strong></p>

<pre><code>PRIMARY KEY (id, name, description)
</code></pre>

<p>whenever searching Cassandra I need to provide the three keys, but now I have a use case where I want to delete, update, and get just based on ID. </p>

<p>So I created a materialized view against this table, and reordered the keys to have ID first so I can search just based on ID. </p>

<p>But how do I delete or update record with just an ID ?</p>
",<cassandra><data-modeling>,"<p>It's not clear if you are using a <strong>partition key</strong> with 3 columns, or if you are using a <strong>composite primary key</strong>.</p>

<p>If you are using a <strong>partition key</strong> with 3 columns:</p>

<pre><code>CREATE TABLE tbl (
    id uuid,
    name text,
    description text,
    ...
    PRIMARY KEY ((id, name, description))
);
</code></pre>

<p>notice the <strong>double</strong> parenthesis you need all 3 components to identify your data. So when you query your data by ID from the materialized view you need to retrieve also both <code>name</code> and <code>description</code> fields, and then issue one delete per tuple <code>&lt;id, name, description&gt;</code>.</p>

<p>Instead, if you use a <strong>composite</strong> primary key with <code>ID</code> being the only <strong>PARTITION KEY</strong>:</p>

<pre><code>CREATE TABLE tbl (
    id uuid,
    name text,
    description text,
    ...
    PRIMARY KEY (id, name, description)
);
</code></pre>

<p>notice the <strong>single</strong> parenthesis, then you can simply issue one delete because you already know the partition and don't need anything else.</p>

<p>Check <a href=""https://stackoverflow.com/a/24953331/6570821"">this</a> SO post for a clear explanation on <em>primary key</em> types.</p>

<p>Another thing you should be aware of is that the materialized view will populate a table under the hood for you, and the same rules/ideas about data modeling should also apply for materialized views. </p>
",['table']
40676589,40677980,2016-11-18 11:57:42,Cassandra received an invalid gossip generation for peer,"<p>We have a basic 2 node Cassandra cluster. Both nodes run version 3.9 with minimal configurations to enable clustering. One of the nodes is sending wrong generation which causes the other node to show the warning </p>

<pre><code>WARN  [GossipStage:1] Gossiper.java:1146 - received an invalid gossip generation for peer /10.3.185.234; local time = 1479469393, received generation = 1872927836
</code></pre>

<p>Node-1 which is causing the issue has this output from </p>

<pre><code>nodetool gossipinfo

/10.3.185.234

generation: 1872927836
</code></pre>

<p>1872927836 epoch is a far away date (Tue, 08 May 2029 09:43:56 GMT). Node 2 is legitimately discarding the packet. Can I somehow fix node-1 to send the correct generation?</p>
",<cassandra><cassandra-3.0><database-cluster>,"<p>Fixed the issue by changing the gossip_generation value in system.local table by using cqlsh</p>

<pre><code>update system.local  set gossip_generation = 1479472176 where key='local';
</code></pre>

<p>Restart the service after this change</p>
",['table']
40682991,40683263,2016-11-18 17:39:26,Write and Update record on Cassandra along with clustering columns,"<p>I have a notification table and additional index</p>

<pre><code>CREATE TABLE notification (
    postid double,
    userid double,
    type text,
    message text,
    hasread boolean,
    postdate timestamp,
    PRIMARY KEY (userid, postdate)
)  WITH  CLUSTERING ORDER BY (postdate DESC);

CREATE INDEX postid ON notification(postid);
</code></pre>

<p>Let's say I inserted several rows</p>

<pre><code>select * from notification;

 userid | postdate                        | hasread | message                | postid | type
--------+---------------------------------+---------+------------------------+--------+---------
    104 | 2016-11-18 17:21:32.692000+0000 |   False | Let\'s do it together! |  70521 | newpost
    104 | 2016-11-18 17:21:26.511000+0000 |   False | Let\'s do it together! |  90521 | newpost
    103 | 2016-11-18 17:20:17.284000+0000 |   False | Let\'s do it together! |  40521 | newpost
    103 | 2016-11-18 17:20:02.925000+0000 |   False | Let\'s do it together! |  40521 | newpost
    103 | 2016-11-18 17:19:55.643000+0000 |   False | Let\'s do it together! |  30521 | newpost
    103 | 2016-11-18 17:19:49.029000+0000 |   False | Let\'s do it together! |  60521 | newpost
</code></pre>

<p>If I do simple queries i.e. </p>

<pre><code>select * from notification where postid=40521;
</code></pre>

<p>then the result seems fine</p>

<pre><code>userid | postdate                        | hasread | message                | postid | type
--------+---------------------------------+---------+------------------------+--------+---------
    103 | 2016-11-18 17:20:17.284000+0000 |   False | Let\'s do it together! |  40521 | newpost
    103 | 2016-11-18 17:20:02.925000+0000 |   False | Let\'s do it together! |  40521 | newpost
</code></pre>

<p>Or let's get a single row like this</p>

<pre><code>select * from notification where postid=60521;
</code></pre>

<p>again the one single row seems fine</p>

<pre><code>userid | postdate                        | hasread | message                | postid | type
--------+---------------------------------+---------+------------------------+--------+---------
    103 | 2016-11-18 17:19:49.029000+0000 |   False | Let\'s do it together! |  60521 | newpost
</code></pre>

<p>However, when I am updating the hasread row in one row, I am getting missing postdate error which is the clustering column</p>

<pre><code>update notification set hasread=true where postid=60521 and userid=103;
InvalidRequest: Error from server: code=2200 [Invalid query] message=""Some clustering keys are missing: postdate""
</code></pre>

<p>I need to get the list in sorted order, that's the reason I have to use postdate for clustering column. 
However at the same time I need to update a specific row. I guess it's about the design but still couldn't figure it out. Any suggestions would be appreciated. </p>
",<cassandra><cqlsh><nosql>,"<p>When updating a row in Cassandra, you must provide your entire PRIMARY KEY.  That's essentially what that error message is telling you.  Remember, Cassandra is <em>NOT</em> a relational database, so you will <em>NOT</em> be able to update by <code>postid</code>.</p>

<p>Additionally, it is important to remember that Cassandra does not differentiate between an INSERT and an UPDATE.  As a specific row's uniqueness is determined by its complete PRIMARY KEY, you must have all PRIMARY KEY components present for all upserts.  Essentially, this is what you need to do:</p>

<pre><code>UPDATE notification SET hasread=true 
  WHERE userid=103 AND postdate='2016-11-18 17:19:49.029+0000';
</code></pre>

<p>Also, it looks like <code>postid</code> is a high-cardinality column.  Therefore, calls relying on that secondary index are not going to perform well.  Like at all.  If you really need to query by <code>postid</code>, you should consider building an additional query table for that pattern.</p>
",['table']
40699125,40742908,2016-11-19 23:42:06,Consistency and timeout issues with Cassandra 2.2,"<p>I'm using Cassandra 2.2 and I've an application that requires a high level of consistency.</p>

<p>I've configured one datacenter cluster with 3 nodes.
My keyspace is created with <code>replication_factor</code> of 2.
In each configuration.yaml files I've set 2 seed_providers (for example NODE_1 and NODE_3).</p>

<p><strong>The important thing is that my app should be full-functional even if one node is down.</strong></p>

<p>Currently I've some issues with the consistency and timeout when my app contacts the cluster.</p>

<p>I've read the whole Cassandra 2.2 documentation and I concluded that the best <code>CONSISTENCY LEVEL</code> for my write operations should be <code>QUORUM</code> and for my read operations <code>ONE</code>, but I still have some consistency issues.</p>

<p>First of all, is it the right choice to have a strong level of consistency?
And also, are <code>UPDATE</code> and <code>DELETE</code> operations considered as write or read operations, since for example an update operation with a <code>WHERE</code> clause still has to 'read' data? I'm not sure, spacially in the context of the cassandra' write workflow.</p>

<p>My second issue is the timeout during the write operations. A simple and lightweight <code>INSERT</code> sometimes get ""<code>Cassandra timeout during write query at consistency QUORUM (2 replicas were required but only 1 acknowledged the write)</code>""
or sometines even ""... 0 acknoledged"" even though all of my 3 nodes are UP.</p>

<p>Are there some other parameters that I should check, like for example write_request_timeout_in_ms, with default value of 2000 ms (which is already a high value)?</p>
",<cassandra><timeout><consistency><cassandra-2.2>,"<p>You will have strong consistency with <code>Replication Factor = 2</code> and <code>Consistency Level = QUORUM</code> for write operations and <code>ONE</code> for read operations. But write operations will fail if one node is down. <code>Consistency Level = QUORUM</code> is the same as <code>ALL</code> in case <code>Replication Factor = 2</code>.</p>

<p>You should use <code>Replication Factor = 3</code> and <code>Consistency Level = QUORUM</code> for both write and read operations, to have strong consistency and full functional app even if one node is down.</p>

<p><code>DELETE</code> and <code>UPDATE</code> operations are write operations.</p>

<p>For the timeout issue please provide table model and queries that fails.</p>

<p><strong>Updated</strong></p>

<p>Consistency level applies to replicas, not nodes.</p>

<p>Replication factor = 2 means that 2 of 3 nodes  will contain data. These nodes will be <strong>replicas</strong>.</p>

<p>QUORUM means that a write operation must be acknowledged by 2 <strong>replicas</strong> (when replication factor=2), not nodes.</p>

<p>Cassandra places the data on each node according to the partition key. Each node is responsible for a <strong>range</strong> of partition keys. Not any node can store any data, so you need have alive replicas (not nodes) to perform operations. Here article <a href=""https://docs.datastax.com/en/cassandra/2.1/cassandra/architecture/architectureDataDistributeAbout_c.html"" rel=""nofollow noreferrer"">about data replication and distribution</a>.</p>

<p>When you perform QUORUM write request to cluster with 2 of 3 alive nodes, there is a chance that the cluster has only 1 alive replica for the partition key, in this case the write request will fail.</p>

<p>In additional: here is a <a href=""http://www.ecyrd.com/cassandracalculator/"" rel=""nofollow noreferrer"">simple calculator for Cassandra parameters</a></p>
",['table']
40715529,40715638,2016-11-21 08:16:35,Cassandra timeuuid comparison,"<p>I have a table, </p>

<pre><code>CREATE TABLE log (
    uuid uuid,
    time timeuuid,
    user text,
    ....
    PRIMARY KEY (uuid, time, user)
)  
</code></pre>

<p>and</p>

<blockquote>
  <p>CREATE CUSTOM INDEX time_idx on Log(time) USING 'org.apache.cassandra.index.sasi.SASIIndex';</p>
</blockquote>

<p>then I want to select base on time</p>

<blockquote>
  <p>select * from Log where time >  84bfd880-b001-11e6-918c-24eda6ab1677;</p>
</blockquote>

<p>and nothing return, it will return if I use equal(=). Which step did I go wrong ?</p>
",<cassandra><cql><timeuuid>,"<p>You need to make time_idx index as SPARSE index.</p>

<blockquote>
  <p>The SPARSE index is meant to improve performance of querying large, dense number ranges like timestamps for data inserted every millisecond. If the data is numeric, millions of columns values with a small number of partition keys characterize the data, and range queries will be performed against the index, then SPARSE is the best choice. For numeric data that does not meet this criteria, PREFIX is the best choice.</p>
</blockquote>

<p>drop the time_idx and create with the below query</p>

<pre><code>CREATE CUSTOM INDEX time_idx on Log(time) USING 'org.apache.cassandra.index.sasi.SASIIndex' WITH OPTIONS = { 'mode': 'SPARSE' };
</code></pre>

<p>Now you can query with The inequalities >=, > and &lt;= .</p>

<p>Limitation : SPARSE indexing is used only for numeric data, so LIKE queries do not apply.</p>

<p>and Another thing your table creation is not right. It should be</p>

<pre><code>CREATE TABLE log (
    uuid uuid,
    time timeuuid,
    user text,
    PRIMARY KEY (uuid, time, user)
) 
</code></pre>
",['table']
40758846,40758978,2016-11-23 07:57:18,How to solve 'Secondary indexes cardinality' for cfs.inode?,"<p>In OpsCenter 6.0.3, I got the following problem</p>

<p><a href=""https://i.stack.imgur.com/bzDHz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bzDHz.png"" alt=""enter image description here""></a></p>

<p>The above figure appeared after clicking <code>'Services' -&gt; 'Best Practice Service' -&gt; 'Performance Service - Table Metrics Advisor' -&gt; 'Secondary indexes cardinality'</code> in turn.</p>

<p>The <code>inode</code> table viewed in DevCenter looks as follows:</p>

<p><a href=""https://i.stack.imgur.com/PQbFT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PQbFT.png"" alt=""enter image description here""></a> </p>

<p>As far as I know, <code>[inode]</code><a href=""http://www.datastax.com/dev/blog/cassandra-file-system-design"" rel=""nofollow noreferrer"">link</a> tracks each files metadata and block locations.  But, what can I do to fix this problem ?</p>

<p>OpsCenter Version: 6.0.3 Cassandra Version: 2.1.15.1423 DataStax Enterprise Version: 4.8.10</p>
",<cassandra><cfs>,"<p><strong>Don't use Secondary index for high cardinality column</strong>.</p>

<blockquote>
  <p>High-cardinality refers to columns with values that are very uncommon or unique. High-cardinality column values are typically identification numbers, email addresses, or user names. An example of a data table column with high-cardinality would be a USERS table with a column named USER_ID.</p>
</blockquote>

<p><strong>Problems using a high-cardinality column index datastax doc :</strong> </p>

<blockquote>
  <p>If you create an index on a high-cardinality column, which has many distinct values, a query between the fields will incur many seeks for very few results. In the table with a billion songs, looking up songs by writer (a value that is typically unique for each song) instead of by their artist, is likely to be very inefficient. It would probably be more efficient to manually maintain the table as a form of an index instead of using the Cassandra built-in index. For columns containing unique data, it is sometimes fine performance-wise to use an index for convenience, as long as the query volume to the table having an indexed column is moderate and not under constant load.</p>
</blockquote>

<p><strong>Solution :</strong> </p>

<p><strong>Create another table with that column in the partition key</strong> </p>
",['table']
40765237,40765412,2016-11-23 12:59:42,Create Cassandra Materialized view on different keyspace fail - unconfigured table,"<p>I created 2 keyspaces :</p>

<pre><code>CREATE KEYSPACE test1 WITH replication = {'class': 'SimpleStrategy', 'replicatio                                                                                                             n_factor': '3'} 

CREATE KEYSPACE test2 WITH replication = {'class': 'SimpleStrategy', 'replicatio                                                                                                             n_factor': '3'} 
</code></pre>

<h2>Created table users on test1</h2>

<pre><code>CREATE TABLE test1.users (
  user_name varchar ,
  password varchar,
  id varchar,
  primary key(user_name,id)
);
</code></pre>

<h2>create Materialized view on test2</h2>

<pre><code>CREATE MATERIALIZED VIEW test2.user_by_id AS
SELECT * FROM test1.users where id is not null and user_name is not null
PRIMARY KEY (id,user_name);
</code></pre>

<p>Get error :</p>

<blockquote>
  <p>InvalidRequest: Error from server: code=2200 [Invalid query]
  message=""unconfigured table users""</p>
</blockquote>

<hr>

<pre><code>CREATE MATERIALIZED VIEW test1.user_by_id AS
       SELECT * FROM test1.users where id is not null and user_name is not null
       PRIMARY KEY (id,user_name);
</code></pre>

<p>Works Good .</p>

<p>According to Cassandra documenation this should work .</p>

<p><a href=""https://docs.datastax.com/en/cql/3.3/cql/cql_reference/refCreateMV.html"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/cql/3.3/cql/cql_reference/refCreateMV.html</a></p>

<p>""To create a materialized view in a keyspace other than the current keyspace, put the keyspace name in front of the materialized view name, followed by a period""</p>

<p>Checked it On Cassandra 3.0 and 3.9 - same issue .</p>

<p>I connect from cassandra node using cqlsh.</p>

<p>test1.users is accessible from test2:</p>

<pre><code>cqlsh:test2&gt; select * from test1.users;

 user_name | id | password
-----------+----+----------
</code></pre>

<p>(0 rows)</p>

<p>Do I miss  something .</p>

<p>Thanks</p>

<p>Alon</p>
",<cassandra>,"<p><strong>The table and the Materialized view must be in a same keyspace.</strong><br>
You misunderstood the documentation.</p>

<blockquote>
  <p>To create a materialized view in a keyspace other than the current keyspace, put the keyspace name in front of the materialized view name, followed by a period.</p>
</blockquote>

<p>If your current keyspace is not same as table's keyspace then you have to put the table's keyspace name in front of the materialized view name, followed by a period.
Materialized view will be created in the table's keyspace</p>
",['table']
40790427,40791132,2016-11-24 15:50:33,How to batch select data from Cassandra effectively?,"<p>I know Cassandra doesn't support batch query, and it also doesn't recommend to use <code>IN</code>, because it can degrade performance. But I have to get the data by id, for example:</p>

<pre><code>select * from visit where id in ([visit_id array])
</code></pre>

<p>desc table:</p>

<pre><code>CREATE TABLE visit (
    enterprise_id int,
    id text,
    ........
    PRIMARY KEY (enterprise_id, id)
</code></pre>

<p>The array maybe has thousands of elements. Is there any way can make it effectively? </p>
",<cassandra><cql>,"<p>Large In query create GC pauses and heap pressure that leads to overall slower performance. When you execute large in query this means you’re waiting on this single coordinator node to give you a response, it’s keeping all those queries and their responses in the heap, and if one of those queries fails, or the coordinator fails, you have to retry the whole thing.</p>

<p>Approach 1 :  </p>

<p>Try to convert your in query into range query (>=, &lt;=)</p>

<pre><code>SELECT * visit WHERE enterprise_id = ? and id &gt;= ? and id &lt;= ?
</code></pre>

<p>Approach 2 :</p>

<p>Use executeAsync, Java Example</p>

<pre><code>PreparedStatement statement = session.prepare(""SELECT * FROM visit where enterprise_id = ? and id = ?"");

List&lt;ResultSetFuture&gt; futures = new ArrayList&lt;&gt;();
for (int i = 1; i &lt; 4; i++) {
    ResultSetFuture resultSetFuture = session.executeAsync(statement.bind(i, i));
    futures.add(resultSetFuture);
}

List&lt;String&gt; results = new ArrayList&lt;&gt;();
for (ResultSetFuture future : futures){
     ResultSet rows = future.getUninterruptibly();
     Row row = rows.one();
     results.add(row.getString(""name""));
}
return results; 
</code></pre>

<p>Approach 3 : </p>

<p>If possible then instead of in query, create another table and when a data that you will perform in query are about to insert or update also insert the data to new table, then you can just query from the new table without in query </p>

<p>Source :<br>
<a href=""http://www.datastax.com/dev/blog/a-deep-look-to-the-cql-where-clause"" rel=""nofollow noreferrer"">http://www.datastax.com/dev/blog/a-deep-look-to-the-cql-where-clause</a>
<a href=""https://lostechies.com/ryansvihla/2014/09/22/cassandra-query-patterns-not-using-the-in-query-for-multiple-partitions/"" rel=""nofollow noreferrer"">https://lostechies.com/ryansvihla/2014/09/22/cassandra-query-patterns-not-using-the-in-query-for-multiple-partitions/</a></p>
",['table']
40838383,40838502,2016-11-28 06:35:01,Handling huge number of fields in scalatra model,"<p>I'm building a rest api using scalatra and cassandra. My cassandra data model is having 1000+ fields. I need to read these fields into scalatra middleware and do a lot of json manipulation as per business logic. What are the ways in which I can automatically/easily map the cassandra fields -> scalatra object -> JSON response?</p>

<p>Thanks in advance.</p>
",<scala><cassandra><scalatra>,"<p><strong>In Cassandra 2.2 added JSON support</strong><br>
You can use <code>SELECT JSON</code></p>

<blockquote>
  <p>The SELECT statement has also be extended to support retrieval of rows in a JSON-encoded map format. The results for SELECT JSON will only include a single column named [json]. This column will contain the same JSON-encoded map representation of a row that is used for INSERT JSON. For example, if we have a table like the following:</p>
</blockquote>

<p>Let you schema be </p>

<pre><code>CREATE TABLE users (
    id text PRIMARY KEY,
    age int,
    state text
);
</code></pre>

<p>You can use </p>

<pre><code>SELECT JSON * FROM users;
</code></pre>

<p>The results will look like this:</p>

<pre><code>{""id"": ""user123"", ""age"": 42, ""state"": ""TX""}
</code></pre>

<p>or you can use </p>

<pre><code>SELECT JSON id, writetime(age), ttl(state) as ttl FROM users;
</code></pre>

<p>Output : </p>

<pre><code>{""id"": ""user123"", ""writetime(age)"": 1434135381782986, ""ttl"": null}
</code></pre>

<p>Source : <a href=""http://www.datastax.com/dev/blog/whats-new-in-cassandra-2-2-json-support"" rel=""nofollow noreferrer"">http://www.datastax.com/dev/blog/whats-new-in-cassandra-2-2-json-support</a></p>
",['table']
40898470,40930564,2016-11-30 21:24:02,Cassandra datamodelling,"<p>I have the following table to store time-series data:</p>

<pre><code>CREATE TABLE alerts_by_year_day (
    day_of_year int,
    year int,
    alert_timestamp timestamp,
    serial_number text,
    alert_id uuid,
    alert_type text,
    ....
    ....
  PRIMARY KEY((year, day_of_year), alert_timestamp, serial_number, alert_id)
) WITH CLUSTERING ORDER BY (alert_timestamp DESC, serial_number DESC);
</code></pre>

<ol>
<li>For the UI report I want to retrieve all the alerts for a given period of time. I have this query:</li>
</ol>

<blockquote>
  <p>select * from alerts_by_year_day where year = 2015 and day_of_year in (241, 240);</p>
</blockquote>

<p>But this query is returning me results that are in the ASC order of the year and then ASC order of day. 
So results like this </p>

<blockquote>
  <p>2015 | 240 |.....</p>
  
  <p>2015 | 241 |.....</p>
</blockquote>

<p>But I want to display the latest results first or in the descending order. Adding 'order by alert_timestamp;' gives an error
So how can I display results that are in the descending order ?</p>

<ol start=""2"">
<li>Then for a given period of time I want to retrieve only certain types of alert based on alert_type.</li>
</ol>

<p>So I created a mat view like this:</p>

<pre><code>CREATE MATERIALIZED VIEW alerts_by_type_and_timestamp AS 
    SELECT *
    FROM alerts_by_year_day
    WHERE alert_timestamp IS NOT NULL AND
        alert_type IS NOT NULL AND 
        day_of_year IS NOT NULL AND 
        year IS NOT NULL AND serial_number IS NOT NULL AND 
        alert_id IS NOT NULL
    PRIMARY KEY ((year, day_of_year, alert_type), alert_timestamp, serial_number, alert_id)
    WITH CLUSTERING ORDER BY (alert_timestamp DESC, serial_number DESC, alert_id DESC);
</code></pre>

<p>But of course it returns results by type first and in that by timestamp. 
What I am looking for is only a subset of types and in the desc order they were generated in. 
Is that possible in Cassandra ?</p>

<p>Thanks</p>
",<cassandra><cql>,"<p>The order of the partitions is in token order. Its the order of the murmur3 hash of the primary key. ie:</p>

<pre><code>cqlsh:test&gt; select * from alerts_by_year_day ;

 year | day_of_year | alert_timestamp                 | serial_number | alert_id                             | alert_type
------+-------------+---------------------------------+---------------+--------------------------------------+------------
 2015 |          10 | 1970-01-01 00:00:00.001000+0000 |          s123 | b7baa710-b87b-11e6-9137-eb2177fd2cc2 |       type
 2015 |         110 | 1970-01-01 00:00:00.001000+0000 |          s123 | bf110270-b87b-11e6-9137-eb2177fd2cc2 |       type
 2015 |          11 | 1970-01-01 00:00:00.001000+0000 |          s123 | bce08de1-b87b-11e6-9137-eb2177fd2cc2 |       type
 2016 |         110 | 1970-01-01 00:00:00.001000+0000 |          s123 | c2e22eb1-b87b-11e6-9137-eb2177fd2cc2 |       type
</code></pre>

<p>Its because of your IN query that its walking them in that order (which you cant control). Under cover has to make a separate query for each combination of the primary key.</p>

<p>This requires multiple fetches per value in the <code>in</code> clause and can get inefficient fast if you put too much in it since it puts a lot of burden on your coordinator. It costs almost same to just make two <em>async</em> <code>select</code> queries. Which you then can read in the order you want. This also saves you from making a single coordinator in your cluster manage fetches to many nodes, ultimately this can help with cluster health. 1 query per day isn't bad at all to iterate through in your application.</p>

<p>If the days are not ""everyday"" may want to consider a 2nd table thats just <code>(year, day_of_year)</code> and <code>(type, year, day_of_year)</code> that you write to when you do your insert before you do your query you can.</p>

<blockquote>
  <p>note: can keep local in memory cache so you dont have thousands of unnecessary writes, can write just once but it is ok to write multiple times incase multiple instances of app or restarts</p>
</blockquote>

<pre><code>year = 2015
days = query('select * from alert_day_index where year = %s', year)
results = []
for day in days:
  results.extend(query('select * from alerts_by_year_day where year = %s and day_of_year = %s', year, day))
</code></pre>

<p>If you have a lot of days just need to make queries async so the latency of the queries doesn't block throughput of app.</p>
",['table']
40957339,40999313,2016-12-04 09:49:14,How to handle race condition with cassandra updates?,"<p>I am learning Cassandra. I am modeling the cassandra table for a specific use case. The use case described below -</p>

<p>A user can write a post. 
Other users can reply to the post.
Users can also ""up vote"" or ""down vote"" a post.
User sort the posts by date or up votes or down votes.</p>

<p>This is my table definition -</p>

<pre><code>CREATE TABLE post.comments_by_post (
postid text,
parentpostid text,
createdon bigint,
username text,
userid text,
displayname text,
upvotes int,
downvotes int,
comment text,
PRIMARY KEY ((postid, parentpostid), createdon)
) WITH CLUSTERING ORDER BY (createdon DESC);
</code></pre>

<p>To increment ""upvote"" I have a update query -</p>

<pre><code>UPDATE post.comments_by_post SET upvotes = incrementedValue where postid=1 and parentpostid = 2 ;
</code></pre>

<p><strong>incrementedValue</strong> is calculated adding 1 in previous value.</p>

<p><strong>incrementedValue = previousValue + 1</strong></p>

<p>My question is, if i have to calculate the increment based on the previous value, which is in the table, it will cause race condition and data corruption.</p>

<p>Do we have better way?</p>

<p>I know that cassandra has counter column definition type, which can be used for such incremental values, but it requires additional table. Counter column cannot be used with normal columns which are not part of primary key.</p>
",<cassandra><race-condition><cassandra-2.0>,"<p>The following table and secondary index will allow you to implement counting without the Counter table and without any locks:</p>

<pre><code>CREATE TABLE votes_by_comment (
   postid text,
   parentpostid text,
   userid text,
   vote text, //can be 'up' or 'down'
PRIMARY KEY (( postid, parentpostid ), userid))

CREATE INDEX ON votes_by_comment (vote);
</code></pre>

<p>When a user does 'up votes':</p>

<pre><code>INSERT INTO votes_by_comment (postid, parentpostid, userid, vote) VALUES ('comment1', 'post1', 'user1', 'up');
</code></pre>

<p>When a user does 'down votes':</p>

<pre><code>INSERT INTO votes_by_comment (postid, parentpostid, userid, vote) VALUES ('comment1', 'post1', 'user1', 'down');
</code></pre>

<p><code>userid</code> as clustering column will allow it to avoid race condition and restrict multiple voting by one user.</p>

<p>To count up votes:</p>

<pre><code>SELECT count(*) from votes_by_comment WHERE postid='comment1' AND parentpostid='post1' and vote='up';
</code></pre>

<p>The secondary index will allow it to perform select by <code>vote</code> value, since the select by the secondary index will be performed within a partition key, it will have good performance.    </p>

<p>But this approach doesn't allow you to implement ordering by votes on Cassandra side, and it should be implemented on the application side.</p>
",['table']
40977718,40979455,2016-12-05 15:30:41,Slow Select from big table in Cassandra,"<p>I have table like this in Cassandra(2.1.15.1423) with over the 14 000 000 records: </p>

<pre><code>CREATE TABLE keyspace.table (
    field1 text,
    field2 text,
    field3 text,
    field4 uuid,
    field5 map&lt;text, text&gt;,
    field6 list&lt;text&gt;,
    field7 text,
    field8 list&lt;text&gt;,
    field9 list&lt;text&gt;,
    field10 text,
    field11 list&lt;text&gt;,
    field12 text,
    field13 text,
    field14 text,
    field15 list&lt;frozen&lt;user_defined_type&gt;&gt;,
    field16 text,
    field17 text,
    field18 text,
    field19 text,
    PRIMARY KEY ((field1, field2, field3) field4)
) WITH bloom_filter_fp_chance = 0.01
    AND caching = '{""keys"":""ALL"", ""rows_per_partition"":""NONE""}'
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy'}
    AND compression = {'sstable_compression': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99.0PERCENTILE';
</code></pre>

<p>In application I use Python (cassandra-driver==3.1.1) and Go (gocql).</p>

<p>Problem:</p>

<p>I need to move records from this table to another.
When I try to get data (even without filters) all stops and I get timeout error.
I tried to change fetch_size/page_size - result the same but after few minutes of waiting.</p>
",<cassandra><datastax><datastax-enterprise><nosql>,"<p>If you are going to move records from this table to a different table you should do this one partition range at a time.  Doing something similar to a</p>

<pre><code>SELECT * FROM keyspace.table
</code></pre>

<p>will not work in a highly distributed datastore such as Cassandra.  This is becasue a query like the one above requires a full cluster scan and scatter/gather operation to be performed in order to satisfy it.  This is an anti-pattern in C* and will cause timeouts in most cases.  A better approach is to only query one partition at a time.  This data can be retrieved very quickly by the data store.  A common pattern for this sort of operation is to iterate through the token ranges of the table one at a time and process each one individually.Here is an example (sorry it is in Java) of how you can slice the token ranges in Cassandra to only deal with a small subset of the data at a time:</p>

<p><a href=""https://github.com/brianmhess/cassandra-count"" rel=""nofollow noreferrer"">https://github.com/brianmhess/cassandra-count</a></p>
",['table']
40996347,40997199,2016-12-06 13:17:30,Cassandra: Batch with conditions cannot span multiple tables,"<p>I am trying to execute 3 conditional inserts to different tables inside a batch by using the Cassandra cpp-driver:</p>

<pre><code>BEGIN BATCH 
insert into table1 values (...) IF NOT EXISTS 
insert into table2 values (...) IF NOT EXISTS 
insert into table3 values (...) IF NOT EXISTS 
APPLY BATCH
</code></pre>

<p>But I am getting the following error:</p>

<pre><code>Batch with conditions cannot span multiple tables
</code></pre>

<p>If the above is not possible in Cassandra, what is the alternative to perform multiple conditional inserts as a transaction and ensure that all succeed or all fail?</p>
",<cassandra><batch-processing><datastax><cassandra-3.0><nosql>,"<p>I'm afraid there are no alternatives. Conditional statements in a <code>BATCH</code> environment are limited to a single table only, and I don't think there's room for changes in future. </p>

<p>This is due to how Cassandra works internally: a batch containing a conditional update (it is called <em>lightweight transaction</em>) can only be used in one partition because they are based on the Paxos implementation, because the Paxos itself works at partition level only. Moreover, in a batch with multiple conditional statements in the same BATCH, all the conditions must be verified to the batch succeed. Even if one (and only) conditional update fails, the entire batch will fail. </p>

<p>You can read more about <code>BATCH</code> statements in the <a href=""https://docs.datastax.com/en/cql/3.3/cql/cql_reference/batch_r.html"" rel=""noreferrer"">documentation</a>.</p>

<p>You'd basically get a performance hit for the conditional update, and a performance hit for a batched operation, and C* stops you to get so far. </p>

<p>It seems to me you designed it RDBMS-like. A No-SQL alternative solution, I don't know if it can be applied to your use case though, you could denormalize your data in a 4th table that combines all the other 3 tables, and then supply a single update to this 4th table.</p>
",['table']
41009781,41031703,2016-12-07 05:03:06,Partition keys retrieval for usage with joinWithCassandraTable,"<p>I have following Cassandra table:</p>

<pre><code>CREATE TABLE listener.snapshots_geohash 
(
    created_date text, -- date when record have come to the system
    geo_part text, -- few signs of geo hash - just for partitioning
    when timestamp, -- record creation date
    device_id text, -- id of device produced json data (see snapshot column)
    snapshot text, -- json data, should be aggregated by spark
    PRIMARY KEY ((created_date, geo_part), when, device_id)
)
</code></pre>

<p>Every morning aggregating application should load data for previous day and aggregate JSON data from snapshot column. Aggregation will group data by geohash, that's why its part were selected to be part of partition key.</p>

<p>I know that it is efficient to load data from Cassandra by using joinWithCassandraTable - but for that I have to get RDD constructed from (created_date, geo_part) pairs. While I know created_date value, I can't list geo_part values - since it is just part of geohash and its values are not continuous. So I have somehow to run <code>select distinct created_date, geo_part from ks.snapshots</code> and create RDD from its results. The question is how to run this select with spark 2.0.2 and cassandra-connector 2.0.0-M3 or perhaps there alternative way?</p>
",<apache-spark><cassandra><spark-cassandra-connector>,"<p>I found the way to fetch Cassandra partition keys by running CQL query with CassandraConnector:</p>

<pre><code> val cassandraConnector = CassandraConnector(spark.sparkContext.getConf)
 val distinctRows = cassandraConnector.withSessionDo(session =&gt; {
     session.execute(s""select distinct created_date, geo_part from ${keyspace}.$snapshots_table"")
 }).all().map(row =&gt; {TableKeyM(row.getString(""created_date""), row.getString(""geo_part""))}).filter(k =&gt; {days.contains(k.created_date)})
 val data_x = spark.sparkContext.parallelize(distinctRows)
</code></pre>

<p>The table structure design has following problem: Cassandra disallows to add <strong>WHERE created_date='...'</strong> clause to <strong>select distinct created_date, geo_part</strong> and it is required to fetch whole list of pairs and filter it in application.</p>

<p>Alternative solution could be making partition keys continuous. In case if aggregation would be done by hours - then partition key could be (created_date, hour) and 24 hours could be listed in application. If 24 partitions per day is not enough, and aggregation have <strong>group by</strong> by geohash, it is possible to stick with geohash significant part - but it should be translated to something countable - for example <strong>geoPart.hash() % desiredNumberOfSubpartitions</strong></p>
",['table']
41017805,41018523,2016-12-07 12:42:40,How to do negation for 'CONTAINS',"<p>I have Cassandra table with one column defined as set.
How can I achieve something like this:</p>

<pre><code>SELECT * FROM &lt;table&gt; WHERE &lt;set_column_name&gt; NOT CONTAINS &lt;value&gt;
</code></pre>

<p>Proper secondary index in was already created.</p>
",<cassandra><cassandra-2.0><cassandra-2.1>,"<p>From the <a href=""https://docs.datastax.com/en/cql/3.1/cql/cql_reference/select_r.html"" rel=""nofollow noreferrer"">documentation</a>:</p>

<blockquote>
  <p>SELECT select_expression   FROM keyspace_name.table_name   WHERE
  relation AND relation ...    ORDER BY ( clustering_column ( ASC | DESC
  )...)   LIMIT n   ALLOW FILTERING</p>
</blockquote>

<p>then later:</p>

<blockquote>
  <p>relation is:</p>
  
  <p>column_name op term</p>
</blockquote>

<p>and finally:</p>

<blockquote>
  <p>op is = | &lt; | > | &lt;= | > | = | CONTAINS | CONTAINS KEY</p>
</blockquote>

<p>So there's no native way to perform such query. You have to workaround by designing a new table to specifically satisfy this query.</p>
",['table']
41023957,41032100,2016-12-07 17:40:48,"Spark2 session for Cassandra , sql queries","<p>In Spark-2.0 what is the best way to create a Spark session. Because in both  Spark-2.0 and Cassandra- the APIs have been reworked, essentially deprecating the SqlContext (and also CassandraSqlContext). So for executing SQL- either I create a Cassandra Session <code>(com.datastax.driver.core.Session) and use execute( "" "")</code>. Or I have to create a <code>SparkSession (org.apache.spark.sql.SparkSession) and execute sql(String sqlText)</code> method.</p>

<p>I don't know the SQL limitations of either - can someone explain.</p>

<p>Also if I have to create the SparkSession - how do I do it- couldn't find any suitable example. With APIs getting reworked the old examples don't work. 
I was going thru this code sample- <a href=""https://github.com/datastax/spark-cassandra-connector/blob/master/doc/14_data_frames.md"" rel=""nofollow noreferrer"">DataFrames</a>- not clear what sql context is being used here (is that the right approach going forward.)
(For some reason deprecated APIs are not even compiling - need to check my eclipse settings)</p>

<p>Thanks</p>
",<java><apache-spark><apache-spark-sql><cassandra><spark-cassandra-connector>,"<p>You would need Cassandra Session for create/drop keyspace and table from Cassandra DB. In Spark application, in order to create Cassandra Session you need to pass SparkConf to CassandraConnector. In Spark 2.0 you can do it like below.</p>

<pre><code> SparkSession spark = SparkSession
              .builder()
              .appName(""SparkCassandraApp"")
              .config(""spark.cassandra.connection.host"", ""localhost"")
              .config(""spark.cassandra.connection.port"", ""9042"")
              .master(""local[2]"")
              .getOrCreate();

CassandraConnector connector = CassandraConnector.apply(spark.sparkContext().conf());
Session session = connector.openSession();
session.execute(""CREATE TABLE mykeyspace.mytable(id UUID PRIMARY KEY, username TEXT, email TEXT)"");
</code></pre>

<p>If you have existing Dataframe then you can create table in Cassandra using <code>DataFrameFunctions.createCassandraTable(Df)</code> as well. See api details <a href=""http://datastax.github.io/spark-cassandra-connector/ApiDocs/2.0.0/spark-cassandra-connector/#com.datastax.spark.connector.DataFrameFunctions"" rel=""noreferrer"">here</a>. </p>

<p>You can read data from Cassandra DB using api provided by spark-cassandra-connector like below. </p>

<pre><code>Dataset&lt;Row&gt; dataset = spark.read().format(""org.apache.spark.sql.cassandra"")
            .options(new HashMap&lt;String, String&gt;() {
                {
                    put(""keyspace"", ""mykeyspace"");
                    put(""table"", ""mytable"");
                }
            }).load();

dataset.show(); 
</code></pre>

<p>You can use SparkSession.sql() method to run query on temporary table created on Dataframe returned by spark cassandra connector like below.</p>

<pre><code>dataset.createOrReplaceTempView(""usertable"");
Dataset&lt;Row&gt; dataset1 = spark.sql(""select * from usertable where username = 'Mat'"");
dataset1.show();
</code></pre>
",['table']
41137109,41137196,2016-12-14 07:35:09,I can not repository.save(mydata) I am looking for a pretty alternative to JOIN with Spring + Cassandra,"<p>I am beginning to touch Cassandra, but I am in trouble because I can not do JOIN.
Since JOIN can not be done with CQL as it is, I thought about looking for alternative means and joining it on the Java application side.</p>

<p>Specifically, I used @OneToMany and I tried joining Entities, but the following error appears.</p>

<p>Is there any good solution?</p>

<p>■Project structure</p>

<p>SpringBoot + Spring Data for Apache Cassandra</p>

<p>Version:</p>

<ul>
<li>Spring Boot :: (v1.3.5.RELEASE)</li>
<li>spring-data-cassandra-1.3.5.RELEASE</li>
<li>cassandra 2.1.16</li>
</ul>

<p>■Error log</p>

<pre><code>com.datastax.driver.core.exceptions.InvalidQueryException: Unknown identifier emp
at com.datastax.driver.core.Responses$Error.asException(Responses.java:102) ~[cassandra-driver-core-2.1.9.jar:na]
at com.datastax.driver.core.DefaultResultSetFuture.onSet(DefaultResultSetFuture.java:149) ~[cassandra-driver-core-2.1.9.jar:na]
at com.datastax.driver.core.RequestHandler.setFinalResult(RequestHandler.java:183) ~[cassandra-driver-core-2.1.9.jar:na]
at com.datastax.driver.core.RequestHandler.access$2300(RequestHandler.java:44) ~[cassandra-driver-core-2.1.9.jar:na]
at com.datastax.driver.core.RequestHandler$SpeculativeExecution.setFinalResult(RequestHandler.java:751) ~[cassandra-driver-core-2.1.9.jar:na]
</code></pre>

<p>■ Source: Controller</p>

<pre><code>@RequestMapping(value = ""/DepartmentsCassandra/form"", method = RequestMethod.POST)
@Transactional(readOnly=false)
public ModelAndView form(
        @RequestParam(""department_id"") int department_id,
        @RequestParam(""department_name"") String department_name,
        ModelAndView mav){
    Departments mydata = new Departments();
    mydata.setDepartment_id(department_id);
    mydata.setDepartment_name(department_name);
    repository.save(mydata);// ← Error occurred !!!
    return new ModelAndView(""redirect:/DepartmentsCassandra"");
}
</code></pre>

<p>■ Source: Entity: Departments</p>

<pre><code>package com.example.cassandra.entity;

import java.util.ArrayList;
import java.util.List;
import javax.persistence.FetchType;
import javax.persistence.JoinColumn;
import javax.persistence.OneToMany;
import org.springframework.cassandra.core.PrimaryKeyType;
import org.springframework.data.cassandra.mapping.Column;
import org.springframework.data.cassandra.mapping.PrimaryKeyColumn;
import org.springframework.data.cassandra.mapping.Table;
@Table(value=""departments"")
public class Departments {

@PrimaryKeyColumn(name = ""department_id"",ordinal = 1,type = PrimaryKeyType.PARTITIONED)
private int department_id;

@Column(value = ""department_name"")
private String department_name;

public Departments(int department_id,String department_name){
    this.department_id = department_id;
    this.department_name = department_name;
}

@OneToMany(fetch=FetchType.EAGER)
@JoinColumn(name=""department_id"",insertable=false,updatable=false)
private List&lt;Employees&gt; emp = new ArrayList&lt;Employees&gt;();
</code></pre>
",<java><spring><spring-boot><cassandra><spring-data>,"<p>Sooo, ""client side join"" is a general anti-pattern using Cassandra, since you do two queries instead of one every time and hence lose the performance gain.
The way to go is creating a broad table for each query - including all ""joined"" data.
So in your case, create a table employee_by_department or something alike.
Check out the introductory courses on datastax.com - they are great :-)</p>
",['table']
41154491,41154790,2016-12-15 00:25:48,Cassandra Statement set KeySpace,"<p>Using Cassandra 2.2.8 with 3.0 Connector.
I am trying to create a Statement with QueryBuilder. When I execute Statement it complains <strong>no keyspace defined</strong>. The only way I know to set keyspace is as below (There is no setKeyspace method in Statement). When I do a getKeySpace - I actually get null</p>

<pre><code> Statement s = QueryBuilder.select().all()
            .from(""test.tests"")
System.out.println(""getKeyspace:""+ s.getKeyspace()); &gt;&gt; null
</code></pre>

<p>Am I doing something wrong, Is there any other (more reliable) way to setKeyspace?
Thanks</p>
",<cassandra><datastax-java-driver>,"<p><a href=""http://docs.datastax.com/en/drivers/java/3.1/com/datastax/driver/core/querybuilder/Select.Builder.html#from-java.lang.String-"" rel=""nofollow noreferrer""><code>from(String)</code></a> expects a table name.  While what you are doing is technically valid and cassandra will interpret it correctly, the driver is not able to derive the keyspace name in this way.</p>

<p>Instead you could use <a href=""http://docs.datastax.com/en/drivers/java/3.1/com/datastax/driver/core/querybuilder/Select.Builder.html#from-java.lang.String-java.lang.String-"" rel=""nofollow noreferrer""><code>from(String, String)</code></a> which takes the first parameter as the keyspace.</p>

<pre class=""lang-java prettyprint-override""><code>    Statement s = QueryBuilder.select().all()
            .from(""test"", ""tests"");
    System.out.println(""getKeyspace:"" + s.getKeyspace()); // &gt;&gt; test
</code></pre>
",['table']
41194429,41197250,2016-12-17 01:07:56,cassandra error keyspace does not exist,"<pre><code>org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'JournalActions': Invocation of init method failed; nested exception is com.datastax.driver.core.exceptions.InvalidQueryException: Keyspace 'audit' does not exist
    at org.springframework.beans.factory.annotation.InitDestroyAnnotationBeanPostProcessor.postProcessBeforeInitialization(InitDestroyAnnotationBeanPostProcessor.java:136)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsBeforeInitialization(AbstractAutowireCapableBeanFactory.java:408)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1570)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:545)
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:482)
    at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:306)
    at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230)
    at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:302)
    at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:197)
    at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:772)
    at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:839)
    at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:538)
    at org.springframework.boot.context.embedded.EmbeddedWebApplicationContext.refresh(EmbeddedWebApplicationContext.java:118)
    at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:766)
    at org.springframework.boot.SpringApplication.createAndRefreshContext(SpringApplication.java:361)
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:307)
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1191)
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1180)
    at com.paypal.pps.activity.init.RaptorApplication.main(RaptorApplication.java:23)
Caused by: com.datastax.driver.core.exceptions.InvalidQueryException: Keyspace 'audit' does not exist
</code></pre>
<p>But when i manually login to cqlsh , i'm able to select the table.</p>
<blockquote>
<p>cqlsh:journal&gt; use audit;</p>
<p>cqlsh:audit&gt; select * from event_log;</p>
<p>event_id | event_type</p>
</blockquote>
<hr />
<p>i'm using Cassandra 3.9 running on a docker</p>
",<ubuntu><docker><cassandra>,"<p>You have already narrowed down the problem(in cqlsh the queries are working), 
so the problem is on the driver side.</p>

<p>Please ensure the following</p>

<p>1) Driver used (datastax.cassandra - cassandra-driver-core - version 3.0.0-alpha5) is compatible with Cassandra DB Version</p>

<p>2) Remove if you have any spring-data-cassandra libraries reference in your project</p>

<p>Please Refer these posts for more info</p>

<p><a href=""https://docs.datastax.com/en/developer/driver-matrix/doc/javaDrivers.html#java-drivers"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/developer/driver-matrix/doc/javaDrivers.html#java-drivers</a></p>

<p><a href=""https://stackoverflow.com/questions/34117374/com-datastax-driver-core-exceptions-invalidqueryexception-unconfigured-table-sc"">com.datastax.driver.core.exceptions.InvalidQueryException: unconfigured table schema_keyspaces</a></p>
",['table']
41329229,41329573,2016-12-26 09:43:49,Cassandra - Batch too large,"<p>I have a list of Products which have to be added to a Purchase Order. The Purchase order has a sequence number and once the Products are added, their status should be changed to indicate that these are out for purchase.</p>

<p>The typical number of Products being processed in 1 Purchase Order would be 500. </p>

<p>On the DB - I have 2 tables -> 1 for Products and another for Purchase Orders. Which means I need 500 updates and 1 insert to be done.
When I try to do this in a BatchStatement I get the error - Batch too large.</p>

<p>Suggestions from various quarters tell me that I should use multiple async queries. My concern however is atomicity of the entire operation.
Please suggest what would be the best way forward given my requirement.</p>

<p>Thanks in advance.</p>
",<cassandra><datastax>,"<p>This is interesting. Inserting a lot of inserts (> 10) into a batch (to achieve atomicity) is really going to be a bad performancer, so raising the batch limit is not really an option.</p>

<p>Since Cassandra manages atomicity at single row level also, you could exploit that by changing your model by adding a table to ""bookmark"" your purchase orders, where you store there in one row only both the purchase order id and the items into a map, so you have idempotency in your queries. You can then expand or post process this table to continue your workflow as needed.</p>
",['table']
41393238,41400526,2016-12-30 08:46:51,Cassandra data model with multiple conditions,"<p>I'm new to Cassandra, so I read a dozen articles about it and thus I know the basics. All the tutorials show efficient data retrieval by 1 or 2 columns and a time range. What I could not find was how to correctly model your data if you have more conditions.</p>

<p>I have a big events normalised database, with quite a few columns, say: </p>

<ul>
<li>Event type</li>
<li>time </li>
<li>email</li>
<li>User_age</li>
<li>user_country</li>
<li>user_language</li>
<li>and so on.</li>
</ul>

<p>I would need to be able to query by all columns. So in RDBMS I would query:</p>

<p><code>SELECT email FROM table WHERE time &gt; X AND user_age BETWEEN X AND X AND user_language = 'nl'</code> etc..</p>

<p>I know I can make a separate table for each column, but then I would still need to combine the results. Maybe this is not a bad approach, but I doubt it since there are no subqueries.</p>

<p>My question is obviously, how can I model this kind of data correctly in Cassandra?</p>

<p>Thanks a lot!</p>
",<cassandra><data-modeling><cassandra-2.0><cql3><nosql>,"<blockquote>
  <p>I would need to be able to query by all columns.</p>
</blockquote>

<p>Let me stop you right there.  In Cassandra, you create your tables based on your anticipated query patterns, and usually a table supports a single query.  In your case, you have ""quite a few"" columns and you will need to duplicate that data into a table designed to support each possible query.  That is going to get big and ungainly, very quickly.</p>

<blockquote>
  <p>Could we just add the rest as secondary indexes? there could potentially still be millions of rows in the eventtype table + merchant_id + time selection.</p>
</blockquote>

<p>Secondary indexes are intended to be used on middle-of-the-road cardinality columns.  So both, extremely low and extremely high cardinality columns are bad for secondary indexes.  The problem, is that Cassandra will have to pick one of your nodes as a coordinator, scan the index on each node (incurring lots of network time), and then build and return the result set.  It's a prescription for poor performance, that flies in-the-face of the best practices for working with a distributed database.</p>

<p>In short, Cassandra is not a good solution for use cases like this.  It sounds like you want to be able to do OLAP-type queries, and for that you should use a tool that is better-suited for that purpose.</p>
",['table']
41440627,41446688,2017-01-03 09:49:52,Cassandra Hints table for a write-heavy system can exceed 2 Bn cells limit,"<ol>
<li><p>I am using Cassandra 2.2 in which hints are stored in system.hints table. Since, the node-Id is the partition key, the table can easily exceed 2 Bn cells limit per partition in 3 hours(default hint window) for a write-heavy system. Is it preferred to reduce this window for a write heavy system?</p></li>
<li><p>Also, for testing purposes I brought a node down and then up after a while. Ideally after hints replay, the hints table should be truncated and compacted,but I notice  that that table still has all the entries, why is not cleared? Also, how to validate if replay has happened or not?</p></li>
</ol>
",<cassandra>,"<p>Upgrading to 3.0 will fix a lot of the HH issues when theres a ton of them like this. Can see <a href=""http://www.datastax.com/dev/blog/whats-coming-to-cassandra-in-3-0-improved-hint-storage-and-delivery"" rel=""nofollow noreferrer"">here</a> for some more details, but basically using a C* table as a queue like this is kinda an anti-pattern in itself so it was changed to be more similar to the commitlog which works far better. That is absolutely your best option here.</p>

<ol>
<li><p>Decreasing HH window will work but be aware that you must do repairs if a node is down longer than that window since you will lose data.</p></li>
<li><p>They are not truncated, but deleted and you need to wait for a compaction for them to be cleared. Before delivering the hints when node comes back up it should do a major compaction that would clear an up. To validate I would just read (<code>select count(*)</code>) from the table, any deletes would be accounted for.</p></li>
</ol>

<p>ie turn off node2, insert a value and can observe:</p>

<pre><code>cqlsh:keyspace1&gt; select count(*) from system.hints where target_id = 8e821294-50de-46d4-b668-f4dce69797aa;

 count
-------
     1

(1 rows)
</code></pre>

<p>Start node2, and wait for HH drain :</p>

<pre><code>cqlsh:keyspace1&gt; select count(*) from system.hints where target_id = 8e821294-50de-46d4-b668-f4dce69797aa;

 count
-------
     0
</code></pre>
",['table']
41497408,41499675,2017-01-06 01:10:58,Distinct users in Cassandra,"<p>I have the following problem: 
In my Cassandra database I have several messages sent by several users.
My messages table has the following structure:</p>

<pre><code>CREATE TABLE messages (
  recipient bigint,
  sender bigint,
  created_at text,
  content text,
  PRIMARY KEY((recipient, sender),created_at)
);
</code></pre>

<p>I need to count the number of messages sent by a user in a day. For example, between the date 2017-01-01 and 2017-01-05</p>

<pre><code>sender | created_at
  1       2017-01-01
  1       2017-01-01
  2       2017-01-01
  3       2017-01-02  
  3       2017-01-02
  4       2017-01-03
  4       2017-01-04
  5       2017-01-04
</code></pre>

<p>I would have the result</p>

<pre><code>2017-01-01 = 2
2017-01-02 = 1
2017-01-03 = 1  
2017-01-04 = 2
</code></pre>
",<cassandra>,"<p>From what I can see, you cannot do that with your table structure because your partition key contains the <code>recipient</code>. To be told, you should not count at all, because <a href=""https://www.datastax.com/dev/blog/counting-keys-in-cassandra"" rel=""nofollow noreferrer"">counting keys in cassandra</a> is hard. </p>

<p>However, if you insist on counting these keys, I suggest you two approaches:</p>

<hr>

<h2>Create a new counter table</h2>

<pre><code>CREATE TABLE counters_by_user (
    sender bigint,
    ts timestamp,
    messages counter,
    PRIMARY KEY (sender, ts)
)
</code></pre>

<p>This table will allow you to directly fetch the value you are looking for. It allows you to select the appropriate ""granularity"" of the counters, that is if you want a day-by-day counter simply store the timestamp in the <code>ts</code> field in the <code>yyyy-mm-dd</code> format. If you want hourly-based counting, store it in <code>yyyy-mm-dd HH:00</code> format, etc... You'll need the exact <code>sender</code> only to fetch the results, and can range query by specifying the <code>ts</code> component of the primary key. Have a look at the <a href=""https://docs.datastax.com/en/cql/3.1/cql/cql_using/use_counter_t.html"" rel=""nofollow noreferrer"">Counters</a> page documentation on how to use them, and beware that the main drawback of this approach is that Cassandra can over/under count, so watch your steps if you need to be pedantic on counting.</p>

<hr>

<h2>Create a new message table, aka denormalize your data</h2>

<pre><code>CREATE TABLE messages_by_sender (
    sender bigint,
    created_at timestamp,
    PRIMARY KEY (sender, created_at)
);
</code></pre>

<p>Every time you insert a row in the <code>messages</code> table you'll insert a row here, and when you need to count the messages sent you simply run a <code>SELECT COUNT(*) FROM messages_by_sender WHERE sender=?</code> to count them all, or <code>SELECT COUNT(*) FROM messages_by_sender WHERE sender=? AND created_at &gt; ? AND created_at &lt; ?;</code> to specify a range. If you have a lot of messages per <code>sender</code>, however this will lead to inefficiency, since counting keys in Cassandra requires a partition scan.</p>
",['table']
41522856,41526185,2017-01-07 15:07:32,"Cassandra, partition/primary key: partitioning and constraint","<p>I'm trying to design some column-family in Cassandra but I have some doubt about the primary/partition key.<br>
What I want is the following:</p>

<ul>
<li>Spread data evenly around the cluster</li>
<li>Minimize the number of partitions read</li>
</ul>

<p>I know that these goals are achievable by choosing the appropriate partition/primary key, but suppose that the partition/primary key choosed allows invalid data in your colum-family, what do you do?<br></p>

<p>Suppose that I have the following colum-family:</p>

<pre><code>CREATE TABLE group (
  groupname text,
  username text,
  PRIMARY KEY (groupname, username))
</code></pre>

<p>Suppose that a username must belong only in <strong>one group</strong>, with this primary key I can achieve (with some assumption) the previous goals, but there is no way to avoid that a username is placed in more than one group.<br></p>

<p>Can I simply make assumption on what will be in the column-family or there are ways to avoid the insertion of invalid data? </p>
",<database-design><cassandra><primary-key><partitioning><nosql>,"<p>Declaring a table with username as the primary key will ensure only one username-to-group relationship exists:</p>

<pre><code>CREATE TABLE group (username text, groupname text, PRIMARY KEY (username));
</code></pre>

<p>With <a href=""http://docs.datastax.com/en/cql/3.1/cql/cql_using/use_ltweight_transaction_t.html"" rel=""nofollow noreferrer"">lightweight transactions</a> you can prevent updates to a user's group:</p>

<pre><code>INSERT INTO group (username, groupname) VALUES ('joe', 'wheel') IF NOT EXISTS;
</code></pre>

<p>If you wish to query by group, create a materialized view:</p>

<pre><code>CREATE MATERIALIZED VIEW usersbygroup AS
   SELECT groupname, username FROM group
PRIMARY KEY (groupname, username);
</code></pre>
",['table']
41536495,41539255,2017-01-08 18:51:05,nosql separate data by client,"<p>I have to develop a project using a NoSql base, either couchbase or cassandra.
I would like to know if it is recommended to partition the data of each customer in a bucket?</p>

<p>In my case, there will never be requests between the different clients.
The data can be completely separated.</p>

<p>For couchbase, I saw that for each bucket a memory capacity, was reserved for him.</p>

<p>Where does the separation have to be done at another place document or super column for cassandra.</p>

<p>Thank you</p>
",<cassandra><couchbase><nosql>,"<blockquote>
  <p>Where does the separation have to be done at another place document or super column for cassandra.</p>
</blockquote>

<p>Tip #1, when working with Cassandra, <strong>completely erase</strong> the word ""super column"" from your vocabulary.</p>

<blockquote>
  <p>I would like to know if it is recommended to partition the data of each customer in a bucket?</p>
</blockquote>

<p>That depends.  It sounds like your queries would be mostly based on a customer id, so it makes sense to have it as a <em>part</em> of your partition key.  However, if each customer partition has millions of rows and/or columns underneath it, that's going to get very big.</p>

<p>Tip #2, proper Cassandra modeling is done based on what your required queries look like.  So without actually seeing the kinds of queries you need to serve, it's going to be difficult to be any more specific than that.</p>

<p>If you have customer data relating to accounts and addresses, etc, then building a <code>customers</code> table with a PRIMARY KEY of only <code>customer_id</code> might make sense.  But if you find that you need to query your customers (for example) by <code>email_address</code>, then you'll want to create a <code>customers_by_email</code> table, duplicate your data into that, and create a PRIMARY KEY that supports that.</p>

<p>Additionally, if you find yourself storing data on customer activity, you may want to consider a <code>customer_activity</code> table with a PRIMARY KEY of <code>PRIMARY KEY ((customer_id,month),activity_time)</code>.  That will use both <code>customer_id</code> and <code>month</code> as a partition key, storing the customer's activity clustered by <code>activity_time</code>.  In this case, if we didn't use <code>month</code> as an additional partition key, each <code>customer_id</code> partition would be continually written to, until it became too ungainly to write to or query (unbound row growth).</p>

<p><strong>Summary</strong>:</p>

<ul>
<li>If anyone tells you to use a super column in Cassandra, <strong>slap them</strong>.</li>
<li>You need to know your queries <em>before</em> you design your tables.</li>
<li>Yes, <code>customer_id</code> would be a good way to keep your data separate and ensure that each query is restricted to a single node.
-Build your partition keys to account for unbound row growth, to save you from writing too much data into the same partition.</li>
</ul>
",['table']
41554993,41558242,2017-01-09 18:56:47,Cassandra NoHostAvailable and Failed to open native connection to Cassandra,"<p>I'm trying to get cassandra setup, and having some issues where google and other questions here are not helpful. </p>

<p>From cqlsh, I get <code>NoHostAvailable:</code> when I try to query tables after creating them:</p>

<pre><code>Connected to DS Cluster at 10.101.49.129:9042.
[cqlsh 5.0.1 | Cassandra 3.0.9 | CQL spec 3.4.0 | Native protocol v4]
Use HELP for help.
cqlsh&gt; use test;
cqlsh:test&gt; describe kv;

CREATE TABLE test.kv (
    key text PRIMARY KEY,
    value int
) WITH bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';

cqlsh:test&gt; select * from kv;
NoHostAvailable:
</code></pre>

<p>All of the nodes are up and running according to <code>nodetool</code>. </p>

<p>When I try to connect from Spark, I get something similar -- everything works fine I can manipulate and connect to tables, until I try to access any data, and then it fails.</p>

<pre><code>val df = sql.read.format(""org.apache.spark.sql.cassandra"").options(Map(""keyspace"" -&gt; ""test2"", ""table"" -&gt; ""words"")).load
df: org.apache.spark.sql.DataFrame = [word: string, count: int]
df.show
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 25, HOSTNAME): java.io.IOException: Failed to open native connection to Cassandra at {10.101.49.129, 10.101.50.24, 10.101.61.251, 10.101.49.141, 10.101.60.94, 10.101.63.27, 10.101.49.5}:9042
    at com.datastax.spark.connector.cql.CassandraConnector$.com$datastax$spark$connector$cql$CassandraConnector$$createSession(CassandraConnector.scala:162)
    at com.datastax.spark.connector.cql.CassandraConnector$$anonfun$3.apply(CassandraConnector.scala:148)
    at com.datastax.spark.connector.cql.CassandraConnector$$anonfun$3.apply(CassandraConnector.scala:148)
    at com.datastax.spark.connector.cql.RefCountedCache.createNewValueAndKeys(RefCountedCache.scala:31)
    at com.datastax.spark.connector.cql.RefCountedCache.acquire(RefCountedCache.scala:56)
    at com.datastax.spark.connector.cql.CassandraConnector.openSession(CassandraConnector.scala:81)
    at com.datastax.spark.connector.rdd.CassandraTableScanRDD.compute(CassandraTableScanRDD.scala:325)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
...
Caused by: java.lang.NoSuchMethodError: com.google.common.util.concurrent.Futures.withFallback(Lcom/google/common/util/concurrent/ListenableFuture;Lcom/google/common/util/concurrent/FutureFallback;Ljava/util/concurrent/Executor;)Lcom/google/common/util/concurrent/ListenableFuture;
    at com.datastax.driver.core.Connection.initAsync(Connection.java:177)
    at com.datastax.driver.core.Connection$Factory.open(Connection.java:731)
    at com.datastax.driver.core.ControlConnection.tryConnect(ControlConnection.java:251)
</code></pre>

<p>I apologize if this is a naive question, and thank you in advance.</p>
",<apache-spark><cassandra>,"<h2>NoHostAvailable</h2>
<p><a href=""https://docs.datastax.com/en/cassandra/3.0/cassandra/architecture/archDataDistributeReplication.html?hl=replication"" rel=""nofollow noreferrer"">Replication</a> in Cassandra is done via one of two strategies which can be specified on a particular keyspace.</p>
<h3><code>SimpleStrategy</code> :</h3>
<p>Represents a naive approach and spreads data globally among nodes based on the token ranges owned by each node. There is no differentiation between nodes that are in different datacenters.</p>
<p>There is one parameter for <code>SimpleStrategy</code> which chooses how many replicas for any partition will exist within the entire cluster.</p>
<h3><code>NetworkTopologyStrategy</code> :</h3>
<p>Represents a per Datacenter replication strategy. With this strategy data is replicated based on the token ranges owned by nodes but only within a datacenter.</p>
<p>This means that if you have a two datacenters with nodes <code>[Token]</code> and a full range of [0-20]</p>
<pre><code> Datacenter A : [1], [11]
 Datacenter B : [2], [12]
</code></pre>
<p>Then with simple strategy the range would be viewed as being split like this</p>
<pre><code>[1] [2-10] [11] [12 -20]
</code></pre>
<p>Which means we would end up with two very unbalanced nodes which only own a single token.</p>
<p>If instead we use <code>NetworkTopologyStrategy</code> the responsibilities look like</p>
<pre><code>Datacenter A : [1-10], [11-20]
Datacenter B : [2-11], [12-01]
</code></pre>
<p>The strategy itself can be described with a dictionary as a parameter which lists each datacenter and how many replicas should exist in that datacenter.</p>
<p>For example you can set the replication as</p>
<pre><code>'A' : '1'
'B' : '2'
</code></pre>
<p>Which would create 3 replicas for the data, 2 replicas in B but only 1 in A.</p>
<p>This is where a lot of users run into trouble since you could specify</p>
<pre><code>a_mispelled : '4'
</code></pre>
<p>Which would mean that a datacenter which doesn't exist should have replicas for that particular keyspace. Cassandra would then respond whenever doing requests to that keyspace that it could not obtain replicas because it can't find the datacenter.</p>
<p>With <code>VNodes</code> you can get skewed replication (if required) by giving different nodes different numbers of VNodes. Without VNodes it just requires shrinking the ranges covered by nodes which have less capacity.</p>
<h3>How data gets read</h3>
<p>Regardless of the replication, data can be read from any node because the mapping is completely deterministic. Given a keyspace, table and partition key, Cassandra can determine on which nodes any particular token should exist and obtain that information as long as the Consistency Level for the query can be met.</p>
<h2>Guava Error</h2>
<p>The error you are seeing most commonly comes from a bad package of Spark Cassandra Connector being used. There is a difficulty with working with the Java Cassandra Driver and Hadoop since both require different (incompatible) versions of Guava. To get around this the SCC <a href=""https://github.com/datastax/spark-cassandra-connector/blob/master/doc/FAQ.md#how-do-i-fix-guava-classpath-errors"" rel=""nofollow noreferrer"">provides builds</a> with the SCC guava version shaded but re-including the Java Driver as a dependency or using an old build can break things.</p>
",['table']
41558640,41559469,2017-01-09 23:21:25,Cassandra: High volume sensor data clarification,"<p>We are considering transitioning our time series application from SQL Server to Cassandra, as the volume of data is becoming too large for SQL Server to handle. We can have up to 100 sensors working at the same time (sometimes for a whole year, sometimes shorter, but usually at least 50 of them simultaneously), and each of them is capable of transmitting up to 20 different measurements at up to 60 Hz (potentially 120 in the future).</p>

<p>Most online resources (e.g. <a href=""http://academy.datastax.com"" rel=""nofollow noreferrer"">DataStax</a>) recommend partitioning into ""manageable partitions"", which is presumably something below 1,000,000 rows (actually, something below say 50MB is probably the actual metric). So for 1 Hz reporting rates, partitioning each sensor quantity by a single week would produce <code>(7 * 24 * 60 * 60) = 604,800</code> measurements per partition:</p>

<pre><code>CREATE TABLE measurements (
    sensor_id       TEXT,
    quantity        TEXT,
    start_of_week   TIMESTAMP,
    offset_seconds  INT,        -- offset from week start (0..604799)
    value           FLOAT,
    PRIMARY KEY ((sensor_id, quantity, start_of_week), offset_seconds)
) WITH CLUSTERING ORDER BY (offset_seconds DESC)
</code></pre>

<p>So, naturally, for <strong>60 Hz</strong> reporting rates I might partition <strong>by hour</strong> to keep it simple and get <code>(60 * 60 * 60) = 216,000</code> measurements per partition. Or several hours, of course. </p>

<p>However, I have a couple of uncertainties on how this will work in practice.</p>

<p>Until now, we had a rather denormalized SQL Server database, where we would put all 20 values from a single sensor in a single row, and the server was able to keep up (albeit with CPU constantly being ~30%) for up to 50 devices (that's basically 3,000 rows per second, and we presume SQL Server can max at about 10,000 rows/s). Needless to say, this cannot scale at all if new quantities are added per device, while at the same time lots of space is wasted for devices which report less than 20 quantities.</p>

<p>However, with the C* approach above, it seems the number of key-value pairs stored per second (presuming 100 sensors, 20 measurements, 60 Hz) will be <strong>120,000 per second</strong>. </p>

<ol>
<li><p>Is it possible to achieve this with the ""base"" 3 node setup? How many Cassandra nodes are needed in practice, for such insertion rates? </p></li>
<li><p>Would moving all sub-second (60 Hz) values for a single quantity into a single <strong>blob</strong> improve performance? This would mean total insertion rate would be 2,000 <em>blobs</em>, which seems much more manageable (and even a blob size of 240 bytes for 60 float32 values doesn't seem that huge).</p></li>
</ol>

<p>Most of the time the data will be displayed from a different table which holds pre-calculated min/max/average aggregates (the user can create a full-resolution range query at any time, but for smaller ranges), so our emphasis is to <strong>maximize write throughput</strong>. If you believe any other schema might provide better throughput (I don't have a clue, perhaps multiple tables/some other partitioning/clustering strategies), please suggest. We are even open to switching to a different NoSQL database, if it can fit our requirements better.</p>
",<cassandra><time-series><nosql>,"<blockquote>
  <p>Is it possible to achieve this with the ""base"" 3 node setup? How many Cassandra nodes are needed in practice, for such insertion rates?</p>
</blockquote>

<p>120,000 per second with a 3 node cluster is certainly possible but it depends a lot on your hardware. I would expect 20k-30k/sec with normal instances. A rule of thumb I stick with is 4k/sec/core but its very hardware/load dependent.</p>

<blockquote>
  <p>Would moving all sub-second (60 Hz) values for a single quantity into a single blob improve performance? This would mean total insertion rate would be 2,000 blobs, which seems much more manageable (and even a blob size of 240 bytes for 60 float32 values doesn't seem that huge).</p>
</blockquote>

<p>Yes it will help throughput.</p>

<p>Some notes:</p>

<p>Make sure with this kind of load your client is performing your queries efficiently or you may see bottle necks there. Making inserts async and applying back pressure based on in flight requests is a good idea. There is batching, but its only efficient if all the writes go into same partition. So if you have 60 readings for the second and are inserting them all into the same partition it will be faster to do an <strong>unlogged</strong> batch. Logged batches will be massively slower. There is a point that this ends up becoming worse for performance though but the number is different for different workloads so may want to play with it.</p>

<p>I would recommend Timed Window Compaction Strategy (TWCS) for this workload.</p>

<p>In the 2.x versions 100mb per partition is a good limit to try to stay under. In later 3.x releases and 4.x future releases you are much safer going higher yet (ie gb). That said partitions are still the lowest level so if they get too large it can still lead to hot spots, and it makes things like repairs much less efficient. Keep eye on the max/mean table partition size in monitoring.</p>

<p>With <code>offset_seconds</code> that is actually better to be a full timestamp. When you do a read it checks the min/max clustering keys vs whats queried to determine which sstable to read. Each sstable stores its min/max clustering key to optimize time series like this. If you have TWCS and you are querying a range of time it will be able to narrow down the sstables to just the relevant ones. Otherwise it will likely include a lot of them since the offsets will reset in every bucket and have overlaps. The bloom filter should catch some of the ones with different partition keys but it will have false positives and requires checking the bloom filters on all the sstables (this is done after picking from min/max and is far from free). It may seem a bit wasteful for space but it will improve reads a lot.</p>
",['table']
41572333,41621509,2017-01-10 15:26:00,CQL from Cassandra Model (datastax driver: python),"<p>Datastax Cassandra Driver (Python): Is there a way to generate CQL files from the data models we create? As an example if I have a data model:</p>

<pre><code>class ExampleModel(Model):
example_id      = columns.UUID(primary_key=True, default=uuid.uuid4)
example_type    = columns.Integer(index=True)
created_at      = columns.DateTime()
description     = columns.Text(required=False)
</code></pre>

<p>I know I can create table in Cassandra using the <code>sync_table</code> but my aim here is to derive the equivalent CQL statements for the create table statement generated from the model above. Is there a way?</p>
",<python><cassandra>,"<p>If you are okay creating the table, the most robust way would be to sync the table, then use the driver's metadata API to reproduce the string.</p>

<p><a href=""http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.metadata"" rel=""nofollow noreferrer"">Cluster.metadata.keyspaces[ks].tables[t]</a>
and
<a href=""http://datastax.github.io/python-driver/api/cassandra/metadata.html#cassandra.metadata.TableMetadata.export_as_string"" rel=""nofollow noreferrer"">TableMetadata.export_as_string</a></p>

<p>This is the exact code that generates the <code>desc</code> output for cqlsh.</p>

<p>Alternatively, you can avoid creating the table by using the cqlengine management function that generates CQL from these models:</p>

<p><a href=""https://github.com/datastax/python-driver/blob/master/cassandra/cqlengine/management.py#L389"" rel=""nofollow noreferrer"">cassandra.cqlengine.management._get_create_table</a></p>

<p>Note that this is not part of the API, and very likely to change as it is replaced with the core driver metadata equivalent.</p>
",['table']
41575900,41816383,2017-01-10 18:32:09,Accessing cassandra without hardcoded username password,"<p>I have an existing Datastax Cassandra setup that is working. We just added authentication to the system and now we can log in with our AD accounts. This is very nice and certainly works. However applications need to use a hard-coded username/password in order to connect.</p>

<p>In SQL Server we were able to setup a user to run the service as and then it would connect and work through AD. However in Cassandra it is not the same.</p>

<p>If I don't want to include usernames and especially passwords in my app.config files what are my options?</p>
",<windows><authentication><cassandra><datastax-enterprise>,"<p>You can use authentication via LDAP with DSE (Datastax Enterprise), so the authentication stage is done with LDAP instead of the internal authentication in DSE which you're using at the moment. Note that my comments here apply to DSE5.0 onwards but you can use LDAP auth with earlier versions of DSE from 4.6 onwards.</p>

<p>The documentation (link below) covers this. The basic steps are as follows:</p>

<ol>
<li><p>Configure your authenticator in the <code>cassandra.yaml</code> to use the DSE authenticator</p>

<p><code>authenticator: com.datastax.bdp.cassandra.auth.DseAuthenticator</code></p></li>
<li><p>Create an internal role in cassandra to map to the LDAP group(s) in your LDAP server using the <code>CREATE ROLE</code> command</p></li>
<li><p>Ensure all the users you need to use map to the relevant LDAP group (part of your LDAP config)</p></li>
<li><p>Configure your <code>dse.yaml</code> to have the correct settings for your LDAP server</p></li>
<li><p>Restart the DSE process for the settings to take effect</p></li>
</ol>

<p>The following documentation gives some good examples and background information:</p>

<p><a href=""https://docs.datastax.com/en/latest-dse/datastax_enterprise/unifiedAuth/unifiedAuthConfig.html"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/latest-dse/datastax_enterprise/unifiedAuth/unifiedAuthConfig.html</a></p>

<p><a href=""https://docs.datastax.com/en/latest-dse/datastax_enterprise/sec/authLdapConfig.html"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/latest-dse/datastax_enterprise/sec/authLdapConfig.html</a></p>

<p>Note: when configuring the <code>dse.yaml</code> note the comment in the docs regarding <code>user_search_filter</code>:</p>

<blockquote>
  <p>When using Active Directory set the filter to <code>(sAMAccountName={0})</code></p>
</blockquote>
",['authenticator']
41578751,41583779,2017-01-10 21:28:20,Cassandra: modelling a small table without partitioning,"<p>I want to model a small table using Cassandra (purely for convenience reasons) that puts all the data on a single node (and possibly replicated data on some other nodes). The reason for this is that I want to do a lot of <code>SELECT * FROM my_table</code> queries from it afterwards.</p>

<p>I know that there is a way to achieve this by creating a table with a constant Partition Key and putting my current Primary Key into Clustering Key (Columns), but this feels very hacky. This would superficially store the whole table on r nodes, where r = replication factor.</p>

<p>Example:
Current table with <code>Primary Key (param1)</code>. Move this to <code>Primary Key ('some constant', param1)</code>.</p>

<p>Is there a better way of achieving this, e.g. using some Cassandra table or keyspace configuration that I missed?</p>
",<cassandra><cql>,"<blockquote>
  <p>I know that there is a way to achieve this by creating a table with a
  constant Partition Key and putting my current Primary Key into
  Clustering Key (Columns), but this feels very hacky. This would
  superficially store the whole table on r nodes, where r = replication
  factor.</p>
</blockquote>

<p>I disagree that this is hacky or a superficial solution. A key part of the Cassandra design is to choose an appropriate partition key so that all your data for a single query is in a single partition. In your case, if you want all of your table data to be read then its perfectly reasonably to have a constant partition key. </p>

<p>If there's no field in your dataset that will be constant, then using an arbitrary value (a partition ID) is fine. This is basically the same as adding an arbitrary value as an additional clustering column in your schema to bucket your data, which is a very common use case. </p>

<p>To answer your question directly, no I don't think there are settings to achieve what you want. </p>
",['table']
41579551,41586865,2017-01-10 22:27:14,Cassandra help : Supporting fast queries using either part of composite key,"<p>I'm new to Cassandra and was unclear on the best way to store my data to support my query needs.  I want to be able to search my data based on either of the keys, or both.  To illustrate I will use this table example:</p>

<pre><code>CREATE TABLE temperature (
weatherstation_id text,
event_time timestamp,
temperature text,
PRIMARY KEY (weatherstation_id,event_time)
);
</code></pre>

<p>This works great for queries like these two:</p>

<pre><code>SELECT event_time,temperature FROM temperature WHERE weatherstation_id=’1234ABCD’;
</code></pre>

<p>...because it goes directly to a single partition</p>

<pre><code>SELECT temperature FROM temperature WHERE weatherstation_id=’1234ABCD’ AND event_time &gt; ’2013-04-03 07:01:00′ AND event_time &lt; ’2013-04-03 07:04:00′;
</code></pre>

<p>...because its still going to a single partition and getting a slice of results from an ordered list</p>

<p>However what if I wanted to something like this:</p>

<pre><code>SELECT temperature FROM temperature WHERE event_time &gt; ’2013-04-03 07:01:00′ AND event_time &lt; ’2013-04-03 07:04:00′;
</code></pre>

<p>If my understanding serves me right, wouldn't this be inefficient since it would need to iterate over every partition?  Not only that but it would then need to be resorted to get it back in time order.</p>

<p>What's the best design to get around this?</p>
",<cassandra>,"<p>Actually your query:</p>

<pre><code>SELECT temperature FROM temperature WHERE event_time &gt; ’2013-04-03 07:01:00′ AND event_time &lt; ’2013-04-03 07:04:00′;
</code></pre>

<p>will fail to run. Cassandra really <strong>must</strong> know in which partition has to look for the data you're requesting, that is you always <strong>must</strong> specify the partition key. </p>

<p>In order to efficiently retrieve data for this query you need to model your data around that query too:</p>

<pre><code>CREATE TABLE temperature_by_time (
    granularity timestamp,
    event_time timestamp,
    weatherstation_id text,
    temperature text,
    PRIMARY KEY (granularity, event_time)    
);
</code></pre>

<p>Here I added the field <code>granularity</code>. This field allows you to control how wide your partitions will get. A good rule of thumb is to have at most around 10k-100k rows in each partition. Depending on how fast you write to this table you can proceed in different ways. Examples:</p>

<hr>

<h2>Case 1</h2>

<ul>
<li>You have 10 sensors</li>
<li>Each sensor gives 1 measure every second</li>
</ul>

<p>In this case you're going to write 10 measures/second, 36k measures/hour. A good granularity value is then something like <code>yyyy-mm-dd HH:00:00</code>, that is you partition your data on hour-by-hour basis:</p>

<pre><code>INSERT INTO temperature_by_time (granularity, event_time, ..) VALUES ('2017-01-11 10:00:00', '2017-01-11 10:05:01', ...);
INSERT INTO temperature_by_time (granularity, event_time, ..) VALUES ('2017-01-11 10:00:00', '2017-01-11 10:19:15', ...);
INSERT INTO temperature_by_time (granularity, event_time, ..) VALUES ('2017-01-11 10:00:00', '2017-01-11 10:39:35', ...);
INSERT INTO temperature_by_time (granularity, event_time, ..) VALUES ('2017-01-11 10:00:00', '2017-01-11 10:59:49', ...);

SELECT * FROM temperature_by_time WHERE granularity='2017-01-11 10:00:00';
SELECT * FROM temperature_by_time WHERE granularity='2017-01-11 10:00:00' AND event_time &gt;= '2017-01-1 10:30:00' AND event_time &lt; '2017-01-1 11:00:00';
</code></pre>

<p>that is you ""truncate"" the <code>event_time</code> to the integer hour, and can get records on hour-per-hour only.</p>

<hr>

<h2>Case 2</h2>

<ul>
<li>You have 100 sensors</li>
<li>Each sensor gives 1 measure every second</li>
</ul>

<p>In this case you're going to write 100 measures/second, 360k measures/hour. Good granularity values are then something like <code>yyyy-mm-dd HH:00:00</code>, <code>yyyy-mm-dd HH:15:00</code>, <code>yyyy-mm-dd HH:30:00</code>, <code>yyyy-mm-dd HH:45:00</code>, that is you partition your data on quarters of an hour basis:</p>

<pre><code>INSERT INTO temperature_by_time (granularity, event_time, ..) VALUES ('2017-01-11 10:00:00', '2017-01-11 10:05:01', ...);
INSERT INTO temperature_by_time (granularity, event_time, ..) VALUES ('2017-01-11 10:15:00', '2017-01-11 10:19:15', ...);
INSERT INTO temperature_by_time (granularity, event_time, ..) VALUES ('2017-01-11 10:30:00', '2017-01-11 10:39:35', ...);
INSERT INTO temperature_by_time (granularity, event_time, ..) VALUES ('2017-01-11 10:45:00', '2017-01-11 10:59:49', ...);

SELECT * FROM temperature_by_time WHERE granularity='2017-01-11 10:00:00';
SELECT * FROM temperature_by_time WHERE granularity='2017-01-11 10:00:00' AND event_time &gt;= '2017-01-1 10:30:00' AND event_time &lt; '2017-01-1 10:33:00';
</code></pre>

<p>that is you ""truncate"" the <code>event_time</code> to the quarter of the hour, and can get records on quarters of an hour only.</p>

<hr>

<h2>Case 3</h2>

<p>You already know how to proceed...</p>
",['table']
41582657,41584807,2017-01-11 04:22:05,Spark read() works but sql() throws Database not found,"<p>I'm using Spark 2.1 to read data from Cassandra in Java.
I tried the code posted in <a href=""https://stackoverflow.com/a/39890996/1151472"">https://stackoverflow.com/a/39890996/1151472</a> (with SparkSession) and it worked. However when I replaced spark.read() method with spark.sql() one, the following exception is thrown:</p>

<pre><code>Exception in thread ""main"" org.apache.spark.sql.AnalysisException: Table or view not found: `wiki`.`treated_article`; line 1 pos 14;
'Project [*]
+- 'UnresolvedRelation `wiki`.`treated_article`

    at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
</code></pre>

<p>I'm using same spark configuration for both read and sql methods</p>

<p>read() code:
Dataset dataset = </p>

<pre><code>spark.read().format(""org.apache.spark.sql.cassandra"")
                .options(new HashMap&lt;String, String&gt;() {
                    {
                        put(""keyspace"", ""wiki"");
                        put(""table"", ""treated_article"");
                    }
                }).load();
</code></pre>

<p>sql() code:</p>

<pre><code>spark.sql(""SELECT * FROM WIKI.TREATED_ARTICLE"");
</code></pre>
",<java><apache-spark><cassandra><apache-spark-sql>,"<p>Spark Sql uses a <code>Catalogue</code> to look up database and table references. When you write in a table identifier that isn't in the catalogue it will throw errors like the one you posted. The <code>read</code> command doesn't require a catalogue since you are required to specify all of the relevant information in the invocation.</p>

<p>You can add entries to the catalogue either by </p>

<h3>Registering DataSets as Views</h3>

<p>First create your DataSet</p>

<pre><code>spark.read().format(""org.apache.spark.sql.cassandra"")
                .options(new HashMap&lt;String, String&gt;() {
                    {
                        put(""keyspace"", ""wiki"");
                        put(""table"", ""treated_article"");
                    }
                }).load();
</code></pre>

<p>Then use one of the catalogue registry functions</p>

<pre><code>void    createGlobalTempView(String viewName)
Creates a global temporary view using the given name.
void    createOrReplaceTempView(String viewName)
Creates a local temporary view using the given name.
void    createTempView(String viewName)
Creates a local temporary view using the given name
</code></pre>

<h3>OR Using a SQL Create Statement</h3>

<pre><code>   CREATE TEMPORARY VIEW words
     USING org.apache.spark.sql.cassandra
     OPTIONS (
       table ""words"",
       keyspace ""test"",
       cluster ""Test Cluster"",
       pushdown ""true""
     )
</code></pre>

<p>Once added to the catalogue by either of these methods you can reference the table in all <code>sql</code> calls issued by that context. </p>

<hr>

<p>Example</p>

<pre><code>CREATE TEMPORARY VIEW words
  USING org.apache.spark.sql.cassandra
  OPTIONS (
    table ""words"",
    keyspace ""test""
  );

SELECT * FROM words;
// Hello    1
// World    2
</code></pre>

<hr>

<p>The Datastax (My employer) Enterprise software automatically registers all Cassandra tables by placing entries in the Hive Metastore used by Spark as a Catalogue. This makes all tables accessible without manual registration.</p>

<p>This method allows for select statements to be used without an accompanying <code>CREATE VIEW</code></p>
",['table']
41587806,41590463,2017-01-11 10:03:31,cassandra sharding and replication,"<p>I am new to Cassandra was going though <a href=""https://academy.datastax.com/resources/brief-introduction-apache-cassandra"" rel=""noreferrer"">this Article</a> explaining sharding and replication and I am stuck at a point that is - </p>

<p>I have a cluster with 6 Cassandra nodes configured at my local machine. I create a new keyspace ""TestKeySpace"" with replication factor as 6 and a table in keyspace ""employee"" and primary key is auto-increment-number named RID.
I am not able to understand how this data will be partitioned and replicated. What I want to know is since I am keeping my replication factor to be 6, and data will be distributed on multiple nodes, then will each node will be having exactly same data as the other nodes or not?</p>

<p>What If my cluster has following configuration - </p>

<pre><code>    Number of nodes - 6 (n1, n2 ,n3, n4, n5 and n6).
    replication_factor - 3. 
</code></pre>

<p>How can I determine that for any one node (let say n1), on which other two nodes the data is replicated and which other nodes are behaving as different shards.</p>

<p>Thanks in Advance.</p>

<p>Regards,
Vibhav</p>

<p>PS - If anybody down votes this question kindly do mention in comments what went wrong.</p>
",<cassandra><replication><database-replication><sharding>,"<p>I will explain this with simple example.
A keyspace in cassandra is equivalent to database schema name in RDBMS.</p>

<p>First create a keyspace - </p>

<pre><code>CREATE KEYSPACE MYKEYSPACE WITH REPLICATION = { 
 'class' : 'SimpleStrategy', 
 'replication_factor' : 3 
};
</code></pre>

<p>Lets create a simple table -</p>

<pre><code>CREATE TABLE USER_BY_USERID(
 userid int,
 name text,
 email text,
 PRIMARY KEY(userid, name)
) WITH CLUSTERING ORDER BY(name  DESC);
</code></pre>

<p>In this example, <code>userid</code> is your partition key and name is clustering key. Partition is also called row key, this key determines on which node row will be saved.</p>

<p>Your first question - </p>

<blockquote>
  <p>I am not able to understand how this data will be partitioned? </p>
</blockquote>

<p>Data will be partitioned based on your partition key. By default C* uses <code>Murmur3partitioner</code>. You can change the partitioner in  cassandra.yaml  configuration file. How partitions happens, is also depends on your configuration. You can specify range of tokens for each node, for example take a look at below cassandra.yaml configuration file. I have specified 6 node form your question.</p>

<p>cassandra.yaml for Node 0:</p>

<pre><code>cluster_name: 'MyCluster'
initial_token: 0
seed_provider:
    - seeds:  ""198.211.xxx.0""
listen_address: 198.211.xxx.0
rpc_address: 0.0.0.0
endpoint_snitch: RackInferringSnitch
</code></pre>

<p>cassandra.yaml for Node 1:</p>

<pre><code>cluster_name: 'MyCluster'
initial_token: 3074457345618258602
seed_provider:
    - seeds:  ""198.211.xxx.0""
listen_address: 192.241.xxx.0
rpc_address: 0.0.0.0
endpoint_snitch: RackInferringSnitch
</code></pre>

<p>cassandra.yaml for Node 2:</p>

<pre><code>cluster_name: 'MyCluster'
initial_token: 6148914691236517205
seed_provider:
    - seeds:  ""198.211.xxx.0""
listen_address: 37.139.xxx.0
rpc_address: 0.0.0.0
endpoint_snitch: RackInferringSnitch
</code></pre>

<p>.......Node3 ...... Node4 ....</p>

<p>cassandra.yaml for Node 5:</p>

<pre><code>cluster_name: 'MyCluster'
initial_token: {some large number}
seed_provider:
    - seeds:  ""198.211.xxx.0""
listen_address: 37.139.xxx.0
rpc_address: 0.0.0.0
endpoint_snitch: RackInferringSnitch
</code></pre>

<p>lets take this insert statement -</p>

<pre><code>INSERT INTO USER_BY_USERID VALUES(
 1,
 ""Darth Veder"",
 ""darthveder@star-wars.com""
);
</code></pre>

<p>Partitioner will calculate the hash of the PARTITION key (in above example userid - 1), and decides which node this row will be saved. Lets say calculated hash is something 12345, this row will be saved at Node 0 (look for the initial_token value for Node0 in above configuration).</p>

<p>Complete cassandra.yaml configuration <a href=""https://docs.datastax.com/en/cassandra/2.1/cassandra/configuration/configCassandra_yaml_r.html"" rel=""noreferrer"">configCassandra_yaml_r</a></p>

<p>You can go through this  <a href=""https://docs.datastax.com/en/datastax_enterprise/4.7/datastax_enterprise/deploy/deployCalcTokens.html"" rel=""noreferrer"">deployCalcTokens</a> to know how to generate tokens.</p>

<p>Second question - </p>

<blockquote>
  <p>how data gets replicated?</p>
</blockquote>

<p>Depending on your replication strategy and replication factor, the data gets replicated on each node. you have to specify Replication factor and replication strategy while creating keyspace.
For example, in above example, I have used <code>SimpleStrategy</code> as replication strategy. This strategy is suitable for small cluster. For geologically distributed application you can use <code>NetworkTopologyStrategy</code>. replication_factor specifies, how many copies of a row to be created, in this example three copies of each row will be created. With simple strategy, cassandra will use clockwise direction to copy the row. </p>

<p>In above example, the row is saved at Node0 and the same node gets copied on Node1 and Node2. 
Let's take another example -</p>

<pre><code>INSERT INTO USER_BY_USERID VALUES(
 448454,
 ""Obi wan kenobi"",
 ""obiwankenobi@star-wars.com""
);
</code></pre>

<p>For user id 448454, the calculated hash is say 3074457345618258609, so this row will be save at Node2 (look for the initial_token value for node 2 in above configuration) and also get copied in clockwise direction to Node3 and Node4 (remember we have specified replication factor of 3, so only three copies Noe2, Node3, Node4). </p>

<p>Hope this helps.</p>
","['table', 'partitioner', 'initial_token']"
41678142,41686156,2017-01-16 14:02:35,How are nodes decided for replication in Cassandra,"<p>I am trying to understand how exactly data is replicated on multiple nodes in Cassandra. Lets assume we have 6 nodes and replication factor is 3. For all simplicity, lets assume single datacenter and single rack. Since RF is 3,data is stored in 3 replicas. I want to understand how the 3 replicas are decided.</p>

<p>Referring to example in <a href=""http://www.datastax.com/dev/blog/virtual-nodes-in-cassandra-1-2"" rel=""nofollow noreferrer"">http://www.datastax.com/dev/blog/virtual-nodes-in-cassandra-1-2</a> (first image second part i.e, with virtual nodes), lets say our row falls under virtual node 'E' as decided by partitioner. So the row must be present in Node 1, 5, 6 according to distribution of virtual nodes among different nodes.</p>

<p>But coming to documentation - <a href=""http://docs.datastax.com/en/cassandra/2.1/cassandra/architecture/architectureDataDistributeReplication_c.html"" rel=""nofollow noreferrer"">http://docs.datastax.com/en/cassandra/2.1/cassandra/architecture/architectureDataDistributeReplication_c.html</a> , it says even in simple case of SimpleStrategy, first replica on a node is determined by the partitioner. Additional replicas are placed on the next nodes clockwise to the ring.  So will data be stored in E, F, G virtual nodes or may be Node 1, 2, 3 ? </p>

<p>Which one is correct ? 1st link or documentation ? </p>

<p>Thanks!</p>
",<cassandra><database-replication>,"<p>And if it really interest you where your partition data ends up in the cluster you can use:</p>

<pre><code>nodetool getendpoints
</code></pre>

<p><a href=""https://docs.datastax.com/en/cassandra/2.1/cassandra/tools/toolsGetEndPoints.html"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/cassandra/2.1/cassandra/tools/toolsGetEndPoints.html</a></p>

<p>Please take into account that documentation is simplified so that people understand it easier when seeing for the first time. In reality it's consistent hashing on steroids.</p>

<p>Previously every node had a single token and tokens were boundaries on ring that was used for consistent hashing. Basically you had a whole range divided into number of nodes that you had in the cluster. When you needed to do an operation on some partition, you took partition key, hashed it and then you knew to which node to go to. Basically after hashing you get a number in a range of -2^63 to 2^63 - 1. Then you go clockwise on the ring until you ""find"" a marker and this is how you know to which node a partition belongs initially. If you have greater replication factor, you just continue going clockwise on the ring until you ""find"" all the nodes that you need to satisfy replication factor. And this is how you know what nodes in the cluster have your partition.</p>

<p>With virtual nodes there is a property num_tokens and every node selects that many random tokens (In range previously mentioned) when joining the ring and they are then used for consistent hashing. Basically every node then sees that new node wants to have portions of the ring and streams the data to it. Also when new writes comes in they are sent to the new node that is going to own them (until the node fully joins the ring, it's responses are ignored when counted up for consistency levels).</p>

<p>This is how it was before (single token per node in cluster):
<a href=""https://i.stack.imgur.com/Mkme8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Mkme8.png"" alt=""Standard Consistent Hashing""></a></p>

<p>This is how the ring looks like with virtual nodes:
<a href=""https://i.stack.imgur.com/l977w.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/l977w.jpg"" alt=""Consistent Hashing with vnodes""></a></p>

<p>Absolutely the same rules apply with virtual nodes and ordinary consistent hashing, you go around the ring to select
the replicas. If during your going around the ring you stumble upon the same node again you just skip it and continue until you find all the nodes that own the data according to the replication factor that you desire.</p>
",['num_tokens']
41768069,41793745,2017-01-20 16:19:59,Get latest entry per group in Cassandra,"<p>As per Gunwant's request, I wanted to provide more information about my problem.</p>

<p>I have a database with >10^7 rows. Each row is a product with many different attributes (columns), e.g. title, description, price, weight, color, volume, warehouse_location and many more. However, all of these attributes are subject to change - the price might go up or down, the description might change, it might be moved to another location in the warehouse, etc. All data is stored historically, e.g.:</p>

<pre><code>description |       date | price | warehouse_location |  color
   Cucumber | 2017-01-14 |    50 |                23A |  green
   Cucumber | 2017-01-16 |    55 |                23A |  green
   Cucumber | 2017-01-19 |    52 |                14B |  green
  Pineapple | 2017-01-12 |    80 |                23A | yellow
  Pineapple | 2017-01-17 |    75 |                23A | yellow
  Pineapple | 2017-01-22 |    80 |                23A | yellow
      Lemon | 2017-01-18 |    60 |                 9C | yellow
      Lemon | 2017-01-19 |    70 |                33E | yellow
      Lemon | 2017-01-20 |    80 |                 9A | yellow
</code></pre>

<p>I now want to create arbitrary reports, I need to be able to filter every column.</p>

<p>For example: Price of all objects with warehouse_location 23A from 2017-01-12 to 2017-01-18. If the same object has multiple matches for a given query, only the most recent entry within that timespan should be returned. In this case, ""55"" should be returned for ""Cucumber"" and ""75"" for pineapple.</p>

<p>I need to be able to filter multiple columns at once. Another example would be ""Color of all objects with price > 60 and price &lt; 90 and date > 2017-01-11 and date &lt; 2017-01-22"" which should return { yellow; yellow } for the above dataset.</p>

<p>Original question:</p>

<p>I want to store historical data in a Cassandra database:</p>

<pre><code>objectid |       date | price | foo
       1 | 2017-01-18 |   200 |   A
       1 | 2017-01-19 |   300 |   A
       1 | 2017-01-20 |   400 |   B
       2 | 2017-01-18 |   100 |   C
       2 | 2017-01-19 |   150 |   C
       2 | 2017-01-20 |   200 |   D
       3 | 2017-01-18 |   400 |   E
       3 | 2017-01-19 |   350 |   E
       3 | 2017-01-20 |   300 |   F
</code></pre>

<p>I now want to select the latest entry for the ""foo"" column of each object that satisfies a condition. For example, for the query price between 300 and 500 I want to get the following:</p>

<pre><code>objectid |       date | price | foo
       1 | 2017-01-20 |   400 |   B
       3 | 2017-01-18 |   400 |   E
</code></pre>

<p>Are queries like these possible in Cassandra?</p>

<p>Edit:
Thanks everyone for your effort. Marko Švaljek's answer seems to work if you only want to get unique values of foo. In my use case, I have dozens of different ""foo columns"" and >10^7 rows. I apparantly would have to create hundreds of different ""report"" tables to allow arbitrary filtering - I'm not sure if Cassandra is the right solution for that use case.</p>
",<database><cassandra>,"<p>As always with cassandra you would need to denormalize this. I will assume
your base table is something like following:</p>

<pre><code>create table base (
    objectid int,
    date timestamp,
    price int,
    foo text,
    primary key (objectid, date)
);
</code></pre>

<p>please be very careful with this create statements because
historical data usually grows way above 100 000'ts</p>

<p>Then I created following insert statements:</p>

<pre><code> insert into base (objectid, date, price, foo) values (1, '2017-01-18', 200, 'A');
 insert into base (objectid, date, price, foo) values (1, '2017-01-19', 300, 'A');
 insert into base (objectid, date, price, foo) values (1, '2017-01-20', 400, 'B');
 insert into base (objectid, date, price, foo) values (2, '2017-01-18', 100, 'C');
 insert into base (objectid, date, price, foo) values (2, '2017-01-19', 150, 'C');
 insert into base (objectid, date, price, foo) values (2, '2017-01-20', 200, 'D');
 insert into base (objectid, date, price, foo) values (3, '2017-01-18', 400, 'E');
 insert into base (objectid, date, price, foo) values (3, '2017-01-19', 350, 'E');
 insert into base (objectid, date, price, foo) values (3, '2017-01-20', 300, 'F');
</code></pre>

<p>It's not possible to get the query you want out of the box. But you can go
around it.</p>

<p>You need to create another table:</p>

<pre><code>create table report (
    report text,
    price int,
    objectid int,
    date timestamp,
    foo text,
    primary key (report, price, foo)
);

-- in cassandra if you want to search for something it has to go into clustering columns
-- and price is your first goal ... foo is there just for uniqueness 
-- now you do inserts with data that you have above
-- perfectly o.k. to create multiple inserts in cassandra 
insert into report (report, objectid, date, price, foo) values ('latest', 1, '2017-01-18', 200, 'A');
insert into report (report, objectid, date, price, foo) values ('latest', 1, '2017-01-19', 300, 'A');
insert into report (report, objectid, date, price, foo) values ('latest', 1, '2017-01-20', 400, 'B');
insert into report (report, objectid, date, price, foo) values ('latest', 2, '2017-01-18', 100, 'C');
insert into report (report, objectid, date, price, foo) values ('latest', 2, '2017-01-19', 150, 'C');
insert into report (report, objectid, date, price, foo) values ('latest', 2, '2017-01-20', 200, 'D');
insert into report (report, objectid, date, price, foo) values ('latest', 3, '2017-01-18', 400, 'E');
insert into report (report, objectid, date, price, foo) values ('latest', 3, '2017-01-19', 350, 'E');
insert into report (report, objectid, date, price, foo) values ('latest', 3, '2017-01-20', 300, 'F');
</code></pre>

<p>This would return you:</p>

<pre><code>select objectid, date, price, foo from report where report='latest' and price &gt; 300 and price &lt; 500;

 objectid | date                            | price | foo
----------+---------------------------------+-------+-----
        3 | 2017-01-18 23:00:00.000000+0000 |   350 |   E
        1 | 2017-01-19 23:00:00.000000+0000 |   400 |   B
        3 | 2017-01-17 23:00:00.000000+0000 |   400 |   E
</code></pre>

<p>And this is not what you want. You have couple of options now.</p>

<p>Basically if you exclude price from primary key you will get:</p>

<pre><code>create table report2 (
    report text,
    price int,
    objectid int,
    date timestamp,
    foo text,
    primary key (report, foo)
 );

insert into report2 (report, objectid, date, price, foo) values ('latest', 1, '2017-01-18', 200, 'A');
insert into report2 (report, objectid, date, price, foo) values ('latest', 1, '2017-01-19', 300, 'A');
insert into report2 (report, objectid, date, price, foo) values ('latest', 1, '2017-01-20', 400, 'B');
insert into report2 (report, objectid, date, price, foo) values ('latest', 2, '2017-01-18', 100, 'C');
insert into report2 (report, objectid, date, price, foo) values ('latest', 2, '2017-01-19', 150, 'C');
insert into report2 (report, objectid, date, price, foo) values ('latest', 2, '2017-01-20', 200, 'D');
insert into report2 (report, objectid, date, price, foo) values ('latest', 3, '2017-01-18', 400, 'E');
insert into report2 (report, objectid, date, price, foo) values ('latest', 3, '2017-01-19', 350, 'E');
insert into report2 (report, objectid, date, price, foo) values ('latest', 3, '2017-01-20', 300, 'F');

select objectid, date, price, foo from report2 where report='latest';

 objectid | date                            | price | foo
----------+---------------------------------+-------+-----
        1 | 2017-01-18 23:00:00.000000+0000 |   300 |   A
        1 | 2017-01-19 23:00:00.000000+0000 |   400 |   B
        2 | 2017-01-18 23:00:00.000000+0000 |   150 |   C
        2 | 2017-01-19 23:00:00.000000+0000 |   200 |   D
        3 | 2017-01-18 23:00:00.000000+0000 |   350 |   E
        3 | 2017-01-19 23:00:00.000000+0000 |   300 |   F
</code></pre>

<p>If you don't have too much foo, you might get away with it by filtering it on the client side, but most
of the time this is an anti pattern.</p>

<p>you can also make it with query:</p>

<pre><code>select objectid, date, price, foo from report2 where report='latest' and price &gt; 300 and price &lt; 500 allow filtering;


 objectid | date                            | price | foo
----------+---------------------------------+-------+-----
        1 | 2017-01-19 23:00:00.000000+0000 |   400 |   B
        3 | 2017-01-18 23:00:00.000000+0000 |   350 |   E
</code></pre>

<p>Which is not ideal, but it kind of works.</p>

<p>The reason I'm creating partition latest is so that the partition remains on the same host. Depending
on the workload you get this might become a hot row for you.</p>

<p>This is more or less the relational side of the story ...</p>

<p>If you are truly working with cassandra you have to prepare the view up front. So you would have report 2
but would insert the data for every statistical group you want to get out i.e.</p>

<pre><code>insert into report2 (report, objectid, date, price, foo) values ('300-500', 1, '2017-01-19', 300, 'A');
... and so on
</code></pre>

<p>and then you would do:</p>

<pre><code>select objectid, date, price, foo from report2 where report='300-500'
</code></pre>

<p>But I guess you want to set the ranges dynamically so this is not what you want. This is more or less what basic cassandra does.</p>

<p>Then there are always materialized views (at the moment they have some issues) personally I wouldn't use them with some super important reports.</p>

<p>And if the access pattern is unknown, there is always apache spark or some scripting solution that check the data and creates views you need.</p>
",['table']
41776345,41807621,2017-01-21 06:21:01,Cassandra failed to connect,"<p>I'm newbie in cassandra apache. In the tutorial video, it says type bin/nodetools status to check the status of node but when I tried to input it. Terminal returns </p>

<pre><code>Failed to connect to '127.0.0.1:7199' - ConnectException: 'Connection
refused (Connection refused)'.
</code></pre>

<p><a href=""https://i.stack.imgur.com/pUA1s.png"" rel=""nofollow noreferrer"">Check this image</a></p>

<p>I tried to change JVM_OPTS to ""$JVM_OPTS -Djava.rmi.server.hostname=localhost"" in cassandra-env.sh
but still can't connect.
What I gonna do to fix this error?</p>

<p>Debug.logs</p>

<pre><code>DEBUG [main] 2017-01-21 13:57:48,095 ColumnFamilyStore.java:881 - Enqueuing flush of local: 38.338KiB (0%) on-heap, 0.000KiB (0%) off-heap
DEBUG [PerDiskMemtableFlushWriter_0:1] 2017-01-21 13:57:48,167 Memtable.java:435 - Writing Memtable-local@858986260(8.879KiB serialized bytes, 1 ops, 0%/0% of on/off-heap limit), flushed range = (min(-9223372036854775808), max(9223372036854775807)]
DEBUG [PerDiskMemtableFlushWriter_0:1] 2017-01-21 13:57:48,168 Memtable.java:464 - Completed flushing /usr/lib/cassandra/apache-cassandra-3.9/data/data/system/local-7ad54392bcdd35a684174e047860b377/mc-56-big-Data.db (5.367KiB) for commitlog position CommitLogPosition(segmentId=1484978256521, position=32861)
DEBUG [MemtableFlushWriter:1] 2017-01-21 13:57:48,471 ColumnFamilyStore.java:1184 - Flushed to [BigTableReader(path='/usr/lib/cassandra/apache-cassandra-3.9/data/data/system/local-7ad54392bcdd35a684174e047860b377/mc-56-big-Data.db')] (1 sstables, 9.527KiB), biggest 9.527KiB, smallest 9.527KiB
DEBUG [CompactionExecutor:1] 2017-01-21 13:57:48,472 CompactionTask.java:150 - Compacting (896b3470-df9e-11e6-9508-7dc463a45cc9) [/usr/lib/cassandra/apache-cassandra-3.9/data/data/system/local-7ad54392bcdd35a684174e047860b377/mc-53-big-Data.db:level=0, /usr/lib/cassandra/apache-cassandra-3.9/data/data/system/local-7ad54392bcdd35a684174e047860b377/mc-54-big-Data.db:level=0, /usr/lib/cassandra/apache-cassandra-3.9/data/data/system/local-7ad54392bcdd35a684174e047860b377/mc-55-big-Data.db:level=0, /usr/lib/cassandra/apache-cassandra-3.9/data/data/system/local-7ad54392bcdd35a684174e047860b377/mc-56-big-Data.db:level=0, ]
DEBUG [main] 2017-01-21 13:57:48,539 StorageService.java:2084 - Node localhost/127.0.0.1 state NORMAL, token [-1035692197905104867, -1103547951527719073, -1136980347732340590, -1150272208899529050, -1184340318934652250, -1251847845785777189, -1355083122390358187,
INFO  [main] 2017-01-21 13:57:48,539 StorageService.java:2087 - Node localhost/127.0.0.1 state jump to NORMAL
DEBUG [main] 2017-01-21 13:57:48,545 StorageService.java:1336 - NORMAL
DEBUG [PendingRangeCalculator:1] 2017-01-21 13:57:48,575 PendingRangeCalculatorService.java:66 - finished calculation for 3 keyspaces in 19ms
INFO  [main] 2017-01-21 13:57:49,125 NativeTransportService.java:70 - Netty using native Epoll event loop
DEBUG [CompactionExecutor:1] 2017-01-21 13:57:49,286 CompactionTask.java:230 - Compacted (896b3470-df9e-11e6-9508-7dc463a45cc9) 4 sstables to [/usr/lib/cassandra/apache-cassandra-3.9/data/data/system/local-7ad54392bcdd35a684174e047860b377/mc-57-big,] to level=0.  9.869KiB to 4.938KiB (~50% of original) in 812ms.  Read Throughput = 12.145KiB/s, Write Throughput = 6.077KiB/s, Row Throughput = ~2/s.  4 total partitions merged to 1.  Partition merge counts were {4:1, }
INFO  [main] 2017-01-21 13:57:49,368 Server.java:159 - Using Netty Version: [netty-buffer=netty-buffer-4.0.39.Final.38bdf86, netty-codec=netty-codec-4.0.39.Final.38bdf86, netty-codec-haproxy=netty-codec-haproxy-4.0.39.Final.38bdf86, netty-codec-http=netty-codec-http-4.0.39.Final.38bdf86, netty-codec-socks=netty-codec-socks-4.0.39.Final.38bdf86, netty-common=netty-common-4.0.39.Final.38bdf86, netty-handler=netty-handler-4.0.39.Final.38bdf86, netty-tcnative=netty-tcnative-1.1.33.Fork19.fe4816e, netty-transport=netty-transport-4.0.39.Final.38bdf86, netty-transport-native-epoll=netty-transport-native-epoll-4.0.39.Final.38bdf86, netty-transport-rxtx=netty-transport-rxtx-4.0.39.Final.38bdf86, netty-transport-sctp=netty-transport-sctp-4.0.39.Final.38bdf86, netty-transport-udt=netty-transport-udt-4.0.39.Final.38bdf86]
INFO  [main] 2017-01-21 13:57:49,369 Server.java:160 - Starting listening for CQL clients on localhost/127.0.0.1:9042 (unencrypted)...
INFO  [main] 2017-01-21 13:57:49,429 CassandraDaemon.java:521 - Not starting RPC server as requested. Use JMX (StorageService-&gt;startRPCServer()) or nodetool (enablethrift) to start it
</code></pre>
",<database><cassandra><cassandra-3.0>,"<ol>
<li>Get rid of JVM_OPTS to &quot;$JVM_OPTS -Djava.rmi.server.hostname=localhost.</li>
<li>Set listen_address and broadcast_rpc_address to local ip (ifconfig &gt; ip-address-of-system).</li>
<li>Restart Cassandra.</li>
</ol>
","['broadcast_rpc_address', 'listen_address']"
41901797,41901911,2017-01-27 19:40:53,insert dummy data to Cassandra Counter table,"<p>I know I cannot insert in counter table , but how to use update if the table is empty , the update statement requires k_where </p>

<p>I tried many update statement but without hope if any example to fix this will be highly appreciated </p>
",<cassandra>,"<pre><code>create table test_counter(a text primary key, v counter);

update test_counter set v = v + 1 where a = 'dummy';

select * from test_counter;

 dummy             | v
-------------------+---
 some_counter_name | 1
</code></pre>

<p>take into account that besides primary key column you may only have counter columns in the table that you define with counters. Also you always update the data, partition key is simply upserted.</p>

<p>If you want to put counter back to zero you have to read it's current value and then you have to set to v = v - read_value. Note that there might be additional concurrent updates.</p>

<p>Counters may sometimes be updated even if the client got a message the update failed so they are not 100% reliable.</p>

<p>After receiving the comments :) it looks like you are running into some sort of Cassandra naming bug. Here is my exploration of your problem:</p>

<p>I started with your original problem:</p>

<pre><code>Update ""QueueMetric""
    Set ""TotalCompletedMessages"" = TotalCompletedMesseages + 1
    where ""QueueName"" = 'Test Queue one'
    and ""Time"" = now(); 
</code></pre>

<p>So I created a simple table:</p>

<pre><code>create table QueueMetric(
    QueueName text primary key,
    Time timeuuid,
    TotalCompletedMessages counter
)
</code></pre>

<blockquote>
  <p>InvalidRequest: code=2200 [Invalid query] message=""Cannot mix counter and non counter columns in the same table""</p>
</blockquote>

<p>Then I figured your create statement is something like:</p>

<pre><code>create table QueueMetric(
    QueueName text,
    Time timeuuid,
    TotalCompletedMessages counter,
    primary key (QueueName, Time)
)

Update QueueMetric Set TotalCompletedMessages = TotalCompletedMesseages + 1 where QueueName='Test Queue one' and Time = now();
</code></pre>

<p>I get the same exception here ... I also tried with a predefined uuid instead of now</p>

<p>Your example also doesn't make sense because why would you cluster counters by timeuuid?</p>

<p>Thing is it's one time thing, even with 10000 req per second you would have to generate them for 100 years to get a collision. I think what you actually need is a timestamp.</p>

<p>So then I try to define the time column as string:</p>

<pre><code>create table QueueMetric(
    QueueName text,
    Time text,
    TotalCompletedMessages counter,
    primary key (QueueName, Time)
)

Update QueueMetric Set TotalCompletedMessages = TotalCompletedMesseages + 1 where QueueName='Test Queue one' and Time = 'd14d44b2-e4d1-11e6-bf01-fe55135034f3';
</code></pre>

<p>I also failed with this one ...</p>

<p>so now my idea is that time is somehow wrong name</p>

<pre><code>create table QueueMetric(
    QueueName text,
    Test text,
    TotalCompletedMessages counter,
    primary key (QueueName, Test)
);

 Update QueueMetric Set TotalCompletedMessages = TotalCompletedMesseages + 1 where QueueName='Test Queue one' and Test = 'd14d44b2-e4d1-11e6-bf01-fe55135034f3';
</code></pre>

<p>This also didn't work ... let's try with just partition key:</p>

<pre><code>create table QueueMetric(
    QueueName text,
    Time timeuuid,
    TotalCompletedMessages counter,
    primary key ((QueueName, Time))
);

Update QueueMetric Set TotalCompletedMessages = TotalCompletedMesseages + 1 where QueueName='Test Queue one' and Time = now();
</code></pre>

<p>Now it's really getting strange ... I'll try with different name;</p>

<pre><code>create table QueueMetric2(
    QueueName text,
    Time timeuuid,
    TotalCompletedMessages counter,
    primary key ((QueueName, Time))
);

Update QueueMetric2 Set TotalCompletedMessages = TotalCompletedMesseages + 1 where QueueName='Test Queue one' and Time = now();
</code></pre>

<p>ok it might be the time:</p>

<pre><code>create table QueueMetric3(
    QueueName text,
    SomethingOther text,
    TotalCompletedMessages counter,
    primary key ((QueueName, SomethingOther))
);

Update QueueMetric3 Set TotalCompletedMessages = TotalCompletedMesseages + 1 where QueueName='Test Queue one' and SomethingOther = 'd14d44b2-e4d1-11e6-bf01-fe55135034f3';
</code></pre>

<p>It simply looks like there has to be primary key and column?</p>

<pre><code>create table QueueMetric4(
    QueueNamePlusTimeUUID text,
    TotalCompletedMessages counter,
    primary key (QueueNamePlusTimeUUID)
);

Update QueueMetric4 Set TotalCompletedMessages = TotalCompletedMesseages + 1 where QueueName='Test Queue one d14d44b2-e4d1-11e6-bf01-fe55135034f3';
</code></pre>

<p>now I'm really confused, lowercase everything?</p>

<pre><code>create table queuemetric5(
    queuenameplustimeUUID text primary key,
    totalcompletedmessages counter
);

update queuemetric5 set totalcompletedmessages = totalcompletedmessages + 1 where queuename='Test Queue one d14d44b2-e4d1-11e6-bf01-fe55135034f3';
</code></pre>

<p>This also doesn't work :)</p>

<pre><code>create table abc(
    a text primary key,
    t counter
);

update abc set t = t + 1 where a='Test Queue one d14d44b2-e4d1-11e6-bf01-fe55135034f3';
</code></pre>

<p>and this totally works now :) it looks to me you are having some strange names actually
let's try the same thing you had in the beginning but with different names:</p>

<pre><code>create table hohoho(
    n text,
    o timeuuid,
    m counter,
    primary key (n, o)
);

update hohoho set m = m + 1 where n='Test Queue one' and o = now();
</code></pre>

<p>I guess we should file this as some sort of a bug ... it looks to me your
problem is naming only :) Looks like Queue is something special in cql</p>

<p>:)</p>
",['table']
41931666,41931882,2017-01-30 08:37:57,Insert timestamp to Cassandra using Spark Scala,"<p>I am trying to read a file containing names and insert the name along with timestamp data to cassandra table using Spark and Scala.  Below is my code</p>

<pre><code>case class Names(name:String, auditDate:DateTime )

def main(args: Array[String]): Unit = {
    System.setProperty(""hadoop.home.dir"", ""D:\\backup\\lib\\winutils"");
    val conf = new SparkConf()
      .set(""spark.cassandra.connection.host"", ""172.16.109.202"")
      //.set(""spark.cassandra.connection.host"", ""192.168.1.17"")
      .setAppName(""CassandraLoader"")
      .setMaster(""local"")
    var context = new SparkContext(conf)

    var namesFile = context.textFile(""src/main/resources/names.txt"")

    namesFile.map(x=&gt;Names(x,DateTime.now()))
      .saveToCassandra(""practice"",""names"",SomeColumns(""name"", ""insert_date""))

  }
</code></pre>

<p>The cassandra table details is in below</p>

<pre><code>CREATE TABLE practice.names (
    name text PRIMARY KEY,
    insert_date timestamp
)
</code></pre>

<p>When I try to execute the code, I am getting the below error</p>

<pre><code>Exception in thread ""main"" java.lang.IllegalArgumentException: requirement failed: Columns not found in com.sample.practice.Names: [insert_date]
    at scala.Predef$.require(Predef.scala:233)
    at com.datastax.spark.connector.mapper.DefaultColumnMapper.columnMapForWriting(DefaultColumnMapper.scala:108)
    at com.datastax.spark.connector.writer.MappedToGettableDataConverter$$anon$1.&lt;init&gt;(MappedToGettableDataConverter.scala:29)
    at com.datastax.spark.connector.writer.MappedToGettableDataConverter$.apply(MappedToGettableDataConverter.scala:20)
    at com.datastax.spark.connector.writer.DefaultRowWriter.&lt;init&gt;(DefaultRowWriter.scala:17)
    at com.datastax.spark.connector.writer.DefaultRowWriter$$anon$1.rowWriter(DefaultRowWriter.scala:31)
    at com.datastax.spark.connector.writer.DefaultRowWriter$$anon$1.rowWriter(DefaultRowWriter.scala:29)
    at com.datastax.spark.connector.writer.TableWriter$.apply(TableWriter.scala:271)
    at com.datastax.spark.connector.RDDFunctions.saveToCassandra(RDDFunctions.scala:36)
    at com.sample.practice.CqlInsertDate$.main(CqlInsertDate.scala:30)
    at com.sample.practice.CqlInsertDate.main(CqlInsertDate.scala)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:147)
</code></pre>

<p>When I try to print the RDD instead of saving to cassandra, I am getting the below output</p>

<pre><code>Names(Frank,2017-01-30T14:03:16.911+05:30)
Names(Jean,2017-01-30T14:03:17.115+05:30)
Names(Joe,2017-01-30T14:03:17.116+05:30)
</code></pre>

<p>Below is my SBT file details</p>

<pre><code>version := ""1.0""

scalaVersion := ""2.10.6""

libraryDependencies += ""com.datastax.spark"" % ""spark-cassandra-connector_2.10"" % ""2.0.0-M3""

libraryDependencies += ""org.apache.spark"" % ""spark-core_2.10"" % ""2.0.2""

libraryDependencies += ""org.apache.spark"" % ""spark-sql_2.10"" % ""2.0.2""

libraryDependencies += ""org.apache.spark"" % ""spark-hive_2.10"" % ""2.0.2""
</code></pre>

<p>I am using Cassandra 2.1.  Please help.  Thanks in advance.</p>
",<scala><apache-spark><cassandra>,"<p>Try to change your class field to insert_date or vice versa table column to auditDate</p>
",['table']
42119104,53299104,2017-02-08 16:53:03,RangeError: index out of range when connect to cassandra databse in node js,"<p>I'm working on NodeJS project and I've tried to select data from table stored in cassandra database using cassandra-driver package,
I start the client connection using this line:</p>

<pre><code>const cassandra = require('cassandra-driver');
const cassandraClient = new cassandra.Client({ contactPoints: ['192.168.0.253'], keyspace: 'test' });
</code></pre>

<p>sometimes when I call the url, the engine fetchs the result successfully, by the way sometimes raises this error:</p>

<pre><code>{ [RangeError: index out of range]
  coordinator: '192.168.0.253:9042',
  query: 'SELECT * FROM table where hidden=false ALLOW FILTERING' }
</code></pre>

<p>this error usually raise when the cassandra table updated recently!
I don't know if this important, but there is another service connect to cassandra database and insert new data, is this related to problem? and how would I solve this error?</p>

<p>if the two services are the reason of this error, Are there any way to make cassandra table not blocked for (reading and writing) when there is active service write to cassandra table, mean no problem with ghost reading?</p>

<p>stack error:</p>

<pre><code>{ [RangeError: index out of range]
  coordinator: '192.168.0.253:9042',
  query: 'SELECT * FROM table where hidden=false ALLOW FILTERING' }
RangeError: index out of range
    at checkOffset (buffer.js:663:11)
    at Buffer.readInt32BE (buffer.js:828:5)
    at Function.Long.fromBuffer (/home/l.alassadi/alarm-socket/node_modules/cassandra-driver/lib/types/index.js:466:25)
    at Encoder.decodeLong (/home/l.alassadi/alarm-socket/node_modules/cassandra-driver/lib/encoder.js:133:17)
    at Encoder.decodeTimestamp (/home/l.alassadi/alarm-socket/node_modules/cassandra-driver/lib/encoder.js:142:26)
    at Encoder.decode (/home/l.alassadi/alarm-socket/node_modules/cassandra-driver/lib/encoder.js:1202:18)
    at Parser.parseRows (/home/l.alassadi/alarm-socket/node_modules/cassandra-driver/lib/streams.js:377:36)
    at Parser.parseResult (/home/l.alassadi/alarm-socket/node_modules/cassandra-driver/lib/streams.js:335:10)
    at Parser.parseBody (/home/l.alassadi/alarm-socket/node_modules/cassandra-driver/lib/streams.js:185:19)
    at Parser._transform (/home/l.alassadi/alarm-socket/node_modules/cassandra-driver/lib/streams.js:137:10)
</code></pre>
",<javascript><node.js><cassandra>,"<p><strong>TLDR</strong> fixed in <code>cassandra-driver</code> v3.6.0</p>

<hr>

<p><code>RangeError</code> is caused by a problem in <code>cassandra-driver</code>, specifically in how it decodes the data from db server. It appears in the following conditions:</p>

<ol>
<li>The table contains a column with a <code>map</code> type</li>
<li>The map's value type is not nullable</li>
<li>The selected row has the said column</li>
<li>The value of the column (the map) has a key without a value (not quite sure how it's possible but it is)</li>
</ol>

<p>Here are all nullable types:</p>

<ul>
<li>text</li>
<li>ascii</li>
<li>varchar</li>
<li>custom</li>
<li>blob</li>
</ul>

<p>Long story short, the driver does not handle the case when a field of a not-nullable type (like int) inside a map is serialized as an empty buffer. In this case the driver is unconditionally trying to read from the buffer the amount of bytes equivalent to the type's size and fails with the said error.</p>

<p>Having this investigated, I've submitted a <a href=""https://github.com/datastax/nodejs-driver/pull/294"" rel=""nofollow noreferrer"">fix</a> to <code>cassandra-driver</code>. It has been accepted and released in 3.6.0.</p>
",['table']
42249428,42253079,2017-02-15 12:27:24,"Cassandra, how to filter and update a big table dynamically?","<p>I'm trying to find the best data model to adapt a very big mysql table in Cassandra.
This table is structured like this:</p>

<pre><code>CREATE TABLE big_table (
  social_id, 
  remote_id,
  timestamp,
  visibility,
  type,
  title,
  description,
  other_field,
  other_field,
  ...
  )
</code></pre>

<p>A page (which is not here) can contain many socials, which can contain many remote_ids.</p>

<p>Social_id is the partitioning key, remote_id and timestamp are the clustering key: ""Remote_id"" gives unicity, ""Time"" is used to order the results. So far so good.</p>

<p>The problem is that users can also search on their page contents, filtering by one or more socials, one or more types, visibility (could be 0,1,2), a range of dates or even nothing at all.
Plus, based on the filters, users should be able to set visibility.</p>

<p>I tried to handle this case, but I really can find a sustainable solution.
The best I've got is to create another table, which I need to keep up with the original one.
This table will have: </p>

<ul>
<li>page_id: partition key</li>
<li>timestamp, social_id, type, remote_id: clustering key</li>
</ul>

<p>Plus, create a Materialized View for each combination of filters, which is madness.</p>

<p>Can I avoid creating the second table? What wuold be the best Cassandra model in this case? Should I consider switching to other technologies?</p>
",<database><database-design><cassandra>,"<p>I start from last questions.</p>

<p><em>> What would be the best Cassandra model in this case?</em></p>

<p>As stated in <a href=""http://shop.oreilly.com/product/0636920043041.do"" rel=""nofollow noreferrer"">Cassandra: The Definitive Guide, 2nd edition</a> (which I highly recommend to read before choosing or using Cassandra),</p>

<blockquote>
  <p>In Cassandra you don’t start with the data model; <strong>you start with the query model</strong>.</p>
</blockquote>

<p>You may want to read an available chapter about data design at <a href=""https://www.safaribooksonline.com/library/view/cassandra-the-definitive/9781449399764/ch04.html"" rel=""nofollow noreferrer"">Safaribooksonline.com</a>. Basically, Cassandra wants you to think about queries only and don't care about normalization. </p>

<p>So the answer on </p>

<p><em>> Can I avoid creating the second table?</em> </p>

<p>is <strong>You shouldn't avoiding it</strong>.</p>

<p><em>> Should I consider switching to other technologies?</em></p>

<p>That depends on what you need in terms of replication and partitioning. You may end up creating master-master synchronization based on RDBMS or something else. In Cassandra, you'll end up with duplicated data between tables and that's perfectly normal for it. You trade disk space in exchange for reading/writing speed.</p>

<p><em>> how to filter and update a big table dynamically?</em></p>

<p>If after all of the above you still want to use normalized data model in Cassandra, I suggest you look on <a href=""https://docs.datastax.com/en/cql/3.3/cql/cql_using/useSecondaryIndex.html"" rel=""nofollow noreferrer"">secondary indexes</a> at first and then move on to custom indexes like <a href=""https://github.com/Stratio/cassandra-lucene-index"" rel=""nofollow noreferrer"">Lucene index</a>.</p>
",['table']
42331153,42689476,2017-02-19 18:29:49,rpc_address and broadcast_rpc address for cassandra.yaml for Datastax OpsCenter,"<p>So I have a single node cassandra running on an AWS machine which also has the OpsCenter installed. I'm trying to manage it with OpsCenter GUI from a windows machine (which is in the same private network as the cassandra node)however I keep getting the following error </p>

<p>""No HTTP communication to the agent""</p>

<p>Opscenter logs show the following information - </p>

<p>2017-02-19 18:08:17,622 [Test_Cluster]  INFO: Node 172.18.51.175 changed its mode to normal (MainThread)
2017-02-19 18:08:17,773 [Test_Cluster]  INFO: Using 1.2.3.4 as the RPC address for node 172.18.51.175 (MainThread)
2017-02-19 18:09:12,046 [Test_Cluster]  WARN: These nodes reported this message, Nodes: ['172.18.51.175'] Message: HTTP request <a href=""http://1.2.3.4:61621/connection-status"" rel=""nofollow noreferrer"">http://1.2.3.4:61621/connection-status</a>? failed: User timeout caused connection failure. (MainThread)
2017-02-19 18:10:12,045 [Test_Cluster]  WARN: These nodes reported this message, Nodes: ['172.18.51.175'] Message: HTTP request <a href=""http://1.2.3.4:61621/connection-status"" rel=""nofollow noreferrer"">http://1.2.3.4:61621/connection-status</a>? failed: User timeout caused connection failure. (MainThread)
2017-02-19 18:11:12,046 [Test_Cluster]  WARN: These nodes reported this message, Nodes: ['172.18.51.175'] Message: HTTP request <a href=""http://1.2.3.4:61621/connection-status"" rel=""nofollow noreferrer"">http://1.2.3.4:61621/connection-status</a>? failed: IPv4Address(TCP, '1.2.3.4', 61621) (MainThread)
2017-02-19 18:12:12,045 [Test_Cluster]  WARN: These nodes reported this message, Nodes: ['172.18.51.175'] Message: HTTP request <a href=""http://1.2.3.4:61621/connection-status"" rel=""nofollow noreferrer"">http://1.2.3.4:61621/connection-status</a>? failed: IPv4Address(TCP, '1.2.3.4', 61621) (MainThread)
2017-02-19 18:13:12,433 [Test_Cluster]  WARN: These nodes reported this message, Nodes: ['172.18.51.175'] Message: HTTP request <a href=""http://1.2.3.4:61621/connection-status"" rel=""nofollow noreferrer"">http://1.2.3.4:61621/connection-status</a>? failed: IPv4Address(TCP, '1.2.3.4', 61621) (MainThread)
2017-02-19 18:14:12,045 [Test_Cluster]  WARN: These nodes reported this message, Nodes: ['172.18.51.175'] Message: HTTP request <a href=""http://1.2.3.4:61621/connection-status"" rel=""nofollow noreferrer"">http://1.2.3.4:61621/connection-status</a>? failed: IPv4Address(TCP, '1.2.3.4', 61621) (MainThread)
2017-02-19 18:15:12,045 [Test_Cluster]  WARN: These nodes reported this message, Nodes: ['172.18.51.175'] Message: HTTP request <a href=""http://1.2.3.4:61621/connection-status"" rel=""nofollow noreferrer"">http://1.2.3.4:61621/connection-status</a>? failed: User timeout caused connection failure. (MainThread)
2017-02-19 18:16:12,044 [Test_Cluster]  WARN: These nodes reported this message, Nodes: ['172.18.51.175'] Message: HTTP request <a href=""http://1.2.3.4:61621/connection-status"" rel=""nofollow noreferrer"">http://1.2.3.4:61621/connection-status</a>? failed: IPv4Address(TCP, '1.2.3.4', 61621) (MainThread)
2017-02-19 18:17:12,044 [Test_Cluster]  WARN: These nodes reported this message, Nodes: ['172.18.51.175'] Message: HTTP request <a href=""http://1.2.3.4:61621/connection-status"" rel=""nofollow noreferrer"">http://1.2.3.4:61621/connection-status</a>? failed: IPv4Address(TCP, '1.2.3.4', 61621) (MainThread)
2017-02-19 18:18:12,045 [Test_Cluster]  WARN: These nodes reported this message, Nodes: ['172.18.51.175'] Message: HTTP request <a href=""http://1.2.3.4:61621/connection-status"" rel=""nofollow noreferrer"">http://1.2.3.4:61621/connection-status</a>? failed: IPv4Address(TCP, '1.2.3.4', 61621) (MainThread)</p>

<p>So I guess my cassandra.yaml file needs some change ? </p>

<p>Currently I have set listen_address as private IP of my node</p>

<p>my rpc_address is 0.0.0.0</p>

<p>and my broadcast_rpc_address is set as 1.2.3.4 </p>

<p>Which is how the datastax doc recommended. </p>

<p>I tried setting the rpc_address and broadcast_rpc_address to the node's private IP and it failed in that scenario as well.</p>

<p>netstat --listen shows the below line for the port 61621 and 61620</p>

<p>tcp6       0      0 [::]:61620              [::]:*                  LISTEN
tcp6       0      0 [::]:61621              [::]:*                  LISTEN</p>

<p>I'm not sure what I'm doing wrong or how to set these parameters in cassandra.yaml for it to work with Opscenter. </p>

<p>Note : I seem to be having issues only with OpsCenter with the above config. Cassandra services start up fine and my web application is connecting to the cluster using the datastax driver. Any one have comments on what might be going wrong ? </p>

<p>Thanks</p>
",<cassandra><datastax><cql><datastax-enterprise><opscenter>,"<blockquote>
  <p>my rpc_address is 0.0.0.0</p>
  
  <p>and my broadcast_rpc_address is set as 1.2.3.4</p>
</blockquote>

<p>That is your mistake, change the rpc_address to the local IP --> 172.18.51.175 [if this is the nodes IP] </p>

<p>Check in cassandra.yaml file that the listen_address is also set to --> 172.18.51.175</p>
","['broadcast_rpc_address', 'rpc_address', 'listen_address']"
42341685,42355466,2017-02-20 10:10:21,PHP-CASSANDRA-LUCENE Paging with sorting,"<p>I am trying to fetch sorted records with paging in cassandra which is using lucene indexes for searching and sorting</p>

<p>Note : There are total 26 rows in user table</p>

<p>Case 1 : Sorting in ascending order with paging</p>

<pre><code>cqlsh &gt; paging 10;
cqlsh &gt; SELECT user_id FROM user WHERE category_id = 'e4da3b7f-bbce-2345-d777-2b0674a318d5' AND expr(user_index, '{filter:[{type:""match"", field:""is_primary"", value:true}], sort:[{field: ""user_id"",reverse:false}]}');
</code></pre>

<p>This will give all 26 rows in ascending order of user_id first 10 rows,second 10 rows then last 6 rows that is fine but issue in case 2</p>

<p>Case 2 : Sorting in descending order with paging</p>

<pre><code>cqlsh &gt; paging 10;
cqlsh &gt; SELECT user_id FROM user WHERE category_id = 'e4da3b7f-bbce-2345-d777-2b0674a318d5' AND expr(user_index, '{filter:[{type:""match"", field:""is_primary"", value:true}], sort:[{field: ""user_id"",reverse:true}]}');
</code></pre>

<p>This will give only 19 rows in descending order of user_id first 10 rows,then 9 rows same of first page</p>

<p>Is this indicate that cassandra paging and lucene sorting can't use together? If yes then any alternative to use sorting on lucen indexes with cassandra paging? While answering please consider timeline based sorting and paging can't apply here because I have to consider too many columns while sorting.</p>
",<php><cassandra><lucene><cassandra-lucene-index><bigdata>,"<p>Paging and sorting should work together. Which version of both Apache Cassandra and the Lucene index plugin are you using?</p>

<p>Could you please provide the table and index creation statements and the rows returned by each query? The values of <code>user_id</code> and <code>mac_address</code> would be enough to reproduce the issue.</p>
",['table']
42346532,42352728,2017-02-20 13:58:21,Is increasing `max_hint_window_in_ms` to days a bad idea?,"<p>I'm considering raising <code>max_hint_window_in_ms</code> to something like 72 hours. Anyone see issues with this? Essentially, it would allow us much longer downtime of nodes over a weekend without having to do a full repair.</p>
",<cassandra><datastax><datastax-enterprise><cassandra-2.1><datastax-startup>,"<p>It depends on the version. After <a href=""https://issues.apache.org/jira/browse/CASSANDRA-6230"" rel=""nofollow noreferrer"">C* 3.0</a> or DSE 5.0 when the hinted handoff storage was refactored its actually a very good idea to increase it. Before then (given your 2.1 tag assuming this is you) theres a lot of issues with accumulating too many hints highlighted in <a href=""http://www.datastax.com/dev/blog/whats-coming-to-cassandra-in-3-0-improved-hint-storage-and-delivery"" rel=""nofollow noreferrer"">this blog post</a>. Unless using a version after 3.0 I would not recommend increasing it too much.</p>

<p>To highlight some pre 3.0 issues:</p>

<ul>
<li>Hints are stored in a C* table and acts like a queue which is a known antipattern, builds up many tombstones and slow/expensive reads</li>
<li>Hints are partitioned by node, so if one node is down a long time the partition gets very huge. This is handled better in the latest of C*/DSE but particularly in 2.1 this impacts compactions, and gcs significantly.</li>
<li>Compactions are called regularly and are required, but if there is nothing getting removed this means just rewriting the mutations over and over while the node is down (wasteful)</li>
<li>Individual mutations need to go through memtables and full write path vs just appended to disk</li>
</ul>
",['table']
42362240,42369667,2017-02-21 08:31:58,How to manipulate timestamp columns in Apache Cassandra,"<p>I have a table with a <code>timestamp</code> column, and I'd like to manipulate the values of that column. For instance, I need to do something along the line:</p>

<p><code>UPDATE mytable SET datetimecolumn = datetimecolumn + 10mins</code></p>

<p>How is it done in Apache Cassandra?</p>

<p><strong>UPDATE</strong>: The answer seems to be ""you can't"". But the selected answer is the closest we can get apparently.</p>
",<cassandra><cql><cql3>,"<p><strong>You can query similar this one, only if the data type is counter.</strong></p>
<p><strong>Using Counter :</strong></p>
<blockquote>
<p>A counter is a special column used to store a number that is changed in increments. For example, you might use a counter column to count the number of times a page is viewed.</p>
<p>Define a counter in a dedicated table only and use the counter data type. You cannot index, delete, or re-add a counter column. All non-counter columns in the table must be defined as part of the primary key.</p>
</blockquote>
<p>Example :</p>
<pre><code>CREATE TABLE mytable (
   pk1 int PRIMARY KEY,
   datetimecolumn counter
);
</code></pre>
<p>Here you have to use datetimecolumn value in millisecond.<br />
For the first time, you have to use update query with the time in millisecond value let's say 1487686182403</p>
<pre><code>UPDATE mytable SET datetimecolumn = datetimecolumn + 1487686182403 where pk1 = 1
</code></pre>
<p>Now mytable with pk = 1 contains datetimecolumn = 1487686182403 value.</p>
<p>If you want to increment datetimecolumn by 10mins (600000 millisecond)</p>
<pre><code>UPDATE mytable SET datetimecolumn = datetimecolumn + 600000 where pk1 = 1
</code></pre>
<p>Source : <a href=""https://docs.datastax.com/en/cql/3.1/cql/cql_using/use_counter_t.html"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/cql/3.1/cql/cql_using/use_counter_t.html</a></p>
",['table']
42411568,42411908,2017-02-23 09:23:48,Get last location of multiple users?,"<p>Users are always sending their location (long, lat) to Cassandra to store their location into a time series table.</p>

<pre><code>CREATE TABLE locations (
    organization_id int,
    user_id int,
    date text,
    unix_time bigint,
    lat double,
    long double,
    PRIMARY KEY ((organization_id, user_id, date), unix_time)
) WITH CLUSTERING ORDER BY (unix_time DESC); 
</code></pre>

<p>What's the advice when it comes to retrieve the last location of multiple users? </p>

<p>As I will be using <code>LIMIT</code> as something like that:</p>

<pre><code>SELECT * from myapp.locations WHERE organization_id=1 and user_id in (15,16, 17, 18, 19, 20)  and date='2017-2-23' LIMIT 6;
</code></pre>

<p>What if last 6 records belongs to the same user? thus, won't get last locations of other users..</p>

<p>Shall create another table that keeps only one location of each user? and keep updating location of each user in it?</p>

<p>What's the advice here?</p>
",<cassandra>,"<p><strong>You should query for each user with executeAsync, and then merge the result.</strong></p>

<p>For every user use this query to get his latest location : </p>

<pre><code>SELECT * from myapp.locations WHERE organization_id=1 and date='2017-2-23' and user_id = ? LIMIT 1
</code></pre>

<p>and execute this using executeAsync, and then merge the result</p>

<p>If you make separate table, you need to partition it by userid, then you need to make in query with userids, which is slower, for faster you have to make separate query with executeAsync for each user on this table also. </p>

<p>Also there is another thing location will update more frequently, So every update cassandra will need to do more work on compaction, can slow down your performance </p>
",['table']
42476481,42481524,2017-02-27 01:34:58,Most efficient way to query cassandra in small time-based chunks,"<p>My Cassandra-based application needs to read the rows changed since last read.
For this purpose, we are planning to have a table <code>changed_rows</code> that will contain two columns - </p>

<ol>
<li>ID - The ID of the changed row and</li>
<li>Updated_Time - The timestamp when it was changed.</li>
</ol>

<p>What is the best way to read such a table such that it reads small group of rows ordered by time.
Example: if the table is:</p>

<pre><code>ID   Updated_Time
foo    1000
bar    1200
abc    2000
pqr    2500
zyx    2900
 ...
xyz   901000
 ...
</code></pre>

<p>I have shown IDs to be simple 3-letter keys, in reality they are UUIDs.
Also, time shown above is shown as an integer for the sake of simplicity, but its an actual Cassandra timestamp (Or Java Date). <strong>The <code>Updated_Time</code> column is a monotonically increasing one.</strong></p>

<p>If I query this data with:</p>

<pre><code>SELECT * FROM changed_rows WHERE Updated_Time &lt; toTimestamp(now())
</code></pre>

<p>I get the following error:</p>

<pre><code>Cannot execute this query as it might involve data filtering and 
thus may have unpredictable performance... Use Allow Filtering
</code></pre>

<p>But I think <code>Allow Filtering</code> in this case would kill the performance.
The Cassandra index page warns to avoid indexes for high cardinality columns and the <code>Updated_Time</code> above sure seems like high cardinality.</p>

<p>I do not know the ID column before-hand because the purpose of the query is to know the IDs updated between given time intervals.</p>

<p>What is the best way to query Cassandra in this case then?<br>
Can I change my table somehow to run the time-chunk query more efficiently?  </p>

<p>Note: This should sound somewhat similar to <a href=""http://cassandra.apache.org/doc/latest/operating/cdc.html"" rel=""nofollow noreferrer"">Cassandra-CDC feature</a> but we cannot use the same because our solution should work for all the Cassandra versions</p>
",<optimization><cassandra><datastax><cql><cqlsh>,"<p>Assuming you know the time intervals you want to query, you need to create another table like the following:</p>

<pre><code>CREATE TABLE modified_records (
    timeslot timestamp,
    updatedtime timestamp,
    recordid timeuuid,
    PRIMARY KEY (timeslot, updatedtime)
);
</code></pre>

<p>Now you can split your ""updated record log"" into time slices, eg 1 hour, and fill the table like this:</p>

<pre><code>INSERT INTO modified_records (timeslot, updatedtime, recordid) VALUES ( '2017-02-27 09:00:00', '2017-02-27 09:36:00', 123);
INSERT INTO modified_records (timeslot, updatedtime, recordid) VALUES ( '2017-02-27 09:00:00', '2017-02-27 09:56:00', 456);
INSERT INTO modified_records (timeslot, updatedtime, recordid) VALUES ( '2017-02-27 10:00:00', '2017-02-27 10:00:13', 789);
</code></pre>

<p>where you use a part of your <code>updatedtime</code> timestamp as a partition key, eg in this case you round to the integral hour. You then query by specifying the time slot only, eg:</p>

<pre><code>SELECT * FROM modified_records WHERE timeslot = '2017-02-27 09:00:00';
SELECT * FROM modified_records WHERE timeslot = '2017-02-27 10:00:00';
</code></pre>

<p>Depending on how often your records get updated, you can go with smaller or bigger time slices, eg every 6 hours, or 1 day, or every 15 minutes. This structure is very flexible. You only need to know the timeslot you want to query. If you need to span multiple timeslots you'll need to perform multiple queries.</p>
",['table']
42509583,42903689,2017-02-28 13:07:34,LookupTables consistency,"<p>I've been working with cassandra for a year and in one of my projects I had to handle data in various lookup tables, to update, insert and delete... all of them were orchestrated at ""service"" layer. One of my concerns was consistency, I know cassandra gave up that to offer Availability and Partitioning (what could be tuned, but the project required A and P instead of C).</p>

<p>When I said consistency I'm thinking about this scenario:</p>

<pre><code>Keyspace [User] {
  userId,
  email,
  phoneNumber,
  firstName,
  lastName
} Primary Key (userID)
</code></pre>

<p>LookupTables:</p>

<ul>
<li>UserByPhoneNumber</li>
<li>UserByEmail</li>
<li>UserByLastName</li>
</ul>

<p>based on the architecture we used, when a client calls service.save(User user) it triggers actions on lookupTables, filling data in all of them, given that what if during insert process the insert fails in one of them? Should I keep control of it in my code either? We managed it using BatchStatement, was it the best approach?</p>

<p>Cassandra version: 2x</p>
",<database-design><cassandra><cassandra-2.0><nosql>,"<p>First I would like to define consistency. I think you have mixed up the concept of Cassandra Consistency Level Vs Atomicity. I think your concern is about how to keep data consistent among related tables.</p>

<p>Cassandra Tunable Consistency</p>

<blockquote>
  <blockquote>
    <p>Consistency refers to how up-to-date and synchronized a row of Cassandra data is on all of its replicas. <br/>
    Cassandra is typically classified as an AP system, meaning that availability and partition tolerance are generally considered to be more important than consistency in Cassandra. But Cassandra can be tuned with replication factor and consistency level to also meet C.</p>
  </blockquote>
</blockquote>

<p>Cassandra is best suited where strong consistency is not needed. You will get the most up-to-date data eventually. </p>

<p>Now get to the Data Modeling Part. You are in the right path. :)</p>

<p>It is very important to prepare your query before you design your model. There are some possible solutions for this case.</p>

<ol>
<li>Usage of Cassandra Secondary Index</li>
</ol>

<p>You could create secondary index on those columns to query and get your desired data. In this case you don't have to manage any lookup tables and the situation of inconsistent data among tables won't arise. But this is not the good solution for this scenario. The reason for this is described in below link:</p>

<p><a href=""https://docs.datastax.com/en/cql/3.1/cql/ddl/ddl_when_use_index_c.html"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/cql/3.1/cql/ddl/ddl_when_use_index_c.html</a></p>

<blockquote>
  <blockquote>
    <p>It would probably be more efficient to manually maintain the table as a form of an index instead of using the Cassandra built-in index.</p>
  </blockquote>
</blockquote>

<p>Also reads will be slower cause every node has to queried to get required results. As Cassandra writes are much faster, we maintain tables (tables per query if needed) to do the index and serve the queries and also denormalize data to make read faster. But now arise the problem of maintaining data consistency among those tables. If an update happens, how to ensure to keep indexed/denormalized data consistent in all tables.</p>

<ol start=""2"">
<li>Using Batch operation</li>
</ol>

<p>To maintain data consistency between these tables (depends on use case) if you want to ensure atomicity among these updates batch is the  solution. </p>

<p>If your system (cluster health )is okay, Cassandra ensures all writes to be successful. But if in case any write fails (you cannot find user by their email/mobile is okay), then you may avoid the batch (coordinator needs to do a lot of work for maintaining a batch). But here you can use batch.</p>

<p>Additionally if you are using Cassandra 3.0 you can use the materialized view concept where Cassandra maintains data consistency between tables. </p>

<p>There are so many questions related this</p>

<p><a href=""https://stackoverflow.com/questions/34231718/how-to-ensure-data-consistency-in-cassandra-on-different-tables"">How to ensure data consistency in Cassandra on different tables?</a></p>
",['table']
42519927,42866327,2017-02-28 22:28:39,Configuring what is written to Kafka Topic when using Kafka Connect Cassandra Source,"<p>I am doing a spike in which we want to publish data as it is written in a Cassandra table to a Kafka Topic. We are looking at using Kafka Connect and the Stream Reactor Connectors. </p>

<p>I am using Kafka 0.10.0.1</p>

<p>I am using DataMountaineer <a href=""http://docs.datamountaineer.com/en/latest/install.html#install"" rel=""nofollow noreferrer"">Stream Reactor</a> 0.2.4</p>

<p>I placed the jar file for Stream Reactor into the Kafka libs folder and am running Kafka Connect in distributed mode</p>

<pre><code>bin/connect-distributed.sh config/connect-distributed.properties
</code></pre>

<p>I added the Cassandra Source connector as follows:</p>

<pre><code>curl -X POST -H ""Content-Type: application/json"" -d @config/connect-idoc-cassandra-source.json.txt localhost:8083/connectors
</code></pre>

<p>When I add data to the Cassandra table I see it being added to the topic using the Kafka command line consumer</p>

<pre><code>bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic idocs-topic --from-beginning
</code></pre>

<p>Here is a sample of what is being written to the Topic right now:</p>

<pre><code>{
""schema"": {
    ""type"": ""struct"",
    ""fields"": [{
        ""type"": ""string"",
        ""optional"": true,
        ""field"": ""idoc_id""
    }, {
        ""type"": ""string"",
        ""optional"": true,
        ""field"": ""idoc_event_ts""
    }, {
        ""type"": ""string"",
        ""optional"": true,
        ""field"": ""json_doc""
    }],
    ""optional"": false,
    ""name"": ""idoc.idocs_events""
},
""payload"": {
    ""idoc_id"": ""dc4ab8a0-fdf8-11e6-8285-1bce55915fdd"",
    ""idoc_event_ts"": ""dc4ab8a1-fdf8-11e6-8285-1bce55915fdd"",
    ""json_doc"": ""{\""foo\"":\""bar\""}""
}}
</code></pre>

<p>What I would like written to the topic is the value of the <code>json_doc</code> column.</p>

<p>Here is what I have in my config for the Cassandra source</p>

<pre><code>{
""name"": ""cassandra-idocs"",
""config"": {
    ""tasks.max"": ""1"",
    ""connector.class"": ""com.datamountaineer.streamreactor.connect.cassandra.source.CassandraSourceConnector"",
    ""connect.cassandra.key.space"": ""idoc"",
    ""connect.cassandra.source.kcql"": ""INSERT INTO idocs-topic SELECT json_doc FROM idocs_events PK idoc_event_ts"",
    ""connect.cassandra.import.mode"": ""incremental"",
    ""connect.cassandra.contact.points"": ""localhost"",
    ""connect.cassandra.port"": 9042,
    ""connect.cassandra.import.poll.interval"": 10000
}}
</code></pre>

<p>How do I change the way Kafka Connect Cassandra Source is configured so that only the value of <code>json_doc</code> is written to the Topic so it would look something like this:</p>

<pre><code>{""foo"":""bar""}
</code></pre>

<p>The <a href=""http://docs.datamountaineer.com/en/latest/kcql.html#why"" rel=""nofollow noreferrer"">Kassandra Connect Query Language</a> seemed to be the way to go but it isn't limiting what is written to the column specified in the KCQL.</p>

<p><strong>UPDATE</strong></p>

<p>Saw this <a href=""https://stackoverflow.com/questions/40878365/why-is-meta-data-added-to-the-output-of-this-kafka-connector"">answer on StackOverflow</a> and changed the converters in the <code>connect-distributed.properties</code> file from <code>JsonConverter</code> to <code>StringConverter</code>. </p>

<p>The result is this is now written to the Topic:</p>

<pre><code>Struct{idoc_id=74597cf0-fdf7-11e6-8285-1bce55915fdd,idoc_event_ts=74597cf1-fdf7-11e6-8285-1bce55915fdd,json_doc={""foo"":""bar""}}
</code></pre>

<p><strong>UPDATE 2</strong></p>

<p>Changed the converters in the <code>connect-distributed.properties</code> file back to <code>JsonConverter</code>. Then also disabled the schemas.</p>

<pre><code>key.converter.schemas.enable=false
value.converter.schemas.enable=false 
</code></pre>

<p>The result is this is now written to the Topic:</p>

<pre><code>{
    ""idoc_id"": ""dc4ab8a0-fdf8-11e6-8285-1bce55915fdd"",
    ""idoc_event_ts"": ""dc4ab8a1-fdf8-11e6-8285-1bce55915fdd"",
    ""json_doc"": ""{\""foo\"":\""bar\""}""
}
</code></pre>

<p><strong>Note</strong>
Using code from snapshot release and changing the KCQL to </p>

<pre><code>INSERT INTO idocs-topic 
SELECT json_doc, idoc_event_ts
FROM idocs_events 
IGNORE idoc_event_ts
PK idoc_event_ts
</code></pre>

<p>Yields this result on the Topic</p>

<pre><code>{""json_doc"": ""{\""foo\"":\""bar\""}""}
</code></pre>

<p>Thanks</p>
",<cassandra><apache-kafka><apache-kafka-connect>,"<p>Turns out what I was attempting to do was not possible in the Cassandra Source in DataMountaineer Stream Reactor 0.2.4. However, the snapshot release (of what I assume will become release 0.2.5) will support this.</p>

<p>Here is how it will work:</p>

<p>1) Set the converters in the <code>connect-distributed.properties</code> file to <code>StringConverter</code>.</p>

<p>2) Set the KCQL in the JSON configuration for the Cassandra Source connector to</p>

<pre><code>INSERT INTO idocs-topic 
SELECT json_doc, idoc_event_ts 
  FROM idocs_events 
IGNORE idoc_event_ts 
    PK idoc_event_ts 
WITHUNWRAP
</code></pre>

<p>This will result in the value of the <code>json_doc</code> column being published to the Kafka Topic without any schema information or the column name itself.</p>

<p>So if the column <code>json_doc</code> contained the value <code>{""foo"":""bar""}</code> then this is what would appear on the Topic: </p>

<pre><code>{""foo"":""bar""}
</code></pre>

<p>Here is some background information on how the KCQL works in the snapshot release.</p>

<p>The <code>SELECT</code> will now retrieve only the columns in that table that are specified in the KCQL. Originally it was always retrieving all of the columns. It is important to note that the PK column must be part of the <code>SELECT</code> statement when using the <code>incremental</code> import mode. If the value of the PK column is not something that should be included in the message published to the Kafka Topic then add it to the <code>IGNORE</code> statement (as in the example above). </p>

<p>The <code>WITHUNWRAP</code> is the new feature to KCQL that will tell the Cassandra Source connector to create a <code>SourceRecord</code> using the String <code>Schema</code> type (instead of Struct). In this mode only the values of the columns that are in the <code>SELECT</code> statement will be stored as the value of the <code>SourceRecord</code>. If there is more than one column in the <code>SELECT</code> statement after applying the <code>IGNORE</code> statement then the values are appended to together and separated by a comma.</p>
",['table']
42542845,42543413,2017-03-01 21:55:15,Apache Cassandra 3.10 IllegalArgumentException - Invalid token for Murmur3Partitioner,"<p>The token value that I'm using:</p>

<pre><code>initial_token: 85070591730234615865843651857942052864
</code></pre>

<p>Is causing the following Java exception when I try to start Cassandra:</p>

<pre><code>Exception (java.lang.IllegalArgumentException) encountered during startup: Invalid token for Murmur3Partitioner. Got 85070591730234615865843651857942052864 but expected a long value (unsigned 8 bytes integer).
java.lang.IllegalArgumentException: Invalid token for Murmur3Partitioner. Got 85070591730234615865843651857942052864 but expected a long value (unsigned 8 bytes integer).
    at org.apache.cassandra.dht.Murmur3Partitioner$2.fromString(Murmur3Partitioner.java:333)
    at org.apache.cassandra.dht.Murmur3Partitioner$2.validate(Murmur3Partitioner.java:317)
    at org.apache.cassandra.config.DatabaseDescriptor.applyInitialTokens(DatabaseDescriptor.java:885)
    at org.apache.cassandra.config.DatabaseDescriptor.applyAll(DatabaseDescriptor.java:321)
    at org.apache.cassandra.config.DatabaseDescriptor.daemonInitialization(DatabaseDescriptor.java:141)
    at org.apache.cassandra.service.CassandraDaemon.applyConfig(CassandraDaemon.java:646)
    at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:581)
    at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:735)
ERROR [main] 2017-03-01 19:57:15,861 CassandraDaemon.java:752 - Exception encountered during startup
java.lang.IllegalArgumentException: Invalid token for Murmur3Partitioner. Got 85070591730234615865843651857942052864 but expected a long value (unsigned 8 bytes integer).
    at org.apache.cassandra.dht.Murmur3Partitioner$2.fromString(Murmur3Partitioner.java:333) ~[apache-cassandra-3.10.jar:3.10]
    at org.apache.cassandra.dht.Murmur3Partitioner$2.validate(Murmur3Partitioner.java:317) ~[apache-cassandra-3.10.jar:3.10]
    at org.apache.cassandra.config.DatabaseDescriptor.applyInitialTokens(DatabaseDescriptor.java:885) ~[apache-cassandra-3.10.jar:3.10]
    at org.apache.cassandra.config.DatabaseDescriptor.applyAll(DatabaseDescriptor.java:321) ~[apache-cassandra-3.10.jar:3.10]
    at org.apache.cassandra.config.DatabaseDescriptor.daemonInitialization(DatabaseDescriptor.java:141) ~[apache-cassandra-3.10.jar:3.10]
    at org.apache.cassandra.service.CassandraDaemon.applyConfig(CassandraDaemon.java:646) [apache-cassandra-3.10.jar:3.10]
    at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:581) [apache-cassandra-3.10.jar:3.10]
    at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:735) [apache-cassandra-3.10.jar:3.10]
</code></pre>

<p>Cassandra starts fine on my first node with <code>initial_token: 0</code></p>

<p>What am I doing wrong?</p>

<p><strong>Any help is much appreciated!</strong></p>
",<java><cassandra><yaml><virtualbox><cluster-computing>,"<p>My main question would be why are you trying to set it?</p>

<p>That token (<code>85070591730234615865843651857942052864</code>) is likely for the random partitioner, not murmur partitioner. Can check other nodes in your cluster but perhaps set in the cassandra.yaml:</p>

<pre><code>partitioner: org.apache.cassandra.dht.RandomPartitioner
</code></pre>

<p>Murmur3 partitioner is better idea though, not sure where you got that token or why you expect it to work. Is better to let it assign tokens itself if your just adding nodes. Just do not set it.</p>

<pre><code># initial_token:
</code></pre>
",['partitioner']
42553767,42575856,2017-03-02 11:07:45,Efficient Filtering on a huge data frame in Spark,"<p>I have a Cassandra table with 500 million rows. I would like to filter based on a field which is a partition key in Cassandra using spark.</p>

<p>Can you suggest the best possible/efficient approach to filter in Spark/Spark SQL based on the list keys which is also a pretty large.</p>

<p>Basically i need only those rows from the Cassandra table which are present in the list of keys.</p>

<p>We are using DSE and its features.
The approach i am using is taking lot of time roughly around an hour.</p>
",<apache-spark><cassandra><apache-spark-sql><spark-dataframe><spark-cassandra-connector>,"<p>Have you checked <code>repartitionByCassandraReplica</code> and <code>joinWithCassandraTable</code> ?</p>

<p><a href=""https://github.com/datastax/spark-cassandra-connector/blob/75719dfe0e175b3e0bb1c06127ad4e6930c73ece/doc/2_loading.md#performing-efficient-joins-with-cassandra-tables-since-12"" rel=""nofollow noreferrer"">https://github.com/datastax/spark-cassandra-connector/blob/75719dfe0e175b3e0bb1c06127ad4e6930c73ece/doc/2_loading.md#performing-efficient-joins-with-cassandra-tables-since-12</a></p>

<blockquote>
  <p><code>joinWithCassandraTable</code> utilizes the java drive to execute a single
  query for every partition required by the source RDD so no un-needed
  data will be requested or serialized. This means a join between any
  RDD and a Cassandra Table can be performed without doing a full table
  scan. When performed between two Cassandra Tables which share the same
  partition key this will not require movement of data between machines.
  In all cases this method will use the source RDD's partitioning and
  placement for data locality.</p>
  
  <p>The method <code>repartitionByCassandraReplica</code> can be used to relocate data
  in an RDD to match the replication strategy of a given table and
  keyspace. The method will look for partition key information in the
  given RDD and then use those values to determine which nodes in the
  Cluster would be responsible for that data.</p>
</blockquote>
",['table']
42607183,42608589,2017-03-05 10:27:12,Spark with Cassandra python setup,"<p>I am trying to use spark to do some simple computations on Cassandra tables, but I am quite lost. </p>

<p>I am trying to follow: <a href=""https://github.com/datastax/spark-cassandra-connector/blob/master/doc/15_python.md"" rel=""nofollow noreferrer"">https://github.com/datastax/spark-cassandra-connector/blob/master/doc/15_python.md</a></p>

<p>So I'm running the PySpark shell: with</p>

<pre><code>./bin/pyspark \
  --packages com.datastax.spark:spark-cassandra-connector_2.11:2.0.0-M3
</code></pre>

<p>But I am not sure how to set things up from here. How do I let Spark know where my Cassandra cluster is? I've seen that <code>CassandraSQLContext</code> can be used for this, but I also read that this is deprecated.</p>

<p>I have read this: <a href=""https://stackoverflow.com/questions/37082562/how-to-connect-spark-with-cassandra-using-spark-cassandra-connector"">How to connect spark with cassandra using spark-cassandra-connector?</a></p>

<p>But if I use </p>

<pre><code>import com.datastax.spark.connector._
</code></pre>

<p>Python says that it can't find the module. 
Can someone maybe point me in the right direction on how to set things up properly? </p>
",<python><apache-spark><pyspark><cassandra><spark-cassandra-connector>,"<p>Cassandra connector doesn't provide any Python modules. All functionality is provided with <a href=""https://databricks.com/blog/2015/01/09/spark-sql-data-sources-api-unified-data-access-for-the-spark-platform.html"" rel=""nofollow noreferrer"">Data Source API</a> and as long as required jars are present, everything should work out of the box.</p>

<blockquote>
  <p>How do I let Spark know where my Cassandra cluster is? </p>
</blockquote>

<p>Use <code>spark.cassandra.connection.host</code> property. You can for exampel pass it as an argument for <code>spark-submit</code> / <code>pyspark</code>:</p>

<pre><code>pyspark ... --conf spark.cassandra.connection.host=x.y.z.v
</code></pre>

<p>or set in your configuration:</p>

<pre><code>(SparkSession.builder
    .config(""cassandra.connection.host"", ""x.y.z.v""))
</code></pre>

<p>Configuration like table name or keyspace can be set directly on reader:</p>

<pre><code>(spark.read
    .format(""org.apache.spark.sql.cassandra"")
    .options(table=""kv"", keyspace=""test"", cluster=""cluster"")
    .load())
</code></pre>

<p>So you can follows <a href=""https://github.com/datastax/spark-cassandra-connector/blob/master/doc/14_data_frames.md"" rel=""nofollow noreferrer"">Dataframes</a> documentation.</p>

<p>As a side note</p>

<pre><code>import com.datastax.spark.connector._
</code></pre>

<p>is a Scala syntax and is accepted in Python only accidentally.</p>
",['table']
42654524,42656570,2017-03-07 17:28:41,Proper way to insert iterative data into Cassandra using Python,"<p>Let's say I have cassandra table define like this:</p>

<pre><code>CREATE TABLE IF NOT EXISTS {} (
            user_id bigint ,
            username text,
            age int,
            PRIMARY KEY (user_id)
        );
</code></pre>

<p>I have 3 list of same size let's <code>1 000 000</code> records in each list. Is it a good practice to insert data using a for loop like this:</p>

<pre><code>for index, user_id in enumerate(user_ids):
    query = ""INSERT INTO TABLE (user_id, username, age) VALUES ({0}, '{1}', {1});"".format(user_id, username[index] ,age[index])
    session.execute(query)
</code></pre>
",<python><python-3.x><cassandra><datastax>,"<p>Prepared statements with concurrent execution will be your best bet. The driver provides utility functions for concurrent execution of statements with sequences of parameters, just as you have with your lists: <a href=""http://datastax.github.io/python-driver/api/cassandra/concurrent.html#cassandra.concurrent.execute_concurrent_with_args"" rel=""nofollow noreferrer"">execute_concurrent_with_args</a></p>

<p><a href=""https://docs.python.org/3/library/functions.html#zip"" rel=""nofollow noreferrer"">Zipping</a> your lists together will produce a sequence of parameter tuples suitable for input to that function.</p>

<p>Something like this:</p>

<pre><code>prepared = session.prepare(""INSERT INTO table (user_id, username, age) VALUES (?, ?, ?)"")
execute_concurrent_with_args(session, prepared, zip(user_ids, username, age))
</code></pre>
",['table']
42676334,42676607,2017-03-08 16:10:47,trouble modeling cassandra table,"<p>I want to use cassandra to save logs and read them later</p>

<p>Here is what I've done so far :</p>

<pre><code>CREATE TABLE logs
(
    id uuid,
    type int,
    start_date timestamp,
    end_date timestamp,
    ip text,
    log_event text,
    user_id text,
    user_agent text,
    PRIMARY KEY (id, type, start_date, user_id)
) WITH CLUSTERING ORDER BY (type ASC, start_date DESC, profil_token ASC);
</code></pre>

<p>What I need is always this kind of query :</p>

<pre><code>SELECT * FROM logs WHERE type = 1 AND user_id = 'test' AND start_date = '2017-03-08';
</code></pre>

<p>I need to query without id, but I can't because id is my real primary key</p>

<p>I don't see how to achieve this kind of query without ALLOW FILTERING</p>
",<cassandra><cql>,"<p>This depends on if you need the query to be fast. If you don't and can live with scanning over all rows, then <code>ALLOW FILTERING</code> is your fix.</p>

<p>If you need the query to be fast, you can either restructure the table so you're querying on a prefix or you can create a new table, denormalizing your data for faster queries.</p>

<p>I don't know enough about your use case, but making <code>user_id</code> part of the clustering key might work for you:</p>

<pre><code>PRIMARY KEY (user_id, start_date, type, id)
</code></pre>

<p>But this means you can't look up by <code>id</code> without knowing the <code>user_id</code> (and other fields). You could add a secondary index for that, though.</p>
",['table']
42694324,42695309,2017-03-09 11:33:25,"Cassandra (replication factor: 2, nodes: 3) and lightweight transactions","<p>We have a cassandra cluster running with 3 nodes and a replication factor of 2 -> maybe we should have selected 3 from the start, but this is not the case.</p>

<p>Our quorum is therefore = 2/2 + 1 = 2</p>

<p>Lets say we lose one node - so now only two cassandra nodes are online.</p>

<p>We still have the possibility to read from the cluster if we set our consistency level to ""ONE"" and then read -> so this is not a problem.</p>

<p>The thing I do not understand is the following.</p>

<p>We still have two nodes running, so why is it not possible to do a serial (lightweight transaction) insert into our keyspace? We have two nodes up, so shouldn't it be possible to get a quorum of 2 when trying to insert?</p>

<p>Is it because one of the row's is already put on the missing node?</p>
",<database><cassandra>,"<p>When you are trying to insert a data, the data is stored based on the token values(based on the partitioner configured) and replicated in a circular way.</p>

<p>For e.g. If you are inserting a data X in a keyspace with replication factor of 2 in a 3 node cluster Node1 (owning token A), Node2 (owning token B) and Node3 (Owning token C). Say if the data X is computed to token B, then Cassandra starts inserting data from Node2 and Node3 (till it completes the replicas). Say if the data X is computed to token C, then Cassandra starts inserting data from Node3 and Node1.</p>

<p>So setting consistency level of 2 means the data must be written in 2 nodes.
In your case even though you have 2 nodes up Node1 (token A) and Node2 (token B) and one node down Node3 (token C), if the data is computed and selected as token B, then Cassandra tries to insert in Node2 and Node3 and you get consistency error as it cannot insert in Node3. </p>

<p>So to insert you must either increase replication to 3 or decrease the consistency to 1.</p>

<p>To know more on consistency see this docs <a href=""https://docs.datastax.com/en/cassandra/2.1/cassandra/dml/dml_config_consistency_c.html"" rel=""noreferrer"">https://docs.datastax.com/en/cassandra/2.1/cassandra/dml/dml_config_consistency_c.html</a></p>
",['partitioner']
42736040,42736609,2017-03-11 13:53:12,Calculating the size of a table in Cassandra,"<p>In ""Cassandra The Definitive Guide"" (2nd edition) by Jeff Carpenter &amp; Eben Hewitt, the following formula is used to calculate the size of a table on disk (apologies for the blurred part):</p>

<p><a href=""https://i.stack.imgur.com/p2ZBe.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/p2ZBe.png"" alt=""table size equation""></a> </p>

<ul>
<li>ck: primary key columns</li>
<li>cs: static columns</li>
<li>cr: regular columns</li>
<li>cc: clustering columns</li>
<li>Nr: number of rows</li>
<li>Nv: it's used for counting the total size of the timestamps (I don't get this part completely, but for now I'll ignore it).</li>
</ul>

<p>There are two things I don't understand in this equation.</p>

<p><strong>First:</strong> why do clustering columns size gets counted for every regular column? Shouldn't we multiply it by the number of rows? It seems to me that by calculating this way, we're saying that the data in each clustering column, gets replicated for each regular column, which I suppose is not the case.</p>

<p><strong>Second:</strong> why do primary key columns don't get multiplied by the number of partitions? From my understanding, if we have a node with two partitions, then we should multiply the size of the primary key columns by two because we'll have two different primary keys in that node.</p>
",<cassandra>,"<p>It's because of Cassandra's version &lt; 3 internal structure.   </p>

<ul>
<li>There is only one entry for each distinct partition key value.</li>
<li>For each distinct partition key value there is only one entry for static column</li>
<li>There is an empty entry for the clustering key </li>
<li>For each column in a row there is a single entry for each clustering key column</li>
</ul>

<p>Let's take an example : </p>

<pre><code>CREATE TABLE my_table (
    pk1 int,
    pk2 int,
    ck1 int,
    ck2 int,
    d1 int,
    d2 int,
    s int static,
    PRIMARY KEY ((pk1, pk2), ck1, ck2)
); 
</code></pre>

<p>Insert some dummy data : </p>

<pre><code> pk1 | pk2 | ck1 | ck2  | s     | d1     | d2
-----+-----+-----+------+-------+--------+---------
   1 |  10 | 100 | 1000 | 10000 | 100000 | 1000000
   1 |  10 | 100 | 1001 | 10000 | 100001 | 1000001
   2 |  20 | 200 | 2000 | 20000 | 200000 | 2000001
</code></pre>

<p>Internal structure will be : </p>

<pre><code>             |100:1000:  |100:1000:d1|100:1000:d2|100:1001:  |100:1001:d1|100:1001:d2|  
-----+-------+-----------+-----------+-----------+-----------+-----------+-----------+
1:10 | 10000 |           |  100000   |  1000000  |           |  100001   |  1000001  |


             |200:2000:  |200:2000:d1|200:2000:d2|
-----+-------+-----------+-----------+-----------+ 
2:20 | 20000 |           |  200000   |  2000000  |
</code></pre>

<p>So size of the table will be : </p>

<pre><code>Single Partition Size = (4 + 4 + 4 + 4) + 4 + 2 * ((4 + (4 + 4)) + (4 + (4 + 4))) byte = 68 byte

Estimated Table Size = Single Partition Size * Number Of Partition 
                     = 68 * 2 byte
                     = 136 byte
</code></pre>

<ul>
<li>Here all of the field type is int (4 byte)</li>
<li>There is 4 primary key column, 1 static column, 2 clustering key column and 2 regular column</li>
</ul>

<p>More : <a href=""http://opensourceconnections.com/blog/2013/07/24/understanding-how-cql3-maps-to-cassandras-internal-data-structure/"" rel=""noreferrer"">http://opensourceconnections.com/blog/2013/07/24/understanding-how-cql3-maps-to-cassandras-internal-data-structure/</a></p>
",['table']
42751253,42766954,2017-03-12 18:16:39,Saving JSON in Cassandra via Python,"<p>I'm trying to save JSON into Cassandra: I have defined my table like:</p>

<pre><code>CREATE TABLE IF NOT EXISTS {} (
                node_id bigint PRIMARY KEY,
                tweets list&lt;text&gt;
            );
</code></pre>

<p>I'm using Tweepy to retrieve some tweets via Twitter API. So I got a list of Statut object containing a field <code>_json</code>:</p>

<pre><code>statuses = get_statuses(row.friend_follower_id)
tweets = [status._json for status in statuses]
</code></pre>

<p>Then I tried to save directly this list of JSON in Cassandra, but I got the following error:</p>

<pre><code>cassandra.protocol.SyntaxException: &lt;Error from server: code=2000 [Syntax error in CQL query] message=""line 1:88 no viable alternative at input ',' (... (14862190, [{'in_reply_to_screen_name': [None],...)""&gt;
</code></pre>

<p>And I remember that I specified <code>text</code> in my table definition so I cast thos dictionary into a string:</p>

<pre><code>tweets_string = [str(tweet) for tweet in tweets]
query = ""INSERT INTO tweets (node_id, tweets) VALUES ({0}, {1})"".format(node_id, tweets_string)
session.execute(query)
</code></pre>

<p>But I get the following error:</p>

<pre><code>cassandra.protocol.SyntaxException: &lt;Error from server: code=2000 [Syntax error in CQL query] message=""line 1:69 no viable alternative at character '\'""&gt;
</code></pre>

<p>How can I save those JSON into Cassandra? Is my model innacurate?</p>

<p><strong>EDIT</strong>:</p>

<p>As asked this is a sample of one JSON and a node_id:</p>

<p><em>node_id</em>:</p>

<pre><code>14862190
</code></pre>

<p><em>JSON</em>:</p>

<pre><code>{'quoted_status_id': 775731184256978944, 'truncated': False, 'coordinates': None, 'in_reply_to_user_id_str': None, 'lang': 'en', 'retweeted': False, 'in_reply_to_status_id_str': None, 'in_reply_to_screen_name': None, 'created_at': 'Tue Sep 13 21:05:28 +0000 2016', 'possibly_sensitive': False, 'quoted_status_id_str': '775731184256978944', 'in_reply_to_status_id': None, 'place': {'country_code': 'US', 'name': 'Delaware', 'place_type': 'admin', 'full_name': 'Delaware, USA', 'url': 'https://api.twitter.com/1.1/geo/id/3f5897b87d2bf56c.json', 'attributes': {}, 'id': '3f5897b87d2bf56c', 'bounding_box': {'coordinates': [[[-75.7887564, 38.4510398], [-74.984165, 38.4510398], [-74.984165, 39.839007], [-75.7887564, 39.839007]]], 'type': 'Polygon'}, 'contained_within': [], 'country': 'Etats-Unis'}, 'user': {'is_translator': False, 'is_translation_enabled': False, 'lang': 'en', 'contributors_enabled': False, 'profile_background_color': '1A1B1F', 'profile_background_tile': False, 'default_profile_image': False, 'verified': True, 'name': 'ColinFlaherty', 'id_str': '14862190', 'profile_link_color': '2FC2EF', 'notifications': False, 'geo_enabled': True, 'url': 'https://short', 'id': 14862190, 'protected': False, 'screen_name': 'colinflaherty', 'description': ""Author of two Amazon #1 Best Sellers: Don't Make the Black Kids Angry and White Girl Bleed a Lot. YouTube channel."", 'translator_type': 'none', 'entities': {'url': {'urls': [{'url': 'https://short', 'indices': [0, 23], 'display_url': 'ColinFlaherty.com', 'expanded_url': 'http://www.ColinFlaherty.com'}]}, 'description': {'urls': []}}, 'following': False, 'profile_sidebar_border_color': 'FFFFFF', 'profile_background_image_url_https': 'https://pbs.twimg.com/profile_background_images/378800000021228899/32bec7f67389fe6272155bbc1450054f.jpeg', 'profile_sidebar_fill_color': '252429', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/663924202592931840/l14MTV7z_normal.png', 'profile_image_url': 'http://pbs.twimg.com/profile_images/663924202592931840/l14MTV7z_normal.png', 'created_at': 'Wed May 21 22:40:37 +0000 2008', 'friends_count': 492, 'favourites_count': 7072, 'location': 'California and Delaware.', 'profile_text_color': '666666', 'utc_offset': -14400, 'follow_request_sent': False, 'time_zone': 'Eastern Time (US &amp; Canada)', 'has_extended_profile': True, 'profile_background_image_url': 'http://pbs.twimg.com/profile_background_images/378800000021228899/32bec7f67389fe6272155bbc1450054f.jpeg', 'default_profile': False, 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/14862190/1398365259', 'followers_count': 12246, 'profile_use_background_image': True, 'statuses_count': 37094, 'listed_count': 202}, 'source': '&lt;a href=""http://twitter.com/download/iphone"" rel=""nofollow""&gt;Twitter for iPhone&lt;/a&gt;', 'favorite_count': 4, 'id': 775802602466349056, 'in_reply_to_user_id': None, 'entities': {'symbols': [], 'urls': [{'url': 'https://short', 'indices': [14, 37], 'display_url': 'twitter.com/trotlinedesign…', 'expanded_url': 'https://twitter.com/trotlinedesigns/status/775731184256978944'}], 'user_mentions': [], 'hashtags': []}, 'favorited': False, 'quoted_status': {'truncated': False, 'coordinates': None, 'in_reply_to_user_id_str': '14862190', 'lang': 'en', 'retweeted': False, 'in_reply_to_status_id_str': None, 'in_reply_to_screen_name': 'colinflaherty', 'created_at': 'Tue Sep 13 16:21:41 +0000 2016', 'in_reply_to_status_id': None, 'place': {'country_code': 'US', 'name': 'Florida', 'place_type': 'admin', 'full_name': 'Florida, USA', 'url': 'https://api.twitter.com/1.1/geo/id/4ec01c9dbc693497.json', 'attributes': {}, 'id': '4ec01c9dbc693497', 'bounding_box': {'coordinates': [[[-87.634643, 24.396308], [-79.974307, 24.396308], [-79.974307, 31.001056], [-87.634643, 31.001056]]], 'type': 'Polygon'}, 'contained_within': [], 'country': 'Etats-Unis'}, 'user': {'is_translator': False, 'is_translation_enabled': False, 'lang': 'en', 'contributors_enabled': False, 'profile_background_color': '352726', 'profile_background_tile': False, 'default_profile_image': False, 'verified': False, 'name': 'Ron Joseph', 'id_str': '68369722', 'profile_link_color': 'D02B55', 'notifications': False, 'geo_enabled': True, 'url': 'https://short', 'id': 68369722, 'protected': False, 'screen_name': 'TrotlineDesigns', 'description': ""They say I am apathetic but really... I don't care! \n#AmericaFirst. Disabled Ret. Military 86-09 Artist, Cartoonist work seen on Fox News more than once."", 'translator_type': 'none', 'entities': {'url': {'urls': [{'url': 'https://short', 'indices': [0, 23], 'display_url': 'etsy.com/listing/256807…', 'expanded_url': 'https://www.etsy.com/listing/256807641/2016-trump?ref=shop_home_active_1'}]}, 'description': {'urls': []}}, 'following': False, 'profile_sidebar_border_color': 'FFFFFF', 'profile_background_image_url_https': 'https://pbs.twimg.com/profile_background_images/614017620703117312/sU9TaJrc.jpg', 'profile_sidebar_fill_color': '99CC33', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/820943979315347457/LYA0ZAv3_normal.jpg', 'profile_image_url': 'http://pbs.twimg.com/profile_images/820943979315347457/LYA0ZAv3_normal.jpg', 'created_at': 'Mon Aug 24 09:28:41 +0000 2009', 'friends_count': 3738, 'favourites_count': 7134, 'location': 'Clewiston, FL', 'profile_text_color': '3E4415', 'utc_offset': -14400, 'follow_request_sent': False, 'time_zone': 'Eastern Time (US &amp; Canada)', 'has_extended_profile': False, 'profile_background_image_url': 'http://pbs.twimg.com/profile_background_images/614017620703117312/sU9TaJrc.jpg', 'default_profile': False, 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/68369722/1458797839', 'followers_count': 4351, 'profile_use_background_image': True, 'statuses_count': 108098, 'listed_count': 148}, 'source': '&lt;a href=""http://twitter.com"" rel=""nofollow""&gt;Twitter Web Client&lt;/a&gt;', 'favorite_count': 0, 'retweet_count': 0, 'in_reply_to_user_id': 14862190, 'entities': {'symbols': [], 'urls': [], 'user_mentions': [{'name': 'ColinFlaherty', 'id': 14862190, 'id_str': '14862190', 'screen_name': 'colinflaherty', 'indices': [0, 14]}], 'hashtags': []}, 'favorited': False, 'id': 775731184256978944, 'geo': None, 'is_quote_status': False, 'id_str': '775731184256978944', 'contributors': None, 'text': '@colinflaherty Honestly just talk to a prison guard. Not me.. I am not asking for anything but you have to have followers that are guards.'}, 'geo': None, 'is_quote_status': True, 'retweet_count': 1, 'id_str': '775802602466349056', 'contributors': None, 'text': 'Tons of them  https://short'}
</code></pre>
",<python><json><python-3.x><cassandra>,"<p>The problem here is the size of my list which is 3200. I review the model of my table to solve the problem.</p>
",['table']
42766090,42767459,2017-03-13 14:27:35,Insert big list into Cassandra using python,"<p>I have a problem inserting big list into Cassandra using python. I have a list of 3200 string that I want to save in Cassandra:</p>

<pre><code>CREATE TABLE IF NOT EXISTS my_test (
                id bigint PRIMARY KEY,
                list_strings list&lt;text&gt;
            );
</code></pre>

<p>When I'm reducing my list I have no problem. It works.</p>

<pre><code>prepared_statement = session.prepare(""INSERT INTO my_test (id, list_strings) VALUES (?, ?)"")
        session.execute(prepared_statement, [id, strings[:5]])
</code></pre>

<p>But if I keep the totality of my list I have an error:</p>

<pre><code>Error from server: code=1500 [Replica(s) failed to execute write] message=""Operation failed - received 0 responses and 1 failures"" info={'required_responses': 1, 'consistency': 'LOCAL_ONE', 'received_responses': 0, 'failures': 1}
</code></pre>

<p>How can I insert big list into Cassandra? </p>
",<python><list><python-3.x><cassandra>,"<p>A DB array type is not supossed to hold that ammount of data. Using different rows of the table to store each string would be better:</p>

<pre><code>    id     |    time    | strings
-----------+------------+---------
  bigint   | timestamp  | string
 partition | clustering |
</code></pre>

<p>Using id as the clustering key would be a bad solution as when requesting all the tweets from a user id, it will require to do a read in multiple nodes while when used as a partition key it will only require to read in one node per user.</p>
",['table']
42770658,42770911,2017-03-13 18:15:33,Cassandra DevCenter get Cassandra Version,"<p>I use Cassandra dev center to write queries against Cassandra database. </p>

<p>If I was using the cqlsh shell, getting the version of cassandra is very easy. you can just read it when you run the shell. or you can issue queries like</p>

<pre><code>show version
select @@version
</code></pre>

<p>But sadly none of these approaches work when you are using the cassandra devcenter. </p>

<p>Does anyone know how to query the version of cassandra when you are using the devcenter?</p>
",<cassandra>,"<p>In DevCenter you should be able to query the system.local table of (whichever) node serves your query:</p>

<pre><code>SELECT release_version FROM system.local;
</code></pre>

<p>ex:
<a href=""https://i.stack.imgur.com/6kaB8.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/6kaB8.png"" alt=""enter image description here""></a></p>
",['table']
42786410,42793733,2017-03-14 12:45:18,Datastax Cassandra C/C++ driver cass_cluster_set_blacklist_filtering functionality,"<p>Datastax C/C++ driver has a blacklist filtering functionality as part of its load balancing controls.</p>

<p><a href=""https://docs.datastax.com/en/developer/cpp-driver/2.5/topics/configuration/"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/developer/cpp-driver/2.5/topics/configuration/</a></p>

<p>Correct me If I missing something but my understanding is that a CQL client can't connect to blacklisted hosts.</p>

<p>I'm using C/C++ driver v2.5 and the below codeblock and trying to connect to a multinode cluster:</p>

<pre><code>CassCluster* cluster = cass_cluster_new();
CassSession* session = cass_session_new();
const char* hosts    = ""192.168.57.101"";
cass_cluster_set_contact_points(cluster, hosts);
cass_cluster_set_blacklist_filtering(cluster, hosts);
CassFuture* connect_future = cass_session_connect(session, cluster);
</code></pre>

<p>In this codeblock the host to which the CQL client is trying to connect is set as blacklisted. However, CQL client seems to connect to this host and executes any queries. Is there something wrong with the above codeblock? If not so, is this the expected behavior? Does it behaves differently because it is a multinode cluster and establish connection to the other peers?</p>

<p>Any help will be appreciated.</p>

<p>Thank you in advance</p>
",<cassandra><datastax>,"<p>Since you are supplying only one contact point, that IP address is being used to establish the control connection into the cluster. Once that control connection is established and the peers table is read to determine other nodes available in the cluster, connections are made to those other nodes. At this point all queries will be routed to those other nodes and not your initial/blacklisted contact point; however the connection to the initial contact point will remain as it is the control connection into the cluster.</p>

<p>To get a better look at what is going on inside the driver you can enable logging in the driver. Here is an example to enable logging via the console:</p>

<pre><code>void on_log(const CassLogMessage* message, void* data) {
  fprintf(stderr, ""%u.%03u [%s] (%s:%d:%s): %s\n"",
  (unsigned int) (message-&gt;time_ms / 1000),
  (unsigned int) (message-&gt;time_ms % 1000),
  cass_log_level_string(message-&gt;severity),
  message-&gt;file, message-&gt;line, message-&gt;function,
  message-&gt;message);
}

/* Log configuration *MUST* be done before any other driver call */
cass_log_set_level(CASS_LOG_TRACE);
cass_log_set_callback(on_log, NULL);
</code></pre>

<p>In order to reduce the extra connection on a node that will be blacklisted you can supply a different contact point into the cluster that is not the same as the node (or nodes) that will be blacklisted.</p>
",['table']
42813635,42835376,2017-03-15 15:08:46,Read optimisation cassandra using python,"<p>I have a table with the following model:</p>

<pre><code>CREATE TABLE IF NOT EXISTS {} (
                user_id bigint ,
                pseudo text,
                importance float,
                is_friend_following bigint,
                is_friend boolean,
                is_following boolean,
                PRIMARY KEY ((user_id), is_friend_following)
            );
</code></pre>

<p>I also have a table containing my seeds. Those (20) users are the starting point of my graph. So I select their ID and search in the table above to get their Followers and friends, and from there I build my graph (networkX).</p>

<pre><code>def build_seed_graph(cls, name):
    obj = cls()
    obj.name = name
    query = ""SELECT twitter_id FROM {0};""
    seeds = obj.session.execute(query.format(obj.seed_data_table))
    obj.graph.add_nodes_from(obj.seeds)
    for seed in seeds:
        query = ""SELECT friend_follower_id, is_friend, is_follower FROM {0} WHERE user_id={1}""
        statement = SimpleStatement(query.format(obj.network_table, seed), fetch_size=1000)
        friend_ids = []
        follower_ids = []
        for row in obj.session.execute(statement):
            if row.friend_follower_id in obj.seeds:
                if row.is_friend:
                    friend_ids.append(row.friend_follower_id)
                if row.is_follower:
                    follower_ids.append(row.friend_follower_id)
        if friend_ids:
            for friend_id in friend_ids:
                obj.graph.add_edge(seed, friend_id)
        if follower_ids:
            for follower_id in follower_ids:
                obj.graph.add_edge(follower_id, seed)
    return obj
</code></pre>

<p>The problem is that the time it takes to build the graph is too long and I would like to optimize it.
I've got approximately 5 millions rows in  my table <code>'network_table'</code>. </p>

<p>I'm wondering if it would be faster for me if instead of doing a query with a where clauses to just do a single query on whole table? Will it fit in memory? Is that a good Idea? Are there better way?</p>
",<python><python-3.x><cassandra><datastax>,"<p>I suspect the real issue may not be the queries but rather the processing time.</p>

<blockquote>
  <p>I'm wondering if it would be faster for me if instead of doing a query with a where clauses to just do a single query on whole table? Will it fit in memory? Is that a good Idea? Are there better way?</p>
</blockquote>

<p>There should not be any problem with doing a single query on the whole table if you enable paging (<a href=""https://datastax.github.io/python-driver/query_paging.html"" rel=""nofollow noreferrer"">https://datastax.github.io/python-driver/query_paging.html</a> - using fetch_size). Cassandra will return up to the fetch_size and will fetch additional results as you read them from the result_set.</p>

<p>Please note that if you have many rows in the table that are non seed related then a full scan may be slower as you will receive rows that will not include a ""seed""</p>

<p>Disclaimer - I am part of the team building ScyllaDB - a Cassandra compatible database.</p>

<p>ScyllaDB have published lately a blog on how to efficiently do a full scan in parallel <a href=""http://www.scylladb.com/2017/02/13/efficient-full-table-scans-with-scylla-1-6/"" rel=""nofollow noreferrer"">http://www.scylladb.com/2017/02/13/efficient-full-table-scans-with-scylla-1-6/</a> which applies to Cassandra as well - if a full scan is relevant and you can build the graph in parallel than this may help you.</p>
",['table']
42878994,43495516,2017-03-18 19:40:21,Remove Duplicates without shuffle Spark,"<p>I have a Cassandra table XYX with columns(
id uuid,
insert a timestamp,
header text)</p>

<p>Where id and insert are composite primary key.</p>

<p>I'm using Dataframe and in my spark shell I'm fetching id and header column.
I want to have distinct rows based on id and header column.</p>

<p>I'm seeing lot of shuffles which not be the case since Spark Cassandra connector ensures that all rows for a given Cassandra partition are in same spark partition.</p>

<p>After fetching I'm using dropDuplicates to get distinct records.</p>
",<apache-spark><cassandra><spark-cassandra-connector>,"<p>Spark Dataframe API does not support custom partitioners yet. So the Connector could not introduce the C* partitioner to Dataframe engine.
An RDD Spark API supports custom partitioner from other hand. Thus you could load your data into RDD and then covert it to df.
Here is a Connector doc about C* partitioner usage: <a href=""https://github.com/datastax/spark-cassandra-connector/blob/master/doc/16_partitioning.md"" rel=""nofollow noreferrer"">https://github.com/datastax/spark-cassandra-connector/blob/master/doc/16_partitioning.md</a></p>

<p>keyBy() function allow you to define key columns to use for grouping</p>

<p>Here is working example. It is not short, so I expect someone could improve it:</p>

<pre><code>//load data into RDD and define a group key
val rdd = sc.cassandraTable[(String, String)] (""test"", ""test"")
   .select(""id"" as ""_1"", ""header"" as ""_2"")
   .keyBy[Tuple1[Int]](""id"")
// check that partitioner is CassandraPartitioner
rdd.partitioner
// call distinct for each group, flat it, get two column DF
val df = rdd.groupByKey.flatMap {case (key,group) =&gt; group.toSeq.distinct}
    .toDF(""id"", ""header"")
</code></pre>
",['partitioner']
42916659,42917345,2017-03-21 01:04:14,How to model inbox,"<p>How would ago about modelling the data if I have a web app for messaging and I expect the user to either see all the messages ordered by date, or see the messages exchanged with a specific contact, again ordered by date.</p>

<p>Should I have two tables, called ""global_inbox"" and ""contacts_inbox"" where I would add each message to both?</p>

<p>For example:</p>

<pre><code>CREATE TABLE global_inbox(user_id int, timestamp timestamp, 
                          message text, PRIMARY KEY(user_id, timestamp)

CREATE TABLE inbox(user_id int, contact_id int, 
                   timestamp timestapm, message text, 
                   PRIMARY KEY(user_id, contact_id, timestamp)
</code></pre>

<p>This means that every message should be copied 4 times, 2 for sender and 2 for receiver. Does it sound reasonable?</p>
",<cassandra><data-modeling>,"<p>Yes, It's reasonable.<br>
You need some modification.    </p>

<ul>
<li>Inbox table : If a user have many contact and every contact send message, then a huge amount of data will be inserted into a single partition (user_id). So add contact_id to partition key.</li>
</ul>

<p>Updated Schema : </p>

<pre><code>CREATE TABLE inbox (
     user_id int, 
     contact_id int, 
     timestamp timestamp, 
     message text, 
     PRIMARY KEY((user_id, contact_id), timestamp)
);
</code></pre>

<ul>
<li>global_inbox : Though It's global inbox, a huge amount of data can be inserted into a single partition (user_id). So add more key to partition key to more distribution.</li>
</ul>

<p>Updated Schema : </p>

<pre><code>CREATE TABLE global_inbox (
     user_id int,
     year int,
     month int, 
     timestamp timestamp, 
     message text, 
     PRIMARY KEY((user_id,year,month), timestamp)
);
</code></pre>

<p>Here you can also add also add week to partition key, if you have huge data in a single partition in a week. Or remove month from partition key if you think not much data will insert in a year.</p>
",['table']
42932204,42935733,2017-03-21 15:58:03,Cassandra - Which IP address do you use to connect to a cluster when using a driver?,"<p>I have deployed a 3-node Cassandra cluster on AWS (EC2).  I'm trying to connect to the cluster from a .NET console application running on my computer.  The Datastax website provides the following code sample for connecting to a local instance:</p>

<p><code>Cluster cluster = Cluster.Builder().AddContactPoint(""127.0.0.1"").Build();</code></p>

<p>I've set cassandra.yaml such that <code>nodetool status</code> displays the node's address as the EC2 instance's private IP address.  However, if I use the private address in the <code>AddContactPoint()</code> function, the .NET driver returns an error saying that <code>none of the hosts tried for query are available</code>.</p>

<p><a href=""https://i.stack.imgur.com/nsuaT.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nsuaT.jpg"" alt=""enter image description here""></a></p>

<p>Which IP address do you use (instead of 127.0.0.1) in the <code>AddContactPoint()</code> function for connecting to the Cassandra cluster in AWS?  The EC2 instance's public or private IP address?  Or something else?</p>

<p>My settings in cassandra.yaml are:</p>

<pre><code>rpc_address: 0.0.0.0
broadcast_rpc_address: *EC2's public IP address*
listen_address: *EC2's private IP address*
</code></pre>

<p>Thanks in advance.</p>
",<.net><amazon-web-services><amazon-ec2><cassandra>,"<p>For the broadcast_rpc_address you would use the EC2's Public IP address as that's the address at which all your clients be connecting. listen_address is used for internode communication between nodes in the cluster. </p>
","['broadcast_rpc_address', 'listen_address']"
42932895,42933196,2017-03-21 16:26:07,Performance impact of Allow filtering on same partition query in cassandra,"<p>I have table like this.</p>

<pre><code>CREATE TABLE posts (
topic text
country text,
bookmarked text,
id uuid,
PRIMARY KEY (topic,id)
);
</code></pre>

<p>First query on single partition with allow filtering.</p>

<pre><code>select * from posts where topic='cassandra' allow filtering;
</code></pre>

<p>Second query on single partition without allow filtering.</p>

<pre><code>select * from posts where topic='cassandra';
</code></pre>

<p>My question is what is performance difference between first query and second query? Will first query(with allow filtering) get result from all partition before filtering though we have requested from single partition.</p>

<p>Thanks.</p>
",<cassandra>,"<p>Allow filtering will allow you to run queries without specifying partition key. But if you using one, it will use only specific partition.</p>

<p>In this specific example you should see no difference.</p>

<p>Ran both queries on my test table with tracing on, got single partition in both execution plans:</p>

<pre><code> Executing single-partition query on table_name
</code></pre>
",['table']
42937387,42944516,2017-03-21 20:19:11,How do I get log4j messages to log to Cassandra?,"<p>I’m getting killed here trying to redirect log4j to a Cassandra database. I’ve spent an enormous amount of time trying everything I could think of, so I won’t be able to cover everything I’ve tried, but I’ll try to describe what I’m trying to do and what I’m running into as concisely as I can.</p>

<p>Our codebase currently uses log4j 1.2.17, so my first try is to get com.datastax.logging.appender.CassandraAppender working.<br>
The relevant Maven dependency is this:</p>

<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;com.datastax.logging&lt;/groupId&gt;
    &lt;artifactId&gt;cassandra-log4j-appender&lt;/artifactId&gt;
    &lt;version&gt;3.1.0&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<p>The relevant appender entry in log4j.xml is this: </p>

<pre><code>&lt;appender name=""cassandra"" class=""com.datastax.logging.appender.CassandraAppender""/&gt;
</code></pre>

<p>The appender defaults to “localhost”, or “127.0.0.1”, I don’t remember which, and I’ve tried both explicitly. It also defaults to port 9042, which is what my Cassandra is configured for. In fact, here’s the console output from Cassandra:</p>

<pre><code>Binding thrift service to localhost/127.0.0.1:9160
</code></pre>

<p>All I can get out of this is an exception:</p>

<pre><code>log4j:ERROR Error
com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: localhost/127.0.0.1:9042 (null))
    at com.datastax.driver.core.ControlConnection.reconnectInternal(ControlConnection.java:196)
    at com.datastax.driver.core.ControlConnection.connect(ControlConnection.java:80)
    at com.datastax.driver.core.Cluster$Manager.init(Cluster.java:1145)
    at com.datastax.driver.core.Cluster.init(Cluster.java:149)
    at com.datastax.driver.core.Cluster.connect(Cluster.java:225)
    at com.datastax.logging.appender.CassandraAppender.initClient(CassandraAppender.java:141)
    at com.datastax.logging.appender.CassandraAppender.append(CassandraAppender.java:97)
    at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
    at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
    at org.apache.log4j.Category.callAppenders(Category.java:206)
    &lt;snip&gt;
log4j:ERROR Error setting up cassandra logging schema: com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: localhost/127.0.0.1:9042 (null))
</code></pre>

<p>After banging my head against the wall, I thought I would try to get the Apache Cassandra appender working. Unfortunately, it’s only for log4j2, and updating our codebase is non-trivial. That said, I’m having even less luck with it.</p>

<p>I created a small test project to try to get something working. In log4j2, Cassandra is supposed to be a built-in appender type, according to <a href=""https://logging.apache.org/log4j/2.x/manual/appenders.html#CassandraAppender"" rel=""noreferrer"">https://logging.apache.org/log4j/2.x/manual/appenders.html#CassandraAppender</a>. However, it absolutely can’t find it when I specify it in my log4j2.xml file. Here’s my file:</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;Configuration status=""INFO""&gt;
&lt;Appenders&gt;
    &lt;Console name=""Console"" target=""SYSTEM_OUT""&gt;
        &lt;PatternLayout pattern=""%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n"" /&gt;
    &lt;/Console&gt;
    &lt;File name=""MyFile"" fileName=""all.log"" immediateFlush=""false"" append=""false""&gt;
        &lt;PatternLayout pattern=""%d{yyy-MM-dd HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n""/&gt;
    &lt;/File&gt;
    &lt;Cassandra name=""Cassandra"" clusterName=""Test Cluster"" keyspace=""gii"" table=""gii_event_log"" bufferSize=""10"" batched=""true""&gt;
        &lt;SocketAddress host=""localhost"" port=""9042""/&gt;
        &lt;ColumnMapping name=""id"" pattern=""%uuid{TIME}"" type=""java.util.UUID""/&gt;
        &lt;ColumnMapping name=""identifier"" pattern=""%marker""/&gt;
        &lt;ColumnMapping name=""message"" pattern=""%message""/&gt;
        &lt;ColumnMapping name=""priority"" pattern=""%level""/&gt;
        &lt;ColumnMapping name=""scope"" pattern=""%level""/&gt;
        &lt;ColumnMapping name=""time_stamp"" literal=""now()""/&gt;
        &lt;ColumnMapping name=""type"" pattern=""%level""/&gt;
    &lt;/Cassandra&gt;
&lt;/Appenders&gt;
&lt;Loggers&gt;
    &lt;Root level=""debug""&gt;
        &lt;AppenderRef ref=""Console""/&gt;
        &lt;AppenderRef ref=""MyFile""/&gt;
        &lt;AppenderRef ref=""Cassandra""/&gt;
    &lt;/Root&gt;
&lt;/Loggers&gt;
&lt;/Configuration&gt;
</code></pre>

<p>Here’s what happens:</p>

<pre><code>Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/logging/log4j/LogManager
        at com.ge.enconn.TestClass.&lt;init&gt;(TestClass.java:10)
        at com.ge.enconn.App.main(App.java:11)
Caused by: java.lang.ClassNotFoundException: org.apache.logging.log4j.LogManager
        at java.net.URLClassLoader.findClass(Unknown Source)
        at java.lang.ClassLoader.loadClass(Unknown Source)
        at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source)
        at java.lang.ClassLoader.loadClass(Unknown Source)
        ... 2 more
</code></pre>

<p>Here are the relevant dependencies from my pom file:</p>

<pre><code>&lt;dependency&gt;
  &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;
  &lt;artifactId&gt;log4j-api&lt;/artifactId&gt;
  &lt;version&gt;${log4j.version}&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
  &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;
  &lt;artifactId&gt;log4j-core&lt;/artifactId&gt;
  &lt;version&gt;${log4j.version}&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
  &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;
  &lt;artifactId&gt;log4j-nosql&lt;/artifactId&gt;
  &lt;version&gt;${log4j.version}&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
  &lt;groupId&gt;com.datastax.cassandra&lt;/groupId&gt;
  &lt;artifactId&gt;cassandra-driver-core&lt;/artifactId&gt;
  &lt;version&gt;3.1.0&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<p>The pom for log4j-nosql is here: <a href=""https://github.com/apache/logging-log4j2/blob/master/log4j-nosql/pom.xml"" rel=""noreferrer"">https://github.com/apache/logging-log4j2/blob/master/log4j-nosql/pom.xml</a>. This should get it into my classpath, right?</p>
",<maven><logging><cassandra><log4j><log4j2>,"<p>First Use this inside dependencies tag of the pom file : </p>

<pre><code>&lt;!-- Apache Cassandra --&gt;
&lt;dependency&gt;
    &lt;groupId&gt;com.datastax.cassandra&lt;/groupId&gt;
    &lt;artifactId&gt;cassandra-driver-core&lt;/artifactId&gt;
    &lt;version&gt;3.1.4&lt;/version&gt;
&lt;/dependency&gt;

&lt;!-- Log4j2 --&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;
    &lt;artifactId&gt;log4j-api&lt;/artifactId&gt;
    &lt;version&gt;2.8.1&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;
    &lt;artifactId&gt;log4j-core&lt;/artifactId&gt;
    &lt;version&gt;2.8.1&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;
    &lt;artifactId&gt;log4j-1.2-api&lt;/artifactId&gt;
    &lt;version&gt;2.8.1&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;
    &lt;artifactId&gt;log4j-jcl&lt;/artifactId&gt;
    &lt;version&gt;2.8.1&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;
    &lt;artifactId&gt;log4j-slf4j-impl&lt;/artifactId&gt;
    &lt;version&gt;2.8.1&lt;/version&gt;
&lt;/dependency&gt;

&lt;dependency&gt;
    &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;
    &lt;artifactId&gt;log4j-nosql&lt;/artifactId&gt;
    &lt;version&gt;2.8.1&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<p>Second Create table in Cassandra : </p>

<pre><code>CREATE TABLE gii_event_log (
    id timeuuid PRIMARY KEY,
    identifier text,
    message text,
    priority text,
    scope text,
    time_stamp timeuuid,
    type text
);
</code></pre>

<p>Third Update your log4j2 file check host value if you get No Host available ERROR. And add username and password inside Cassandra tag if you get Authentication ERROR.</p>

<p>Sample Java Code : </p>

<pre><code>import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

/**
 *
 * @author Ashraful Islam
 */
public class CassandraLog {

    private final static Logger LOGGER = LogManager.getLogger();

    public void printDemo() {
        for (int i = 0; i &lt; 10; i++) {
            System.out.println(i);
            LOGGER.info(""Testing {}"", i);
            LOGGER.error(""Testing {}"", i);
            LOGGER.debug(""Testing {}"", i);
        }
    }

    /**
     * @param args the command line arguments
     */
    public static void main(String[] args) {
        new CassandraLog().printDemo();
    }

}
</code></pre>

<p>Output : </p>

<pre><code>id                                   | identifier | message   | priority | scope | time_stamp                           | type
--------------------------------------+------------+-----------+----------+-------+--------------------------------------+-------
 b85caa03-0ecb-11e7-af5e-b083fe92c73a |            | Testing 1 |    ERROR | ERROR | b85222b2-0ecb-11e7-a374-55d83eefb705 | ERROR
 b85eccec-0ecb-11e7-af5e-b083fe92c73a |            | Testing 4 |    DEBUG | DEBUG | b85222bb-0ecb-11e7-a374-55d83eefb705 | DEBUG
 b8602c83-0ecb-11e7-af5e-b083fe92c73a |            | Testing 6 |    ERROR | ERROR | b85222c2-0ecb-11e7-a374-55d83eefb705 | ERROR
 b862285b-0ecb-11e7-af5e-b083fe92c73a |            | Testing 9 |    DEBUG | DEBUG | b85222ca-0ecb-11e7-a374-55d83eefb705 | DEBUG
</code></pre>
",['table']
42997099,42999813,2017-03-24 10:29:40,Override TTL Cassandra,"<p>I am using a Cassandra database for my application, and I am setting a TTL per request. For testing purpose I am using another database (same schema, but in local) and I am willing to keep data and I was wondering if there is a way to override the ttl, I don't know, by setting it to default while creating tables or something like that.</p>

<p>Thank you.</p>
",<cassandra><cql><ttl>,"<blockquote>
  <p>Setting a TTL for a specific column</p>
  
  <p>Use CQL to set the TTL for data.</p>
  
  <p>To change the TTL of a specific column, you must re-insert the data
  with a new TTL. Cassandra upserts the column with the new TTL,
  replacing the old value with the old TTL, if any exists.</p>
  
  <p>Setting a TTL for a table</p>
  
  <p>The CQL table definition supports the default_time_to_live property,
  which applies a specific TTL to each column in the table. After the
  default_time_to_live TTL value has been exceed, Cassandra tombstones
  the entire table. Apply this default TTL to a table in CQL using
  CREATE TABLE or ALTER TABLE.</p>
</blockquote>

<p>If your table has TTL value not equal to 0 in local environment use </p>

<pre><code>ALTER TABLE table_name
  WITH  default_time_to_live= 0
</code></pre>

<p>This will change table level TTL.</p>

<p>If column level TTL is set change the code to insert record with 0 or some higher TTL.</p>

<p><a href=""https://docs.datastax.com/en/cql/3.1/cql/cql_using/use_expire_c.html"" rel=""nofollow noreferrer"">Details</a></p>
",['table']
43033703,43035469,2017-03-26 19:48:24,Migrate data model from MySQL to Cassandra,"<p><strong>Structure in MySql</strong> (for compactness i am using a simplified notation)</p>

<p><em>Notation: table name->[column1(key or index), column2, …]</em></p>

<pre><code>documents-&gt;[doc_id(primary key), title, description]
elements-&gt;[element_id(primary key), doc_id(index), title, description]
</code></pre>

<p>Each document can contain a large number of elements (between 1 and 100k+) </p>

<p>We have two key requirements:</p>

<ul>
<li>Load all elements for a given doc_id quickly</li>
<li>Update the value of one individual element by his element_id quickly</li>
</ul>

<p><strong>Structure in Cassandra</strong></p>

<p>1st solution</p>

<pre><code>documents-&gt;[doc_id(primary key), title, description, elements] (elements could be a SET or a TEXT, each time new elements are added (they are never removed) we would append it to this column)
elements-&gt;[element_id(primary key), title, description]
</code></pre>

<p>To load a document we would need:</p>

<ul>
<li><p>Load document with given  and get all element ids: SELECT * from documents where doc_id=‘id’</p></li>
<li><p>Load all elements with the given ids: SELECT * FROM elements where element_id IN (ids loaded from query a)</p></li>
</ul>

<p>Updating elements would be done by their primary key.</p>

<p>2nd solution</p>

<pre><code>documents-&gt;[doc_id(primary key), title, description]
elements-&gt;[element_id(primary key), doc_id(secondary index), title, description]
</code></pre>

<p>To load a document we would need:</p>

<ul>
<li>SELECT * from elements where doc_id=‘id’</li>
</ul>

<p>Updating elements would be done by their primary key.</p>

<p>Questions regarding our solutions: </p>

<ul>
<li><p>1st: Will it be efficient to query 100k+ primary keys in the elements table?</p>

<pre><code>SELECT * FROM elements WHERE element_id IN (element_id1,.... element_id100K+)?
</code></pre></li>
<li><p>2nd: Will it be efficient to query just by a secondary index?</p></li>
</ul>

<p>Could anyone give any advice how would we create a model for our use case?</p>
",<mysql><cassandra>,"<p>With cassandra it's all about the access pattern (I hope I understood it correctly, if not please comment)</p>

<p>1st</p>

<p>documents should not use sets because set is limited to 65 535 elements and has to be read, updated in it's entirety every time a change is made. Since you need 100k+ it's not what you want. You could use frozen collections etc, but then again, reading everything in memory every time is bound to be slow.</p>

<p>2nd</p>

<p>secondary indexes, well, small cardinality data might be fine But from I understand you have 100k per document, this might even be fine but then again It's not the best practice. I would simply try it out in your concrete case.</p>

<p>3rd - disk is cheap approach - always write the data the way you are going to read it - cassandra's writes are dirt cheap so prepare the views at write time,</p>

<p>this one satisfies reading of all the elements belonging to doc_id</p>

<pre><code>documents-&gt;[doc_id(primary key), title_doc (static), description_doc(static), element_id(clustering key), title, description]
</code></pre>

<p>elements remain pretty much as they were:</p>

<pre><code>elements-&gt;[element_id(primary key), doc_id, title, description]
</code></pre>

<p>When doing updates, you update it in documents and elements (for consistency you can use batch operation - should you need it) If having element_id you can quickly issue another query after you get it's doc Id.
Depending on your updating needs documentId could also be a set. (I might have not gotten this part right because not sure what data is available when updating the element do you have the doc_id also and can one element be in more docs?)</p>

<p>Also since having 100k+ elements in single partition is not the best thing to have because of the retrievals ( all requests will go to one node) I would propose to have composite partitioning key (bucket) I think in your case a simple fixed int would be just fine. So every time you go to retrieve the elements you just issue selects to documentid + (1, 2, 3, 4 ...) and then merge the result at the client - this will be significantly faster.</p>

<p>One tricky part would be that you don't go into every single bucket for elementid that is stored in the document ... when I think about it then it would be better to use a base of two for buckets. In your case 16 would be ideal ... then when you look to update specific element just use some simple hash function known to you and use last 4 bits.</p>

<p>Now when I think about it if the element id + doc id is always known to you you might not even need the elements table at all.</p>

<p>Hope this helps</p>
",['table']
43041059,43041768,2017-03-27 08:10:51,is it possible to insert json into cassandra without creating table,"<p>How can I insert json objects to Cassandra table without creating table? Can Cassandra parse json to table which is not created? Or, Can I create a table with no column and insert json? </p>

<p>Thanks.</p>
",<cassandra><nosql>,"<p>After Cassandra 2.2 you can <a href=""https://www.datastax.com/dev/blog/whats-new-in-cassandra-2-2-json-support"" rel=""nofollow noreferrer"">insert json directly</a>, but the table still should be created beforehead.</p>
",['table']
43042276,43055550,2017-03-27 09:10:49,Cassandra data model for interval and event based time series,"<p>I have to collect time series data from various IoT sensors. Based on my research there are two different types of time series data streams.</p>

<p><strong>Case 1 : Fixed interval</strong></p>

<p>This type of data stream has a fixed interval and its very easy to select data points between a given range. A typical use case would be a <strong>counter</strong>.</p>

<p><a href=""https://i.stack.imgur.com/IXmr7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IXmr7.png"" alt=""Interval based stream""></a></p>

<p><strong>Case 2 : Event based</strong></p>

<p>This type of data stream comes at irregular points in time and only occurs when something is about to change. A typical use case would be a <strong>power switch</strong> when a sensor is going offline or online.</p>

<p><a href=""https://i.stack.imgur.com/eRNdJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eRNdJ.png"" alt=""Event based stream""></a></p>

<p><strong>Requirements</strong></p>

<p>Selecting all affected data points between a given time window</p>

<p><strong>Data model</strong></p>

<p>This is my cassandra data model. Any point in the stream can be modeled by</p>

<pre><code>CREATE TABLE sensor_raw (
  sensor_id    text,
  bucket_id    date,
  sensor_time  timestamp,
  sensor_value  double,
  PRIMARY KEY ((sensor_id, bucket_id), sensor_time )
) WITH CLUSTERING ORDER BY (sensor_time DESC);
</code></pre>

<p><strong>Solution for case 1</strong></p>

<p>This is very easy and needs no further discussion</p>

<pre><code>SELECT * FROM sensor_raw where 
sensor_id = '1' AND
bucket_id = '2017' AND 
sensor_time &gt;= '2017-01-01 10:00' 
AND sensor_time &lt; '2017-01-01 10:14'
</code></pre>

<p><strong>Solution for case 2</strong></p>

<p>Here i have the problem that events from outside the window can overlap into the selected range. For example <strong>E1</strong></p>

<p>Another problem is the last event <strong>E3</strong> where the event has not yet finished.</p>

<p><strong>I need</strong></p>

<ol>
<li><p>Partial duration from <strong>window start</strong> to <strong>E1</strong>. </p>

<p><em>To get this info i would have to look back from the first event in the   stream to get the previous one. Then calculate the difference from window start to E2.</em></p></li>
<li><p>Duration from <strong>E2</strong> to <strong>E3</strong></p>

<p><em>This is easy</em></p></li>
<li><p>Duration from <strong>E2</strong> to <strong>window end</strong> ( not yet finished )</p>

<p><em>Would have to check if last event has same timestamp as window end and if not last event is still running.</em></p></li>
</ol>

<p><strong>Result</strong></p>

<p><a href=""https://i.stack.imgur.com/dV3o1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dV3o1.png"" alt=""Wanted result""></a></p>

<p><strong>Question</strong></p>

<p>Is there any better data model for case 2 ? </p>

<p>Is there any way to not have an additional query to get the solution i need ?</p>
",<cassandra><time-series><data-modeling>,"<p>I think you pretty much covered all the scenarios. One thing that could help you is if you could create an events table where the data with the ""event"" type and end_time would go. Something on the lines of:</p>

<pre><code>CREATE TABLE sensor_raw_events (
  sensor_id         text,
  bucket_id         date,
  event_end_time    timestamp,
  event_begin_time  timestamp,
  event_type        text,
  PRIMARY KEY ((sensor_id, bucket_id), sensor_end_time )
) WITH CLUSTERING ORDER BY (sensor_end_time DESC);
</code></pre>

<p>The prerequisite for that would be that you actually have some sort of layer that would be able to track the events switching on the application level. A project I worked on had to keep sessions when connecting to devices due to protocol requirements so this wasn't really a problem I guess.</p>

<p>We basically had small in memory grid that was keeping the current state of every sensor with periodic flushing to cassandra - this was only for recovering should all the application go down, but this never happened.</p>

<p>This approach would probably require a lot of memory resources for running it so if you are having millions of sensors this might get too expensive and it doesn't add much value so basically it all depends on scale that you actually have.</p>

<p>Plus one down side of the idea is that you wouldn't really catch the event that is currently ongoing because it's not written to the table yet. But would actually be o.k. for analytical workload because you wouldn't have to make additional query to fetch the beginning of the E1, it would already be there for you.</p>

<p>Some approaches with one table with begin_time and end_time might also be possible but then again this just wastes space (and with sensors it get's packed pretty quick).</p>

<p>Your model and how you described it is pretty much very similar to stuff that I did before and with cassandra alone there simply isn't much more that is known to me that you can do :(</p>
",['table']
43047158,43048096,2017-03-27 12:58:19,compaction history out of order,"<p>I'm using nodetool to view compaction history, and get the attached image <a href=""https://i.stack.imgur.com/yL9wV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yL9wV.png"" alt=""enter image description here""></a> </p>

<p>As you can see the times are completely out of order, and range from Match 17th (1489786637207) to 21st (1490122248602)</p>

<p>We are using Cassandra 3.0.8, uploading the sstables with the <a href=""http://www.datastax.com/dev/blog/using-the-cassandra-bulk-loader-updated"" rel=""nofollow noreferrer"">bulk loader</a>, and are using TimeWindowCompactionStrategy compaction. </p>

<p>Any ideas what could be causing this? </p>
",<cassandra><cassandra-3.0>,"<p>The compaction history table isn't ordered:</p>

<pre><code>CREATE TABLE system.compaction_history (
  id uuid PRIMARY KEY,
  bytes_in bigint,
  bytes_out bigint,
  columnfamily_name text,
  compacted_at timestamp,
  keyspace_name text,
  rows_merged map&lt;int, bigint&gt;
)
</code></pre>

<p>It just has a <code>default_time_to_live = 604800</code> which expires data after 1 week. Nodetool is really just printing this table.</p>

<p>edit:
So actually nodetool is supposed sort the result from jmx on client side. <a href=""https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/tools/nodetool/stats/CompactionHistoryHolder.java#L118"" rel=""nofollow noreferrer"">CompactionHistoryHolder</a> will sort it, and the compareTo checks the <a href=""https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/tools/nodetool/stats/CompactionHistoryHolder.java#L71"" rel=""nofollow noreferrer"">compactedAt</a> field. So it not being sorted by the time of the compaction is probably a bug. I would recommend opening a <a href=""https://issues.apache.org/jira/browse/CASSANDRA"" rel=""nofollow noreferrer"">jira</a> with more details.</p>
",['table']
43085443,43085692,2017-03-29 05:57:38,Cassandra query failure (Tombstones),"<p>So this is driving me crazy. i tried querying one of my table in Cassandra and it showed query failure. i tried digging dip in to the reason behind it and found that it was because of tombstone. i changed GC_GRACE_SECONDS to Zero and triggered Compaction using nodetool, And when i queried again it worked fine. however on a subsequent calls query failed again with a same reason. i am using cassandra-nodejs driver.
This is my data model.</p>
<pre><code>CREATE TABLE my_table (
    firstname text,
    lastname text,
    email text,
    mobile text,
    date timeuuid,
    value float,
    PRIMARY KEY (firstname, lastname, email, mobile)
) WITH CLUSTERING ORDER BY (lastname ASC, email ASC, mobile ASC);
</code></pre>
<p>this is the query i want to perform on that data model.</p>
<pre><code>SELECT firstname, email, toDate(date) as date, mobile, value FROM my_table  WHERE date &gt;= minTimeuuid('2017-03-25 00:00:00+0000') AND date &lt;= minTimeuuid('2017-03-28 23:59:59+0000') ALLOW FILTERING;
</code></pre>
<p>the result will have approx 40k rows.
<a href=""http://www.datastax.com/dev/blog/cassandra-anti-patterns-queues-and-queue-like-datasets"" rel=""nofollow noreferrer"">this</a> shows that if we delete something it will be marked as tombstone and will get deleted After GC_GRACE_SECONDS setted for given table. If i understand it correctly then.</p>
<ol>
<li>how come there be tombstone problem when i never delete any row of table?</li>
<li>Is that true a row will be marked as Tombstone if and only if we delete a row?</li>
<li>clearing tombstones and then querying the same works sometimes and sometimes it does not, why is so?.</li>
<li>is it a good idea to increase <strong>tombstone_failure_threshold</strong> value? (single node cluster application)</li>
</ol>
<p>I am using cassandra 3.5, with cqlsh version 5.0.1. And the query works fine with terminal, but gives error when we execute using external client (express app using nodejs driver for cassandra). i have a single node cluster app.</p>
<h3>EDIT 1</h3>
<p>This is the log of my Inserted null value in field (i inserted only name and timestamp);</p>
<pre><code>  activity                                                                                        | timestamp                  | source        | source_elapsed
-------------------------------------------------------------------------------------------------+----------------------------+---------------+----------------
                                                                              Execute CQL3 query | 2017-03-29 10:28:27.342000 | 172.31.34.179 |              0
                   Parsing select * FROM testtomb WHERE name = 'Dhaval45'; [SharedPool-Worker-2] | 2017-03-29 10:28:27.342000 | 172.31.34.179 |             64
                                                       Preparing statement [SharedPool-Worker-2] | 2017-03-29 10:28:27.342000 | 172.31.34.179 |            101
                              Executing single-partition query on testtomb [SharedPool-Worker-3] | 2017-03-29 10:28:27.342000 | 172.31.34.179 |            210
                                              Acquiring sstable references [SharedPool-Worker-3] | 2017-03-29 10:28:27.342000 | 172.31.34.179 |            223
 Skipped 0/0 non-slice-intersecting sstables, included 0 due to tombstones [SharedPool-Worker-3] | 2017-03-29 10:28:27.342000 | 172.31.34.179 |            243
                                 Merged data from memtables and 0 sstables [SharedPool-Worker-3] | 2017-03-29 10:28:27.342000 | 172.31.34.179 |            288
                                         Read 2 live and 0 tombstone cells [SharedPool-Worker-3] | 2017-03-29 10:28:27.342001 | 172.31.34.179 |            310
                                 Merged data from memtables and 0 sstables [SharedPool-Worker-3] | 2017-03-29 10:28:27.342001 | 172.31.34.179 |            323
                                                                                Request complete | 2017-03-29 10:28:27.342385 | 172.31.34.179 |            385
</code></pre>
<p>And this is the log when i query on filed which i have executed a delete query. Initially  user <strong>Dhaval15</strong> has firstname 'aaaa' and then i the cell aaa. then again executing select query on same user gave me this log.</p>
<pre><code>       activity                                                                                        | timestamp                  | source        | source_elapsed
-------------------------------------------------------------------------------------------------+----------------------------+---------------+----------------
                                                                              Execute CQL3 query | 2017-03-29 10:35:18.581000 | 172.31.34.179 |              0
                   Parsing select * FROM testtomb WHERE name = 'Dhaval15'; [SharedPool-Worker-1] | 2017-03-29 10:35:18.581000 | 172.31.34.179 |             65
                                                       Preparing statement [SharedPool-Worker-1] | 2017-03-29 10:35:18.581000 | 172.31.34.179 |            113
                              Executing single-partition query on testtomb [SharedPool-Worker-3] | 2017-03-29 10:35:18.581000 | 172.31.34.179 |            223
                                              Acquiring sstable references [SharedPool-Worker-3] | 2017-03-29 10:35:18.581000 | 172.31.34.179 |            235
 Skipped 0/0 non-slice-intersecting sstables, included 0 due to tombstones [SharedPool-Worker-3] | 2017-03-29 10:35:18.581000 | 172.31.34.179 |            256
                                 Merged data from memtables and 0 sstables [SharedPool-Worker-3] | 2017-03-29 10:35:18.581001 | 172.31.34.179 |            305
                                         Read 1 live and 1 tombstone cells [SharedPool-Worker-3] | 2017-03-29 10:35:18.581001 | 172.31.34.179 |            338
                                 Merged data from memtables and 0 sstables [SharedPool-Worker-3] | 2017-03-29 10:35:18.581001 | 172.31.34.179 |            351
                                                                                Request complete | 2017-03-29 10:35:18.581430 | 172.31.34.179 |            430
</code></pre>
",<cassandra><cassandra-3.0><cassandra-node-driver>,"<p>In Cassandra tombstone created even if you don't execute delete query, when you insert null value. </p>

<p>Tombstone consume space. When you execute select query cassandra needs to filter out data by tombstone. If huge tombstone generated your select query performance will degrade.   </p>

<p>Your query failing because of huge tombstone and <code>ALLOW FILTERING</code>. Don't use <code>ALLOW FILTERING</code> on production. it's very costy. When you execute query without specifying partition key, Cassandra needs to scan all the row of all the nodes. </p>

<p>Change your data model to like the below one : </p>

<pre><code>CREATE TABLE my_table (
    year int,
    month int,
    date timeuuid,
    email text,
    firstname text,
    lastname text,
    mobile text,
    value float,
    PRIMARY KEY ((year, month), date)
);
</code></pre>

<p>Here you can specify year and month extract from the date.<br>
Now you can query with specifying partition key :</p>

<pre><code>SELECT * FROM my_table WHERE year = 2017 AND month = 03 AND date &gt;= minTimeuuid('2017-03-25 00:00:00+0000') AND date &lt;= minTimeuuid('2017-03-28 23:59:59+0000') ;
</code></pre>

<p>This will return result very efficiently and will not fail.</p>

<p>If you need to query with firstname and lastname create an index on them</p>

<pre><code>CREATE INDEX index_firstname ON my_table (firstname) ;
CREATE INDEX index_lastname ON my_table (lastname) ;
</code></pre>

<p>Then you can query with firstname or last name</p>

<pre><code>SELECT * FROM my_table WHERE firstname = 'ashraful' ;
SELECT * FROM my_table WHERE lastname  = 'islam' ;
</code></pre>

<p>Here i have not create index on email and phone because of high cardinality problem. Instead create materialized view or another table to query with phone or email</p>

<pre><code>CREATE MATERIALIZED VIEW mview_mobile AS
    SELECT *
    FROM my_table
    WHERE mobile IS NOT NULL AND year IS NOT NULL AND month IS NOT NULL AND date IS NOT NULL
    PRIMARY KEY (mobile, year, month, date);


CREATE MATERIALIZED VIEW mview_email AS
        SELECT *
        FROM my_table
        WHERE email IS NOT NULL AND year IS NOT NULL AND month IS NOT NULL AND date IS NOT NULL
        PRIMARY KEY (email, year, month, date);
</code></pre>

<p>Now you can query with phone or email </p>

<pre><code>SELECT * FROM mview_mobile WHERE mobile = '018..';
SELECT * FROM mview_email WHERE email = 'ashraful@...';
</code></pre>

<p>More about cassandra tombstone : <a href=""http://thelastpickle.com/blog/2016/07/27/about-deletes-and-tombstones.html"" rel=""nofollow noreferrer"">http://thelastpickle.com/blog/2016/07/27/about-deletes-and-tombstones.html</a></p>
",['table']
43140525,43152095,2017-03-31 12:19:29,Cassandra Materialized Views,"<p>I have few queries related to the materialized views(MV) in Cassandra 3.x version. Given below is  my understanding of how materialized views work. Can anyone please tell me if my understanding is correct</p>

<p>1.) MV is a replica of a base table. Say I have a base table with 100 GB of data and my replication factor is 3. MV on the base table is nothing but another table with a different partition occupying the same storage as that of the base table. If I am creating an MV for my above table, I will need an addition of 100GB * 3(replication factor)  for storing the new materialized view data?</p>

<p>2.) If the above case is valid, then as such there will be two write process happening for the same data. In case of INSERT, there will be a WRITE overhead, since data will be inserted both to the table and the MV? In case of updates, there will be a read before write on the MV?</p>

<p>3.) The changes to MV when a base table changes will happen asynchornously? What happens if the entire table data is deleted, will it be reflected in MV's immediately or is there some cleanup we need to take?</p>

<p>4.) Any scenarios which needs to be considered where use of MV's could be avoided?</p>

<p>Please help me in clarifying the above questions?</p>

<p>Thanks in advance.</p>
",<cassandra><materialized-views>,"<blockquote>
  <p>Materialized views handle automated server-side denormalization, removing the need for client side handling of this denormalization and ensuring eventual consistency between the base and view data. This denormalization allows for very fast lookups of data in each view using the normal Cassandra read path</p>
</blockquote>

<p>Your Questions Answered below : </p>

<ol>
<li><p>If you used <code>select *</code> when creating MV it will take addition of 100GB * 3 storage</p></li>
<li><p>The base replica performs a local read of the data in order to create the correct update for the view and use batchlog to provide an equivalent eventual consistency between base table and view. So Each MV will cost you about 10% performance at write time</p></li>
<li><p>It will reflected immediately and cassandra will take care of that cleanup.</p></li>
<li><p>When not to use MV :</p>

<ul>
<li>Low cardinality data will create hotspots around the ring
If the partition key of all of the data is the same, those nodes would become overloaded</li>
<li>If there will be a large number of partition tombstones, the performance may suffer</li>
</ul></li>
</ol>
",['table']
43164047,43165262,2017-04-02 02:39:16,Cassandra: Who creates/distributes virtual Nodes among nodes - Leader?,"<p>In Cassandra, virtual nodes are created and distributed among nodes as given in <a href=""http://www.datastax.com/dev/blog/virtual-nodes-in-cassandra-1-2"" rel=""nofollow noreferrer"">http://www.datastax.com/dev/blog/virtual-nodes-in-cassandra-1-2</a>. But who does that process ? Creating the virtual nodes, distributing among peers. Is it some sort of leader ? How does it work ?</p>

<p>Also in case a node joins, virtual nodes are re-distributed. Lot more similar actions are present. Who does all those ?</p>

<p><strong>Edit</strong>: Is it like when a node joins, it takes up some part of virtual nodes from existing cluster thus eliminating the need of leader ?</p>
",<cassandra><distributed-system>,"<p>New node retrieves data about the cluster using seed nodes. </p>

<p>The new node will take his part of the cluster, based of num_tokens parameter (by default it will be distributed evenly between all nodes existing nodes), and will bootstrap it's part of data.
The rest of the cluster will be aware about the changes by ""gossiping"" - using gossip protocol.</p>

<p>Except the seed nodes part, there's no need for any ""master"" in the process.</p>

<p>Old nodes will not delete partitions automatically, you need to run nodetool cleanup on the old nodes after adding a new node.</p>

<p>Here's good article about it:</p>

<p><a href=""http://cassandra.apache.org/doc/latest/operating/topo_changes.html"" rel=""nofollow noreferrer"">http://cassandra.apache.org/doc/latest/operating/topo_changes.html</a></p>
",['num_tokens']
43183778,43184334,2017-04-03 11:44:01,i have a complex json i want to push in cassandra.how should I design my table in Cassandra?,"<p>I have a complex JSON I want to push in Cassandra where I will have questions and options mapping.how should I design my table in Cassandra?what should be my table architecture?</p>

<pre><code>insert into questions_options JSON'{
  ""qid"": ""12736467"",
  ""chapter_id"": ""12"",
  ""subject_id"": 19482065,
  ""topic_id"": 216.28,
  ""type"": ""picked"",
  ""content"": ""on time"",
  ""difficulty"": 1,
  ""marks"": 12,
  ""hint"": ""do something"",
  ""explanations"": ""do something dude"",
  ""created_by"": 1232,
  ""create_on"": ""2013-06-10"",
  ""options"": [
    {
      ""option_id"": 1,
      ""option_text"": ""ACBD"",
      ""sort_order"": 1,
      ""correct_option"": true
    },
    {
      ""option_id"": 2,
      ""option_text"": ""bcdd"",
      ""sort_order"": 2,
      ""correct_option"": true
    }
  ]
}'
</code></pre>

<p>I wrote the below create table query.Is it right?</p>

<pre><code>create table questions_options (
    qid bigint,
    chapter_id bigint,
    subject_id int,
    topic_id int,
    class varchar,
    type varchar,
    content text,
    difficulty tinyint,
    marks int,
    hint text,
    explanations text,
    created_by bigint,
    create_on timestamp,
    option_id bigint,
    option_text text,
    sort_order int,
    correct_option boolean,
    PRIMARY KEY((qid), option_id) 
);
</code></pre>
",<json><cassandra><nested>,"<p>In Cassandra you have to design your table the way you want to query, not by the one you want to insert.</p>

<p>By the way, you asked what will be the table that support your json.    </p>

<p>Here is how you can convert :   </p>

<ul>
<li>For each json key create a column in cassandra with the same name</li>
<li>Define that field data type by the json field value.</li>
<li>If the value is also a json object then create custom data type in cassandra </li>
<li>Define primary key By analyzing your your read query.</li>
</ul>

<p>For your json this will be the schema :    </p>

<pre><code>CREATE TYPE option (
    option_id bigint,
    option_text text,
    sort_order int,
    correct_option boolean
);

CREATE TABLE questions_options (
    qid bigint PRIMARY KEY,
    chapter_id bigint,
    class text,
    content text,
    create_on date,
    created_by bigint,
    difficulty int,
    explanations text,
    hint text,
    marks int,
    options list&lt;frozen&lt;option&gt;&gt;,
    subject_id int,
    topic_id double,
    type text
);
</code></pre>
",['table']
43183915,43185618,2017-04-03 11:51:23,Distinct count in Cassandra,"<p>I have a table chat_matches in ""cassandra"". And want to get the count of all messages from_id to distinct to_id with count of rows(group by to_id).</p>

<pre><code>CREATE TABLE chat_matches(
    chat_id uuid,
    from_id bigint,
    to_id bigint,
    message  text,
    message_send_time timestamp,
    PRIMARY KEY ((chat_id,from_id),message_send_time)
);
</code></pre>
",<cassandra><count><distinct>,"<p>In cassandra <code>count(*)</code> is a very costly operation, need to scan all the row from all the node just to give you the count and can generate timeout exception.</p>

<p>So Instead of using <code>count(*)</code> maintain a counter table like the below one :</p>

<pre><code>CREATE TABLE message_counter (
    from_id bigint,
    to_id bigint,
    count counter,
    primary key((from_id, to_id ))
);
</code></pre>

<p>When a new message appears just increment the value of count by one.  </p>

<pre><code> UPDATE message_counter SET count = count + 1 WHERE from_id = 1 AND to_id = 2;
</code></pre>

<p>Now you can select the message count group by from_id to to_id very efficiently </p>

<pre><code>SELECT * FROM message_counter WHERE from_id = 1 AND to_id = 2;
</code></pre>
",['table']
43191843,43192104,2017-04-03 18:29:21,Is Apache Spark appropriate for generic non-analytics OLTP tasks.,"<p>I am looking into getting into Apache Spark to use with a cassandra database with scala and Akka and I ahve been trying to find the answer to the question of whether i could actually drop my existing Cassandra driver and use Spark exclusively. Does it have means to find records by partition keys and so on or can it only take the entire table and filter it. I knoe you could filter to a single record but that means iterating through a potentially massive table. I want spart to essentially issue CQL where clauses and allow me to fetch only a single row if I choose or a set of rows. If this is not possible then I need to stick with my existing driver for the normal db operations and spark for the analytics. </p>
",<scala><apache-spark><cassandra>,"<p><strong>It is possible to issue CQL where clause in Spark with CassandraRDD.where()</strong></p>

<blockquote>
  <p>To filter rows, you can use the filter transformation provided by Spark. However, this approach causes all rows to be fetched from Cassandra and then filtered by Spark. Also, some CPU cycles are wasted serializing and deserializing objects that wouldn't be included in the result. To avoid this overhead, CassandraRDD offers the where method, which lets you pass arbitrary CQL condition(s) to filter the row set on the serve</p>
</blockquote>

<p>Here is a simple example on how to use CassandraRDD.where()</p>

<p>If you have a table </p>

<pre><code>CREATE TABLE test.data (
   id int PRIMARY KEY,
   data text
);
</code></pre>

<p>You can use spark to select and filter with primary key.</p>

<pre><code>sc.cassandraTable(""test"", ""data "").select(""id"", ""data"").where(""id = ?"", 1).toArray.foreach(println)
</code></pre>

<p>More on : <a href=""https://github.com/datastax/spark-cassandra-connector/blob/master/doc/3_selection.md"" rel=""nofollow noreferrer"">https://github.com/datastax/spark-cassandra-connector/blob/master/doc/3_selection.md</a></p>

<p>But In Cassandra driver you have more flexibility and control over your query and also Spark will cost you more cpu, time and memory than the cassandra driver.  </p>

<p>As RussS Says </p>

<blockquote>
  <p>""While this is correct and the where clause allows you to run a single partition request, This is orders of magnitude more expensive than running analogous queries directly through the Java Driver""</p>
</blockquote>
",['table']
43198775,43201971,2017-04-04 05:14:51,How to get Last 6 Month data comparing with timestamp column using cassandra query?,"<p>How to get Last 6 Month data comparing with <code>timestamp</code> column using cassandra query?
I need to get all account statement which belongs to last 3/6 months comparing with <code>updatedTime(TimeStamp column)</code> and <code>CurrentTime</code>.
For example in <strong>SQL</strong> we are using <code>DateAdd()</code> function tor this to get. i dont know how to proceed this in cassandra.
If anyone know,reply.Thanks in Advance.</p>
",<cassandra><cassandra-2.0><cassandra-2.1><cassandra-cli>,"<p>Cassandra 2.2 and later allows users to define functions (UDT) that can be applied to data stored in a table as part of a query result.</p>

<p>You can create your own method if you use Cassandra 2.2 and later <a href=""http://docs.datastax.com/en//cql/latest/cql/cql_using/useCreateUDF.html#useCreateUDF"" rel=""nofollow noreferrer"">UDF</a></p>

<pre><code>CREATE FUNCTION monthadd(date timestamp, month int)
    CALLED ON NULL INPUT
    RETURNS timestamp
    LANGUAGE java
    AS $$java.util.Calendar c = java.util.Calendar.getInstance();c.setTime(date);c.add(java.util.Calendar.MONTH, month);return c.getTime();$$
</code></pre>

<p>This method receive two parameter </p>

<ul>
<li>date timestamp: The date from you want add or subtract number of month</li>
<li>month int: Number of month you want to or add(+) subtract(-) from date </li>
</ul>

<p>Return the date timestamp</p>

<p>Here is how you can use this : </p>

<pre><code>SELECT * FROM ttest WHERE id = 1 AND updated_time &gt;= monthAdd(dateof(now()), -6) ;
</code></pre>

<p>Here monthAdd method subtract 1 mont from the current timestamp, So this query will data of last month</p>

<p>Note : By default User-defined-functions are disabled in cassandra.yaml - set enable_user_defined_functions=true to enable if you are aware of the security risks </p>
",['table']
43199198,43202763,2017-04-04 05:48:51,Mapping cassandra list<frozen<list<int>>> field to Java in spring-data-cassandra,"<p>can someone point to me how a field declared <code>list&lt;frozen&lt;list&lt;int&gt;&gt;&gt;</code> can be mapped back into java in spring-data-cassandra. I'm able to simply save data through <code>List&lt;List&lt;Integer&gt;&gt;&gt;</code> but doesn't work when reading from the database, a codec not found exception pops.</p>

<p>Help is much appreciated.</p>
",<cassandra><spring-data-cassandra>,"<p>Your Declaration is correct. But for nested collection read you need to create Custom RowMapper to convert row to DTO.</p>

<p>Example : </p>

<p>Let's we have the table ctest</p>

<pre><code>CREATE TABLE ctest (
    id int PRIMARY KEY,
    data list&lt;frozen&lt;list&lt;int&gt;&gt;&gt;
);
</code></pre>

<p>And DTO </p>

<pre><code>public class CTest {

    @PrimaryKey
    private int id;

    private List&lt;List&lt;Integer&gt;&gt; data;

    public CTest() {
    }

    private void setData(List&lt;List&lt;Integer&gt;&gt; data) {
        this.data = data;
    }

    public List&lt;List&lt;Integer&gt;&gt; getData() {
        return data;
    }

    public void setId(int id) {
        this.id = id;
    }

    public int getId() {
        return id;
    }

}
</code></pre>

<p>Now we want to query data from it. </p>

<pre><code>List&lt;CTest&gt; results = cassandraOperations.query(""SELECT * FROM ctest  WHERE id = 1"", new RowMapper&lt;CTest&gt;() {

    private final TypeToken&lt;List&lt;Integer&gt;&gt; listOfInt = new TypeToken&lt;List&lt;Integer&gt;&gt;() {};

    public CTest mapRow(Row row, int rowNum) throws DriverException {
        CTest test = new CTest();
        test.setId(row.getInt(""id""));
        test.setData(row.getList(""data"", listOfInt));
        return test;
    }
});
</code></pre>
",['table']
43322421,43324501,2017-04-10 11:52:05,Cassandra - MATERIALIZED VIEW working 1 out of 3 times,"<p>I am new to cassandra and wanted to use Materialized view to change the way I expose my data but It works sometimes and doesn't some other times.
So I wondered, does anyone as an idea of why such a thing could occur?</p>

<p>Here is the exact script I am running (in a docker container)</p>

<pre><code>CREATE KEYSPACE IF NOT EXISTS demo3 WITH REPLICATION = {'class': 'SimpleStrategy', 'replication_factor': 1 };

/*
** Drop view to allow droping table
*/

DROP MATERIALIZED VIEW IF EXISTS demo3.tabletest_view;

/*
** TABLE demo3.tabletest (
**  cardKey text, fidelity card key
**  dateKey Date, date of the transaction
**  kind text,
**  power text,
**  sid text, //avro schema id
**  data blob,
**  PRIMARY KEY ((cardKey), dateKey, kind, power)
** ) WITH CLUSTERING ORDER BY (cardKey, trxDate DESC);
*/

DROP TABLE IF EXISTS demo3.tabletest;
CREATE TABLE IF NOT EXISTS demo3.tabletest (
    cardkey text,
    datekey date,
    kind text,
    power text,
    sid text, 
    data blob,
    PRIMARY KEY ((cardkey), datekey, kind, power)
) WITH CLUSTERING ORDER BY (datekey DESC);

/*
** FIXTURES
*/

INSERT INTO demo3.tabletest ( cardkey, datekey, kind, power, sid, data) VALUES ( 'card1', '2016-05-01', 'kind1', 'power1', 'Id', bigintAsBlob(99) );
INSERT INTO demo3.tabletest ( cardkey, datekey, kind, power, sid, data) VALUES ( 'card1', '2016-05-08', 'kind1', 'power2', 'Id', bigintAsBlob(99) );
INSERT INTO demo3.tabletest ( cardkey, datekey, kind, power, sid, data) VALUES ( 'card1', '2016-05-09', 'kind2', 'power1', 'Id', bigintAsBlob(99) );
INSERT INTO demo3.tabletest ( cardkey, datekey, kind, power, sid, data) VALUES ( 'card1', '2016-05-15', 'kind1', 'power3', 'Id', bigintAsBlob(99) );
INSERT INTO demo3.tabletest ( cardkey, datekey, kind, power, sid, data) VALUES ( 'card1', '2016-05-22', 'kind1', 'power4', 'Id', bigintAsBlob(99) );
INSERT INTO demo3.tabletest ( cardkey, datekey, kind, power, sid, data) VALUES ( 'card1', '2016-05-25', 'kind2', 'power2', 'Id', bigintAsBlob(99) );
INSERT INTO demo3.tabletest ( cardkey, datekey, kind, power, sid, data) VALUES ( 'card1', '2016-05-30', 'kind1', 'power5', 'Id', bigintAsBlob(99) );
INSERT INTO demo3.tabletest ( cardkey, datekey, kind, power, sid, data) VALUES ( 'card1', '2016-06-06', 'kind2', 'power3', 'Id', bigintAsBlob(99) );
INSERT INTO demo3.tabletest ( cardkey, datekey, kind, power, sid, data) VALUES ( 'card1', '2016-06-14', 'kind2', 'power4', 'Id', bigintAsBlob(99) );
INSERT INTO demo3.tabletest ( cardkey, datekey, kind, power, sid, data) VALUES ( 'card1', '2016-06-17', 'kind2', 'power5', 'Id', bigintAsBlob(99) );

INSERT INTO demo3.tabletest ( cardkey, datekey, kind, power, sid, data) VALUES ( 'card2', '2016-05-01', 'kind1', 'power1', 'Id', bigintAsBlob(99) );
INSERT INTO demo3.tabletest ( cardkey, datekey, kind, power, sid, data) VALUES ( 'card2', '2016-05-08', 'kind1', 'power2', 'Id', bigintAsBlob(99) );
INSERT INTO demo3.tabletest ( cardkey, datekey, kind, power, sid, data) VALUES ( 'card2', '2016-05-09', 'kind2', 'power1', 'Id', bigintAsBlob(99) );
INSERT INTO demo3.tabletest ( cardkey, datekey, kind, power, sid, data) VALUES ( 'card2', '2016-05-15', 'kind1', 'power3', 'Id', bigintAsBlob(99) );
INSERT INTO demo3.tabletest ( cardkey, datekey, kind, power, sid, data) VALUES ( 'card2', '2016-05-22', 'kind1', 'power4', 'Id', bigintAsBlob(99) );
INSERT INTO demo3.tabletest ( cardkey, datekey, kind, power, sid, data) VALUES ( 'card2', '2016-05-25', 'kind2', 'power2', 'Id', bigintAsBlob(99) );
INSERT INTO demo3.tabletest ( cardkey, datekey, kind, power, sid, data) VALUES ( 'card2', '2016-05-30', 'kind1', 'power5', 'Id', bigintAsBlob(99) );
INSERT INTO demo3.tabletest ( cardkey, datekey, kind, power, sid, data) VALUES ( 'card2', '2016-06-06', 'kind2', 'power3', 'Id', bigintAsBlob(99) );
INSERT INTO demo3.tabletest ( cardkey, datekey, kind, power, sid, data) VALUES ( 'card2', '2016-06-14', 'kind2', 'power4', 'Id', bigintAsBlob(99) );
INSERT INTO demo3.tabletest ( cardkey, datekey, kind, power, sid, data) VALUES ( 'card2', '2016-06-17', 'kind2', 'power5', 'Id', bigintAsBlob(99) );

INSERT INTO demo3.tabletest ( cardkey, datekey, kind, power, sid, data) VALUES ( 'card1', '2017-05-01', 'kind1', 'power6', 'Id', bigintAsBlob(99) );
INSERT INTO demo3.tabletest ( cardkey, datekey, kind, power, sid, data) VALUES ( 'card1', '2017-05-08', 'kind1', 'power7', 'Id', bigintAsBlob(99) );
INSERT INTO demo3.tabletest ( cardkey, datekey, kind, power, sid, data) VALUES ( 'card1', '2017-05-09', 'kind2', 'power6', 'Id', bigintAsBlob(99) );
INSERT INTO demo3.tabletest ( cardkey, datekey, kind, power, sid, data) VALUES ( 'card1', '2017-05-15', 'kind1', 'power8', 'Id', bigintAsBlob(99) );
INSERT INTO demo3.tabletest ( cardkey, datekey, kind, power, sid, data) VALUES ( 'card1', '2017-05-22', 'kind1', 'power9', 'Id', bigintAsBlob(99) );
INSERT INTO demo3.tabletest ( cardkey, datekey, kind, power, sid, data) VALUES ( 'card1', '2017-05-25', 'kind2', 'power7', 'Id', bigintAsBlob(99) );
INSERT INTO demo3.tabletest ( cardkey, datekey, kind, power, sid, data) VALUES ( 'card1', '2017-05-30', 'kind1', 'power10', 'Id', bigintAsBlob(99) );
INSERT INTO demo3.tabletest ( cardkey, datekey, kind, power, sid, data) VALUES ( 'card1', '2017-06-06', 'kind2', 'power8', 'Id', bigintAsBlob(99) );
INSERT INTO demo3.tabletest ( cardkey, datekey, kind, power, sid, data) VALUES ( 'card1', '2017-06-14', 'kind2', 'power9', 'Id', bigintAsBlob(99) );
INSERT INTO demo3.tabletest ( cardkey, datekey, kind, power, sid, data) VALUES ( 'card1', '2017-06-17', 'kind2', 'power10', 'Id', bigintAsBlob(99) );

INSERT INTO demo3.tabletest ( cardkey, datekey, kind, power, sid, data) VALUES ( 'card2', '2017-05-01', 'kind1', 'power6', 'Id', bigintAsBlob(99) );
INSERT INTO demo3.tabletest ( cardkey, datekey, kind, power, sid, data) VALUES ( 'card2', '2017-05-08', 'kind1', 'power7', 'Id', bigintAsBlob(99) );
INSERT INTO demo3.tabletest ( cardkey, datekey, kind, power, sid, data) VALUES ( 'card2', '2017-05-09', 'kind2', 'power6', 'Id', bigintAsBlob(99) );
INSERT INTO demo3.tabletest ( cardkey, datekey, kind, power, sid, data) VALUES ( 'card2', '2017-05-15', 'kind1', 'power8', 'Id', bigintAsBlob(99) );
INSERT INTO demo3.tabletest ( cardkey, datekey, kind, power, sid, data) VALUES ( 'card2', '2017-05-22', 'kind1', 'power9', 'Id', bigintAsBlob(99) );
INSERT INTO demo3.tabletest ( cardkey, datekey, kind, power, sid, data) VALUES ( 'card2', '2017-05-25', 'kind2', 'power7', 'Id', bigintAsBlob(99) );
INSERT INTO demo3.tabletest ( cardkey, datekey, kind, power, sid, data) VALUES ( 'card2', '2017-05-30', 'kind1', 'power10', 'Id', bigintAsBlob(99) );
INSERT INTO demo3.tabletest ( cardkey, datekey, kind, power, sid, data) VALUES ( 'card2', '2017-06-06', 'kind2', 'power8', 'Id', bigintAsBlob(99) );
INSERT INTO demo3.tabletest ( cardkey, datekey, kind, power, sid, data) VALUES ( 'card2', '2017-06-14', 'kind2', 'power9', 'Id', bigintAsBlob(99) );
INSERT INTO demo3.tabletest ( cardkey, datekey, kind, power, sid, data) VALUES ( 'card2', '2017-06-17', 'kind2', 'power10', 'Id', bigintAsBlob(99) );


/*
** DISPLAY RESULT:
*/

SELECT * FROM demo3.tabletest;

/*
** Creating view
*/

CREATE MATERIALIZED VIEW demo3.tabletest_view 
AS SELECT cardkey, datekey, kind, power, sid, data 
FROM demo3.tabletest
WHERE cardkey IS NOT NULL AND datekey IS NOT NULL AND kind IS NOT NULL AND power IS NOT NULL
PRIMARY KEY ((cardkey), datekey, kind, power);

SELECT * FROM demo3.tabletest_view;
</code></pre>
",<cassandra>,"<p>You are not getting data from MATERIALIZED VIEW <code>tabletest_view</code> right ?<br>
It's because of tabletest_view is build in progress. When you create MATERIALIZED VIEW on top of a table with existing data, It needs some time to build and data propagation.</p>

<p>Meanwhile you’re able to get the current status of all view through</p>

<pre><code> SELECT * FROM system.views_builds_in_progress ;
</code></pre>

<p>and a list of all built view by executing</p>

<pre><code>SELECT * FROM system.built_views ;
</code></pre>

<p>When your view is built you will get data.</p>
",['table']
43347197,43347311,2017-04-11 13:21:18,Cassandra - Impact of a Materialized View on table delete optimisation,"<p>I know that there is an increase of 10% when using a materialized view but I would like to know (and haven't find any clue about it yet) if there is a repercussion on the table delete optimization when doing a big delete based on the primary key.</p>

<p>Here is a case example:</p>

<pre><code>TABLE a_simple_table (
    year int,
    fulldate date,
    ref1 text,
    ref2 text,
    data blob,
    PRIMARY KEY ((year), fulldate, ref1, ref2)
);

MATERIALIZED VIEW demo.a_simple_table_view 
AS SELECT year, fulldate, ref1, ref2, data
FROM demo.a_simple_table
WHERE ref1 IS NOT NULL AND year IS NOT NULL AND fulldate IS NOT NULL AND ref2 IS NOT NULL
PRIMARY KEY ((ref1), year, fulldate, ref2)
WITH CLUSTERING ORDER BY (year DESC, fulldate DESC, ref2 ASC);
</code></pre>

<p>For what I understand and what I have been told, when we do the following:</p>

<pre><code>DELETE from a_simple_table WHERE year = 2017;
</code></pre>

<p>Cassandra mark only one Tombstone and we therefor don't do 100 delete if there is a 100 rows in the table under the primary key value 2017.</p>

<p>But, since the materialized view has to find each rows to delete into his own table, what does the delete cost becomes?</p>
",<cassandra><cql>,"<p>Delete operation is no different than the insert:
<a href=""http://www.doanduyhai.com/blog/?p=1930"" rel=""nofollow noreferrer"">http://www.doanduyhai.com/blog/?p=1930</a></p>

<p>from <a href=""https://www.datastax.com/dev/blog/new-in-cassandra-3-0-materialized-views"" rel=""nofollow noreferrer"">https://www.datastax.com/dev/blog/new-in-cassandra-3-0-materialized-views</a></p>

<p>When a deletion occurs, the materialized view will query all of the deleted values in the base table and generate tombstones for each of the materialized view rows, because the values that need to be tombstoned in the view are not included in the base table’s tombstone...</p>

<p>Basically the ""hit"" would be as if you try to insert all the values in the base table row. And the reading will take the hit because of increased number of tombstones in materialized view.</p>
",['table']
43363861,43364710,2017-04-12 08:11:11,How to programmatically know the primary keys of a table in Cassandra,"<p>I'm using <code>Datastax driver</code> to access <code>cassandra</code>. My method takes a <code>cql string paramater</code>.
Because the cql is arbitrary,  I don't know the <code>primary keys</code> of the table in the cql string.</p>

<p>In ResultSet, I didn't find the <code>metadata</code> associated with the <code>primary keys</code>.
I can only get the <code>names</code> and <code>values</code> of <code>all columns</code>.</p>

<p>I'd like to know which columns are primary keys.
How to <code>programmatically</code> get the primary keys of the table in the cql?</p>

<pre><code>public Map&lt;String, Object&gt; search(String cql) {

    SimpleStatement statement = new SimpleStatement(cql);
    ResultSet result = session.execute(statement);
    // ...
}
</code></pre>
",<java><cassandra><datastax-java-driver>,"<p><strong>You can use <code>TableMetadata.getPrimaryKey()</code> method</strong></p>

<p>Here is a sample demo to get the primary key of keyspace 'test' and table <code>ashraful_test</code></p>

<pre><code>try (Cluster cluster = Cluster.builder().addContactPoints(""127.0.0.1"").withCredentials(""cassandra"", ""cassandra"").build(); Session session = cluster.connect(""test"")) {
    System.out.println(cluster.getMetadata().getKeyspace(session.getLoggedKeyspace()).getTable(""ashraful_test"").getPrimaryKey());
}
</code></pre>
",['table']
43371462,43371555,2017-04-12 13:49:11,"How to pass variable parameter for ""IN clause""","<p>I am preparing a session.execute statement as below. 
I have few conditions and one IN clause. I am getting the below error. 
I know I am doing a mistake but not able to make it work.<br>
<code>filter_site_value = ['filter 1', 'filter 2']</code>  </p>

<pre><code>session = get_session()
query = 'SELECT * FROM table where cv = %s AND dt &gt; %s and dt &lt; %s AND st IN  (%s)' % ','.join('%s' for i in filter_site_value)
data = (filter_customer_value,filter_date_start_value, filter_date_end_value, filter_site_value)
rows = session.execute(query, data)
</code></pre>

<p>""errorType"": ""TypeError"",
  ""errorMessage"": ""not enough arguments for format string""</p>

<p>Please help. </p>
",<python><python-2.7><cassandra><datastax>,"<p>your string has 3 places where it needs to format a value into it (<code>%s</code>), you only provide one value: <code>','.join('%s' for i in filter_site_value)</code></p>

<p>So if you have:</p>

<pre><code>x = string_to_format % values
</code></pre>

<p>and string_to_format contains X amount of <code>%s</code> (or <code>%d %r ...</code>) then you need X values in <code>values</code></p>

<p>see:
<a href=""https://docs.python.org/2/library/string.html"" rel=""nofollow noreferrer"">https://docs.python.org/2/library/string.html</a> , <a href=""https://pyformat.info/"" rel=""nofollow noreferrer"">https://pyformat.info/</a></p>

<p>What you probably want to do is:</p>

<pre><code>query = 'SELECT * FROM table where cv = %s AND dt &gt; %s and dt &lt; %s AND st IN  ('+ ','.join(filter_site_value)+')'
data = (filter_customer_value,filter_date_start_value, filter_date_end_value)
</code></pre>

<p>or</p>

<pre><code>query = 'SELECT * FROM table where cv = %s AND dt &gt; %s and dt &lt; %s AND st IN  ('+ ','.join(%s for i in filter_site_value)+')'
data = (filter_customer_value,filter_date_start_value, filter_date_end_value)+tuple(filter_site_value)
</code></pre>
",['table']
43412723,43413973,2017-04-14 13:38:30,Load data from one table to another every 10 mins - Cassandra,"<p>We have a stream of data coming to Table A every 10 mins. No history preserved. The existing data has to be flushed to a new table B every time data is loaded in Table A. Can this be done dynamically or automated in Cassandra?</p>

<p>I can think of loading the Table A into a CSV file and then loading back to Table B every time Table A is flushed. But i would like to have something done at the database level itself.
Any ideas or suggestions appreciated.</p>

<p>Thanks,
Arun</p>
",<cassandra><cassandra-2.1>,"<p>For smaller amounts of data you could put this into cron:</p>

<p><a href=""https://dba.stackexchange.com/questions/58901/what-is-a-good-way-to-copy-data-from-one-cassandra-columnfamily-to-another-on-th"">https://dba.stackexchange.com/questions/58901/what-is-a-good-way-to-copy-data-from-one-cassandra-columnfamily-to-another-on-th</a></p>

<p>If larger and running newer versions of cassandra (3.8+)</p>

<p><a href=""http://cassandra.apache.org/doc/latest/operating/cdc.html"" rel=""nofollow noreferrer"">http://cassandra.apache.org/doc/latest/operating/cdc.html</a>
<a href=""https://issues.apache.org/jira/browse/CASSANDRA-8844"" rel=""nofollow noreferrer"">https://issues.apache.org/jira/browse/CASSANDRA-8844</a></p>

<p>and then replay the data to the table that you need (by some sort of outside process, script, app etc ...).</p>

<p>Basically there are already some tools around like:
<a href=""https://github.com/carloscm/cassandra-commitlog-extract"" rel=""nofollow noreferrer"">https://github.com/carloscm/cassandra-commitlog-extract</a></p>

<p>You could use the samples there to cover your use-case.</p>

<p>But for most use cases this is handled at the application level, writes are relatively cheap with cassandra.</p>
",['table']
43430227,43430402,2017-04-15 19:13:46,Cassandra get latest entry for each element contained within IN clause,"<p>So, I have a Cassandra CQL statement that looks like this:</p>

<pre><code>SELECT * FROM DATA WHERE APPLICATION_ID = ? AND PARTNER_ID = ? AND LOCATION_ID = ? AND DEVICE_ID = ? AND DATA_SCHEMA = ?
</code></pre>

<p>This table is sorted by a timestamp column.</p>

<p>The functionality is fronted by a REST API, and one of the filter parameters that they can specify to get the most recent row, and then I appent ""LIMIT 1"" to the end of the CQL statement since it's ordered by the timestamp column in descending order. What I would like to do is allow them to specify multiple device id's to get back the latest entries for. So, my question is, is there any way to do something like this in Cassandra:</p>

<pre><code>SELECT * FROM DATA WHERE APPLICATION_ID = ? AND PARTNER_ID = ? AND LOCATION_ID = ? AND DEVICE_ID IN ? AND DATA_SCHEMA = ?
</code></pre>

<p>and still use something like ""LIMIT 1"" to only get back the latest row for each device id? Or, will I simply have to execute a separate CQL statement for each device to get the latest row for each of them?</p>

<p>FWIW, the table's composite key looks like this:</p>

<pre><code>PRIMARY KEY ((application_id, partner_id, location_id, device_id, data_schema), activity_timestamp)
) WITH CLUSTERING ORDER BY (activity_timestamp DESC);
</code></pre>
",<cassandra><cql>,"<blockquote>
  <p>IN is not recommended when there are a lot of parameters for it and under the hood it's making reqs to multiple partitions anyway and it's putting pressure on the coordinator node. </p>
</blockquote>

<p>Not that you can't do it. It is perfectly legal, but most of the time it's not performant and is not suggested. If you specify limit, it's for the whole statement, basically you can't pick just the first item out from partitions. The simplest option would be to issue multiple queries to the cluster (every element in <code>IN</code> would become one query) and put a <code>limit 1</code> to every one of them.</p>

<p>To be honest this was my solution in a lot of the projects and it works pretty much fine. Basically coordinator would under the hood go to multiple nodes anyway but would also have to work more for you to get you all the requests, might run into timeouts etc.</p>

<blockquote>
  <p>In short it's far better for the cluster and more performant if client asks multiple times (using multiple coordinators with smaller requests) than to make single coordinator do to all the work.</p>
</blockquote>

<p>This is all in case you can't afford more disk space for your cluster</p>

<p><strong><em>Usual Cassandra solution</em></strong></p>

<p>Data in cassandra is suggested to be ready for query (query first). So basically you would have to have one additional table that would have the same partitioning key as you have it now, and you would have to drop the clustering column <code>activity_timestamp</code>. i.e.</p>

<pre><code>PRIMARY KEY ((application_id, partner_id, location_id, device_id, data_schema))
</code></pre>

<p><code>double (())</code> is intentional.</p>

<p>Every time you would write to your table you would also write data to the <code>latest_entry</code> (table without <code>activity_timestamp</code>) Then you can specify the query that you need with in and this table contains the latest entry so you don't have to use the limit 1 because there is only one entry per partitioning key ... that would be the usual solution in cassandra.</p>

<p>If you are afraid of the additional writes, don't worry , they are inexpensive and cpu bound. With cassandra it's always ""bring on the writes"" I guess :)</p>

<p>Basically it's up to you:</p>

<ol>
<li>multiple queries - a bit of refactoring, no additional space cost</li>
<li>new schema - additional inserts when writing, additional space cost</li>
</ol>
",['table']
43575543,43943515,2017-04-23 19:24:51,SELECT Error Cassandra 'Row' object has no attribute 'values',"<p>I'm trying to setup and run Cassandra 3.10 in my local docker (<a href=""https://hub.docker.com/_/cassandra/"" rel=""nofollow noreferrer"">https://hub.docker.com/_/cassandra/</a>). Everything goes well until I try to select from one table.</p>

<p>This is the error I get everytime I run select whatever from whatever:</p>

<pre><code>'Row' object has no attribute 'values'
</code></pre>

<p>The steps that I followed:</p>

<ol>
<li><p>I created a new keyspace using the default superuser: cassandra.  create <code>keyspace test with replication = {'class':'SimpleStrategy','replication_factor' : 2};</code> and <code>USE test;</code></p></li>
<li><p>I created a new table: <code>create table usertable (userid int primary key, usergivenname varchar, userfamilyname varchar, userprofession varchar);</code></p></li>
<li><p>Insert some data: <code>insert into usertable (userid, usergivenname, userfamilyname, userprofession) values (1, 'Oliver', 'Veits', 'Freelancer');</code></p></li>
<li><p>Try to select: <code>select * from usertable where userid = 1;</code></p></li>
</ol>

<p>I got this steps from: <a href=""https://oliverveits.wordpress.com/2016/12/08/cassandra-hello-world-example/"" rel=""nofollow noreferrer"">https://oliverveits.wordpress.com/2016/12/08/cassandra-hello-world-example/</a> just to copy &amp; paste some working code (I was getting mad with the syntax and typos)</p>

<p>This are the logs of my docker image:</p>

<pre><code>INFO  [Native-Transport-Requests-1] 2017-04-23 19:09:12,543 MigrationManager.java:303 - Create new Keyspace: KeyspaceMetadata{name=test2, params=KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=2}}, tables=[], views=[], functions=[], types=[]}
INFO  [Native-Transport-Requests-1] 2017-04-23 19:09:41,415 MigrationManager.java:343 - Create new table: org.apache.cassandra.config.CFMetaData@1b484e82[cfId=6757f460-2858-11e7-9787-6d2c86545d91,ksName=test2,cfName=usertable,flags=[COMPOUND],params=TableParams{comment=, read_repair_chance=0.0, dclocal_read_repair_chance=0.1, bloom_filter_fp_chance=0.01, crc_check_chance=1.0, gc_grace_seconds=864000, default_time_to_live=0, memtable_flush_period_in_ms=0, min_index_interval=128, max_index_interval=2048, speculative_retry=99PERCENTILE, caching={'keys' : 'ALL', 'rows_per_partition' : 'NONE'}, compaction=CompactionParams{class=org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy, options={min_threshold=4, max_threshold=32}}, compression=org.apache.cassandra.schema.CompressionParams@4bce743f, extensions={}, cdc=false},comparator=comparator(),partitionColumns=[[] | [userfamilyname usergivenname userprofession]],partitionKeyColumns=[userid],clusteringColumns=[],keyValidator=org.apache.cassandra.db.marshal.Int32Type,columnMetadata=[usergivenname, userprofession, userid, userfamilyname],droppedColumns={},triggers=[],indexes=[]]
INFO  [MigrationStage:1] 2017-04-23 19:09:41,484 ColumnFamilyStore.java:406 - Initializing test2.usertable
INFO  [IndexSummaryManager:1] 2017-04-23 19:13:25,214 IndexSummaryRedistribution.java:75 - Redistributing index summaries
</code></pre>

<p>Thanks a lot!</p>

<p><strong>UPDATE</strong></p>

<p>I created another table with a uuid column like this: ""uid uuid primary key"". It works when the table is empty but after one insert, I get the same error</p>
",<select><docker><cassandra><cassandra-3.0>,"<p>I had the same problem, I was using cqlsh. You just need to reload. Probably, it is a cqlsh bug the first time or after create a schema.</p>

<pre><code>~$ sudo docker run --name ex1 -d  cassandra:latest                                 
9c1092938d29ec7f94bee773cc2adc0a23ff09344e32500bfeb1898466610d06
~$ sudo docker exec -ti ex1 /bin/bash                                         
root@9c1092938d29:/# cqlsh
Connected to Test Cluster at 127.0.0.1:9042.
[cqlsh 5.0.1 | Cassandra 3.10 | CQL spec 3.4.4 | Native protocol v4]
Use HELP for help.
cqlsh&gt; CREATE SCHEMA simple1 
   ... WITH replication={
   ... 'class': 'SimpleStrategy',
   ... 'replication_factor':1};
cqlsh&gt; use simple1;
cqlsh:simple1&gt; create table users(id varchar primary key, first_name varchar, last_name varchar);
cqlsh:simple1&gt; select * from users;

 id | first_name | last_name
----+------------+-----------

(0 rows)
cqlsh:simple1&gt; insert into users(id, first_name, last_name) values ('U100001', 'James', 'Jones');
cqlsh:simple1&gt; select * from users;
'Row' object has no attribute 'values'
cqlsh:simple1&gt; quit


root@9c1092938d29:/# cqlsh     
Connected to Test Cluster at 127.0.0.1:9042.
[cqlsh 5.0.1 | Cassandra 3.10 | CQL spec 3.4.4 | Native protocol v4]
Use HELP for help.
cqlsh&gt; use simple1;
cqlsh:simple1&gt; select * from users;

 id      | first_name | last_name
---------+------------+-----------
 U100001 |      James |     Jones

(1 rows)
</code></pre>
",['table']
43637779,43641652,2017-04-26 14:57:14,Cassandra greater than '>' issue,"<p>in Cassandra i am trying to retrieve text data from table using <code>&gt;=</code>  operation but , nothing retrieved although trying to use <code>=</code> it returns successfully </p>

<p>this is sample of query </p>

<pre><code>select * from s.vechile_information  where datetimelong &gt;= '1493215758000' and vechile_customerid = '123' and vechileId = '32' allow filetring;  
</code></pre>

<p>but when remove <code>&gt;</code> it works fine </p>

<pre><code>   select * from s.vechile_information  where datetimelong = '1493215758000' and vechile_customerid = '123' and vechileId = '32' allow filetring;
</code></pre>

<p>this is the table structure </p>

<pre><code>CREATE TABLE fcs.vehicle_information (
    vehicle_customerId text,
    vehicleid text,
    cityid text,
    cityname text,
    citynamear text,
    createdby text,
    dateTimeLong text,
    description text,
    driverid text,
    drivername text,
    drivernamear text,
    groupofvehicles text,
    groupofvehiclesystemid text,
    insexpirygregoriandate bigint,
    name text,
    namear text,
    platenumber text,

   vehiclestatus text,
    PRIMARY KEY (vehicle_customerId, vehicleid)
) ;
CREATE CUSTOM INDEX dateTimeLongvehicle_information ON fcs.vehicle_information (dateTimeLong) USING 'org.apache.cassandra.index.sasi.SASIIndex' WITH OPTIONS = {'analyzer_class' : 'org.apache.cassandra.index.sasi.analyzer.NonTokenizingAnalyzer', 'case_sensitive' : 'false'};
</code></pre>

<p>after trail one more scenario the issue still exist , i would like to know why this behavior . </p>

<p>this the new table structure </p>

<pre><code>CREATE TABLE fcs.devicetracking_log (
    customerid text,
    vehiclesystemid text,
    datetime text,
    uniqueid text,
    logaction int,
    logid uuid,
    cid int,
    altitude double,
    angle double,
    assignmentname text,
    assignmentsystemid text,
    cityid text,
    cityname text,
    citynamear text,
    coloronmap text,
    departmentid text,
    departmentname text,
    departmentnamear text,
    departmentsystemid text,
    device text,
    direction double,
    drivername text,
    drivernamear text,
    driversystemid text,
    groupofvehicles text,
    groupofvehiclesystemid text,
    gsm_signal bigint,
    id text,
    lastcid int,
    lastidledate bigint,
    lastoverspeednotificationtime bigint,
    laststoppeddate bigint,
    latitude double,
    longitude double,
    message_id bigint,
    mileage double,
    overspeedallowedperiod int,
    overspeedmaximumspeed int,
    receivingdate bigint,
    regionid text,
    regionname text,
    regionnamear text,
    report text,
    rtc_datetime bigint,
    rtctime bigint,
    satellites int,
    speed double,
    uid text,
    valid text,
    vehiclename text,
    vehiclenamear text,
    vehicleplatenumber text,
    PRIMARY KEY (customerid, vehiclesystemid, datetime, uniqueid, logaction, logid, cid)
) ;
CREATE CUSTOM INDEX ciddevicetrackinglog ON fcs.devicetracking_log (cid) USING 'org.apache.cassandra.index.sasi.SASIIndex' WITH OPTIONS = {'analyzer_class' : 'org.apache.cassandra.index.sasi.analyzer.NonTokenizingAnalyzer', 'case_sensitive' : 'false'};
CREATE CUSTOM INDEX citydevicetracking_log ON fcs.devicetracking_log (cityid) USING 'org.apache.cassandra.index.sasi.SASIIndex' WITH OPTIONS = {'analyzer_class' : 'org.apache.cassandra.index.sasi.analyzer.NonTokenizingAnalyzer', 'case_sensitive' : 'false'};
CREATE CUSTOM INDEX datetimedevicetrackinglog ON fcs.devicetracking_log (datetime) USING 'org.apache.cassandra.index.sasi.SASIIndex' WITH OPTIONS = {'analyzer_class' : 'org.apache.cassandra.index.sasi.analyzer.NonTokenizingAnalyzer', 'case_sensitive' : 'false'};
CREATE CUSTOM INDEX departmentdevicetracking_log ON fcs.devicetracking_log (departmentid) USING 'org.apache.cassandra.index.sasi.SASIIndex' WITH OPTIONS = {'analyzer_class' : 'org.apache.cassandra.index.sasi.analyzer.NonTokenizingAnalyzer', 'case_sensitive' : 'false'};
CREATE CUSTOM INDEX regiondevicetracking_log ON fcs.devicetracking_log (regionid) USING 'org.apache.cassandra.index.sasi.SASIIndex' WITH OPTIONS = {'analyzer_class' : 'org.apache.cassandra.index.sasi.analyzer.NonTokenizingAnalyzer', 'case_sensitive' : 'false'};
CREATE CUSTOM INDEX speeddevicetracking_log ON fcs.devicetracking_log (speed) USING 'org.apache.cassandra.index.sasi.SASIIndex' WITH OPTIONS = {'analyzer_class' : 'org.apache.cassandra.index.sasi.analyzer.NonTokenizingAnalyzer', 'case_sensitive' : 'false'};
CREATE CUSTOM INDEX vehiclenameardevicetracking_log ON fcs.devicetracking_log (vehiclenamear) USING 'org.apache.cassandra.index.sasi.SASIIndex' WITH OPTIONS = {'analyzer_class' : 'org.apache.cassandra.index.sasi.analyzer.NonTokenizingAnalyzer', 'case_sensitive' : 'false'};
CREATE CUSTOM INDEX vehiclenamedevicetrackinglog ON fcs.devicetracking_log (vehiclename) USING 'org.apache.cassandra.index.sasi.SASIIndex' WITH OPTIONS = {'analyzer_class' : 'org.apache.cassandra.index.sasi.analyzer.NonTokenizingAnalyzer', 'case_sensitive' : 'false'};
CREATE CUSTOM INDEX vehiclesystemiddevicetrackinglog ON fcs.devicetracking_log (vehiclesystemid) USING 'org.apache.cassandra.index.sasi.SASIIndex' WITH OPTIONS = {'analyzer_class' : 'org.apache.cassandra.index.sasi.analyzer.NonTokenizingAnalyzer', 'case_sensitive' : 'false'};
</code></pre>

<p>the select statement as following . </p>

<pre><code>select * from fcs.devicetracking_log where customerId='179_gov'    and regionid='0000015b648d225c-0242ac11000e0001' and dateTime&gt;='1493208398000' allow filtering; 
</code></pre>

<p>but in case of using <code>=</code> it works </p>

<p>and also when adding one more condition or removing regionid it will work fine </p>

<pre><code>and vehiclesystemid='0000015b64937c79-0242ac1100090001' 
</code></pre>

<p>can any one help me  ?
it's  a production issue </p>
",<cassandra><nosql>,"<p>In general you shouldn't use <code>ALLOW FILTERING</code> in production. <a href=""https://www.datastax.com/dev/blog/allow-filtering-explained-2"" rel=""nofollow noreferrer"">See as explanation</a>.</p>

<p>In order to be able to do range queries on <code>dateTimeLong</code> it needs to be part of your key. In cassandra you normally tempt to create tables by your queries. This would mean in your scenario that you could create another table where <code>dateTimeLong</code> would be part of your key.</p>

<pre><code>CREATE TABLE fcs.vehicle_information_byDateTime (
    vehicle_customerId text,
    vehicleid text,
    cityid text,
    cityname text,
    citynamear text,
    createdby text,
    dateTimeLong text,
    description text,
    driverid text,
    drivername text,
    drivernamear text,
    groupofvehicles text,
    groupofvehiclesystemid text,
    insexpirygregoriandate bigint,
    name text,
    namear text,
    platenumber text,

   vehiclestatus text,
    PRIMARY KEY (vehicle_customerId, vehicleid, dateTime)
) ;
</code></pre>

<p>Have in mind that if you want to be able to do range queries on <code>dateTime</code> you need to specify <code>vehicleid</code>.</p>

<p>Let's assume (customerid, (vehiclesystemid, datetime)) is your compound key. <code>customerid</code> is your primary key which you have to specify anyway. </p>

<p>If you know do:</p>

<p><code>select * from s.vechile_information  where vechile_customerid = '123' AND vehiclesystemid &gt;= '32';</code></p>

<p>This will work perfectly fine. This is introduced by the nature how cassandra stores data on disk. Your primary key specifies the location of the data in your cluster. The clustering columns <code>vehiclesystemid, datetime</code> specify <strong>how</strong> the data is stored on disk.</p>

<p>If you now want to do a range query for <code>datetime</code> you can do it by specifying the <code>vehiclesystemid</code> first:</p>

<p><code>select * from s.vechile_information  where vechile_customerid = '123' AND vehiclesystemid = '32' AND datetime &gt;= '1493215758000';</code></p>

<p>When inserting new data you have to insert then into both tables.</p>

<p>With Cassandra 3 there have been introduced <a href=""https://www.datastax.com/dev/blog/new-in-cassandra-3-0-materialized-views"" rel=""nofollow noreferrer"">materialized views</a> which might be a fit for your use case. By this you would avoid the multiple inserts.</p>
",['table']
43668357,43669791,2017-04-27 21:38:38,Create Cassandra CQL with IN and ORDER BY,"<p>I need a CQL to get all rows from the table based on set of current user friends (I'm using IN for that) and sort them based on created date.</p>

<p>I'm trying to play with key and clustering key, but got no ideas.</p>

<p>Here is my Cassandra table:</p>

<pre><code>CREATE TABLE chat.news_feed(
  id_news_feed                           uuid,
  id_user_sent         uuid,
  first_name text,
  last_name text,
  security int,
  news_feed text, 
  image blob,
  image_preview text,
  image_name text,
  image_length int,
  image_resolution text,
  is_image int,
  created_date        timestamp,
  PRIMARY KEY ((id_news_feed, id_user_sent), created_date))
    WITH CLUSTERING ORDER BY (created_date DESC) AND comment = 'List of all news feed by link id';
</code></pre>

<p>and here is my CQL (formed in Java):</p>

<pre><code>SELECT JSON id_news_feed,  first_name, last_name, id_user_sent, news_feed, image_name, image_preview, image_length, created_date, is_image, image_resolution FROM chat.news_feed WHERE id_user_sent in (b3306e3f-1f1d-4a87-8a64-e22d46148316,b3306e3f-1f1d-4a87-8a64-e22d46148316) ALLOW FILTERING;
</code></pre>

<p>I coul not run it cause there is no key in my WHERE part of CQL.</p>

<p>Is there any way how I could get all rows created by set of users with Order By (I tried to create table different ways, but no results yet)?</p>

<p>Thank you!</p>
",<cassandra><cql>,"<p>Unlike the relational databases here you will probably need denormalization of the tables. First of all, you cannot effectively query everything from a single table. Also Cassandra does not support joins natively. I suggest to split up your table into several. </p>

<p>Let's start with the friends: the current user id should be part of the primary key and the friends should go as a clustering column. </p>

<pre><code>CREATE TABLE chat.user_friends (
  user_id uuid,
  friend_id uuid,
  first_name text,
  last_name text,
  security int,
  PRIMARY KEY ((user_id), friend_id));
</code></pre>

<p>Now you can find the friend for each particular user by querying as follows:</p>

<pre><code>SELECT * FROM chat.user_friends WHERE user_id = 'a001-...';
</code></pre>

<p>or</p>

<pre><code>SELECT * FROM chat.user_friends WHERE user_id = 'a001-...' and friend_id in ('a121-...', 'a156-...', 'a344-...');
</code></pre>

<p>Next let's take care of news feed: before putting remaining columns into this table I'd think about the desired query against this table. The news feeds needs to be filtered by the user ids with <code>IN</code> listing and at the same time be sortable by time. So we put the <code>created_date timestamp</code> as a clustering key and friends' user_id as a partitioning key. Note that the timestamps will be sorted per user_id not globally (you can re-sort those on the client side). What's really important is to keep news_feed_id out of the primary key. This column still may contain uuid which is unique, but as long as we don't want to query this table to get a particular news feed by id. For this purpose We'd anyway require separate table (denormalization of the data) or materialized view (which I will not cover in this answer but is quite nice solution for some types of denormalization introduced in Cassandra 3.0).</p>

<p>Here is the updated table:</p>

<pre><code>CREATE TABLE chat.news_feed(      
  id_user_sent uuid,
  first_name text,
  last_name text,
  security int,
  id_news_feed uuid,
  news_feed text, 
  image blob,
  image_preview text,
  image_name text,
  image_length int,
  image_resolution text,
  is_image int,
  created_date        timestamp,
  PRIMARY KEY ((id_user_sent), created_date))
    WITH CLUSTERING ORDER BY (created_date DESC) AND comment = 'List of all news feed by link id';
</code></pre>

<p>Some example dataset:</p>

<pre><code>cqlsh:ks_test&gt; select * from news_feed ;

 id_user_sent                         | created_date                    | first_name | id_news_feed                         | image | image_length | image_name | image_preview | image_resolution | is_image | last_name | news_feed | security
--------------------------------------+---------------------------------+------------+--------------------------------------+-------+--------------+------------+---------------+------------------+----------+-----------+-----------+----------
 01b9b9e8-519c-4578-b747-77c8d9c4636b | 2017-02-23 00:00:00.000000+0000 |       null | fd25699c-78f1-4aee-913a-00263912fe18 |  null |         null |       null |          null |             null |     null |      null |      null |     null
 9bd23d16-3be3-4e27-9a47-075b92203006 | 2017-02-21 00:00:00.000000+0000 |       null | e5d394d3-b67f-4def-8f1e-df781130ea22 |  null |         null |       null |          null |             null |     null |      null |      null |     null
 6e05257d-9278-4353-b580-711e62ade8d4 | 2017-02-25 00:00:00.000000+0000 |       null | ec34c655-7251-4af8-9718-3475cad18b29 |  null |         null |       null |          null |             null |     null |      null |      null |     null
 6e05257d-9278-4353-b580-711e62ade8d4 | 2017-02-22 00:00:00.000000+0000 |       null | 5342bbad-0b55-4f44-a2e9-9f285d16868f |  null |         null |       null |          null |             null |     null |      null |      null |     null
 6e05257d-9278-4353-b580-711e62ade8d4 | 2017-02-20 00:00:00.000000+0000 |       null | beea0c24-f9d6-487c-a968-c9e088180e73 |  null |         null |       null |          null |             null |     null |      null |      null |     null
 63003200-91c0-47ba-9096-6ec1e35dc7a0 | 2017-02-21 00:00:00.000000+0000 |       null | a0fba627-d6a7-463c-a00c-dd0472ad10c5 |  null |         null |       null |          null |             null |     null |      null |      null |     null
</code></pre>

<p>And the filtered one:</p>

<pre><code>cqlsh:ks_test&gt; select * from news_feed where id_user_sent in (01b9b9e8-519c-4578-b747-77c8d9c4636b, 6e05257d-9278-4353-b580-711e62ade8d4) and created_date &gt;= '2017-02-22';

 id_user_sent                         | created_date                    | first_name | id_news_feed                         | image | image_length | image_name | image_preview | image_resolution | is_image | last_name | news_feed | security
--------------------------------------+---------------------------------+------------+--------------------------------------+-------+--------------+------------+---------------+------------------+----------+-----------+-----------+----------
 01b9b9e8-519c-4578-b747-77c8d9c4636b | 2017-02-25 00:00:00.000000+0000 |       null | 26dc0952-0636-438f-8a26-6a3fef4fb808 |  null |         null |       null |          null |             null |     null |      null |      null |     null
 01b9b9e8-519c-4578-b747-77c8d9c4636b | 2017-02-23 00:00:00.000000+0000 |       null | fd25699c-78f1-4aee-913a-00263912fe18 |  null |         null |       null |          null |             null |     null |      null |      null |     null
 6e05257d-9278-4353-b580-711e62ade8d4 | 2017-02-25 00:00:00.000000+0000 |       null | ec34c655-7251-4af8-9718-3475cad18b29 |  null |         null |       null |          null |             null |     null |      null |      null |     null
 6e05257d-9278-4353-b580-711e62ade8d4 | 2017-02-22 00:00:00.000000+0000 |       null | 5342bbad-0b55-4f44-a2e9-9f285d16868f |  null |         null |       null |          null |             null |     null |      null |      null |     null
</code></pre>

<p>P.S. As you might notice we got rid of the <code>ALLOW FILTERING</code> clause. Don't use <code>ALLOW FILTERING</code> in any application as it has significant performance penalty. This is only usable to look up some small chunk of data scattered around in different partitions.</p>
",['table']
43676414,43677087,2017-04-28 09:12:23,Get data by a specific query in a cassandra keyspace,"<p>I have a cassandra table with this following scheme: </p>

<pre><code>CREATE TABLE keyspace1.records (
    name text,
    sensor_id text,
    record_hour timestamp,
    record_time timestamp,
    raw_value text,
    value text,
    PRIMARY KEY ((name, sensor_id, record_hour), record_time)
)
</code></pre>

<p>I want to get from the records, how many records each sensor has every hour.
example:</p>

<pre><code>sensor_id 145 has 3 records at 2016-10-13 10:00:00+0000
sensor_id 145 has 12 records at 2016-10-13 12:00:00+0000
sensor_id 227 has 4 records at 2016-10-14 20:00:00+0000
sensor_id 227 has 7 records at 2016-10-14 17:00:00+0000
sensor_id 227 has 2 records at 2016-10-14 08:00:00+0000
</code></pre>

<p>When I do a request like this when trying to get all records of one capture :</p>

<pre><code>SELECT * FROM keyspace1.records WHERE sensor_id='145' ;
</code></pre>

<p>I get this error :</p>

<pre><code>InvalidRequest: Error from server: code=2200 [Invalid query] message=""Partition key parts: name must be restricted as other parts are""
</code></pre>

<p>and when I tried this query I got a result but not what I'm looking for:</p>

<pre><code>SELECT * FROM keyspace1.records WHERE sensor_id = '145' AND name = 'client_NYC' AND record_hour IN ('2016-07-16 17:00:00+0000',  '2016-07-16 22:00:00+0000') ;
</code></pre>

<p>But my problem is I don't want to specify the name nor the record time in the request, I only want to know how many records a sensor has in a day every hour.</p>

<p>How to do that ? what is the right query in this case?</p>
",<cassandra><request><cqlsh>,"<p><strong>You have to specify all the partition key when querying.</strong> </p>

<p>You have specified <code>name, sensor_id, record_hour</code> as partition key, So every time you query you have specify all of these field.</p>

<p>If you just want the every hour count of record for each <code>sensor_id</code>, it is best to create a counter table like below : </p>

<pre><code>CREATE TABLE record_count(
    sensor_id text,
    record_hour timestamp,
    count counter,
    PRIMARY KEY(sensor_id, record_hour)
);
</code></pre>

<p>Every time a record inserted into records, increment the value of count :</p>

<pre><code>UPDATE record_count SET count = count + 1 WHERE sensor_id = ? AND record_hour = ?
</code></pre>

<p>Now you can get the record count by hour for each sensor</p>

<pre><code>SELECT * FROM record_count WHERE sensor_id = ?
</code></pre>

<p>If you already have data on records and you want these data to be in your counter table, then you can use driver pagination to scan all the row and increment the counter.</p>

<p>Check this <a href=""https://datastax.github.io/python-driver/query_paging.html#paging-large-queries"" rel=""nofollow noreferrer"">Python Driver Pagination</a></p>

<p>Or Use Copy command to dump sensor_id and record_hour </p>

<pre><code>COPY records (sensor_id , record_hour ) TO 'records.csv';
</code></pre>

<p>Now just for each line of <code>records.csv</code> append <code>,1</code> at the end of line and write to another file say <code>record_count.csv</code>.Now you have the csv, you can just import it.</p>

<pre><code>COPY record_count (sensor_id , record_hour , count) FROM 'record_count.csv';
</code></pre>
",['table']
43873374,43874010,2017-05-09 15:03:00,How do I select everything where two columns contain equal values in CQL?,"<p>I'm trying to select everything where two columns contain equal values.  Here is my CQL query:</p>

<pre><code>select count(someColumn) from somekeySpace.sometable where columnX = columnY
</code></pre>

<p>This doesn't work.  How can I do this?</p>
",<cassandra><cql>,"<p><strong>You can't query like that, cassandra don't support it</strong></p>

<p>You can do this in different way.<br>
First you have to create a separate counter table.</p>

<pre><code>CREATE TABLE match_counter(
    partition int PRIMARY KEY,
    count counter
);
</code></pre>

<p>At the time of insertion into your main table if <code>columnX = columnY</code> then increment the value here. Though you have only a single count, you can use a static value of partition</p>

<pre><code>UPDATE match_counter SET count = count + 1 WHERE partition = 1;
</code></pre>

<p>Now you can get the count of match column</p>

<pre><code> SELECT * FROM match_counter WHERE partition = 1;
</code></pre>
",['table']
43876906,43877513,2017-05-09 18:03:54,Schema for tick data on cassandra,"<h2>Overview</h2>

<p>I would like to determine the correct schema in cassandra for financial tick data.</p>

<h2>Data and schema</h2>

<p>I have the following sample data in csv:</p>

<pre><code>SYMBOL,DATE,TIME,PRICE,SIZE
A,2011-01-03,9:28:00,41.46,200
A,2011-01-03,9:28:00,41.46,100
A,2011-01-03,9:30:00,41.56,1300
A,2011-01-03,9:30:00,41.56,1300
A,2011-01-03,9:30:00,41.55,100
A,2011-01-03,9:30:19,41.55,200
A,2011-01-03,9:30:23,41.5169,100
A,2011-01-03,9:30:29,41.44,66534
A,2011-01-03,9:30:29,41.45,225
A,2011-01-03,9:30:30,41.44,100
A,2011-01-03,9:30:30,41.43,100
A,2011-01-03,9:30:30,41.49,100
A,2011-01-03,9:30:30,41.45,200
</code></pre>

<p>and I store into the following table:</p>

<pre><code>CREATE TABLE tickdata (
symbol text,
date date,
time time,
price float,
size int,
PRIMARY KEY ((symbol,date),time)
);
</code></pre>

<p>This a slice of a <code>SELECT</code> of the table:</p>

<pre><code> symbol | date       | time               | price   | size
--------+------------+--------------------+---------+-------
      A | 2011-01-03 | 09:28:00.000000000 |   41.46 |   100
      A | 2011-01-03 | 09:30:00.000000000 |   41.56 |  1300
      A | 2011-01-03 | 09:30:19.000000000 |   41.55 |   200
      A | 2011-01-03 | 09:30:23.000000000 | 41.5169 |   100
      A | 2011-01-03 | 09:30:29.000000000 |   41.45 | 66534
</code></pre>

<h2>Use case</h2>

<p>The data will be written to Cassandra once, and mostly read with conditions on <code>date</code> and <code>symbol</code>, e.g. a set of symbols for a given time-period.</p>

<h2>Questions</h2>

<ul>
<li><p>The tuple <code>(symbol,date,time)</code> are not a proper <code>PRIMARY KEY</code>, since my granularity is limited to seconds. Hence, the <code>COPY FROM</code> e.g. drops the second row of the csv during the import due to the repetition in the key. <strong>How can I preserve the record</strong>?</p></li>
<li><p>Assuming the <code>PRIMARY KEY</code> is unique, how can I <strong>avoid storing repeated values</strong> of <code>SYMBOL</code> and <code>DATE</code>? Or is partitioning taking care of that under the hood?</p></li>
<li><p>I was thinking to use the following schema:</p>

<pre><code>CREATE TABLE tickdata (
symbol text,
date date,
time blob,
price blob,
size blob,
PRIMARY KEY ((symbol,date))
);
</code></pre>

<p>to store raw data. Is this the correct way to address the points above?</p></li>
<li><p>The data is <strong>NOT ordered</strong> according to the definition of the <code>PRIMARY KEY</code> when I <code>SELECT</code> it. Is that related to the non-uniqueness problem entioned above?</p></li>
<li><p>Should I stick with my binary <strong>file-store</strong> which keeps a map of symbols and dates and loads the relevant files on request? This avoids repeating symbol and date for each row and is indifferent to limited granularity (repetition) of the timestamp.</p></li>
</ul>
",<cassandra><time-series><schema>,"<blockquote>
  <p>The tuple (symbol,date,time) are not a proper PRIMARY KEY, since my
  granularity is limited to seconds. Hence, the COPY FROM e.g. drops the
  second row of the csv during the import due to the repetition in the
  key. How can I preserve the record?</p>
</blockquote>

<p>The primary key in your first table definition is <code>((symbol,date),time)</code> NOT <code>(symbol,date,time)</code>. Both are different in cassandra.</p>

<p><code>((symbol,date),time)</code> => will store all records for same symbol (A) and date in one node. For same symbol(A) but other date might go on other node.
Row Key will be symbol+date</p>

<p><strong>Physical Data layout (example)</strong></p>

<pre><code>|A_2011-01-03||time1.price &amp; time1.value||time2.price &amp; time2.value|
|A_2011-01-04||time1.price &amp; time1.value||time2.price &amp; time2.value|
|B_2011-01-03||time1.price &amp; time1.value||time2.price &amp; time2.value|
|B_2011-01-04||time1.price &amp; time1.value||time2.price &amp; time2.value|
</code></pre>

<p><code>(symbol,date,time)</code> => All records for same symbol will reside on one node. This might result in wide rows.
Row key will be symbol.</p>

<p><strong>Physical Data layout (example)</strong></p>

<pre><code>|A||date1.time1.price &amp; date1.time1.value||date1.time2.price &amp; date1.time2.value||date2.time1.price &amp; date2.time1.value||date2.time2.price &amp; date2.time2.value|
|B||date1.time1.price &amp; date1.time1.value||date1.time2.price &amp; date1.time2.value||date2.time1.price &amp; date2.time1.value||date2.time2.price &amp; date2.time2.value|
</code></pre>

<p>To Avoid dropping of records you can add one more column like <code>uuid</code> or <code>timeuuid</code></p>

<pre><code>CREATE TABLE tickdata (
symbol text,
date date,
time time,
price float,
size int,
id timeuuid
PRIMARY KEY ((symbol,date),time,id)
);
</code></pre>

<blockquote>
  <p>Assuming the PRIMARY KEY is unique, how can I avoid storing repeated
  values of SYMBOL and DATE? Or is partitioning taking care of that
  under the hood?</p>
</blockquote>

<p>Based on physical storage structure explained above this issue is already taken care of.</p>

<p>The alternate schema you are talking about will have only 1 record for one symbol and a date. You will have to handle the blob at application side... which i think might be overhead.</p>

<blockquote>
  <p>The data is NOT ordered according to the definition of the PRIMARY KEY
  when I SELECT it. Is that related to the non-uniqueness problem
  entioned above?</p>
</blockquote>

<p>By default data is ordered by clustering key in ascending order (in your case time). Though you can change order by changing CLUSTERING ORDER BY property of table to descending.</p>

<p>Example:</p>

<pre><code>CREATE TABLE tickdata (
symbol text,
date date,
time time,
price float,
size int,
id timeuuid
PRIMARY KEY ((symbol,date),time,id)
) WITH CLUSTERING ORDER BY(time desc,id desc); 
</code></pre>

<blockquote>
  <p>Should I stick with my binary file-store which keeps a map of symbols
  and dates and loads the relevant files on request? This avoids
  repeating symbol and date for each row and is indifferent to limited
  granularity (repetition) of the timestamp.</p>
</blockquote>

<p>you can decide this on your own :)</p>
",['table']
43971166,43974947,2017-05-15 03:25:00,how to import csv file to cassandra,"<p>I wanted to import csv file to cassandra so, first I created keyspace and columnfamily like this</p>

<pre><code>CREATE COLUMNFAMILY Consumer_complaints(
    Date_received varchar,
    Product varchar,
    Sub_product varchar,
    Issue varchar,
    Sub_issue varchar,
    Consumer_complaint_narrative varchar,
    Company_public_response varchar,
    Company varchar,
    State varchar,
    ZIP_code varint,
    Tags varchar,
    Consumer_consent_provided varchar,
    Submitted_via varchar,
    Date_sent_to_company varchar,
    Company_response_to_consumer varchar,
    Timely_response varchar,
    Consumer_disputed varchar,
    Complaint_ID varint,
    PRIMARY KEY(Complaint_ID)
);
</code></pre>

<p>I got a csv file from www.data.gov called consumer complaints
and then I typed the command line</p>

<pre><code>COPY consumer_complaints (Date_received,Product,Sub_product, Issue, Sub_issue, Consumer_complaint_narrative, Company_public_response, Company, State, ZIP_code, Tags, Consumer_consent_provided, Submitted_via, Date_sent_to_company, Company_response_to_consumer, Timely_response, Consumer_disputed, Complaint_ID) FROM 'consumer_complaints.csv';
</code></pre>

<p>Sample Input</p>

<pre><code>3/21/2017,Credit reporting,,Incorrect information on credit report,Information is not mine,,Company has responded to the consumer and the CFPB and chooses not to provide a public response,EXPERIAN DELAWARE GP,TX,77075,Older American,N/A,Phone,03/21/2017,Closed with non-monetary relief,Yes,No,2397100
04/19/2017,Debt collection,""Other (i.e. phone, health club, etc.)"",Disclosure verification of debt,Not disclosed as an attempt to collect,,,""Security Credit Services, LLC"",IL,60643,,,Web,04/20/2017,Closed with explanation,Yes,No,2441777
</code></pre>

<p>Error</p>

<pre><code>Failed to import 1 rows: ParseError - Failed to parse 797XX : invalid lit for int() with base 10: '797XX',  given up without retries
Failed to import 1 rows: ParseError - Failed to parse 354XX : invalid lit for int() with base 10: '354XX',  given up without retries
Failed to import 2 rows: ParseError - Failed to parse 313XX : invalid lit for int() with base 10: '313XX',  given up without retries
Failed to import 2 rows: ParseError - Failed to parse 054XX : invalid lit for int() with base 10: '054XX',  given up without retries
</code></pre>

<p>how can I fix it??</p>
",<cassandra>,"<p>Cassandra doesn't preserve the order of column when creating.
You need to specify the column name when importing data.</p>

<p>Try this command :</p>

<pre><code>COPY consumer_complaints (Date_received,Product,Sub_product, Issue, Sub_issue, Consumer_complaint_narrative, Company_public_response, Company, State, ZIP_code, Tags, Consumer_consent_provided, Submitted_via, Date_sent_to_company, Company_response_to_consumer, Timely_response, Consumer_disputed, Complaint_ID) FROM 'c.csv' WITH HEADER = true;
</code></pre>

<p>Sample Input : </p>

<pre><code>Date_received,Product,Sub_product, Issue, Sub_issue, Consumer_complaint_narrative, Company_public_response, Company, State, ZIP_code, Tags, Consumer_consent_provided, Submitted_via, Date_sent_to_company, Company_response_to_consumer, Timely_response, Consumer_disputed, Complaint_ID
07/26/2013,Mortgage,FHA mortgage,""Loan servicing, payments, escrow account"",,,,""CITIBANK, N.A."",NC,28056,,N/A,Web,07/29/2013,Closed with explanation,Yes,No,467750
09/26/2014,Consumer Loan,Vehicle loan,Managing the loan or lease,,,,HSBC NORTH AMERICA HOLDINGS INC.,NY,12572,,N/A,Web,09/26/2014,Closed with explanation,Yes,No,1046323
</code></pre>

<p>Output :</p>

<pre><code>complaint_id  | company                          | company_public_response | company_response_to_consumer | consumer_complaint_narrative | consumer_consent_provided | consumer_disputed | date_received | date_sent_to_company | issue                                    | product       | state | sub_issue | sub_product  | submitted_via | tags | timely_response | zip_code
--------------+----------------------------------+-------------------------+------------------------------+------------------------------+---------------------------+-------------------+---------------+----------------------+------------------------------------------+---------------+-------+-----------+--------------+---------------+------+-----------------+----------
      1046323 | HSBC NORTH AMERICA HOLDINGS INC. |                    null |      Closed with explanation |                         null |                       N/A |                No |    09/26/2014 |           09/26/2014 |               Managing the loan or lease | Consumer Loan |    NY |      null | Vehicle loan |           Web | null |             Yes |    12572
       467750 |                   CITIBANK, N.A. |                    null |      Closed with explanation |                         null |                       N/A |                No |    07/26/2013 |           07/29/2013 | Loan servicing, payments, escrow account |      Mortgage |    NC |      null | FHA mortgage |           Web | null |             Yes |    28056
</code></pre>

<p><strong>Edited</strong></p>

<p>I checked the data from <a href=""https://catalog.data.gov/dataset/consumer-complaint-database"" rel=""nofollow noreferrer"">https://catalog.data.gov/dataset/consumer-complaint-database</a>, some of the zip code has non-integer values like <code>797XX</code>, <code>354XX</code>,  <code>313XX</code> and <code>054XX</code>. You can see that it's clearly not integer. You can either change these value to integer or alter your table and change the type of your ZIP_code field to <code>varchar</code></p>
",['table']
43988390,44013740,2017-05-15 20:35:53,Why read fails with cqlsh query when huge tombstones is present,"<p>I have a table with huge tombstones.So when i performed a spark job (which reads) on that specific table, it gave result without any issues. But i executed same query using cqlsh it gave me error because huge tombstones present in that table.</p>

<blockquote>
  <p>Cassandra failure during read query at consistency one(1 replica
  needed but o replicas responded ,1 failed</p>
</blockquote>

<p>I know tombstones should not be there, i can run repair to avoid them , but apart from that why spark succeeded and cqlsh failed. They both use same sessions and queries.</p>

<p>How <code>spark-cassandra</code> connector works? is it different than cqlsh?
Please let me know .</p>

<p>thank you.</p>
",<apache-spark><cassandra><datastax-enterprise><cassandra-3.0><spark-cassandra-connector>,"<p>The Spark Cassandra Connector is different to cqlsh in a few ways.</p>

<ul>
<li>It uses the Java Driver and not the python Driver</li>
<li>It has significantly more lenient retry policies</li>
<li>It full table scans by breaking the request up into pieces</li>
</ul>

<p>Any of these items could be contributing to why it would work in SCC and not in CQLSH. </p>
",['table']
44028103,44052663,2017-05-17 14:45:01,Flink cassandraOutputFormat tuple needs frozen value,"<p>I have a flink project that will be inserting data in a cassandra table as a batch job. I already have a flink stream project where it is writing a pojo to the same cassandra table, but cassandraOutputFormat needs the data as a Tuple (hope that is changed to accept pojos like CassandraSink does at some point). So here is the pojo that I have that:</p>

<pre><code>@Table(keyspace=""mykeyspace"", name=""mytablename"")
public class AlphaGroupingObject implements Serializable {

    @Column(name = ""jobId"")
    private String jobId;
    @Column(name = ""datalist"")
    @Frozen(""list&lt;frozen&lt;dataobj&gt;"")
    private List&lt;CustomDataObj&gt; dataobjs;
    @Column(name = ""userid"")
    private String userid;

    //Getters and Setters
}
</code></pre>

<p>And the dataset of tuple I am making from this pojo:</p>

<pre><code>DataSet&lt;Tuple3&lt;String, List&lt;CustomDataObj&gt;, String&gt;&gt; outputDataSet = listOfAlphaGroupingObject.map(new AlphaGroupingObjectToTuple3Mapper());
</code></pre>

<p>And here is the line that triggers the output as well:</p>

<pre><code>outputDataSet.output(new CassandraOutputFormat&lt;&gt;(""INSERT INTO mykeyspace.mytablename (jobid, datalist, userid) VALUES (?,?,?);"", clusterThatWasBuilt));
</code></pre>

<p>Now the issue that I have is when I try to run this, I get this error when it tries to output it to the cassandra table:</p>

<pre><code>Caused by: com.datastax.driver.core.exceptions.CodecNotFoundException: 
Codec not found for requested operation: [frozen&lt;mykeyspace.dataobj&gt; &lt;-&gt; flink.custom.data.CustomDataObj]
</code></pre>

<p>So I know when it was a pojo, I just had to add the @Frozen annotation to the field, but I don't know how to do that for a tuple. What is the best/proper way to fix this? Or am I doing something unnecessary because there is actually a way to send pojos through the cassandraOutputFormat I just haven't found?</p>

<p>Thanks for any and all help in advance! </p>

<p><strong>EDIT:</strong></p>

<p>Here is the code for the CustomDataObj class too:</p>

<pre><code>@UDT(name=""dataobj"", keyspace = ""mykeyspace"")
public class CustomDataObj implements Serializable {


    @Field(name = ""userid"")
    private String userId;

    @Field(name = ""groupid"")
    private String groupId;

    @Field(name = ""valuetext"")
    private String valueText;

    @Field(name = ""comments"")
    private String comments;

    //Getters and setters
}
</code></pre>

<p><strong>EDIT 2</strong></p>

<p>Including the table schema in cassandra that the CustomDataObj is tied to and the mytablename schema.</p>

<pre><code>CREATE TYPE mykeyspace.dataobj (
    userid text,
    groupid text,
    valuetext text,
    comments text
);

CREATE TABLE mykeyspace.mytablename (
    jobid text,
    datalist list&lt;frozen&lt;dataobj&gt;&gt;,
    userid text,
    PRIMARY KEY (jobid, userid)
);
</code></pre>
",<java><cassandra><apache-flink>,"<p>I believe I have found a better way than having to provide a tuple to the cassandraOutputFormat, but it technically still doesn't answer this question so I won't mark this as the answer. I ended up using cassandra's object mapper so I can just send the pojo to the table. Still need to validate that data got to the table successfully and that everything is working properly with the way it is implemented, but I felt this would help anyone who is facing a similar problem.</p>

<p>Here is the doc that outlines the solution: <a href=""http://docs.datastax.com/en/developer/java-driver/2.1/manual/object_mapper/using/"" rel=""nofollow noreferrer"">http://docs.datastax.com/en/developer/java-driver/2.1/manual/object_mapper/using/</a></p>
",['table']
44033139,44044427,2017-05-17 19:20:10,Cassandra grouping with filter,"<p>I have a table of events that are done every minute. I want to be able to filter these events by time period and also aggregate data for hour/day/etc.</p>

<p>My data model: </p>

<pre><code>create table min_dev_data (
device TEXT,
event_time BIGINT,
hour BIGINT,
day BIGINT,
value DOUBLE,
PRIMARY KEY ((device), event_time)
)

CREATE MATERIALIZED VIEW hour_dev_data AS
SELECT device, event_time, hour, value
FROM min_dev_data
WHERE hour IS NOT NULL AND value IS NOT NULL 
      and event_time IS NOT NULL AND device IS NOT NULL
PRIMARY KEY ((device), hour, event_time)
</code></pre>

<p>my query is</p>

<pre><code>select hour, sum(value) 
from hour_dev_data 
where device = 'tst' and event_time &lt; 149000000 group by device, hour;
</code></pre>

<p>fails with error 
code=2200 [Invalid query] message=""PRIMARY KEY column ""event_time"" cannot be restricted as preceding column ""hour"" is not restricted""</p>

<p>The only way to make it work is to add ALLOW FILTERING, which is unpredictable.</p>

<p>How can I change my data model to address my query and avoid ALLOW FILTERING mode?</p>
",<cassandra>,"<p>You have to proactively produce these results:</p>

<pre><code>create table min_dev_data (
    device TEXT,
    event_time BIGINT,
    hour BIGINT,
    day BIGINT,
    value DOUBLE,
    PRIMARY KEY ((device), event_time)
) WITH CLUSTERING ORDER BY (event_time DESC);

create table hour_dev_data (
    device TEXT,
    hour BIGINT,
    day BIGINT,
    event_time BIGINT,
    value DOUBLE,
    PRIMARY KEY ((device), event_time)
) WITH CLUSTERING ORDER BY (event_time DESC);

create table day_dev_data (
    device TEXT,
    day BIGINT,
    event_time BIGINT,
    value DOUBLE,
    PRIMARY KEY ((device), event_time)
) WITH CLUSTERING ORDER BY (event_time DESC);
</code></pre>

<p>Each table will satisfy ONE granularity only. </p>

<p>Every hour you query the minute data for the latest hour data for each device with something like:</p>

<pre><code>SELECT * FROM min_dev_data WHERE device = X AND event_time &lt; YYYY
</code></pre>

<p>Sum that at application level and store this value into the hour table:</p>

<pre><code>INSERT INTO hour_dev_data (device, hour, day, event_time, value) VALUES (....);
</code></pre>

<p>And every day you query the hour table to produce the further aggregate data:</p>

<pre><code>SELECT * FROM hour_dev_data WHERE device = X AND event_time &lt; YYYY
</code></pre>

<p>sum at application level and store this value into the day table.</p>

<p>Please consider adding some form of bucketing because, at one minute interval, in two months your minute table will have wide partitions. This should not be a problem if you keep the table in reverse order (like I did) and query only for the last couple of hours. But if you want to query back in time as well then you must definitely use bucketing in your tables.</p>
",['table']
44056790,44062319,2017-05-18 19:59:26,"InvalidRequest: code=2200 [Invalid query] message=""unconfigured table credentials""","<p>I'm trying to reset the <code>cassandra</code> user's password as suggested <a href=""https://stackoverflow.com/questions/18398987/how-to-reset-a-lost-cassandra-admin-users-password"">here</a>:</p>
<pre><code>UPDATE system_auth.credentials SET salted_hash = '$2a$10$H46haNkcbxlbamyj0OYZr.v4e5L08WTiQ1scrTs9Q3NYy.6B..x4O' WHERE username='cassandra';
</code></pre>
<p>I'm getting the following error:</p>
<pre><code>InvalidRequest: code=2200 [Invalid query] message=&quot;unconfigured table credentials&quot;
</code></pre>
<p>My client is cqlsh 5.0.1.</p>
<p>How can I reset the cassandra password?</p>
<p>Cassandra was installed from <code>datastax-ddc-3.7.0-1.noarch.rpm</code></p>
<p>Cassandra version is 3.7.0</p>
",<cassandra><cqlsh>,"<p><strong>In Cassandra 2.2  table name changed</strong><br>
Check the <a href=""https://github.com/apache/cassandra/blob/8b3a60b9a7dbefeecc06bace617279612ec7092d/NEWS.txt#L533"" rel=""nofollow noreferrer"">Cassandra Release News</a></p>

<p><strong>New table is roles</strong> </p>

<pre><code>CREATE TABLE system_auth.roles (
    role text PRIMARY KEY,
    can_login boolean,
    is_superuser boolean,
    member_of set&lt;text&gt;,
    salted_hash text
);
</code></pre>

<p>Use the below query : </p>

<pre><code>UPDATE system_auth.roles SET salted_hash = '$2a$10$1PzCxcMNKgsBEcI1lf.ndut24xyO0N2LzRdRF1tzaMaSH9KFLz/0u' WHERE role = 'cassandra';
</code></pre>
",['table']
44077879,44263303,2017-05-19 19:40:37,issue with frequent truncates in Cassandra and 24 hour ttl create large tombstones,"<p>We have the below table with <code>ttl</code> 24 hours or 1 day. We have 4 <code>cassandra 3.0 node cluster</code> and there will be a <code>spark</code> processing on this table. Once processed, it will truncate all the data in the tables and new batch of data would be inserted. This will be a continuous process.</p>

<p>Problem I am seeing is , we are getting more <code>tombstones</code> because data is truncated frequently everyday after <code>spark</code> finishes processing. </p>

<p>If I set <code>gc_grace_seconds</code> to default , there will be more <code>tombstones</code>. If I reduce <code>gc_grace_seconds</code> to 1 day will it be an issue ? even if I run repair on that table every day will that be enough. </p>

<p>How should I approach this problem, I know frequent deletes is an antipattern in <code>Cassandra</code>, is there any other way to solve this issue?</p>

<pre><code>TABLE b.stag (
    xxxid bigint PRIMARY KEY,
    xxxx smallint,
    xx smallint,
    xxr int,
    xxx text,
    xxx smallint,
    exxxxx smallint,
   xxxxxx tinyint,
    xxxx text,
    xxxx int,
    xxxx text,
 xxxxx text,
    xxxxx timestamp
) WITH bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCom                                                                                        pactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandr                                                                                        a.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 86400
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';
</code></pre>

<p>thank you</p>
",<apache-spark><cassandra><spark-streaming><datastax-enterprise><cassandra-3.0>,"<p>A truncate of a table should not invoke tombstones. So when you're saying ""truncating"" I assume you mean deleting. You can as you have already mentioned drop the <code>gc_grace_seconds</code> value, however this is means you have a smaller window for repairs to run to reconcile any data, make sure each node has the right tombstone for a given key etc or old data could reappear. Its a trade off.</p>

<p>However to be fair if you are clearing out the table each time, why not use the <a href=""http://docs.datastax.com/en/cql/3.3/cql/cql_reference/cqlTruncate.html"" rel=""nofollow noreferrer"">TRUNCATE</a> command, this way you'll flush the table with no tombstones.</p>
",['table']
44109822,44110478,2017-05-22 09:50:43,How to convert Cassandra/ScyllaDB text to bigint and vice-versa?,"<p>In PostgreSQL I can do this:</p>

<pre><code>SELECT (col::BIGINT+1)::TEXT + '%' FROM bar WHERE id = 1
UPDATE bar SET col = (col::BIGINT+1)::TEXT WHERE id = 1 
</code></pre>

<p>How to do this in Cassandra/ScyllaDB? I need to convert TEXT to BIGINT and back to TEXT to update a column value, the column itself must be a TEXT because it doesn't store only number.</p>
",<cassandra><scylla>,"<p>There is no default function to this in cql. But you can create one, see <a href=""http://docs.datastax.com/en/cql/3.3/cql/cql_using/useCreateUDF.html"" rel=""nofollow noreferrer"">UDF</a>. Cassandra support UDF but Scylladb doesn't supports UDF yet</p>

<p>Let's create these function : </p>

<pre><code>CREATE OR REPLACE FUNCTION bigintAstext (input bigint) CALLED ON NULL INPUT RETURNS text LANGUAGE java AS 'return String.valueOf(input);';

CREATE OR REPLACE FUNCTION textAsbigint (input text) CALLED ON NULL INPUT RETURNS bigint LANGUAGE java AS 'return Long.parseLong(input);';
</code></pre>

<p>Here <code>bigintAstext</code> will convert bigint to text and <code>textAsbigint</code> will convert text to bigint</p>

<p>How to use ?</p>

<p>Let's create a table and insert data</p>

<pre><code>CREATE TABLE udf_test (
    id bigint PRIMARY KEY,
    data text
);

INSERT INTO udf_test (id , data ) VALUES ( 10, '10');
</code></pre>

<p>Now you can query like : </p>

<pre><code>SELECT bigintAstext(id), textAsbigint(data) FROM udf_test ;
</code></pre>

<p>Output : </p>

<pre><code> test.bigintastext(id) | test.textasbigint(data)
-----------------------+-------------------------
                    10 |                      10
</code></pre>

<p>Note : UDFs (user defined functions) are disabled by default, you can enable it by setting <code>enable_user_defined_functions: true</code> on <code>cassandra.yaml</code></p>
",['table']
44115579,44117218,2017-05-22 14:31:54,Cassandra Time Series Data Modeling,"<p>I have a table defined as follows</p>

<pre><code>create table events (offset int,key varchar, user uuid,name varchar, 
emitted int, day int, month int, year int,PRIMARY KEY((offset), year, month, 
day) ) 
WITH CLUSTERING  ORDER BY (year DESC, month DESC, day DESC);
</code></pre>

<p>where user is a uuid of the user who submitted the event,
offset is unique, but user is not because a user has many events.
I'd like to select events given user and date range, how can I accomplish this</p>
",<cassandra><time-series><data-modeling>,"<p><strong>You need to a create materialized view or another table</strong></p>

<p>If you are using cassandra >= 3.x, you can create materialized view</p>

<pre><code>CREATE MATERIALIZED VIEW events_by_user AS
    SELECT user, year, month, day, offset
    FROM events
    WHERE user IS NOT NULL AND year IS NOT NULL AND month IS NOT NULL AND day IS NOT NULL AND offset IS NOT NULL
    PRIMARY KEY (user, year, month, day, offset)
    WITH CLUSTERING ORDER BY (year DESC, month DESC, day DESC, offset ASC);
</code></pre>

<p>Or If you use lower version then create another table :</p>

<pre><code>CREATE TABLE events_by_user (
   user uuid, 
   offset int, 
   day int, 
   month int, 
   year int, 
   primary key(user,  year, month, day, offset)
);
</code></pre>

<p>Now you can query by user with date range i.e : </p>

<pre><code>SELECT * FROM events_by_user WHERE user = 6d5c6400-3f04-11e7-b92e-371a840aa4bb AND (year, month, day) &gt;= (2017,05,20) AND (year, month, day) &lt;= (2017,05,25);
</code></pre>
",['table']
44123989,44124444,2017-05-23 00:32:33,Error: value cassandraFormat is not a member of org.apache.spark.sql.DataFrameWriter,"<p>Checking the repo on github I see <code>cassandraFormat</code> <a href=""https://github.com/datastax/spark-cassandra-connector/blob/master/spark-cassandra-connector/src/main/scala/org/apache/spark/sql/cassandra/package.scala"" rel=""nofollow noreferrer"">here</a>. My import statement is not throwing an exception:</p>

<pre><code>import org.apache.spark.sql.cassandra._

df.write
    .cassandraFormat(""keyspace"", ""table"")
    .save()

&lt;console&gt;:34: error: value cassandraFormat is not a member of org.apache.spark.sql.DataFrameWriter[org.apache.spark.sql.Row]
</code></pre>

<p><code>cassandraFormat</code> is not available under <code>df.write</code> but it is under <code>spark.read</code>.</p>

<p>I am using Spark 2.1.1. And my spark-shell is invoked by:</p>

<pre><code>spark-shell --master spark://10.0.0.115:7077 --packages com.databricks:spark-csv_2.11:1.5.0,datastax:spark-cassandra-connector:1.6.6-s_2.11
</code></pre>

<hr>

<p>Edit:</p>

<p>I did realise that <code>cassandraFormat</code> was basically an alias for <code>.format().options()</code>. However a different error was returned:</p>

<pre><code>df.write
      .format(""org.apache.spark.sql.cassandra"")
      .options(Map(""table"" -&gt; ""standard_feed"", ""keyspace"" -&gt; ""testing""))
      .save()

java.lang.AbstractMethodError: org.apache.spark.sql.cassandra.DefaultSource.createRelation
    (Lorg/apache/spark/sql/SQLContext;Lorg/apache/spark/sql/SaveMode;Lscala/collection/
        immutable/Map;Lorg/apache/spark/sql/Dataset;)Lorg/apache/spark/sql/sources/
        BaseRelation;
at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:518)
at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:215)
</code></pre>
",<scala><apache-spark><apache-spark-sql><cassandra><spark-cassandra-connector>,"<p>I'm not so sure about <code>cassandraFormat</code> but saving a dataframe to a cassandra table can easiest be done in the following way:</p>

<pre><code>df.write
  .format(""org.apache.spark.sql.cassandra"")
  .options(Map(""table"" -&gt; ""table_name"", ""keyspace"" -&gt; ""keyspace_name""))
  .save()
</code></pre>

<p>About the error message you received:</p>

<p>I think the error message you get is due to a version mismatch. You use spark 2.1.1 and cassandra-connector version 1.6.6. For spark 2.1.x you need to use version 2.0 of the cassandra-connector, see table <a href=""https://github.com/datastax/spark-cassandra-connector#version-compatibility"" rel=""nofollow noreferrer"">here</a> for a full list of the version compatibilities.</p>
",['table']
44154551,44163441,2017-05-24 09:36:09,Populating Cassandra database using Python,"<p>I am on Linux platform with Cassandra database. I want to insert Images data into Cassandra database using Python Code from a remote server. Previously, I had written a python code that inserts Images' data into MySQL database from a remote server. Please see the code below for MySQL</p>

<pre><code>#!/usr/bin/python
# -*- coding: utf-8 -*-
import MySQLdb as mdb
import psycopg2
import sys
import MySQLdb
def read_image(i):   
    filename=""/home/faban/Downloads/Python/Python-Mysql/images/im""
    filename=filename+str(i)+"".jpg""
    print(filename)
    fin = open(filename)    
    img = fin.read()   
    return img
con = MySQLdb.connect(""192.168.50.12"",""root"",""faban"",""experiments"" )  
with con:
    print('connecting to database')
    range_from=input('Enter range from:')
    range_till=input('Enter range till:')
    for i in range(range_from,range_till):
     cur = con.cursor()
     data = read_image(i)
     cur.execute(""INSERT INTO images VALUES(%s, %s)"", (i,data, ))
     cur.close()
     con.commit()
con.close()
</code></pre>

<hr>

<p>This code successfully inserts data into MySQL database which is located at .12
I want to modify the same code to insert data into Cassandra database which is also located at .12
Please help me out in this regard. </p>
",<python><mysql><database><cassandra>,"<p>If I create a simple table like this:</p>

<pre><code>CREATE TABLE stackoverflow.images (
    name text PRIMARY KEY,
    data blob);
</code></pre>

<p>I can load those images with Python code that is similar to yours, but with some minor changes to use the DataStax Python Cassandra driver (<code>pip install cassandra-driver</code>):</p>

<pre><code>#imports for DataStax Cassandra driver and sys
from cassandra.cluster import Cluster
from cassandra.auth import PlainTextAuthProvider
from cassandra.cluster import SimpleStatement
import sys

#reading my hostname, username, and password from the command line; defining my Cassandra keyspace as as variable.
hostname=sys.argv[1]
username=sys.argv[2]
password=sys.argv[3]
keyspace=""stackoverflow""

#adding my hostname to an array, setting up auth, and connecting to Cassandra
nodes = []
nodes.append(hostname)
auth_provider = PlainTextAuthProvider(username=username, password=password)
ssl_opts = {}
cluster = Cluster(nodes,auth_provider=auth_provider,ssl_options=ssl_opts)
session = cluster.connect(keyspace)

#setting my image name, loading the file, and reading the data
name = ""IVoidWarranties.jpg""
fileHandle = open(""/home/aploetz/Pictures/"" + name)
imgData = fileHandle.read()

#preparing and executing my INSERT statement
strCQL = ""INSERT INTO images (name,data) VALUES (?,?)""
pStatement = session.prepare(strCQL)
session.execute(pStatement,[name,imgData])

#closing my connection
session.shutdown()
</code></pre>

<p>Hope that helps!</p>
",['table']
44170542,44340008,2017-05-25 00:38:45,Cassandra - how to disable memtable flush,"<p>I'm running Cassandra with a very small dataset so that the data can exist on memtable only. Below are my configurations:</p>

<p>In jvm.options: </p>

<pre><code>-Xms4G
-Xmx4G
</code></pre>

<p>In cassandra.yaml, </p>

<pre><code>memtable_cleanup_threshold: 0.50
memtable_allocation_type: heap_buffers
</code></pre>

<p>As per the documentation in cassandra.yaml, the <em>memtable_heap_space_in_mb</em> and <em>memtable_heap_space_in_mb</em> will be set of 1/4 of heap size i.e. 1000MB</p>

<p>According to the documentation here (<a href=""http://docs.datastax.com/en/cassandra/3.0/cassandra/configuration/configCassandra_yaml.html#configCassandra_yaml__memtable_cleanup_threshold"" rel=""nofollow noreferrer"">http://docs.datastax.com/en/cassandra/3.0/cassandra/configuration/configCassandra_yaml.html#configCassandra_yaml__memtable_cleanup_threshold</a>), the memtable flush will trigger if the total size of memtabl(s) goes beyond (1000+1000)*0.50=1000MB.</p>

<p>Now if I perform several write requests which results in almost ~300MB of the data, memtable still gets flushed since I see sstables being created on file system (Data.db etc.) and I don't understand why.</p>

<p>Could anyone explain this behavior and point out if I'm missing something here?</p>
",<cassandra><nodetool>,"<p>Below is the response I got from Cassandra user group, copying it here in case someone else is looking for the similar info.</p>

<p>After thinking about your scenario I believe your small SSTable size might be due to data compression. By default, all tables enable SSTable compression. </p>

<p>Let go through your scenario. Let's say you have allocated 4GB to your Cassandra node. Your memtable_heap_space_in_mb and 
memtable_offheap_space_in_mb  will roughly come to around 1GB. Since you have memtable_cleanup_threshold to .50 table cleanup will be triggered when total allocated memtable space exceeds 1/2GB. Note the cleanup threshold is .50 of 1GB and not a combination of heap and off heap space. This memtable allocation size is the total amount available for all tables on your node. This includes all system related keyspaces. The cleanup process will write the largest memtable to disk.</p>

<p>For your case, I am assuming that you are on a single node with only one table with insert activity. I do not think the commit log will trigger a flush in this circumstance as by default the commit log has 8192 MB of space unless the commit log is placed on a very small disk. </p>

<p>I am assuming your table on disk is smaller than 500MB because of compression. You can disable compression on your table and see if this helps get the desired size.</p>

<p>I have written up a blog post explaining memtable flushing (<a href=""http://abiasforaction.net/apache-cassandra-memtable-flush/"" rel=""nofollow noreferrer"">http://abiasforaction.net/apache-cassandra-memtable-flush/</a>)</p>

<p>Let me know if you have any other question. </p>

<p>I hope this helps.</p>
","['memtable_cleanup_threshold', 'table']"
44217713,44218675,2017-05-27 13:57:10,Can cassandra be used for likes / view counts,"<p>Would cassandra be usable when you are storing data that contains something similar to likes or view counts?  As I see it the issues would be:</p>

<p>1) Constant updates to rows (tombstones)</p>

<p>2) Using a counter or LWT CAS operations would increase latency and be complicated.  Which is troublesome if updating view counts since that would be the most frequent operation.</p>

<p>Using a counter would be a write to the counter 'table' while a read of 100 records would require 100 reads or a select in on the counter table.</p>

<p>Using LWT would be a read of the row and a compare and set until your update sticks. Reads of 100 records would be as normal</p>
",<cassandra>,"<p><strong>Counter is built for these type of Job.</strong></p>

<blockquote>
  <p>A counter is a special column used to store a number that is changed in increments. For example, you might use a counter column to count the number of times a page is viewed.<br>
  Apache Cassandra™ 2.1 counter column improves the implementation of counters and provides a number of configuration options to tune counters</p>
</blockquote>

<p>You can create a counter table like below : </p>

<pre><code>CREATE TABLE video_counter (
    video_id uuid PRIMARY KEY,
    like_count counter,
    view_count counter
);
</code></pre>

<p>Whenever a like or view request received for a video, increment the count.</p>

<pre><code>//Like
UPDATE video_counter SET like_count = like_count + 1 WHERE video_id = ? ;

//View
UPDATE video_counter SET view_count = view_count + 1 WHERE video_id = ? ;
</code></pre>

<p>Now you can get the like and view count of a video very efficiently </p>

<pre><code>SELECT * FROM video_counter WHERE video_id = ? ;
</code></pre>

<p>Source : <a href=""http://docs.datastax.com/en/cql/3.1/cql/cql_using/use_counter_t.html"" rel=""nofollow noreferrer"">http://docs.datastax.com/en/cql/3.1/cql/cql_using/use_counter_t.html</a></p>
",['table']
44219061,44228403,2017-05-27 16:15:22,How to access SparkContext on executors to save DataFrame to Cassandra?,"<p>How can I use <code>SparkContext</code> (to create <code>SparkSession</code> or Cassandra Sessions) on executors?</p>

<p>If I pass it as a parameter to the <code>foreach</code> or <code>foreachPartition</code>, then it will have a <code>null</code> value. Shall I create a new <code>SparkContext</code> in each executor?</p>

<p>What I'm trying to do is as follows:</p>

<p>Read a dump directory with millions of XML files:</p>

<pre><code>dumpFiles = Directory.listFiles(dumpDirectory)
dumpFilesRDD = sparkContext.parallize(dumpFiles, numOfSlices)
dumpFilesRDD.foreachPartition(dumpFilePath-&gt;parse(dumpFilePath))
</code></pre>

<p>In <code>parse()</code>, every XML file is validated, parsed and inserted into several tables using Spark SQL. Only valid XML files will present objects of same type that can be saved. Portion of the data needs to be replaced by other keys before being inserted into one of the tables.</p>

<p>In order to do that, <code>SparkContext</code> is needed in the function <code>parse</code> to use <code>sparkContext.sql()</code>.</p>
",<java><apache-spark><apache-spark-sql><cassandra><spark-cassandra-connector>,"<p>If I'd rephrase your question, what you want is to:</p>

<ol>
<li>Read a directory with millions of XML files</li>
<li>Parse them</li>
<li>Insert them into a database</li>
</ol>

<p>That's a typical <strong>Extract, Transform and Load</strong> (ETL) process that <em>terribly</em> easy in Spark SQL.</p>

<p>Loading XML files can be done using a separate package <a href=""https://github.com/databricks/spark-xml"" rel=""nofollow noreferrer"">spark-xml</a>:</p>

<blockquote>
  <p><strong>spark-xml</strong> A library for parsing and querying XML data with Apache Spark, for Spark SQL and DataFrames. The structure and test tools are mostly copied from CSV Data Source for Spark.</p>
</blockquote>

<p>You can ""install"" the package using <code>--packages</code> command-line option:</p>

<pre><code>$SPARK_HOME/bin/spark-shell --packages com.databricks:spark-xml_2.11:0.4.1
</code></pre>

<p>Quoting spark-xml's <a href=""https://github.com/databricks/spark-xml#scala-api"" rel=""nofollow noreferrer"">Scala API</a> (with some changes to use <code>SparkSession</code> instead):</p>

<pre><code>// Step 1. Loading XML files
val path = ""the/path/to/millions/files/*.xml""
val spark: SparkSession = ???
val files = spark.read
  .format(""com.databricks.spark.xml"")
  .option(""rowTag"", ""book"")
  .load(path)
</code></pre>

<p>That makes the first requirement almost no-brainer. You've got your million XML files taken care by Spark SQL.</p>

<p>Step 2 is about parsing the lines (from the XML files) and marking rows to be saved to appropriate tables.</p>

<pre><code>// Step 2. Transform them (using parse)
def parse(line: String) = ???
val parseDF = files.map { line =&gt; parse(line) }
</code></pre>

<p>Your <code>parse</code> function could return <em>something</em> (as the main result) and the table that <em>something</em> should be saved to.</p>

<p>With the <em>table markers</em>, you split the <code>parseDF</code> into DataFrames per table.</p>

<pre><code>val table1DF = parseDF.filter($""table"" === ""table1"")
</code></pre>

<p>And so on (per table).</p>

<pre><code>// Step 3. Insert into DB    
table1DF.write.option(...).jdbc(...)
</code></pre>

<p>That's just a sketch of what you may <em>really</em> be after, but that's the general pattern to follow. Decompose your pipeline into digestable chunks and tackle one chunk at a time.</p>
",['table']
44253474,44253630,2017-05-30 04:33:00,How to set TTL on one record rather than on an entire row?,"<p>Is there any way in Cassandra to set TTL for only one record (one column), not for entire row. For ex, there is a table which has temporary users data and in it their is a field OTP which expires in 30 minutes. I'm trying to figure out how to set TTL only on OTP record not on entire user field row.</p>

<p>All the examples I found so far sets TTL on entire row not on any specific record.
I'm using Spring Data for Cassandra.</p>
",<spring><cassandra><spring-data><spring-data-cassandra>,"<p><strong>Insert the field using separate query with TTL</strong></p>

<p>If your table name is <code>users</code> and primary key is <code>userid</code> then insert the <code>OTP</code> field with <code>userid</code> using TTL 30 * 60 = 1800 seconds</p>

<pre><code>INSERT INTO users(userid, OTP) values(?, ?) USING TTL 1800;
</code></pre>
",['table']
44264094,44280459,2017-05-30 13:47:15,Filter and sort numeric values represented as text in Cassandra tables,"<p>Suppose I have a text column in a Cassandra table. This Column has numeric values represented textually, e.g.</p>

<pre><code>|...|...|myTextColumn|...|...|
|...|...|   '1000'   |...|...|
|...|...|    '200'   |...|...|
|...|...|    '35'    |...|...|
</code></pre>

<p>If I use indexes I am able to query by myTextColumn and ask for all values that are smaller than let's say 300:</p>

<pre><code>select * from myTable where myTextColumn&lt;'300' ALLOW FILTERING ; 
</code></pre>

<p>And by doing so I have a three-fold problem:</p>

<ol>
<li>the row such that myTextColumn='1000' is listed in the results</li>
<li>the row such that myTextColumn='35' is not listed in the results</li>
<li>sorted results will show '1000' before '200' and '200' before '35'</li>
</ol>

<p>Note that I understand why this happens, the why is <strong>not</strong> my question. My question is whether it is possible to address this and if so how, without recurring to the following approaches: <strong>i)</strong> change the column type <strong>ii)</strong> add leading zeros so that all values have the same number of digits. </p>

<p>Thank you for your time. Best regards.</p>
",<database><cassandra>,"<p>I was not able to solve this problem using only queries. However I ended up solving it by adding an extra digit to the numeric values inserted in myTextColumn.</p>

<p>All numeric variables are prefixed with their own amount of digits, e.g. one thousand is 41000, two hundred is 3200. Of course, forty-one thousand is 541000 and thirty-two thousand is 43200.  </p>

<p>To ensure that the prefix is always a single digit even for large numbers than we can use Base16 (or 24 or 32 if we want to support larger numbers), e.g. A1234567890.</p>

<p>This ensures that the results are always correctly filtered <strong>and sorted</strong>. </p>

<p>To follow-up the original example, the table becomes:</p>

<pre><code>|...|...|myTextColumn|...|...|
|...|...|   '41000'  |...|...|
|...|...|    '3200'  |...|...|
|...|...|    '235'  |...|...|
</code></pre>

<p>hence if I perform the query:</p>

<pre><code>select * from myTable where myTextColumn&lt;'3300' ALLOW FILTERING ; 
</code></pre>

<p>I will get {235,3200}, which contains the correct result in the correct order.</p>
",['table']
44317744,44317854,2017-06-01 21:58:25,How to define constant partition key in Cassandra,"<p>Have small table I'd like to confine to single partition in Cassandra. I tried various constants as partition key, but Cassandra rejects as invalid. Eg <code>primary key(1, other_column)</code>, <code>primary key(true, other_column)</code>, <code>primary key('1', other_column)</code>. Is there some way to limit a table to a single partition w/o adding a dummy constant column to the table?</p>
",<cassandra>,"<p><strong>You can't define constant value as partition key or any other column</strong>    </p>

<p>Instead define a normal column (i.e partition) as partition key and every time you insert/update or delete use a constant value.</p>

<p>Example : </p>

<p>Let's create a table with a column named <code>partition</code> and make that as partition key.</p>

<p>Now CRUD Operation : </p>

<pre><code>INSERT INTO small_table(partition, other_column) VALUES(1, ?);
SELECT * FROM small_table WHERE partition = 1;
UPDATE small_table SET other_column = ? WHERE partition = 1;
DELETE FROM small_table WHERE partition = 1;
</code></pre>

<p>Note : <em>This is a bad design, your data will not distribute across cluster. all your select, insert, update, delete operation will execute on the host and replica that contains this data</em> </p>
",['table']
44324661,44332218,2017-06-02 08:51:33,Add Column in Apache Cassandra,"<p>How to check in node.js that the column does not exist in Apache Cassandra ?</p>

<p>I need to add a column only if it not exists.</p>

<p>I have read that I must make a select before, but if I select a column that does not exist, it will return an error.</p>
",<node.js><cassandra>,"<p>Note that if you're on Cassandra 3.x and up, you'll want to query from the <code>columns</code> table on the <code>system_schema</code> keyspace:</p>

<pre><code>aploetz@cqlsh:system_schema&gt; SELECT * FROm system_schema.columns
     WHERE keyspace_name='stackoverflow'
       AND table_name='vehicle_information'
       AND column_name='name';

 keyspace_name | table_name          | column_name | clustering_order | column_name_bytes | kind    | position | type
---------------+---------------------+-------------+------------------+-------------------+---------+----------+------
 stackoverflow | vehicle_information |        name |             none |        0x6e616d65 | regular |       -1 | text

(1 rows)
</code></pre>
",['table']
44347876,44349140,2017-06-03 19:37:54,SSTable multiple directories,"<p>I have recently started working on Cassandra everything was well documented and easy to understand so far.</p>

<p>However I am unable to find any answer to the following question:</p>

<p>Why do Cassandra data directory (/var/lib/cassandra/data/ks) have multiple subdirectories for the same SSTable? 
At why what point is the new directory is created?</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>[centos@cs1 2017-06-03--19-46-14 cassandra $] ls -l /var/lib/cassandra/data/ks
total 8
drwxr-xr-x. 3 root root 4096 Jun  3 19:46 events-4f35e2c0482911e79119511599d22fe7
drwxr-xr-x. 3 root root 4096 Jun  3 19:41 events-7a34c34047f411e7aee3b9dc2549db1c

[centos@cs1 2017-06-03--19-46-10 cassandra $] tree
.
├── events-4f35e2c0482911e79119511599d22fe7
│   ├── ks-events-ka-4-CompressionInfo.db
│   ├── ks-events-ka-4-Data.db
│   ├── ks-events-ka-4-Digest.sha1
│   ├── ks-events-ka-4-Filter.db
│   ├── ks-events-ka-4-Index.db
│   ├── ks-events-ka-4-Statistics.db
│   ├── ks-events-ka-4-Summary.db
│   ├── ks-events-ka-4-TOC.txt
│   └── snapshots
└── events-7a34c34047f411e7aee3b9dc2549db1c
    └── snapshots
        └── 1496472654574-device_log
            └── manifest.json

5 directories, 9 files</code></pre>
</div>
</div>
</p>

<p>I noticed that flushing or compacting does not create new directory. It Simply adds/compacts the most recent SSTable directory</p>
",<cassandra><cassandra-2.1>,"<p>When you drop a table, by default Cassandra takes a snapshot to prevent data-loss if it was unintended. In your case, the events-7a34c34047f411e7aee3b9dc2549db1c is the older table and it has only snapshot directory in it. </p>

<p>The Cassandra.yaml parameter responsible for that action is as follows</p>

<blockquote>
  <p>auto_snapshot  (Default: true) Enable or disable whether a snapshot
  is taken of the data before keyspace truncation or dropping of tables.
  To prevent data loss, using the default setting is strongly advised.
  If you set to false, you will lose data on truncation or drop.</p>
</blockquote>

<p>Remember to clean up the older table snapshots in production like environments, otherwise it could easily pile up on the data directory size.</p>
",['table']
44378650,44394326,2017-06-05 22:26:28,Cassandra config change to use hostnames after ip's changed,"<p>Initial installation of Cassandra was done using IP addresses and it has been working for 6+ months.  This past weekend DevOps changed security to not allow IP addresses and also reassigned new IP addresses.  I modified the required files (cassandra.yaml, cassandra-rackdc.properties, etc) to contain hostnames.  The issue is nodetool status gives the error, Failed to connect to '127.0.0.1:7199, and I do have JVM_OPTS=""$JVM_OPTS -Djava.rmi.server.hostname=blah"" in cassandra-env.sh. Any ideas how to proceed to the environment back up?  Should I go through the same files and replace hostnames with the new IP addresses?  Thanks.  </p>
",<cassandra>,"<p>There are two options to provide listen address in Cassandra.yaml. </p>

<ul>
<li>listen_address</li>
<li>listen_interface</li>
</ul>

<p>To be completely agnostic of the IP address or hostname, use the option of listen_interface and comment the listen_address. Here is the Cassandra.yaml change required</p>

<pre><code># Address or interface to bind to and tell other Cassandra nodes to  connect to.
#
#listen_address: xx.xxx.xx.xxx

# Set listen_address OR listen_interface, not both. Interfaces must    correspond
# to a single address, IP aliasing is not supported.
listen_interface: eth0
</code></pre>

<p>To figure out the actual listen_interface, issue the command </p>

<ul>
<li><strong>ifconfig -a</strong></li>
<li>Pick the interface that shows, ""UP BROADCAST RUNNING"". (eth0 in mycase) </li>
</ul>

<p>The output should look like</p>

<pre><code>root@ip-xx-xxx-x-xxx:~# ifconfig -a
docker0   Link encap:Ethernet  HWaddr xx:xx:xx:xx:xx:xx
      inet addr:xxx.xx.x.x  Bcast:0.0.0.0  Mask:255.255.0.0
      UP BROADCAST MULTICAST  MTU:1500  Metric:1
      RX packets:2 errors:0 dropped:0 overruns:0 frame:0
      TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
      collisions:0 txqueuelen:0
      RX bytes:152 (152.0 B)  TX bytes:0 (0.0 B)

eth0      Link encap:Ethernet  HWaddr xx:xx:xx:xx:xx:xx
         inet addr:xx.xxx.xx.xx  Bcast:xx.xxx.xx.xx  Mask:255.255.0.0
         UP BROADCAST RUNNING MULTICAST  MTU:9001  Metric:1
         RX packets:169552382 errors:0 dropped:0 overruns:0 frame:0
         TX packets:185182015 errors:0 dropped:0 overruns:0 carrier:0
         collisions:0 txqueuelen:1000
         RX bytes:88406501352 (88.4 GB)  TX bytes:126516101404 (126.5 GB)

lo        Link encap:Local Loopback
          inet addr:xx.xxx.xx.xx  Mask:255.0.0.0
          UP LOOPBACK RUNNING  MTU:65536  Metric:1
          RX packets:38490371 errors:0 dropped:0 overruns:0 frame:0
          TX packets:38490371 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1
          RX bytes:41155731774 (41.1 GB)  TX bytes:41155731774 (41.1 GB)
</code></pre>

<p>Restart Cassandra and you should be good to go. <strong>Another advantage is that Cassandra.yaml no longer has to be different across your nodes</strong> (assuming all have the same network interface).</p>
","['listen_interface', 'listen_address']"
44403843,44404323,2017-06-07 04:59:22,How Cassandra choose clustering key when we only specify Primary key in a table,"<p>We can specify <code>compound primary key</code> in <code>cassandra</code>. When we specify only one column as Primary key then how Cassandra will generate <code>clustering key</code>.
In cassandra primary key automatically becomes <code>partition key</code>.</p>

<p>Example :- 
CASE 1 - K1: primary key has only one partition key and no cluster key.</p>

<p>CASE 2 - (K1, K2): column K1 is a partition key and column K2 is a cluster key.</p>

<p>CASE 3 - (K1,K2,K3,...): column K1 is a partition key and columns K2, K3 and so on make cluster key. </p>

<p>In Case 1 how cassandra will choose the Clustering key for a given table with only Specifying Primary Key</p>

<p>Thank you </p>
",<cassandra>,"<p>In case no clustering key is provided (CASE 1), then cassandra will not choose any clustering key.</p>
<p><a href=""http://opensourceconnections.com/blog/2013/07/24/understanding-how-cql3-maps-to-cassandras-internal-data-structure/"" rel=""nofollow noreferrer"">How CQL3 maps to cassandra internal data structure</a></p>
<p>If you describe a table with such a primary key, you will not get <code>WITH CLUSTERING ORDER BY</code> option.</p>
",['table']
44453091,44454208,2017-06-09 08:37:20,Apache Cassandra alternative time series model with many columns in one row,"<p>we are sucessfully using the skinny row aproach for storing time series in cassandra (incl. bucketing). Nevertheless I am looking for efficient storage models for us (e.g. less storage consumption...). One use case is to store <strong>every second</strong> per value to a table. </p>

<p>The last approach (wide row with many columns) feels like an complete anit-pattern for me (Not in theory, but in practice). Has somebody experience with that approach and can confirm my feeling about it?</p>

<p><strong>1) <s>Skinny Row</s> Wide row (flexible, filtering on timestamp possible)</strong></p>

<pre><code>CREATE TABLE timeseries (
    id int,
    date date,
    timestamp timestamp,    
    value decimal,
    PRIMARY KEY ((id, date), timestamp)
) WITH CLUSTERING ORDER BY (timestamp DESC)
</code></pre>

<p><strong>2) Blob/JSON with all values of a day (less storage consumption, no filtering on timestamp on node)</strong></p>

<pre><code>CREATE TABLE timeseries(
    id int,
    date date,
    json text, -- [{'secondOfDay': 0, 'value': 12.34}, {...} or BLOB
    PRIMARY KEY ((id, date))
) 
</code></pre>

<p><strong>3) <s>Wide Row</s> Skinny row with many columns</strong></p>

<pre><code>CREATE TABLE timeseries(
    id int,
    date date,
    ""0"" decimal, ""1"" decimal,""2"" decimal, -- ... 86400 decimal values
                   -- each column index is the second of the day
    PRIMARY KEY ((id, date))
) 
</code></pre>

<ul>
<li>Inserts on single columns (e.g. insert only some seconds of a day) seem to put the node massively under load. Maybe the full row is loaded before insert?</li>
<li>but really good storage consumption</li>
<li>If a row is not filled completely the tombstone warning is triggered: ERROR ""Scanned over 100001 tombstones during query 'SELECT * FROM ...'"" --> this is a no go</li>
</ul>
",<cassandra><time-series>,"<p><strong>I recommend you to use the First Data Model.</strong></p>

<p>Your first and third data model are similar in cassandra's internal structure. And
Your understanding on wide row and skinny row in cassandra is wrong. The First data model is wide row and Second and Third data model is skinny row.</p>

<p>First Data Model Internal Structure : </p>

<pre><code>{""key"": ""1:2017-06-09"",
 ""cells"": [[""2017-06-09 15\\:05+0600:"","""",1496999149885944],
           [""2017-06-09 15\\:05+0600:value"",""3"",1496999149885944],
           [""2017-06-09 15\\:05+0600:"","""",1496999146862326],
           [""2017-06-09 15\\:05+0600:value"",""2"",1496999146862326],
           [""2017-06-09 15\\:05+0600:"","""",1496999142150486],
           [""2017-06-09 15\\:05+0600:value"",""1"",1496999142150486]]},
{""key"": ""1:2017-06-10"",
 ""cells"": [[""2017-06-09 15\\:06+0600:"","""",1496999171997567],
           [""2017-06-09 15\\:06+0600:value"",""4"",1496999171997567]]}
</code></pre>

<p>Cassandra store each cell in a partition (<code>id, date</code>) key into a single row and Clustering key(<code>timestamp</code>) value as key of each cell. That's why this model is called wide row.</p>

<p>So you can see that 1st and 3rd data model is similar. <strong>So you don't have to create new column for each entry of the value, if you use first model instead of 3rd model</strong></p>

<p><strong>And don't use the 2nd model, for each insert you have to read the entire value and append the new value and reinsert again. It's a very bad design, an anti-pattern</strong>. And also cassandra recommend a column value to be 1 MB.</p>

<blockquote>
  <p>A single column value may not be larger than 2GB; in practice, ""single digits of MB"" is a more reasonable limit, since there is no streaming or random access of blob values.</p>
</blockquote>

<p>Source : <a href=""https://wiki.apache.org/cassandra/CassandraLimitations"" rel=""noreferrer"">https://wiki.apache.org/cassandra/CassandraLimitations</a>  </p>

<p>If you want to reduce your disk space, you can use the <a href=""http://docs.datastax.com/en/cql/3.3/cql/cql_using/useCompactStorage.html"" rel=""noreferrer"">COMPACT STORAGE</a> option. The below result show that compact storage reduce disk space up to 35% </p>

<p><a href=""https://i.stack.imgur.com/fxhIU.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/fxhIU.png"" alt=""enter image description here""></a></p>

<p>Source : <a href=""http://blog.librato.com/posts/cassandra-compact-storage"" rel=""noreferrer"">http://blog.librato.com/posts/cassandra-compact-storage</a></p>

<p>Note : </p>

<ul>
<li><p>Using the WITH COMPACT STORAGE directive prevents you from defining more than one column that is not part of a compound primary key. A compact table with a primary key that is not compound can have multiple columns that are not part of the primary key.</p></li>
<li><p>A compact table that uses a compound primary key must define at least one clustering column. Columns cannot be added nor removed after creation of a compact table. Unless you specify WITH COMPACT STORAGE, CQL creates a table with non-compact storage.</p></li>
<li><p>Collections and static columns cannot be used with COMPACT STORAGE tables.</p></li>
</ul>

<p>Source : <a href=""http://docs.datastax.com/en/cql/3.3/cql/cql_using/useCompactStorage.html"" rel=""noreferrer"">http://docs.datastax.com/en/cql/3.3/cql/cql_using/useCompactStorage.html</a></p>
",['table']
44501087,44501203,2017-06-12 13:48:48,Checking health of all cassandra nodes,"<p>I am writing tests to check metadata information of cassandra keyspace and tables.
I also want to check which node is up or which is down in a cluster. how can I do it?</p>
",<node.js><cassandra><cassandra-node-driver>,"<p>The nodetool utility gives you access to diagnostic and operational information.</p>

<p><code>nodetool ring</code></p>

<p>will give you a list of the nodes in the ring and their state.</p>

<p>From the node.js driver you can also get the information. The <a href=""http://docs.datastax.com/en/developer/nodejs-driver/3.2/api/class.Client/#hosts"" rel=""noreferrer"">Client</a> has a <code>hosts</code> attribute. Each host has a <a href=""http://docs.datastax.com/en/developer/nodejs-driver/3.2/api/class.Host/#is-up"" rel=""noreferrer"">isUp</a> function you can use. An <a href=""https://github.com/datastax/nodejs-driver/blob/master/examples/metadata/metadata-hosts.js"" rel=""noreferrer"">example</a> shows using the metadata:</p>

<pre><code>""use strict"";
const cassandra = require('cassandra-driver');

const client = new cassandra.Client({ contactPoints: ['127.0.0.1'] });
client.connect()
  .then(function () {
    console.log('Connected to cluster with %d host(s): %j', client.hosts.length);
    client.hosts.forEach(function (host) {
      console.log('Host %s v%s on rack %s, dc %s, isUp: %s', host.address, host.cassandraVersion, host.rack, host.datacenter, host.isUp());
    });
    console.log('Shutting down');
    return client.shutdown();
  })
  .catch(function (err) {
    console.error('There was an error when connecting', err);
    return client.shutdown();
  });
</code></pre>
","['rack', 'dc']"
44539007,44541286,2017-06-14 08:06:53,Cequel:: InvalidSchemaMigration,"<p>I am new to Cequel and got this error <code>Cequel::InvalidSchemaMigration: Type changes are not allowed</code> on <code>rake cequel:migrate</code> for configuring Shape Log table. </p>

<p>My shape_log.rb file is: </p>

<pre><code>class ShapeLog
   include Cequel::Record
   key :shape_id, :bigint
   key :id, :uuid, auto: true
   column :controller, :text
   column :action, :text
   map :change_set, :text, :text
   map :object, :text,:text
   column :remark, :text
   column :updated_by, :text
   timestamps
 end
</code></pre>
",<ruby-on-rails><ruby><cassandra><cql>,"<p>I solved with <code>rake cequel:reset</code> which drops keyspace if exists, then create and migrate. Hence, shape log table was configured.</p>
",['table']
44602971,44603433,2017-06-17 09:33:58,How to re-shuffle data in Cassandra when adding a new node?,"<p>I need to add more nodes to our Cassandra cluster but it is unclear to me how to do this properly based on the doc:</p>

<p><a href=""http://docs.datastax.com/en/cassandra/2.1/cassandra/operations/ops_add_node_to_cluster_t.html"" rel=""nofollow noreferrer"">http://docs.datastax.com/en/cassandra/2.1/cassandra/operations/ops_add_node_to_cluster_t.html</a></p>

<p>How do I know if the cluster is using vnodes? We are using it with num_tokens 256 and we only have 3 nodes. I guess if you have that it means we have vnodes.</p>

<p>Is there an easy way to re-shuffle the data?</p>
",<cassandra>,"<p>Cassandra version 1.2 and higher by default uses vnodes (256 vnodes), which splits a nodes token into multiple sub tokens to make data distribution evenly to all nodes in a balanced manner.</p>

<p>Each vnode will be assigned a token. So you can find how many vnodes assigned in configuration file or using nodetool.</p>

<p>As you have said ""num_tokens"" tells number of vnodes in that Cassandra node.</p>

<p>(or) </p>

<p>Execute nodetool ring command which will list the tokens in your cluster for each node.</p>

<pre><code>nodetool ring
</code></pre>

<p>It is recommended to use vnodes which will balance your cluster. Earlier Cassandra versions lesser than 1.1 does not have vnodes, So we used to generate the tokens and configured in initial_token parameter available in cassandra.yaml file. </p>

<p>Hence in Cassandra versions 1.2 or higher, using of vnodes is enough to balance the cluster, no need for re-shuffle data. </p>
",['initial_token']
44603407,44605468,2017-06-17 10:20:42,Why is System.paxos growing up and how to decrease it?,"<p>I'm a newbie with Cassandra and we've started to use it. Sounds good.</p>

<p>Found that table <code>paxos</code> in <code>System</code> is using more space than our data. We are going to transfer all our data from SQL to Cassandra and this will be around 2 TB. </p>

<p>Will this table grow in the same way and, if yes, how to decrease it or maybe it is safe to truncate it?</p>

<pre><code>System.paxos::::
Table: paxos
                Space used (live): 2144801786
                Space used (total): 2144801786
Exdata.data::::
Table: trnsfr
                Space used (live): 1742847712
                Space used (total): 1742847712
</code></pre>
",<cassandra>,"<p><strong>Cassandra uses <code>system.paxos</code> table for Lightweight transactions (IF clause i.e <code>IF NOT EXISTS</code>)</strong></p>
<blockquote>
<p>Lightweight transactions with linearizable consistency ensure transaction isolation level similar to the serializable level offered by RDBMS’s. They are also known as compare and set transactions</p>
<p>Cassandra implements lightweight transactions by extending the Paxos consensus protocol, which is based on a quorum-based algorithm. Using this protocol, a distributed system can agree on proposed data additions/modifications without the need for a master database or two-phase commit.</p>
</blockquote>
<p><strong>If you use an IF clause in CQL statements, such as INSERT is consider as lightweight transactions.</strong></p>
<p>For example :</p>
<pre><code>INSERT INTO customer_account (customerID, customer_email) 
VALUES ('LauraS', 'lauras@gmail.com')
IF NOT EXISTS;

//or

UPDATE customer_account
SET    customer_email='laurass@gmail.com'
IF     customerID='LauraS'; 
</code></pre>
<p>Both of the above statements are Lightweight transactions. For both statement Cassandra make sure linearizable consistency using Paxos consensus protocol. It uses the system <code>system.paxos</code> table internally.</p>
<p>Note : <strong>Paxos make sure linearizable consistency at the cost of four round trips. That sounds like a high cost—perhaps too high, if you have the rare case of an application that requires every operation to be linearizable. But for most applications, only a very small minority of operations require linearizability, and this is a good tool to add to the strong/eventual consistency They've provided so far.</strong></p>
<p>Source :<br />
<a href=""https://www.datastax.com/dev/blog/lightweight-transactions-in-cassandra-2-0"" rel=""nofollow noreferrer"">https://www.datastax.com/dev/blog/lightweight-transactions-in-cassandra-2-0</a>
<a href=""http://docs.datastax.com/en/cassandra/2.1/cassandra/dml/dml_ltwt_transaction_c.html"" rel=""nofollow noreferrer"">http://docs.datastax.com/en/cassandra/2.1/cassandra/dml/dml_ltwt_transaction_c.html</a></p>
",['table']
44664028,44664640,2017-06-20 22:31:34,Access Cassandra from separate docker container using docker-compose,"<p>I am trying to connect to a cassandra container from a separate container (named main). </p>

<p>This is my docker-compose.yml</p>

<pre><code>version: '3.2' 
services:   
  main:
    build:
      context: .
    image: main-container:latest
    depends_on:
      - cassandra
    links:
      - cassandra
    stdin_open: true
    tty: true

  cassandra:
    build:
      context: .
      dockerfile: Dockerfile-cassandra
    ports:
      - ""9042:9042""
      - ""9160:9160""
    image: ""customer-core-cassandra:latest""
</code></pre>

<p>Once I run this using docker-compose up, I run this command:</p>

<p><code>docker-compose exec main cqlsh cassandra 9042</code></p>

<p>but I get this error:</p>

<p><code>Connection error: ('Unable to connect to any servers', {'172.18.0.2': error(111, ""Tried connecting to [('172.18.0.2', 9042)]. Last error: Connection refused"")})</code></p>
",<docker><cassandra><docker-compose>,"<p>I figured out the answer. Basically, in the cassandra.yaml file it sets the default rpc_address to localhost. If this is the case, Cassandra will only listen for requests on localhost, and will not allow connections from anywhere else. In order to change this, I had to set rpc_address to my ""cassandra"" container so my main container (and any other containers) could access Cassandra using the cassandra container ip address.</p>

<p><code>rpc_address: cassandra</code></p>
",['rpc_address']
44669784,44675164,2017-06-21 07:42:03,Cassandra Materialized view,"<p>Assume this schema</p>

<pre><code>CREATE TABLE t(
        a int,
        b int,
        c int,
        d int,
        e text,
        f date,
        g int,
        PRIMARY KEY (a,b)
)
</code></pre>

<p>I we create following mv</p>

<pre><code>CREATE MATERIALIZED VIEW t_mv as
        select a,b,c,d from t where c is not null and d is not null
         PRIMARY KEY (c,d,a,b);
</code></pre>

<p>What happens if we run this query</p>

<pre><code>UPDATE t SET g=1 WHERE a=10 AND b = 20
</code></pre>

<p>As you can see ""g"" is excluded in ""t_mv"" , I want to know what cassandra doing internaly?</p>

<p>Is there any overhead for t_mv , or cassandra smartly detect there is no changes for t_mv and no-operation</p>

<p>for example If we have 10 materialized view like above-mentioned , is Update that excluded in mv impact performance? or the performance in equal to when there is no mv</p>
",<cassandra>,"<p>Cassandra does not send mutation to materialized view in above condition.</p>

<p>Did a quick demo on local system with your table structure and below is TRACE output.<a href=""https://i.stack.imgur.com/Qmzk9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Qmzk9.png"" alt=""enter image description here""></a> </p>

<p>While updating columns which is present in Materialized view gives below TRACE:
<a href=""https://i.stack.imgur.com/ds4m2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ds4m2.png"" alt=""enter image description here""></a></p>

<p>I hope this answers your question.</p>
",['table']
44715598,44716155,2017-06-23 07:32:05,IllegalArgumentException: Table xyz does not exist in keyspace my_ks,"<p>I am developing an application, where I am trying to create a table if not exists and making a Query on it. It is working fine in normal cases. But for the first time , when the table is created , then when trying to Query the same table, the application is throwing :</p>

<pre><code>IllegalArgumentException: Table xyz does not exist in keyspace my_ks
</code></pre>

<p>Same happens if I drop the table, and when my code recreates the table again.</p>

<p>For other cases, when the table exists, it is working fine. Is it some kind of replication issue, or should use a timeout from some time when the table is created for first time.</p>

<p>Following is the code snippet:</p>

<pre><code>  // Oredr 1: First this will be called
     public boolean isSchemaExists() {
            boolean isSchemaExists = false;
        Statement statement = QueryBuilder
                .select()
                .countAll()
                .from(keyspace_name, table_name);
        statement.setConsistencyLevel(ConsistencyLevel.LOCAL_QUORUM);
        try {
            Session session = cassandraClient.getSession(someSessionKey);
            ResultSet resultSet = null;
            resultSet = session.execute(statement);
            if (resultSet.one() != null) {
                isSchemaExists = true;
            }
        } catch (all exception handling)

        }
        return isSchemaExists;
    }

  // Oredr 2: if previous method returns false then this will be get called 
    public void createSchema(String createTableScript) {
        Session session = cassandraClient.getSession(someSessionKey);
        if (isKeySpaceExists(keyspaceName, session)) {
            session.execute(""USE "" + keyspaceName);
        }
        session.execute(createTableScript);
    }

  //Oredr 3: Now read the table, this is throwing the exception when the table 
  // is created for first time
    public int readTable(){
        Session session = cassandraClient.getSession(someSessionKey);
        MappingManager manager = new MappingManager(session);
        Mapper&lt;MyPojo&gt; mapper = manager.mapper(MyPojo.class);
        Statement statement = QueryBuilder
                .select()
                .from(keyspaceName, tableName)
                .where(eq(""col_1"", someValue)).and(eq(""col_2"", someValue));
        statement.setConsistencyLevel(ConsistencyLevel.LOCAL_QUORUM);
        ResultSet resultSet = session.execute(statement);
        result = mapper.map(resultSet);
        for (MyPojo myPojo : result) {
            return myPojo.getCol1();
        }
    }
</code></pre>
",<cassandra><datastax>,"<p>In <code>isSchemaExists</code> function use <code>system.tables</code>.</p>

<p><code>SELECT * FROM system.tables WHERE keyspace_name='YOUR KEYSPACE' AND table_name='YOUR TABLE'</code></p>

<p>Corresponding Java Code:</p>

<pre><code>Statement statement = QueryBuilder
                .select()
                .from(""system"", ""tables"")
                .where(eq(""keyspace_name"", keyspace)).and(eq(""table_name"", table)); 
</code></pre>

<p>It seems like in <code>isSchemaExists</code> you are using actual table and keyspace which will not exist when dropped or not created. That's the reason it is throwing you error table does not exist.</p>
",['table']
44717819,44724722,2017-06-23 09:26:01,Cassandra slow query execution in an empty table,"<p>I'm using apache-Cassandra-2.2.4 and I have encountered this problem :</p>

<p>When I execute select * from TRACKING ; 
it takes more than 70 sec to get the result Despite the fact that the table is empty <strong>(0 rows)</strong>.</p>

<p>As a first solution I execute truncate TRACKING and the problem is fixed (return instantly the result).</p>

<p>Can you help me to know the root cause? I can't truncate the table in my application.</p>
",<cassandra><cassandra-2.0>,"<p>The likely cause is that you have a bunch of data that was deleted, and you were seeing the effects of reading many tombstones in your full table scan. You could confirm by enabling tracing on the query. Truncating would remove the tombstoned data. </p>

<p>This article explains the tombstone pitfall and root cause:
<a href=""http://www.datastax.com/dev/blog/cassandra-anti-patterns-queues-and-queue-like-datasets"" rel=""nofollow noreferrer"">http://www.datastax.com/dev/blog/cassandra-anti-patterns-queues-and-queue-like-datasets</a></p>
",['table']
44751402,44753690,2017-06-25 23:09:25,Same query doesn't run on restored Cassandra database,"<p>I am working on Cassandra migration.
I built a new Cassandra cluster - Cassandra 2.1.8 on Ubuntu 14.04. Database was restored from snapshots.
Source Cassandra cluster is also version 2.1.8.</p>

<p>I am facing with this weird issue.
On the original cluster I can run following query using cqlsh without any errors. cqlsh is version 5.0.1.
<code>
SELECT * FROM ""featureitems"" WHERE ""categoryId"" = 2 LIMIT 100;
</code></p>

<p>On a new cluster same query throws error:
<code>
InvalidRequest: code=2200 [Invalid query] message=""Undefined name categoryId in where clause ('categoryId = 2')""
</code></p>

<p>but it runs perfectly fine when I remove double quotes
<code>
SELECT * FROM featureitems WHERE categoryId = 2 LIMIT 100;
</code></p>

<p>It looks like some configuration issue, but I don't know where to look. Any suggestion in that sense is appreciated.</p>
",<cassandra><cassandra-2.0><cql3><cqlsh>,"<p>Cassandra converts all column/table/keyspace names to lowercase if not provided in double quotes.</p>

<p>So if you need uppercase character in column/table/keyspace name use double quotes. </p>

<p>You can use <code>DESC TABLE featureitems</code> command to describe table.</p>

<p>In your first query you have enclosed <code>categoryId</code> in double quotes, hence it looks for column with capital I.</p>

<p>In your second query  <code>categoryId</code> is not enclosed in double quotes, hence it will be converted to <code>categoryid</code>... which is present in table and hence working.</p>
",['table']
44792836,44793619,2017-06-28 02:47:50,How to getendpoints of a composite partition key which has blob datatype in it,"<p>I have a select query which is timing out, so i tried querying it using consistency all with tracing enabled, so that read_repair will fix it but that didn't helped much and at consistency all i was getting 3 responses out of 9; so i have decided to identify partition and run a repair on it but when i ran getendpoints on composite partition key which has blob datatype it is throwing an exception ""java.lang.NumberFormatException: Non-hex characters"" i also tried using token from cql select statement which also timing out. How can i identify the partition and repair it ??</p>
",<cassandra><datastax><datastax-enterprise><cassandra-2.1>,"<p>If you just run a repair all partitions will be fixed. To repair an individual partition just read it with <code>CL.ALL</code> and read repairs will fix any differences.</p>

<p>That said.</p>

<p><code>nodetool getendpoints</code> takes a <em>token</em> not a partition key. The murmur3 partitioner expects a long token so a large blob wont work. You can get it with CQL with something like a</p>

<pre><code>select token(k1, k2 ...) from table where ...
</code></pre>

<p>and it will give you the token. Alternatively you can get the token from most of the drivers (java driver: <code>cluster.getMetadata().newToken(string)</code>) or from the Cassandra's java api itself (<code>new Murmur3Partitioner().getToken(bytebuffer)</code>)</p>
","['partitioner', 'table']"
44846569,44851706,2017-06-30 12:32:07,How to design Cassandra Scheme for User Actions Log?,"<p>I have a table like this in MYSQL to log user actions :</p>

<pre><code>CREATE TABLE `actions` (
    `id` INT(11) NOT NULL AUTO_INCREMENT,
    `module` VARCHAR(32) NOT NULL,
    `controller` VARCHAR(64) NOT NULL,
    `action` VARCHAR(64) NOT NULL,
    `date` Timestamp NOT NULL,
    `userid` BIGINT(20) NOT NULL,
    `ip` VARCHAR(32) NOT NULL,
    `duration` DOUBLE NOT NULL,
    PRIMARY KEY (`id`),
)
COLLATE='utf8mb4_general_ci'
ENGINE=MyISAM
AUTO_INCREMENT=1
</code></pre>

<p>I have a MYSQL Query Like this to find out count of specific actions per day :</p>

<pre><code>SELECT COUNT(*) FROM actions WHERE actions.action = ""join"" AND 
YEAR(date)=2017 AND MONTH(date)=06 GROUP BY YEAR(date), MONTH(date), 
DAY(date)
</code></pre>

<p>this takes 50 - 60 second to me to have a list of days with count of ""join"" action with only 5 million rows and index in date and action.</p>

<p>So, I want to log actions using Cassandra, so How can I design Cassandra scheme and How to query to get such request less than 1 second.</p>
",<mysql><cassandra><cql>,"<pre><code>CREATE TABLE actions (
    id timeuuid,
    module varchar,
    controller varchar,
    action varchar,
    date_time timestamp,
    userid bigint,
    ip varchar,
    duration double,
    year int,
    month int,
    dt date,
    PRIMARY KEY ((action,year,month),dt,id)
);
</code></pre>

<p>Explanation:
With abobe table Defination</p>

<p><code>SELECT COUNT(*) FROM actions WHERE actions.action = ""join"" AND yaer=2017 AND month=06 GROUP BY action,year,month,dt</code></p>

<p>will hit single partition.
In <code>dt</code> column only date will be there... may be you can change it to only day number with <code>int</code> as datatype and since <code>id</code> is <code>timeuuid</code>.. it will be unique.</p>

<p><strong>Note:</strong> GROUP BY is supported by cassandra 3.10 and above </p>
",['table']
44861684,44862570,2017-07-01 13:51:14,How do I select all rows for a clustering column in cassandra?,"<p>I have a Partion key:  A</p>

<p>Clustering columns: B, C</p>

<p>I do understand I can query like this</p>

<pre><code>Select * from table where A = ?
Select * from table where A = ? and B = ?
Select * from table where A = ? and B = ? and C = ?
</code></pre>

<p>On certain cases, I want the B value to be any value in that column. </p>

<p>Is there a way I can query like the following?</p>

<pre><code>Select * from table where A = ? and B = 'any value' and C = ?
</code></pre>
",<cassandra><cassandra-3.0>,"<p>Option 1:</p>

<p>In Cassandra, you should design your data model to suit your queries. Therefore the proper way to support your fourth query (queries by A and C, but not necessarily knowing B value), is to create a new table to handle that specific query. This table will be pretty much the same, except the CLUSTERING COLUMNS will be in slightly different order:</p>

<pre><code>PRIMARY KEY (A, C, B)
</code></pre>

<p>Now this query will work:</p>

<pre><code>Select * from table where A = ? and C = ? 
</code></pre>

<p>Option 2:</p>

<p>Alternatively you can create a materialized view, with a different clustering order. Now Cassandra will keep the MV in sync with your table data.</p>

<pre><code>create materialized view mv_acbd as 
select A, B, C, D 
from TABLE1 
where A is not null and B is not null and C is not null  
primary key (A, C, B);
</code></pre>

<p>Now the query against this MV will work like a charm</p>

<pre><code>Select * from mv_acbd where A = ? and C = ? 
</code></pre>

<p>Option 3:</p>

<p>Not the best, but you could use the following query with your table as it is</p>

<pre><code>Select * from table where A = ? and C = ? ALLOW FILTERING
</code></pre>

<p>Relying on ALLOW FILTERING is never a good idea, and is certainly not something that you should do in a production cluster. For this particular case, the scan is within the same partition and performance may vary depending on ratio of how many clustering columns per partition your use case has.</p>
",['table']
44910097,44910169,2017-07-04 15:52:38,Cassandra insertion: InvalidQueryException: Invalid null value in condition for column,"<p>Hi I'm starting working with Cassandra, I'm trying to insert data into Cassandra table using this code in Java in REST server created by DropWizard</p>

<pre><code>    public int insertqueueid(Datafilter data) throws UnknownHostException 
    {
    PreparedStatement prep = defaultSession.prepare(""insert into queueid (queueid, filter, nom, date) values (?,?,?,?)"");
    BoundStatement bound = prep.bind(data.getQueued(),data.getFilter(),getName(), new Date()) ;
    defaultSession.execute(bound);
    return 0 ;
    }
</code></pre>

<p>But it gives me this error :</p>

<blockquote>
  <p>ERROR [2017-07-03 23:45:57,514] io.dropwizard.jersey.errors.LoggingExceptionMapper: Error handling a request: 5d2d169dd71c2520
  ! com.datastax.driver.core.exceptions.InvalidQueryException: Invalid null value in condition for column queueid
  ! at com.datastax.driver.core.Responses$Error.asException(Responses.java:136)
  ! at com.datastax.driver.core.DefaultResultSetFuture.onSet(DefaultResultSetFuture.java:179)
  ! at com.datastax.driver.core.RequestHandler.setFinalResult(RequestHandler.java:177)</p>
</blockquote>

<p>What do you think, it's the problem ?<br>
Sorry for bad language  </p>
",<http><cassandra><dropwizard>,"<p>Based on the error it looks like NULL value is being passed to the queueid column. I assume it's the partition key of the table and you can't pass NULL for that column. </p>

<p>Please check the data and fix it accordingly.</p>
",['table']
44950245,45073242,2017-07-06 13:23:54,Generate a script to create a table from the entity definition,"<p>Is there a way to generate the statement <code>CREATE TABLE</code> from an entity definition? I know it is possible using <a href=""https://github.com/doanduyhai/Achilles"" rel=""noreferrer"">Achilles</a> but I want to use the regular <a href=""http://docs.datastax.com/en/developer/java-driver/3.3/manual/object_mapper/creating/#definition-of-mapped-classes"" rel=""noreferrer"">Cassandra entity</a>.</p>

<p>The target is getting the following script from the entity class below.</p>

<h2>Statement</h2>

<pre><code>CREATE TABLE user (userId uuid PRIMARY KEY, name text);
</code></pre>

<h2>Entity</h2>

<pre><code>@Table(keyspace = ""ks"", name = ""users"",
       readConsistency = ""QUORUM"",
       writeConsistency = ""QUORUM"",
       caseSensitiveKeyspace = false,
       caseSensitiveTable = false)
public static class User {
    @PartitionKey
    private UUID userId;
    private String name;
    // ... constructors / getters / setters
}
</code></pre>
",<cassandra><cql>,"<p>From the <a href=""https://stackoverflow.com/a/45039182/1630604"">answer of Ashraful Islam</a>, I have made a functional version in case someone is interested (@Ashraful Islam please feel free to add it to your answer if you prefer).
I also have added the support to <code>ZonedDateTime</code> following the recommendations of Datastax to use a type <code>tuple&lt;timestamp,varchar&gt;</code> (see their <a href=""http://docs.datastax.com/en/developer/java-driver/3.2/manual/custom_codecs/extras/#jdk-8"" rel=""nofollow noreferrer"">documentation</a>).</p>

<pre><code>import com.datastax.driver.core.*;
import com.datastax.driver.mapping.MappedProperty;
import com.datastax.driver.mapping.MappingConfiguration;
import com.datastax.driver.mapping.annotations.Table;
import com.google.common.collect.ImmutableMap;

import java.net.InetAddress;
import java.nio.ByteBuffer;
import java.time.ZonedDateTime;
import java.util.*;
import java.util.function.Predicate;
import java.util.stream.Collectors;

/**
 * Inspired by Ashraful Islam
 * https://stackoverflow.com/questions/44950245/generate-a-script-to-create-a-table-from-the-entity-definition/45039182#45039182
 */
public class CassandraScriptGeneratorFromEntities {

    private static final Map&lt;Class, DataType&gt; BUILT_IN_CODECS_MAP = ImmutableMap.&lt;Class, DataType&gt;builder()
        .put(Long.class, DataType.bigint())
        .put(Boolean.class, DataType.cboolean())
        .put(Double.class, DataType.cdouble())
        .put(Float.class, DataType.cfloat())
        .put(Integer.class, DataType.cint())
        .put(Short.class, DataType.smallint())
        .put(Byte.class, DataType.tinyint())
        .put(long.class, DataType.bigint())
        .put(boolean.class, DataType.cboolean())
        .put(double.class, DataType.cdouble())
        .put(float.class, DataType.cfloat())
        .put(int.class, DataType.cint())
        .put(short.class, DataType.smallint())
        .put(byte.class, DataType.tinyint())
        .put(ByteBuffer.class, DataType.blob())
        .put(InetAddress.class, DataType.inet())
        .put(String.class, DataType.text())
        .put(Date.class, DataType.timestamp())
        .put(UUID.class, DataType.uuid())
        .put(LocalDate.class, DataType.date())
        .put(Duration.class, DataType.duration())
        .put(ZonedDateTime.class, TupleType.of(ProtocolVersion.NEWEST_SUPPORTED, CodecRegistry.DEFAULT_INSTANCE, DataType.timestamp(), DataType.text()))
        .build();
    private static final Predicate&lt;List&lt;?&gt;&gt; IS_NOT_EMPTY = ((Predicate&lt;List&lt;?&gt;&gt;) List::isEmpty).negate();


    public static StringBuilder convertEntityToSchema(final Class&lt;?&gt; entityClass, final String defaultKeyspace, final long ttl) {
    final Table table = Objects.requireNonNull(entityClass.getAnnotation(Table.class), () -&gt; ""The given entity "" + entityClass + "" is not annotated with @Table"");
    final String keyspace = Optional.of(table.keyspace())
            .filter(((Predicate&lt;String&gt;) String::isEmpty).negate())
            .orElse(defaultKeyspace);
    final String ksName = table.caseSensitiveKeyspace() ? Metadata.quote(keyspace) : keyspace.toLowerCase(Locale.ROOT);
    final String tableName = table.caseSensitiveTable() ? Metadata.quote(table.name()) : table.name().toLowerCase(Locale.ROOT);

    final Set&lt;? extends MappedProperty&lt;?&gt;&gt; properties = MappingConfiguration.builder().build().getPropertyMapper().mapTable(entityClass);

    final List&lt;? extends MappedProperty&lt;?&gt;&gt; partitionKeys = Optional.of(
            properties.stream()
                    .filter(((Predicate&lt;MappedProperty&lt;?&gt;&gt;) MappedProperty::isComputed).negate())
                    .filter(MappedProperty::isPartitionKey)
                    .sorted(Comparator.comparingInt(MappedProperty::getPosition))
                    .collect(Collectors.toList())
    ).filter(IS_NOT_EMPTY).orElseThrow(() -&gt; new IllegalArgumentException(""No Partition Key define in the given entity""));

    final List&lt;MappedProperty&lt;?&gt;&gt; clusteringColumns = properties.stream()
            .filter(((Predicate&lt;MappedProperty&lt;?&gt;&gt;) MappedProperty::isComputed).negate())
            .filter(MappedProperty::isClusteringColumn)
            .sorted(Comparator.comparingInt(MappedProperty::getPosition))
            .collect(Collectors.toList());

    final List&lt;MappedProperty&lt;?&gt;&gt; otherColumns = properties.stream()
            .filter(((Predicate&lt;MappedProperty&lt;?&gt;&gt;) MappedProperty::isComputed).negate())
            .filter(((Predicate&lt;MappedProperty&lt;?&gt;&gt;) MappedProperty::isPartitionKey).negate())
            .filter(((Predicate&lt;MappedProperty&lt;?&gt;&gt;) MappedProperty::isClusteringColumn).negate())
            .sorted(Comparator.comparing(MappedProperty::getPropertyName))
            .collect(Collectors.toList());

    final StringBuilder query = new StringBuilder(""CREATE TABLE IF NOT EXISTS "");

    Optional.of(ksName).filter(((Predicate&lt;String&gt;) String::isEmpty).negate()).ifPresent(ks -&gt; query.append(ks).append('.'));

    query.append(tableName).append(""(\n"").append(toSchema(partitionKeys));

    Optional.of(clusteringColumns).filter(IS_NOT_EMPTY).ifPresent(list -&gt; query.append("",\n"").append(toSchema(list)));
    Optional.of(otherColumns).filter(IS_NOT_EMPTY).ifPresent(list -&gt; query.append("",\n"").append(toSchema(list)));

    query.append(',').append(""\nPRIMARY KEY("");
    query.append('(').append(join(partitionKeys)).append(')');

    Optional.of(clusteringColumns).filter(IS_NOT_EMPTY).ifPresent(list -&gt; query.append("", "").append(join(list)));
    query.append(')').append("") with default_time_to_live = "").append(ttl);

    return query;
}

    private static String toSchema(final List&lt;? extends MappedProperty&lt;?&gt;&gt; list) {
    return list.stream()
            .map(property -&gt; property.getMappedName() + ' ' + BUILT_IN_CODECS_MAP.getOrDefault(property.getPropertyType().getRawType(), DataType.text()))
            .collect(Collectors.joining("",\n""));
  }

private static String join(final List&lt;? extends MappedProperty&lt;?&gt;&gt; list) {
    return list.stream().map(MappedProperty::getMappedName).collect(Collectors.joining("", ""));
  }
</code></pre>
",['table']
44963470,44975729,2017-07-07 05:56:19,Spark - move data from tables to new table with extra column,"<p>So we have a Cassandra project and it requires us to migrate a large number of tables from 3 separate tables into one.</p>

<p>e.g. <code>table_d_abc</code>, <code>table_m_abc</code>, <code>table_w_abc</code> to <code>table_t_abc</code></p>

<p>Essentially data needs to be moved to this new table with an extra column with a value that was in the table's name.
There are 100's of tables like this - so you could imagine the huge job it would be to 'hand-make' a migration script. And naturally I thought SPARK should be able to do the job.</p>

<hr>

<p>e.g.:</p>

<pre><code>var tables = List(""table_*_abc"", ""table_*_def"") // etc
var periods = List('d','w','m')

for (table &lt;- tables) {
  for (period &lt;- periods) {
    var rTable = table.replace('*', period)
    var nTable = table.replace('*', 't')
    try {
      var t = sc.cassandraTable(""data"", rTable)
      var fr = t.first
      var columns = fr.toMap.keys.toArray :+ ""period""
      var data = t.map(_.iterator.toArray :+ period)

      // This line does not work as data is a RDD of Array[Any] and not RDD of tuple[...]
      // How to ???
      data.saveToCassandra(""data"", nTable, SomeColumns(columns.map(ColumnName(_)):_*))
    } //catch {}
  }
}
</code></pre>

<p>versus:</p>

<pre><code>var periods = List('d','w','m')

for (period &lt;- periods) {
  sc.cassandraTable(""data"",""table_"" + period + ""_abc"")
    .map(v =&gt; (v.getString(""a""), v.getInt(""b""), v.getInt(""c""), period))
    .saveToCassandra(""data"", ""table_t_abc"", SomeColumns(""a"",""b"",""c"",""period""))

  // ... 100s of other scripts like this
}
</code></pre>

<hr>

<p>Is what I'm trying to do possible?</p>

<p>Is there a way to programatically save an extra column from an source with unknown number of columns and datatypes?</p>
",<scala><apache-spark><cassandra><tuples><rdd>,"<p>The issue here is the RDD objects must be of a type which has a &quot;RowWriter&quot; defined. This maps the data in the object to C* insertable buffers.</p>
<h3>RDD World</h3>
<p>Using &quot;CassandraRow&quot; objects this is possible. These objects allow for generic contents and can be constructed on the file. They are also the default output so making a new one from an old one should be relatively cheap.</p>
<p><a href=""https://github.com/datastax/spark-cassandra-connector/blob/master/spark-cassandra-connector/src/main/scala/com/datastax/spark/connector/CassandraRow.scala"" rel=""nofollow noreferrer"">https://github.com/datastax/spark-cassandra-connector/blob/master/spark-cassandra-connector/src/main/scala/com/datastax/spark/connector/CassandraRow.scala</a></p>
<p>You would make a single RowMetadata (basically schema info) for each table with the additional column, then populate the row with the values of the input row + the new period variable.</p>
<h3>Dataframe World</h3>
<p>If you wanted to switch to Dataframes this would be easier as you could just use the DataFrame add column before saving.</p>
<pre><code>cassandraDF.withColumn(&quot;period&quot;,lit(&quot;Value based on first row&quot;))
</code></pre>
",['table']
45021003,45024011,2017-07-10 20:26:52,What is the optimal way to model one-to-many relationships in Cassandra?,"<p>Say I want to design a system where users can create posts, where each post belongs to one user but a user may have multiple posts. Also assume I want to support finding all posts given a userID in addition to simply finding a post by postId. I also want to store user-specific account details like the date of account creation.</p>

<p>One way of modeling this would be as follows:</p>

<pre><code>CREATE TABLE user (
   userId int,
   name varchar,
   userDetail1,
   userDetail2,
   ...,
   PRIMARY KEY(userId)
);

CREATE TABLE post (
   postId int,
   postDetail1,
   postDetail2,
   ...,
   userId int,
   PRIMARY KEY(postId)
);
</code></pre>

<p>From what I've read, this is supposedly not optimal as querying for posts made by a specific user becomes memory inefficient. Is this correct? And is the reason that Cassandra would not support indexing the post table on userId? </p>

<p>So would the ideal solution be as follows?</p>

<pre><code>CREATE TABLE user (
   userId int,
   name varchar,
   userDetail1,
   userDetail2,
   ...,
   PRIMARY KEY(userId)
);

CREATE TABLE post (
   postId int,
   postDetail1,
   postDetail2,
   ...,
   userId int,
   PRIMARY KEY(postId)
);

CREATE TABLE user_to_post (
   userId int,
   postId int,
   userDetail1,
   userDetail2,
   ...,
   postDetail1,
   postDetail2,
   ...,
   PRIMARY KEY(userId, postId)
);
</code></pre>

<p>Using a composite key, querying for posts for a specific user is much more efficient. But with this design, would having a table for posts specifically be redundant? Again, in this design I want lookups for posts made by a specific user, and also would like to quickly link to a specific user given a post. I have done a lot of reading but am very confused as how to exactly design a one-to-many relationship in Cassandra. </p>
",<cassandra><one-to-many><composite-key><nosql>,"<p>It depends highly all the requests you are trying to achieve. If I understand correctly, you want to be able to:</p>

<ol>
<li>Get a specific user by its ID</li>
<li>Get the list of posts for an user</li>
</ol>

<p>I will base most of my advice from the excellent page <a href=""http://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling"" rel=""noreferrer"">Basic Rules of Cassandra Data Modeling</a> from DataStax. You have to understand first that there is no definite answer to that question. It highly depend on the queries you are trying to run, and on the tradeoffs you are ready to make. For example: do you expect the number of posts for a specific user to be <em>really</em> high (thousands, or millions)? What is the most frequent query (i.e. the one to model the data around)?</p>

<ul>
<li><p>The first model seems to break the rule 2: minimize the number of partition reads. The partition key for the posts table being the post ID (that I will suppose to be random, such as an UUID), the result will be that posts are spread across the cluster. Consequently, supposing that you have the list of posts for a specific user (which actually requires a very inefficient cluster scan), your request will have to hit every server in the cluster if the number of posts per user is sufficiently large. This is the worst case, and definitely not something you want.</p></li>
<li><p>The second model is inherently better, because every request can be achieved using a single request. You are trading storage for read performance, which is usually a very good thing to do. I may just suggest looking at <a href=""https://www.datastax.com/dev/blog/new-in-cassandra-3-0-materialized-views"" rel=""noreferrer"">Materialized Views</a> (Cassandra 3.0+) which do help a lot in maintaining such a table for you – although doing exactly what you propose with MVs is complicated as you can only provide one table as the view source (i.e. the posts).</p></li>
</ul>

<p>I can also suggest an alternative model, which fixes the design flaw from the first proposal without the data duplication (which is, again, not a problem) the key here is to use for the posts the User ID as partition key, and the Post ID as clustering key. This allows all the post for a specific user to be stored on the same node, therefore providing good performance for requesting the posts from a specific user.</p>

<pre><code>CREATE TABLE user (
   userId int,
   name varchar,
   userDetail1,
   userDetail2,
   ...,
   PRIMARY KEY(userId)
);

CREATE TABLE post (
   userId int,
   postId int,
   postDetail1,
   postDetail2,
   PRIMARY KEY(userId, postId)
);
</code></pre>

<p>The main drawback of this solution is that it complexifies slightly the process of retrieving a single post: you have to pass know the user ID in addition to the post ID. This may not be a problem as both are inherently linked.</p>

<p>Once again, remember that except for very simple cases, an optimal way of doing anything in computer science is very unlikely to exist. It depends what set of metrics you are trying to maximize, the tradeoffs you are ready to make, and more importantly for storage systems, the workload you will be running.</p>
",['table']
45075104,45075611,2017-07-13 08:16:03,follower/following in cassandra,"<p>We are designing a twitter like follower/following in Cassandra, and found something similar
<a href=""https://i.stack.imgur.com/UX5U4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UX5U4.png"" alt=""enter image description here""></a></p>

<p>from here <a href=""https://www.slideshare.net/jaykumarpatel/cassandra-at-ebay-13920376/13-Data_Model_simplified_13"" rel=""nofollow noreferrer"">https://www.slideshare.net/jaykumarpatel/cassandra-at-ebay-13920376/13-Data_Model_simplified_13</a></p>

<p>so I think <strong>ItemLike</strong> is a table?
itemid1=>(userid1, userid2...) is a row in the table?
what do you think is the <code>create table</code> of this ItemLike table?</p>
",<cassandra><cassandra-2.0><cassandra-2.1><cassandra-3.0>,"<p><strong>Yes, <code>ItemLike</code> is a table</strong></p>

<p>Schema of the ItemLike table will be Like : </p>

<pre><code>CREATE TABLE itemlike(
    itemid bigint,
    userid bigint,
    timeuuid timeuuid,
    PRIMARY KEY(itemid, userid)
); 
</code></pre>

<p>The picture of the slide is the internal structure of the above table.</p>

<p><a href=""https://i.stack.imgur.com/UX5U4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UX5U4.png"" alt=""enter image description here""></a></p>

<p>Let's insert some data : </p>

<pre><code> itemid | userid | timeuuid
--------+--------+--------------------------------------
      2 |    100 | f172e3c0-67a6-11e7-8e08-371a840aa4bb
      2 |    103 | eaf31240-67a6-11e7-8e08-371a840aa4bb
      1 |    100 | d92f7e90-67a6-11e7-8e08-371a840aa4bb
</code></pre>

<p>Internally cassandra will store the data like below : </p>

<pre><code>--------------------------------------------------------------------------------------|
|    |             100:timeuuid              |            103:timeuuid                | 
|    +---------------------------------------+----------------------------------------|
|2   | f172e3c0-67a6-11e7-8e08-371a840aa4bb  |  eaf31240-67a6-11e7-8e08-371a840aa4bb  |
--------------------------------------------------------------------------------------|

---------------------------------------------|
|    |             100:timeuuid              |
|    +---------------------------------------|
|1   | d92f7e90-67a6-11e7-8e08-371a840aa4bb  |
---------------------------------------------|
</code></pre>
",['table']
45095208,45098851,2017-07-14 05:21:31,Cassandra - how to do group-by and limit query?,"<p>The table data looks like this. Table has clustering order desc on timestamp, and primary key is (name, timestamp):</p>

<pre><code>name - address - timestamp
John - J_Addr 1 - Jan 01, 2017
John - J_Addr 2 - Feb 05, 2017
Mark - M_Addr 1 - Jan 01, 2017
Mark - M_Addr 2 - Mar 05, 2017
</code></pre>

<p>Is there a way to get the latest address for each name?
In above case, the expected result would be:</p>

<pre><code>name - address - timestamp
John - J_Addr 2 - Feb 05, 2017
Mark - M_Addr 2 - Mar 05, 2017
</code></pre>
",<cassandra><datastax><cql><datastax-java-driver>,"<p><strong>If you are using cassandra version >= 3.6 then you can use <code>PER PARTITION LIMIT</code></strong></p>

<p>Example :</p>

<pre><code>SELECT * FROM table_name PER PARTITION LIMIT 1;
</code></pre>

<p><strong>Else If you are inserting every value of timestamp from the current time then you can just create another table like below :</strong></p>

<pre><code>CREATE TABLE user_address (
    name text PRIMARY KEY,
    address text
);
</code></pre>

<p>Whenever you insert into base table also insert into the maintained table. you could use batch if you want to maintain atomicity between these table.</p>

<p>So every time you insert address for a user, address will be upsert. So you will get latest address </p>

<p><strong>Else you have to scan all the row and group by limit from the client side</strong> </p>
",['table']
45113096,45115089,2017-07-14 23:58:05,Cassandra - Internal data storage when no clustering key is specified,"<p>I'm trying to understand the scenario when no clustering key is specified in a table definition.</p>

<p>If a table has only a partition key and no clustering key, what order the rows under the same partition are stored in? Is it even allowed to have multiple rows under the same partition when no clustering key exists? I tried searching for it online but couldn't get a clear explanation.</p>
",<cassandra><cassandra-3.0>,"<p>I got the below explanation from Cassandra user group so posting it here in case someone else is looking for the same info:</p>

<p>""Note that a table always has a partition key, and that if the table has
no clustering columns, then every partition of that table is only
comprised of a single row (since the primary key uniquely identifies
rows and the primary key is equal to the partition key if there is no
clustering columns).""</p>

<p><a href=""http://cassandra.apache.org/doc/latest/cql/ddl.html#the-partition-key"" rel=""nofollow noreferrer"">http://cassandra.apache.org/doc/latest/cql/ddl.html#the-partition-key</a></p>
",['table']
45139240,45139390,2017-07-17 08:23:04,Python Cassandra floating precision loss,"<p>I'm sending data back and forth Python and Cassandra. I'm using both builtin  <code>float</code> types in my python program and the data type for my Cassandra table. If I send a number <code>955.99</code> from python to Cassandra, in the database it shows <code>955.989999</code>. When I send a query in python to return the value I just sent, it is now <code>955.989990234375</code>.</p>

<p>I understand the issue with precision loss in python, I just wanted to know if there's any built-in mechanisms in Cassandra that could prevent this issue.  </p>
",<python><cassandra><floating-point><precision><cassandra-python-driver>,"<p>Python <code>float</code> is an 64-bit IEEE-754 <strong>double</strong> precision binary floating point number. Use <a href=""http://cassandra.apache.org/doc/latest/cql/types.html"" rel=""noreferrer""><code>double</code></a> in Cassandra for a matching type.</p>
",['precision']
45175855,45199781,2017-07-18 19:51:25,"Cassandra column metadata shows ""DateType"" instead of ""timestamp"" after driver upgrade","<p>I have some Java code that performs introspection on the schema of Cassandra tables.  After upgrading the Cassandra driver dependency, this code is no longer working as expected.  With the old driver version, the type for a <code>timestamp</code> column was returned from <a href=""http://docs.datastax.com/en/drivers/java-dse/1.3/com/datastax/driver/core/ColumnMetadata.html#getType--"" rel=""nofollow noreferrer""><code>ColumnMetadata#getType()</code></a> as  <a href=""http://docs.datastax.com/en/drivers/java-dse/1.3/com/datastax/driver/core/DataType.Name.html#TIMESTAMP"" rel=""nofollow noreferrer""><code>DataType.Name#TIMESTAMP</code></a>.  With the new driver, the same call returns <a href=""http://docs.datastax.com/en/drivers/java-dse/1.3/com/datastax/driver/core/DataType.Name.html#CUSTOM"" rel=""nofollow noreferrer""><code>DataType.Name#CUSTOM</code></a> and <a href=""http://docs.datastax.com/en/drivers/java-dse/1.3/com/datastax/driver/core/DataType.CustomType.html#getCustomTypeClassName--"" rel=""nofollow noreferrer""><code>CustomType#getCustomTypeClassName</code></a> returning <code>org.apache.cassandra.db.marshal.DateType</code>.</p>

<p>The old driver version is <code>com.datastax.cassandra:cassandra-driver-core:2.1.9</code>:</p>

<pre class=""lang-xml prettyprint-override""><code>&lt;dependency&gt;
    &lt;groupId&gt;com.datastax.cassandra&lt;/groupId&gt;
    &lt;artifactId&gt;cassandra-driver-core&lt;/artifactId&gt;
    &lt;version&gt;2.1.9&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<p>The new driver version is <code>com.datastax.cassandra:dse-driver:1.1.2</code>:</p>

<pre class=""lang-xml prettyprint-override""><code>&lt;dependency&gt;
    &lt;groupId&gt;com.datastax.cassandra&lt;/groupId&gt;
    &lt;artifactId&gt;dse-driver&lt;/artifactId&gt;
    &lt;version&gt;1.1.2&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<p>The cluster version is DataStax Enterprise 2.1.11.969:</p>

<pre><code>cqlsh&gt; SELECT release_version FROM system.local;

 release_version
-----------------
      2.1.11.969
</code></pre>

<p>To illustrate the problem, I created a simple console application that prints column metadata for a specified table.  (See below.)  When built with the old driver, the output looks like this:</p>

<pre><code># old driver
mvn -Pcassandra-driver clean package
java -jar target/cassandra-print-column-metadata-cassandra-driver.jar &lt;address&gt; &lt;user&gt; &lt;password&gt; &lt;keyspace&gt; &lt;table&gt;
...
ts timestamp
...
</code></pre>

<p>When built with the new driver, the output looks like this:</p>

<pre><code># new driver
mvn -Pdse-driver clean package
java -jar target/cassandra-print-column-metadata-dse-driver.jar &lt;address&gt; &lt;user&gt; &lt;password&gt; &lt;keyspace&gt; &lt;table&gt;
...
ts 'org.apache.cassandra.db.marshal.DateType'
...
</code></pre>

<p>So far, I have only encountered this problem with <code>timestamp</code> columns.  I have not seen it for any other data types, though my schema does not exhaustively use all of the supported data types.</p>

<p><code>DESCRIBE TABLE</code> shows that the column is <code>timestamp</code>.  <code>system.schema_columns</code> shows that the <code>validator</code> is <code>org.apache.cassandra.db.marshal.DateType</code>.</p>

<pre><code>[cqlsh 3.1.7 | Cassandra 2.1.11.969 | CQL spec 3.0.0 | Thrift protocol 19.39.0]

cqlsh:my_keyspace&gt; DESCRIBE TABLE my_table;

CREATE TABLE my_table (
  prim_addr text,
  ch text,
  received_on timestamp,
  ...
  PRIMARY KEY (prim_addr, ch, received_on)
) WITH
  bloom_filter_fp_chance=0.100000 AND
  caching='{""keys"":""ALL"", ""rows_per_partition"":""NONE""}' AND
  comment='emm_ks' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  read_repair_chance=0.100000 AND
  compaction={'sstable_size_in_mb': '160', 'class': 'LeveledCompactionStrategy'} AND
  compression={'sstable_compression': 'SnappyCompressor'};

cqlsh:system&gt; SELECT * FROM system.schema_columns WHERE keyspace_name = 'my_keyspace' AND columnfamily_name = 'my_table' AND column_name IN ('prim_addr', 'ch', 'received_on');

 keyspace_name | columnfamily_name | column_name | component_index | index_name | index_options | index_type | type           | validator
---------------+-------------------+-------------+-----------------+------------+---------------+------------+----------------+------------------------------------------
     my_keyspace |  my_table |          ch |               0 |       null |          null |       null | clustering_key | org.apache.cassandra.db.marshal.UTF8Type
     my_keyspace |  my_table |   prim_addr |            null |       null |          null |       null |  partition_key | org.apache.cassandra.db.marshal.UTF8Type
     my_keyspace |  my_table | received_on |               1 |       null |          null |       null | clustering_key | org.apache.cassandra.db.marshal.DateType
</code></pre>

<p>Is this a bug in the driver, an intentional change in behavior, or some kind of misconfiguration on my part?</p>

<h2>pom.xml</h2>

<pre class=""lang-xml prettyprint-override""><code>&lt;?xml version=""1.0"" encoding=""UTF-8"" standalone=""no""?&gt;
&lt;project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd""&gt;
    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
    &lt;groupId&gt;cnauroth&lt;/groupId&gt;
    &lt;artifactId&gt;cassandra-print-column-metadata&lt;/artifactId&gt;
    &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;
    &lt;description&gt;Console application that prints Cassandra table column metadata&lt;/description&gt;
    &lt;name&gt;cassandra-print-column-metadata&lt;/name&gt;
    &lt;packaging&gt;jar&lt;/packaging&gt;

    &lt;properties&gt;
        &lt;maven.compiler.source&gt;1.7&lt;/maven.compiler.source&gt;
        &lt;maven.compiler.target&gt;1.7&lt;/maven.compiler.target&gt;
        &lt;slf4j.version&gt;1.7.25&lt;/slf4j.version&gt;
    &lt;/properties&gt; 

    &lt;build&gt;
        &lt;plugins&gt;
            &lt;plugin&gt;
                &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;
                &lt;configuration&gt;
                    &lt;archive&gt;
                        &lt;manifest&gt;
                            &lt;addDefaultImplementationEntries&gt;true&lt;/addDefaultImplementationEntries&gt;
                            &lt;mainClass&gt;cnauroth.Main&lt;/mainClass&gt;
                        &lt;/manifest&gt;
                    &lt;/archive&gt;
                    &lt;descriptorRefs&gt;
                        &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;
                    &lt;/descriptorRefs&gt;
                    &lt;finalName&gt;${project.artifactId}&lt;/finalName&gt;
                    &lt;appendAssemblyId&gt;false&lt;/appendAssemblyId&gt;
                &lt;/configuration&gt;
                &lt;executions&gt;
                    &lt;execution&gt;
                        &lt;id&gt;make-assembly&lt;/id&gt;
                        &lt;phase&gt;package&lt;/phase&gt;
                        &lt;goals&gt;
                            &lt;goal&gt;single&lt;/goal&gt;
                        &lt;/goals&gt;
                    &lt;/execution&gt;
                &lt;/executions&gt;
            &lt;/plugin&gt;
        &lt;/plugins&gt;
    &lt;/build&gt;

    &lt;profiles&gt;
        &lt;profile&gt;
            &lt;id&gt;dse-driver&lt;/id&gt;
            &lt;activation&gt;
                &lt;activeByDefault&gt;true&lt;/activeByDefault&gt;
            &lt;/activation&gt;
            &lt;dependencies&gt;
                &lt;dependency&gt;
                    &lt;groupId&gt;com.datastax.cassandra&lt;/groupId&gt;
                    &lt;artifactId&gt;dse-driver&lt;/artifactId&gt;
                    &lt;version&gt;1.1.2&lt;/version&gt;
                &lt;/dependency&gt;
            &lt;/dependencies&gt;
            &lt;build&gt;
                &lt;plugins&gt;
                    &lt;plugin&gt;
                        &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;
                        &lt;configuration&gt;
                            &lt;finalName&gt;${project.artifactId}-dse-driver&lt;/finalName&gt;
                        &lt;/configuration&gt;
                    &lt;/plugin&gt;
                &lt;/plugins&gt;
            &lt;/build&gt;
        &lt;/profile&gt;
        &lt;profile&gt;
            &lt;id&gt;cassandra-driver&lt;/id&gt;
            &lt;activation&gt;
                &lt;activeByDefault&gt;false&lt;/activeByDefault&gt;
            &lt;/activation&gt;
            &lt;dependencies&gt;
                &lt;dependency&gt;
                    &lt;groupId&gt;com.datastax.cassandra&lt;/groupId&gt;
                    &lt;artifactId&gt;cassandra-driver-core&lt;/artifactId&gt;
                    &lt;version&gt;2.1.9&lt;/version&gt;
                &lt;/dependency&gt;
            &lt;/dependencies&gt;
            &lt;build&gt;
                &lt;plugins&gt;
                    &lt;plugin&gt;
                        &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;
                        &lt;configuration&gt;
                            &lt;finalName&gt;${project.artifactId}-cassandra-driver&lt;/finalName&gt;
                        &lt;/configuration&gt;
                    &lt;/plugin&gt;
                &lt;/plugins&gt;
            &lt;/build&gt;
        &lt;/profile&gt;
    &lt;/profiles&gt;

    &lt;dependencies&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.slf4j&lt;/groupId&gt;
            &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt;
            &lt;version&gt;${slf4j.version}&lt;/version&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.slf4j&lt;/groupId&gt;
            &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;
            &lt;version&gt;${slf4j.version}&lt;/version&gt;
        &lt;/dependency&gt;
    &lt;/dependencies&gt;
&lt;/project&gt;
</code></pre>

<h2>Main.java</h2>

<pre class=""lang-java prettyprint-override""><code>package cnauroth;

import java.util.List;

import com.datastax.driver.core.Cluster;
import com.datastax.driver.core.ColumnMetadata;
import com.datastax.driver.core.Session;

class Main {

    public static void main(String[] args) throws Exception {
        // Skipping validation for brevity
        String address = args[0];
        String user = args[1];
        String password = args[2];
        String keyspace = args[3];
        String table = args[4];

        try (Cluster cluster = new Cluster.Builder()
                .addContactPoints(address)
                .withCredentials(user, password)
                .build()) {
            List&lt;ColumnMetadata&gt; columns =
                    cluster.getMetadata().getKeyspace(keyspace).getTable(table).getColumns();
            for (ColumnMetadata column : columns) {
                System.out.println(column);
            }
        }
    }
}
</code></pre>
",<cassandra><datastax><datastax-java-driver>,"<p>It looks like the internal Cassandra type used for Timestamp changed from <code>org.apache.cassandra.db.marshal.DateType</code> and <code>org.apache.cassandra.db.marshal.TimestampType</code> between Cassandra 1.2 and 2.0 (<a href=""https://issues.apache.org/jira/browse/CASSANDRA-5723"" rel=""nofollow noreferrer"">CASSANDRA-5723</a>).  If you created the table with Cassandra 1.2 (or a DSE compatible version) <code>DateType</code> would be used (even if you upgraded your cluster later).</p>

<p>It appears that the 2.1 version of the java driver was able to account for this (<a href=""https://github.com/datastax/java-driver/blob/2.1/driver-core/src/main/java/com/datastax/driver/core/CassandraTypeParser.java#L64-L65"" rel=""nofollow noreferrer"">source</a>) but starting with 3.0 it does not (<a href=""https://github.com/datastax/java-driver/blob/3.3.0/driver-core/src/main/java/com/datastax/driver/core/DataTypeClassNameParser.java#L66"" rel=""nofollow noreferrer"">source</a>).  Instead, it parses it as a Custom type.  </p>

<p>Fortunately, the driver is still able to serialize and deserialize this column as the cql timestamp type is communicated over the protocol in responses, but it's a bug that the driver parses this as the wrong type.   I went ahead and created <a href=""https://datastax-oss.atlassian.net/browse/JAVA-1561"" rel=""nofollow noreferrer"">JAVA-1561</a> to track this.</p>

<p>If you were to migrate your cluster to C* 3.0+ or DSE 5.0+ I suspect the problem goes away as the schema tables reference the cql name instead of the representative Java class name (unless it is indeed a custom type).</p>
",['table']
45209188,45300459,2017-07-20 08:18:27,Cassandra querying multiple partitions on a single node,"<p>We have less than 50GB of data for a table and we are trying to come up with a reasonable design for our Cassandra database. With so little data we are thinking of having all data on each node (2 node cluster with replication factor of 2 to start with).</p>

<p>We want to use Cassandra for easy replication - safeguarding against failover, having copies of data in different parts of the world and Cassandra is brilliant for that.</p>

<p>Moreover, best model that we currently came up with would imply that a single query (consistency level 1-2) would involve getting data from multiple partitions (avg=2, 90th %=20). Most of the queries would ask for data from &lt;= 2 partitions but some might go up to 5k.</p>

<p>So my question here is whether it is really a problem? Is Cassandra slow to retrieve data from multiple partitions if we ensure that all the partitions are on the single node? </p>
",<cassandra><nodes><partitioning><cassandra-3.0>,"<p>I did some testing on my machine and results are contradicting what Ryan Svihla proposed in another answer.</p>

<p><strong>TL;DR</strong> storing same data in multiple partitions and retrieving via IN operator is <strong>much</strong> slower than storing the data in a single partition and retrieving it in one go. PLEASE NOTE, that all of the action is on a single Cassandra node (as the conclusion should be more than obvious for a distributed Cassandra cluster)</p>

<p><strong>Case A</strong>
Insert X rows into a single partition of the table defined below. Retrieve all of them via <code>SELECT</code> specifying the partition key in <code>WHERE</code>.</p>

<p><strong>Case B</strong>
Insert X rows each into a separate partition of the table defined below. Retrieve all of them via <code>SELECT</code> specifying multiple partition keys using <code>WHERE pKey IN (...)</code>.</p>

<p><strong>Table definition</strong></p>

<pre><code>pKey: Text PARTITION KEY
cColumn: Int CLUSTERING KEY
sParam: DateTime STATIC
param: Text (size of each was 500 B in tests)
</code></pre>

<p><strong>Results</strong></p>

<p>Using Phantom Driver</p>

<ol>
<li><code>X = 100
 A - 10ms
 B - 150ms
 r = 15</code></li>
<li><code>X = 1000
 A - 20ms
 B - 1400ms
 r = 70</code></li>
<li><code>X = 10000
 A - 100ms
 B - 14000ms
 r = 140</code></li>
</ol>

<p>Using DevCenter (it has a limit of 1000 rows retrieved in one go)</p>

<ol>
<li><code>X = 100
 A - 20ms
 B - 900ms
 r = 45</code></li>
<li><code>X = 1000
 A - 30ms
 B - 1300ms
 r = 43</code></li>
</ol>

<p><strong>Technical details:</strong></p>

<ul>
<li>Phantom driver v <code>2.13.0</code></li>
<li>Cassandra <code>3.0.9</code></li>
<li>Windows <code>10</code></li>
<li>DevCenter <code>1.6</code></li>
</ul>
",['table']
45210675,45212477,2017-07-20 09:26:36,Filter on the partition and the clustering key with an additional criteria,"<p>I want to filter on a table that has a partition and a clustering key with another criteria on a regular column. I got the following warning.</p>

<blockquote>
  <p>InvalidQueryException: Cannot execute this query as it might involve
  data filtering and thus may have unpredictable performance. If you
  want to execute this query despite the performance unpredictability,
  use ALLOW FILTERING</p>
</blockquote>

<p>I understand the problem if the partition and the clustering key are not used. In my case, is it a relevant error or can I ignore it?</p>

<p>Here is an example of the table and query.</p>

<pre><code>CREATE TABLE mytable(
    name text,
    id uuid,
    deleted boolean
    PRIMARY KEY((name),id)
)
</code></pre>

<h2> </h2>

<pre><code>SELECT id FROM mytable WHERE name='myname' AND id='myid' AND deleted=false;
</code></pre>
",<cassandra><cql>,"<p><strong>In Cassandra you can't filter data with non-primary key column unless you create index in it.</strong></p>

<p><em>Cassandra 3.0 or up it is allowed to filter data with non primary key but in unpredictable performance</em></p>

<p><strong>Cassandra 3.0 or up, If you provide all the primary key (as your given query) then you can use the query with ALLOW FILTERING, ignoring the warning</strong></p>

<p><strong>Otherwise filter from the client side or remove the field <code>deleted</code> and create another table :</strong></p>

<p>Instead of updating the field to <code>deleted</code> <code>true</code> move your data to another table let's say <code>mytable_deleted</code></p>

<pre><code>CREATE TABLE mytable_deleted (
    name text,
    id uuid
    PRIMARY KEY (name, id)
);
</code></pre>

<p>Now if you only have the non deleted data on <code>mytable</code> and deleted data on <code>mytable_deleted</code> table</p>

<p>or </p>

<p><strong>Create index on it :</strong></p>

<p>The column <code>deleted</code> is a low cardinality column. So remember</p>

<blockquote>
  <p>A query on an indexed column in a large cluster typically requires collating responses from multiple data partitions. The query response slows down as more machines are added to the cluster. You can avoid a performance hit when looking for a row in a large partition by narrowing the search.</p>
</blockquote>

<p>Read More : <a href=""http://docs.datastax.com/en/cql/3.1/cql/ddl/ddl_when_use_index_c.html#concept_ds_sgh_yzz_zj__when-no-index"" rel=""nofollow noreferrer"">When not to use an index</a></p>
",['table']
45211965,45240166,2017-07-20 10:20:06,Cassandra for datawarehouse,"<p>Is Cassandra a good alternative for Hadoop as a data warehouse where data is append only and all updates in source databases should not overwrite the existing rows in the data warehouse but get appended. Is Cassandra really ment to act as a data warehouse or just as a database to store the results of batch / stream queries?</p>
",<hadoop><cassandra>,"<p>Cassandra can be used both as a data warehouse(raw data storage) and as a database (for final data storage). It depends more on the cases you want to do with the data. <br>
You even may need to have both Hadoop and Cassandra for different purposes. <br>
Assume, you need to gather and process data from multiple mobile devices and provide some complex aggregation report to the user. 
So at first, you need to save data as fast as possible (as new portions appear very often) so you use Cassandra here. As Cassandra is limited in aggregation features, you load data into HDFS and do some processing via HQL scripts (assume, you're not very good at coding but great in complicated SQLs). And then you move the report results from HDFS to Cassandra in a dedicated reports table partitioned by user id. <br>
So when the user wants to have some aggregation report about his activity in the last month, the application takes the id of active user and returns the aggregated result from Cassandra (as it is simple key-value search).<br>
So for your question, yes, it could be an alternative, but the selection strategy depends on the data types and your application business cases. <br>
You can read more information about usage of Cassandra 
<a href=""http://www.issart.com/blog/can-use-cassandra-big-data-world/"" rel=""nofollow noreferrer"">here</a></p>
",['table']
45249787,45250948,2017-07-22 01:35:08,"com.datastax.driver.core.exceptions.InvalidQueryException: unconfigured table user""","<p>I am new to Cassandra and this has been giving me issues. I downloaded apache-Cassandra 3.11 and I am using spring boot 1.5.4.RELEASE. I did some research and found a <a href=""https://stackoverflow.com/questions/34117374/com-datastax-driver-core-exceptions-invalidqueryexception-unconfigured-table-sc"">source</a> where it says it may be because Spring data is using a different Cassandra driver core version? But the latest uses cql 3 correct? I also made a java class configuration file. The issue may be here.</p>

<pre><code>import org.springframework.cassandra.config.CassandraCqlClusterFactoryBean;
import org.springframework.cassandra.config.DataCenterReplication;
import org.springframework.cassandra.core.keyspace.CreateKeyspaceSpecification;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.data.cassandra.config.SchemaAction;
import org.springframework.data.cassandra.config.java.AbstractCassandraConfiguration;
import org.springframework.data.cassandra.repository.config.EnableCassandraRepositories;

import java.util.ArrayList;
import java.util.List;

@Configuration
@EnableCassandraRepositories
public class CassandraConfig extends AbstractCassandraConfiguration{
    private static final String KEYSPACE = ""my_keyspace"";

    @Bean
    @Override
    public CassandraCqlClusterFactoryBean cluster() {
        CassandraCqlClusterFactoryBean bean = new CassandraCqlClusterFactoryBean();
        bean.setKeyspaceCreations(getKeyspaceCreations());
        return bean;
    }

    /**
     * if it dont exist , create it
     * @return
     */
    @Override
    public SchemaAction getSchemaAction() {
        return SchemaAction.CREATE_IF_NOT_EXISTS;
    }

    @Override
    protected String getKeyspaceName() {
        return KEYSPACE;
    }

    @Override
    public String[] getEntityBasePackages() {
        return new String[]{""com.cassandra""};
    }

    protected List&lt;CreateKeyspaceSpecification&gt; getKeyspaceCreations() {
        List&lt;CreateKeyspaceSpecification&gt; createKeyspaceSpecifications = new ArrayList&lt;&gt;();
        createKeyspaceSpecifications.add(getKeySpaceSpecification());
        return createKeyspaceSpecifications;
    }

    // Below method creates ""my_keyspace"" if it doesnt exist.
    private CreateKeyspaceSpecification getKeySpaceSpecification() {
        CreateKeyspaceSpecification pandaCoopKeyspace = new CreateKeyspaceSpecification();
        DataCenterReplication dcr = new DataCenterReplication(""dc1"", 3L);
        pandaCoopKeyspace.name(KEYSPACE);
        pandaCoopKeyspace.ifNotExists(true).createKeyspace().withNetworkReplication(dcr);
        return pandaCoopKeyspace;
    }


    @Override
    public String getContactPoints() {
        return ""localhost"";
    }
}
</code></pre>

<p>This is the bean am trying to use</p>

<pre><code>import org.springframework.data.cassandra.mapping.PrimaryKey;
import org.springframework.data.cassandra.mapping.Table;

import java.util.UUID;

@Table(""user"")
public class User {

    @PrimaryKey
    private UUID id;

    private String firstName;

    private String lastName;

    public User() {
        super();
    }

    public User(UUID id, String firstName, String lastName) {
        this.id = id;
        this.firstName = firstName;
        this.lastName = lastName;
    }

    public User(String firstName, String lastName) {
        this.firstName = firstName;
        this.lastName = lastName;
    }

    @Override
    public String toString() {
        return ""User{"" +
                ""id="" + id +
                "", firstName='"" + firstName + '\'' +
                "", lastName='"" + lastName + '\'' +
                '}';
    }
}
</code></pre>

<p>If you need more information, let me know, I cannot for the life of me figure out what is wrong. Thank you.</p>
",<java><spring><spring-boot><cassandra><spring-data>,"<p>Create <code>user</code> table into keyspace <code>my_keyspace</code></p>

<pre><code>CREATE TABLE user(
    id uuid PRIMARY KEY,
    firstName text,
    lastName text
);
</code></pre>
",['table']
45295432,45296737,2017-07-25 06:26:20,Cassandra displaying different time zones,"<p>When I start cassandra the time displaying is correct. It is using my machines time. However, while inserting data I'm using a timestamp column. The time stamp is using UTC time zone. I want both times to sync. </p>

<p>When cassandra is started:</p>

<pre><code>INFO  [main] 2017-07-25 11:46:57,933 StorageService.java:2248 - Node localhost/127.0.0.1 state jump to NORMAL
</code></pre>

<p>In column :</p>

<pre><code> seq     | age | city      | dollar | first     | last    | last_modified                   | pick | state     | street          | zip 
 --------+-----+-----------+--------+-----------+---------+--------------------------------+------+-----------+-----------------+--------
 2100005 |  23 | Bangalore | $40000 | Sushmitha | Vegesna | 2017-07-25 06:19:12.950000+0000 | BLUE | Karnataka | 10th cross road | 500049
</code></pre>
",<cassandra><timezone>,"<blockquote>
  <p>When cassandra is started:</p>
  
  <p>INFO  [main] 2017-07-25 11:46:57,933 StorageService.java:2248 - Node
  localhost/127.0.0.1 state jump to NORMAL</p>
</blockquote>

<p>This is logs that casssandra prints on console/logfile... which uses your system datetime.</p>

<p>When inserting data into cassandra, you can manually specify timezone, to use specific timezone.</p>

<pre><code>Insert into table (timestamp_column1) values ('2017-07-25 00:00:00+0530');
</code></pre>

<p>When data is queried using cqlsh, timestamp columns are displayed using timezone value set in .cqlshrc file.. default is UTC timezone.
To change the display timezone change following in .cqlshrc file</p>

<pre><code>[ui]
;; Display timezone
timezone = Etc/UTC
</code></pre>

<p><a href=""http://docs.datastax.com/en/cql/3.1/cql/cql_reference/timestamp_type_r.html"" rel=""nofollow noreferrer"">TimeStamp in cassandra</a></p>
",['table']
45306061,45403269,2017-07-25 14:19:56,Cassandra WriteTimeoutException exception in CounterMutationStage - node dies eventually,"<p>I'm getting the following exception in my cassandra system.log:</p>

<pre><code>WARN  [CounterMutationStage-25] 2017-07-25 13:25:35,874 AbstractLocalAwareExecutorService.java:169 - Uncaught exception on thread Thread[CounterMutationStage-25,5,main]: {}
java.lang.RuntimeException: org.apache.cassandra.exceptions.WriteTimeoutException: Operation timed out - received only 0 responses.
    at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2490) ~[apache-cassandra-3.9.jar:3.9]
    at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) ~[na:1.8.0_112]
    at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) ~[apache-cassandra-3.9.jar:3.9]
    at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:136) [apache-cassandra-3.9.jar:3.9]
    at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.9.jar:3.9]
    at java.lang.Thread.run(Unknown Source) [na:1.8.0_112]
Caused by: org.apache.cassandra.exceptions.WriteTimeoutException: Operation timed out - received only 0 responses.
    at org.apache.cassandra.db.CounterMutation.grabCounterLocks(CounterMutation.java:150) ~[apache-cassandra-3.9.jar:3.9]
    at org.apache.cassandra.db.CounterMutation.applyCounterMutation(CounterMutation.java:122) ~[apache-cassandra-3.9.jar:3.9]
    at org.apache.cassandra.service.StorageProxy$9.runMayThrow(StorageProxy.java:1473) ~[apache-cassandra-3.9.jar:3.9]
    at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2486) ~[apache-cassandra-3.9.jar:3.9]
    ... 5 common frames omitted
</code></pre>

<p>Whenever this happens, CPU goes down to 0% for a minute or so, node becomes unresponsive but recovers after that. 
But eventually, the node will die completely (i.e. the process keeps running, but it will not respond to commands any more, even shutdown does not work, have to kill the process).</p>

<p>Some more information:</p>

<ul>
<li>Cassandra 3.9</li>
<li>G1 garbage collector</li>
<li>Single Node on Windows Server 2012 R2 (20 Cores, 256 GB RAM)</li>
<li>using a lot of counters and counter mutations</li>
</ul>

<p>Things I have tried:</p>

<ul>
<li>eleminated all other warnings from the log. Used to have warnings about counter batches being too large, rewrote code to not use batching at all. This eleminated the warning, but not the exception problem.</li>
<li>migrated to a bigger machine, used bigger heap and fine tuned GC to make sure the problem is not the machine being overstressed. CPU load is &lt; 20%.</li>
</ul>

<p>Does anyone have an idea what else to do? My main concern is the node dying completely. I am not sure that this exception is causing it but it is the only hint I have...</p>

<p><strong>Update 1:</strong></p>

<p>Updated to Cassandra 3.11 and the node does not seem to die any more now. However, write timeouts presists, node is unresponsive for several minutes but at least recovers now.</p>

<p><strong>Update 2:</strong></p>

<p>Solved the problem (with the help of a professional consultant). Disc I/O speed on our node was terrible, leading to a growing queue of flush writers. Reason is unknown, I/O speed tests on the drive (Raid 1 SSDs) were actually super good.
Moving the node from Windows to Linux (and configuring it according to <a href=""http://docs.datastax.com/en/landing_page/doc/landing_page/recommendedSettings.html"" rel=""nofollow noreferrer"">http://docs.datastax.com/en/landing_page/doc/landing_page/recommendedSettings.html</a>) solved the problem.</p>

<p>Real reason for the problem is unknown; might have been Windows per se or just some freak incompatibility with the RAID setup. In any case, Cassandra is only really tested on Linux and it is far easier to find help for Linux setups. Lesson learned.</p>
",<cassandra><cassandra-3.0>,"<p>It sounds like a beefy machine with 20cores and 256GB RAM. Cassandra is a distributed system aimed to scale horizontally. Rather than pushing the load at a single node, try adding more commodity hardware and scale horizontally. Also you can run multiple nodes of Cassandra within the same box. </p>

<p>Atleast try running a couple of nodes within this box to scale from the unresponsiveness. Most often CPU is not the bottleneck for Cassandra. Its the I/O that a single node can perform. </p>

<ul>
<li>Check the values on concurrent_writes in cassandra.yaml, I guess based on the recommendation for 20 cores it would be 160 (20 * 8).</li>
<li>If feasible, try separating the commitlog directory and data directory storage drives.</li>
<li>Best bet to scale writes is to add more boxes (which could be smaller in configuration).</li>
</ul>
",['concurrent_writes']
45356126,45356361,2017-07-27 16:18:22,In Cassandra why create two tables for users to search by username and email instead of adding an index?,"<p>Reading this article: <a href=""https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling"" rel=""nofollow noreferrer"">Basic Rules of Cassandra Data Modeling</a> they say, if you want to be able to query users by both email and username, you should make two tables:</p>

<pre><code>CREATE TABLE users_by_username (
    username text PRIMARY KEY,
    email text,
    age int
)

CREATE TABLE users_by_email (
    email text PRIMARY KEY,
    username text,
    age int
)
</code></pre>

<p>Why would you do this? Doesn't it make the data much less manageable for such a small thing? Why wouldn't you just do one table and have an index?</p>

<pre><code>-- A table holding the user info
CREATE TABLE users (
    username text,
    email text,
    age int,
    PRIMARY KEY((username),email)
);

-- An index that gives good performance on email searching
CREATE INDEX user_email ON users (email);
</code></pre>
",<cassandra><cql><nosql>,"<p><strong>You should make two table because of high cardinality issue in index</strong></p>

<blockquote>
  <p>If you create an index on a high-cardinality column, which has many distinct values, a query between the fields will incur many seeks for very few results. In the table with a billion emails, looking up user by email (a value that is typically unique for each user) is likely to be very inefficient.</p>
</blockquote>

<p><strong>When you execute query with email, cassandra will execute this query on every node, each node will look up it's local index and send the response. Your merge result will be a single user. You are querying on every node to get a single result, it's very inefficient</strong></p>

<p>Instead if you create a separate table for user by email. And execute query, cassandra only need to look up into a single node by the partition key email.</p>

<p>Or If you are using cassandra version 3.0 or higher you could use <a href=""https://www.datastax.com/dev/blog/new-in-cassandra-3-0-materialized-views"" rel=""nofollow noreferrer"">Materialized Views</a> that will maintain your denormalization automatically. </p>

<p>Source : <a href=""http://docs.datastax.com/en/cql/3.1/cql/ddl/ddl_when_use_index_c.html"" rel=""nofollow noreferrer"">http://docs.datastax.com/en/cql/3.1/cql/ddl/ddl_when_use_index_c.html</a></p>
",['table']
45387160,45390269,2017-07-29 08:31:35,Cassandra NoHostsAvailable error in cqlsh,"<p>I am running Cassandra 3.11 on two nodes. This is the keyspace:</p>

<pre><code>CREATE KEYSPACE backend_platform_dev 
WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'}  AND durable_writes = true;
</code></pre>

<p>I have two nodes, running on VMs on my machine. When both nodes are up, everything is working. But when I take down one node, my application (elixir) starts throwing errors, as well as cqlsh (<code>NoHostAvailable</code> or <code>InvalidRequest: Error from server: code=2200 [Invalid query] message=""unconfigured table test""</code>).</p>

<p>I have searched error, and everyone had the problem of setting <code>NetworkTopologyStrategy</code> for a single node, but that's not my case.</p>

<p>What is happening here?</p>

<p>Edit: This is the error the driver is giving:</p>

<blockquote>
  <p>[unavailable] Cannot achieve consistency level ONE: %{alive: 0,
  consistency: :one, required: 1}</p>
</blockquote>

<p>I'm sure one node is alive, one is down. Using <code>cqlsh</code> from my local system to connect to the Cassandra node confirms this. I'm more confused now.</p>
",<cassandra><cqlsh>,"<p>The problem is with ""replication_factor"" being configured as 1. So there is only one copy of data. Say you are storing 10 records, then each node supposedly gets its share of records and for simplicity lets say first 5 records is stored in node1 and second 5 records is stored in node2. Now when you take down node1 and look for the first record, you will be end up with the driver error as reported, as no node is available to serve up that record. </p>

<ul>
<li>Change the replication_factor to 2 minimum since there are only two nodes situation here. Recommended RF=3 for PROD.</li>
<li>Run NODETOOL REPAIR, for the records in that table to become two copies. Any future inserts will automatically be two copies, but this is to fix the existing records.</li>
<li>The consistency level of READ query is by default ONE which will succeed even in case of node failure.</li>
</ul>

<p>SimleStrategy should work fine for Single DataCenter scenario. Still the recommended configuration for PROD is NetworkTopology Strategy.</p>
",['table']
45402176,45403217,2017-07-30 16:36:55,Using default TTL columns but high number of tombstones in Cassandra,"<p>I use Cassandra 3.0.12.</p>

<p>And I have a cassandra Column Family, or CQL table with the following schema:</p>

<pre><code>CREATE TABLE win30 (
    cust_id text,
    tid timeuuid,
    info text,
    PRIMARY KEY (cust_id , tid )
) WITH CLUSTERING ORDER BY (tid DESC) 
and compaction = {'class': 'DateTieredCompactionStrategy', 'max_sstable_age_days': 31 };

alter table win30 with default_time_to_live = '2592000';
</code></pre>

<p>I have set the default_time_to_live property for the entire table, but when I query the table,</p>

<pre><code>select * from win30 order by tid desc limit 9999
</code></pre>

<p>Cassandra WARN that </p>

<pre><code>Read xx live rows and xxxx tombstone for query  xxxxxx (see tombstone_warn_threshold).
</code></pre>

<p>According to this doc <a href=""https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlAboutDeletes.html?hl=default_time_to_live"" rel=""nofollow noreferrer"">How is data deleted</a>, </p>

<blockquote>
  <p>Cassandra allows you to set a default_time_to_live property for an
  entire table. Columns and rows marked with regular TTLs are processed
  as described above; but when a record exceeds the table-level TTL,
  Cassandra deletes it immediately, without tombstoning or compaction.</p>
</blockquote>

<p><strong>""but when a record exceeds the table-level TTL,Cassandra deletes it immediately, without tombstoning or compaction.""</strong></p>

<p><strong>Why Cassandra still WARN for tombstone since I have set a default_time_to_live?</strong></p>

<p>I insert data using some CQL like, without using TTL.</p>

<pre><code>insert into win30 (cust_id, tid, info ) values ('123', now(), 'sometext'); 
</code></pre>

<p><a href=""https://stackoverflow.com/questions/25798435/high-number-of-tombstones-with-ttl-columns-in-cassandra"">a similar question but it does not use default_time_to_live </a></p>

<p><strong>And it seems that I could set the unchecked_tombstone_compaction to true?</strong></p>

<p><strong>Another question, I select data with ordering the same as the CLUSTERING ORDER,
why Cassandra hit so many tombstones?</strong></p>
",<cassandra><cassandra-3.0><cql3>,"<blockquote>
  <p>Why Cassandra still WARN for tombstone since I have set a default_time_to_live?</p>
</blockquote>

<p>The way TTL works in Cassandra is that once the record is expired, its marked as tombstone (the same process of deletion of a record). So instead of manually having a purge job in RDBMS world, Cassandra enables you to cleanup old records based on their TTL. But it still follows through the same process as DELETE and hence the tombstone. Since your TTL value is '2592000' (30days), anything older than 30 days in the table gets expired (marked as tombstone - deleted). </p>

<p>Now the reason for the warning is that your SELECT statement is looking for records that are alive (non-deleted) and the warning message is for how many tombstoned (expired / deleted) records were encountered in the process. So while trying to serve 9999 alive records, the table hit X number of tombstones along the way. </p>

<p>Since the TTL is set at table level, any inserted record to this table will have a default TTL of 30days.</p>

<p>Here is the documentation reference, in case you want to read more. </p>

<blockquote>
  <p>After the number of seconds since the column's creation exceeds the TTL value, TTL data is considered expired and is included in results. Expired data is marked with a tombstone after on the next read on the read path, but it remains for a maximum of gc_grace_seconds.</p>
</blockquote>

<p>Above reference is from this <a href=""http://docs.datastax.com/en/cql/3.1/cql/cql_using/use_expire_c.html"" rel=""nofollow noreferrer"">link</a> </p>

<blockquote>
  <p>And it seems that I could set the unchecked_tombstone_compaction to true?</p>
</blockquote>

<p>Its nothing related to the warning that you are getting. You could think about reducing gc_grace_seconds value (default 10 days) to get rid of tombstones quicker. But there is a reason for this value to be 10days. </p>

<p>Note that DateTieriedCompactionStrategy is depcreated and once you upgrade to 3.11 Apache Cassandra or DSE 5.1.2 there is TimeWindowCompactionStrategy which does a better job with handling tombstones. </p>
",['table']
45531562,45532725,2017-08-06 11:29:15,Cassandra 3 trigger on delete operation,"<p>I'm trying to implement a trigger on Cassandra 3 to catch delete operations on a specific table by implementing </p>

<pre><code> public Collection&lt;Mutation&gt; augment(Partition partition)
</code></pre>

<p>on <code>ITrigger</code> interface but I can't differentiate between update and delete operations. </p>

<p>How can I catch that the operation is a delete operation?</p>
",<triggers><cassandra>,"<p><strong>Here is how you can catch all type of deletion</strong></p>

<pre><code>public Collection&lt;Mutation&gt; augment(Partition partition) {
    if(partition.partitionLevelDeletion().isLive()) {
        UnfilteredRowIterator it = partition.unfilteredIterator();
        while (it.hasNext()) {
            Unfiltered unfiltered = it.next();
            switch (unfiltered.kind()) {
                case ROW:
                    Row row = (Row) unfiltered;

                    if (!row.deletion().isLive()) {
                        // Row deletion
                    }

                    for (Cell cell : row.cell()) {
                        if (cell.isTombstone() {
                            // Cell deletion
                        } else {
                            // Insert or Update
                        }  
                    }
                    break;
                case RANGE_TOMBSTONE_MARKER:
                    // Range Deletion
                    break;
            }
        }
    } else {
        // Partition Level Deletion
    }
}
</code></pre>

<p>Let's say we have the table :</p>

<pre><code>CREATE TABLE kv (
    pk int,
    ck int,
    d int,
    PRIMARY KEY (pk, ck)
);
</code></pre>

<p><strong>Here pk is the partition key and ck is the clustering key</strong></p>

<p>Partition Level Deletion :</p>

<pre><code>DELETE from kv WHERE pk = 1;
</code></pre>

<p>Range Deletion :</p>

<pre><code>DELETE from kv WHERE pk = 1 AND ck &gt; 10;
</code></pre>

<p>Row deletion : </p>

<pre><code> DELETE from kv WHERE pk = 1 AND ck = 10;
</code></pre>

<p>And cell deletion :</p>

<pre><code>DELETE d from kv WHERE pk = 1 AND ck = 10;
</code></pre>
",['table']
45552822,45553358,2017-08-07 17:48:01,How to insert set type into cassandra from a dataframe in spark,"<p>I have a data frame which looks like this -</p>

<pre><code>+-------------+---------------+-----------------+-------------+-------------+
| Address_Type|    Address_Zip|     Address_City|         Name|           ID|
+-------------+---------------+-----------------+-------------+-------------+
|         HOME|         141101|           Nevada|       George|       SO-123|
+-------------+---------------+-----------------+-------------+-------------+
|       OFFICE|         123561|               LA|       George|       SO-123|
+-------------+---------------+-----------------+-------------+-------------+
|         HOME|         141234|         New York|         Jane|       SC-128|
+-------------+---------------+-----------------+-------------+-------------+
|         BILL|         111009|             UTAH|         Jane|       SC-128|
+-------------+---------------+-----------------+-------------+-------------+
</code></pre>

<p>I'm trying to save the data in cassandra where there is a field named Address which is of type Set. Now I want to save the address which is the combination of all field associated with address tag. So that the new Dataframe looks like -</p>

<pre><code>+-------------+-------------+----------------------------------------------------+
|         Name|           ID|                                             Address|
+-------------+-------------+----------------------------------------------------+
|       George|       SO-123|{""Address_Type: ""HOME"", ""Address_City"": ""Nevada"",...|
+-------------+-------------+----------------------------------------------------+
|         Jane|       SC-128|{""Address_Type: ""HOME"", ""Address_City"": ""New York"",.|
+-------------+-------------+----------------------------------------------------+
</code></pre>

<p>and I can easily save it to the cassandra table.</p>

<p>How can I do this?</p>
",<dataframe><cassandra><set>,"<p>All that needs to happen is to match up the DataFrame with the Cassandra Table. So if you are inserting into a Cassandra table with type Set. You just need a dataframe whose schema contains a column of that name of type Array where the internal structure of those rows matches the <code>Address</code> type.</p>

<p>So in your case the dataframe should look like
<code>
| Name | ID | Addresses Array&lt;Address&gt; |
</code>
Which would match a cassandra table
<code>
| Name String, ID String, Addresses Set&lt;Addresses&gt;|
</code></p>

<p>With that matching the command would be
<code>
df.write.format(""org.apache.spark.sql.cassandra"").options(...).save()
</code></p>
",['table']
45561214,45562016,2017-08-08 06:51:58,Cassandra data modeling for range queries using timestamp,"<p>I need to create a table with 4 columns:</p>

<ul>
<li>timestamp BIGINT</li>
<li>name VARCHAR</li>
<li>value VARCHAR</li>
<li>value2 VARCHAR</li>
</ul>

<p>I have 3 required queries:</p>

<pre><code>SELECT *
FROM table
WHERE timestamp &gt; xxx
AND timestamp &lt; xxx;

SELECT *
FROM table
WHERE name = 'xxx';

SELECT *
FROM table
WHERE name = 'xxx'
AND timestamp &gt; xxx
AND timestamp &lt; xxx;
</code></pre>

<p>The result needs to be sorted by timestamp.<br>
When I use:</p>

<pre><code>CREATE TABLE table (
    timestamp BIGINT,
    name VARCHAR,
    value VARCHAR,
    value2 VARCHAR,
    PRIMARY KEY (timestamp)
);
</code></pre>

<p>the result is never sorted.<br>
When I use:</p>

<pre><code>CREATE TABLE table (
    timestamp BIGINT,
    name VARCHAR,
    value VARCHAR,
    value2 VARCHAR,
    PRIMARY KEY (name, timestamp)
);
</code></pre>

<p>the result is sorted by name > timestamp which is wrong.</p>

<pre><code>name | timestamp
------------------------
   a | 20170804142825729
   a | 20170804142655569
   a | 20170804142650546
   a | 20170804142645516
   a | 20170804142640515
   a | 20170804142620454
   b | 20170804143446311
   b | 20170804143431287
   b | 20170804143421277
   b | 20170804142920802
   b | 20170804142910787
</code></pre>

<p>How do I do this using Cassandra?</p>
",<cassandra><data-modeling><cql>,"<p><strong>Cassandra order data by clustering key group by partition key</strong></p>

<p>In your case first table have only partition key <code>timestamp</code>, no clustering key. So data will not be sorted.</p>

<p>And For the second table partition key is <code>name</code> and clustering key is <code>timestamp</code>. So your data will sorted by <code>timestamp</code> group by <code>name</code>. Means data will be first group by it's <code>name</code> then each group will be sorted separately by <code>timestamp</code>. </p>

<p><strong>Edited</strong></p>

<p>So you need to add a partition key like below :</p>

<pre><code>CREATE TABLE table (
    year BIGINT,
    month BIGINT,
    timestamp BIGINT,
    name VARCHAR,
    value VARCHAR,
    value2 VARCHAR,
    PRIMARY KEY ((year, month), timestamp)
);
</code></pre>

<p>here <code>(year, month)</code> is the composite partition key. You have to insert the year and month from the timestamp. So your data will be sorted by <code>timestamp</code> within a year and month </p>
",['table']
45924282,45933945,2017-08-28 17:19:03,Cassandra data model guidance,"<p>I have a question on Cassandra data modeling. Sorry for little long post.</p>

<p>I am taking a hypothetical situation here. Let's say I have a master server which collects data from the machines (1 or many) on network. The data of the other machines is in the form like machine details, status, is connected or not, is up or down, is desktop or laptop, something like this. I have queries like this</p>

<ol>
<li>Given the master server id get the list of machines connected to it</li>
<li>Given the machine id, get the machine details.</li>
<li>Given the status of machine (nothing else) get the list of machines (one or many)</li>
<li>Given the flag is_connected get the list of machines which are connected to master server?</li>
<li>Given the flag is_up get the list of machines?</li>
</ol>

<p>So as per Cassandra, we should create a column family for each query (approximately). My worry is for query #3, #4, #5 above, the where clause for those queries is status, is_connected and is_up respectively, so to satisfy those queries I must create table which has these flags as either partition key or cluster keys. </p>

<pre><code>CREATE TABLE server (
    server_id text,
    server_name text,
    status text,
    .
    .
    .
    .
    .
    other information,
    PRIMARY KEY (server_id))

CREATE TABLE machine (
    machine_id text,
    machine_name text,
    status boolean,
    is_connected boolean,
    is_up boolean,
    .
    .
    .
    .
    .
    other information,
    PRIMARY KEY (machine_id))

  CREATE TABLE machine_by_status (
      machine_id text,
      machine_name text,
      status boolean,
      is_connected boolean,
      is_up boolean,
      .
      .
      .
      .
      .
      other information,
      PRIMARY KEY (status, machine_id))

  CREATE TABLE machine_by_connected (
      machine_id text,
      machine_name text,
      status boolean,
      is_connected boolean,
      is_up boolean,
      .
      .
      .
      .
      .
      other information,
      PRIMARY KEY (is_connected, machine_id))

  CREATE TABLE machine_by_up_down (
      machine_id text,
      machine_name text,
      status boolean,
      is_connected boolean,
      is_up boolean,
      .
      .
      .
      .
      .
      other information,
      PRIMARY KEY (is_up, machine_id))
</code></pre>

<p>But the value of these flags may change over the period of time. The values could be multiple. If these are part of either partition key or cluster keys then I cannot update or change it. So once my column family is created and a record is added with some value of flag then for the new value how can I update that record or if I add new record then how can I remove the old record? I want to avoid read before write.</p>

<p>The frequency of data collection may vary so I cannot use fixed value of TTL so that Cassandra can remove the old value. I understand that above model has problems like it may create hotspots or may create imbalance cluster and that is the reason I need guidance. How can I handle this situation. My client application can query based on these flags only on few pages. Client does not have other data to query.</p>

<p>So how can I create column family to satisfy query #3, #4, #5? Your guidance will help me to come up with good data model in this case.</p>

<p>Thank you in advance.</p>
",<cassandra><data-modeling><cassandra-3.0>,"<p>The flag <code>status</code>, <code>is_connected</code>, <code>is_up</code> all of them are low cardinality partition.</p>

<p>Let's say all of the machine is up, so all your data will be in a single partition (on same node), will create hotspot, will not be scaleable etc.</p>

<p>So instead of making low cardinality column (<code>status</code>, <code>is_connected</code>, <code>is_up</code>) as partition key. Create separate table for each flag value. </p>

<p>Example <code>is_up</code> : </p>

<pre><code>CREATE TABLE up_machines ( 
    machine_id text PRIMARY KEY, 
    machine_name text, 
); 

CREATE TABLE down_machines ( 
    machine_id text PRIMARY KEY, 
    machine_name text, 
); 
</code></pre>

<p>Now if you need the up machine list then you can just select all from the up_machines table. Similarly create table for other flags. </p>

<p>Another thing instead of selecting all record at ones, use driver pagination system</p>

<p><a href=""https://docs.datastax.com/en/developer/java-driver/2.1/manual/paging/"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/developer/java-driver/2.1/manual/paging/</a></p>

<p><em>Note : If a machine status changed, you have to delete from one table and insert into another. Deleting records create tombstone. If this frequently happens huge tombstone can be generated. <a href=""http://thelastpickle.com/blog/2016/07/27/about-deletes-and-tombstones.html"" rel=""nofollow noreferrer"">About Deletes and Tombstones in Cassandra</a></em></p>
",['table']
45937494,45940837,2017-08-29 11:03:35,how to establish a many to many relationship with itself of a table in cassandra,"<p>Actually i am working on app in which a user table can have many to many relationship with itself.</p>

<p>It is an app in which a particular user can sell things to different buyers and can buy things from different sellers.
that is,a particular user can have many buyers and many sellers.</p>

<p>As a user is selling things to some other user.
User becomes seller for that particular user &amp; that particular user become the buyer of that user.</p>

<p>I am very new to cassandra and i do not how relationships work in cassandra.</p>

<p>Could AnyOne tell me what should i do to establish that relationship</p>

<p>My User table is as follows :-</p>

<pre><code>CREATE TABLE IF NOT EXISTS user (
    id text,
    login text,
    password text,
    firstName text,
    lastName text,
    gender text,
    mobileNo text,
    aadhar text,
    email text,
    stateCode text,
    district text,
    city text,
    zipCode int,
    address text,
    PRIMARY KEY(id)
);
</code></pre>
",<database-design><cassandra><data-modeling><database><nosql>,"<p>Cassandra is a NoSQL database and one particular character of it is regarding storing unstructured data. So, you should not think about relations of tables, instead, think of how would you like to get this information (in other words, what would be your ""select * from"" fields). </p>

<p>You should to take a look at <a href=""https://docs.datastax.com/en/cql/3.1/cql/ddl/dataModelingApproach.html"" rel=""nofollow noreferrer"" title=""this modeling tutorial"">this cassandra data modeling introduction</a>;</p>

<p>After that, you will notice 2 things:</p>

<ol>
<li>Primary keys plays a HUGE role on cassandra storage</li>
<li>You can only filter searches (where clause) using primary keys</li>
</ol>

<p>You are looking for is to have one table for each search cql:</p>

<pre><code>create table sells_by_user_id (
user_id text,
buyer_id text,
date timestamp,
item text,
item_description,
..., &lt;this will depend on which info you would like to obtain
primary key ((user_id), date)); //with this table you will obtain all the things that a user sold and to whom

create table buys_by_user_id (
user_id text,
seller_id text,
date timestamp,
item text,
item_description,
..., &lt;this will depend on which info you would like to obtain
primary key ((user_id), date)); //this table will store all things that an user bought 
</code></pre>

<p>As you can see, its a different mindset of traditional RDBMS.</p>

<p>Another example:
create table transactions_by_item_name (
    item_name text,
    seller_id text,
    seller_name text,
    buyer_id text,
    buyer_name text,
    date timestamp,
    item text,
    item_description,
    ..., 

<p>To accomplish atomic transactions on your writes (as you will probably have to write same info in more than one table) is to use <a href=""https://docs.datastax.com/en/cql/3.3/cql/cql_using/useBatch.html"" rel=""nofollow noreferrer"" title=""batch statements"">batch statements</a></p>
",['table']
45984447,46009516,2017-08-31 14:53:49,Use WHERE or FILTER whe creating TempView,"<p>Is it possible to use <code>where</code> or <code>filter</code> when creating a SparkSQL <code>TempView</code> ?</p>

<p>I have a Cassandra table <code>words</code> with </p>

<pre><code>word | count
------------
apples | 20
banana | 10
</code></pre>

<p>I tried</p>

<pre><code>%spark

val df = sqlContext
.read
.format(""org.apache.spark.sql.cassandra"")
.options( Map (""keyspace""-&gt; ""temp"", ""table""-&gt;""words"" ))
.where($""count"" &gt; 10)
.load()
.createOrReplaceTempView(""high_counted"")
</code></pre>

<p>or</p>

<pre><code> %spark

val df = sqlContext
.read
.format(""org.apache.spark.sql.cassandra"")
.options( Map (""keyspace""-&gt; ""temp"", ""table""-&gt;""words"" ))
.where(""count &gt; 10"")
.load()
.createOrReplaceTempView(""high_counted"")
</code></pre>
",<cassandra><apache-spark-sql><apache-zeppelin>,"<p>You cannot do a <code>WHERE</code> or <code>FILTER</code> without <code>.load()</code>ing the table as @undefined_variable suggested.</p>

<p>Try:</p>

<pre><code>%spark

val df = sqlContext
.read
.format(""org.apache.spark.sql.cassandra"")
.options( Map (""keyspace""-&gt; ""temp"", ""table""-&gt;""words"" ))
.load()
.where($""count"" &gt; 10)
.createOrReplaceTempView(""high_counted"")
</code></pre>

<p>Alternatively, you can do a free form query as documented <a href=""https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-operations"" rel=""nofollow noreferrer"">here</a>.</p>

<p>Spark evaluated statements in <a href=""https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-operations"" rel=""nofollow noreferrer"">lazy fashion</a> and the above statement is a Transformation. (If you are thinking we need to filter before we load)</p>
",['table']
46071299,46104031,2017-09-06 09:16:33,Cassandra partition keys organisation,"<p>I am trying to store the following structure in cassandra.</p>

<pre><code>ShopID, UserID , FirstName , LastName etc....
</code></pre>

<p>The most of the queries on it are </p>

<pre><code>select * from table where  ShopID = ? , UserID = ? 
</code></pre>

<p>That's why it is useful to set (<code>ShopID, UserID</code>) as the primary key.</p>

<p>According to docu the default partitioning key by Cassandra is the first column of primary key - for my case it's <code>ShopID</code>, but I want to distribute the data uniformly on Cassandra cluster, I can not allow that all data from one <code>shopID</code> are stored only in one partition, because some of shops have 10M records and some only 1k. </p>

<p>I can setup (<code>ShopID, UserID</code>) as partitioning keys then I can reach the uniform distribution of records in the Cassandra cluster . But after that I can not receive all users that belong to some <code>shopid</code>.</p>

<pre><code>select * 
from table 
where ShopID = ?
</code></pre>

<p>Its obvious that this query demand full scan on the whole cluster but I have no any possibility to do it. And it looks like very hard constraint. </p>

<p>My question is how to reorganize the data to solve both problem (uniform data partitioning, possibility to make full scan queries) in the same time.</p>
",<cassandra>,"<p>In general you need to make user id a clustering column and add some artificial information to your table and partition key during saving. It allows to break a large natural partition to multiple synthetic. But now you need to query all synthetic partitions during reading to combine back natural partition. So the goal is find a reasonable trade-off between number(size) of synthetic partitions and read queries to combine all of them.</p>

<p>Comprehensive description of possible implementations can be found <a href=""https://medium.com/@foundev/synthetic-sharding-in-cassandra-to-deal-with-large-partitions-2124b2fd788b"" rel=""nofollow noreferrer"">here</a> and <a href=""https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling"" rel=""nofollow noreferrer"">here</a>
 (Example 2: User Groups).</p>

<p>Also take a look at <a href=""https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling"" rel=""nofollow noreferrer"">solution</a> (Example 3: User Groups by Join Date) when querying/ordering/grouping is performed by clustering column of date type. It can be useful if you also have similar queries.</p>
",['table']
46166650,46174958,2017-09-12 01:41:58,View Cassandra Partitions using CQLSH,"<p>Using Cassandra, how do i see how many partitions were created base on how i created the primary key? I have been following a tutorial and it mentions to go to <code>bin/cassandra-cli</code> and use the <code>LIST</code> command. However, the latest Cassandra install does not come with this and I have read other articles online that have indicated that cli is now deprecated. </p>

<p>Is there anyway for me to see the partitions that were created using cqlsh? </p>

<p>Thanks in advance!</p>
",<cassandra><cassandra-2.0><cqlsh><cassandra-2.1><cassandra-3.0>,"<p>First of all you have to investigate your <code>cassandra.yaml</code> file to see the <strong>number of tokens</strong> that are currently configured. This tells you how many partitions each node will own:</p>

<pre><code>$ grep num_tokens conf/cassandra.yaml
...
num_tokens: 128
...
$ grep initial_token conf/cassandra.yaml
...
# initial_token: 1
...
</code></pre>

<p>If initial token is commented out, that means that the node will figure out it's own partition ranges during start-up.</p>

<p>Next you can check partition ranges using <code>nodetool ring</code> command:</p>

<pre><code>$ bin/nodetool ring

Datacenter: DC1
==========
Address    Rack        Status State   Load            Owns                Token                                       
                                                                          9167006318991683417                         
127.0.0.2  r1          Down   Normal  ?               ?                   -9178420363247798328                        
127.0.0.2  r1          Down   Normal  ?               ?                   -9127364991967065057                        
127.0.0.3  r1          Down   Normal  ?               ?                   -9063041387589326037 
</code></pre>

<p>This shows you which partition range belongs to which node in the cluster. </p>

<p>In the example above each node owns <strong>128</strong> partition ranges. The range between <strong>-9178420363247798327</strong> and <strong>-9127364991967065057</strong> belongs to the node <strong>127.0.0.2</strong>.</p>

<p>You can use this simple select to tell each row's partition key:</p>

<pre><code>cqlsh:mykeyspace&gt; select token(key), key, added_date, title from mytable;

 system.token(key)    | key       | added_date               | title
----------------------+-----------+--------------------------+----------------------
 -1651127669401031945 |  first    | 2013-10-16 00:00:00+0000 | Hello World
 -1651127669401031945 |  first    | 2013-04-16 00:00:00+0000 | Bye World
   356242581507269238 | second    | 2014-01-29 00:00:00+0000 | Lorem Ipsum
   356242581507269238 | second    | 2013-03-17 00:00:00+0000 | Today tomorrow
   356242581507269238 | second    | 2012-04-03 00:00:00+0000 | It's good to meet you

(5 rows)
</code></pre>

<p>Finding the partition key in partition ranges will tell you where the record is stored.</p>

<p>Also you can use <code>nodetool</code> to do the same in one simple step:</p>

<pre><code>$ bin/nodetool getendpoints mykeyspace mytable 'first'
127.0.0.1
127.0.0.2
</code></pre>

<p>This tells where the records with the partition key 'first' are located.</p>

<p><strong>NOTE:</strong> If some of the nodes are down, <code>getendpoints</code> command won't list those nodes, even though they should store the record based on replication settings.</p>
","['initial_token', 'num_tokens']"
46236274,46241764,2017-09-15 09:30:01,Cassandra: How to query local Table from local node in Java?,"<p>I have a three-node cluster with replicationfactor 2 and I want to receive the table <code>local</code> from keyspace '<code>system</code>'.
That means I want to acess local data of a Cassandra node.
Is that possible?</p>
",<java><cassandra><local><cqlsh>,"<p>Yes.  You can query the <code>system.local</code> table from within Java just like any other.</p>

<pre><code>    session = cluster.connect();
    ResultSet results = getSession()
        .execute(""SELECT key,broadcast_address,cql_version FROM system.local"");

    for (Row row : results) {
        System.out.println(row.getString(""key"") + "" ""
            + row.getInet(""broadcast_address"") + "" ""
            + row.getString(""cql_version""));
    }

local 127.0.0.1 3.4.4
</code></pre>
",['table']
46259258,46260600,2017-09-16 23:10:27,Cassandra - Is it OK to update a table's column which is aslo used for materialized view's PK?,"<p>I wonder if its OK to change a table's value, which is also used in a materialized view's PK?
If its OK, I will appreciate if someone can explain on how it works (insert and delete?)</p>

<p>For example, Having the following tables:</p>

<pre><code>CREATE TABLE users (
    id uuid,
    username text,
    category int,
    created timestamp,
    PRIMARY KEY (username) //Show users ASC
)

CREATE MATERIALIZED VIEW category_username AS
    SELECT username, category
    FROM keyspace.users
    WHERE username IS NOT NULL AND category IS NOT NULL
    PRIMARY KEY (category, username); //Show users by category ASC
</code></pre>

<p>Then I change the user's category to something different then what he have at the moment:</p>

<pre><code>UPDATE keyspace.users
    SET category = 'SomeUniqueInt'
    WHERE username = 'IAmGroot' IF EXISTS;
</code></pre>

<p>Will the category_username be updated accordingly?</p>

<p>It's an evaluation of this <a href=""https://stackoverflow.com/questions/40922020/put-primary-keys-in-cassandra-for-updating-record"">question</a>.</p>
",<indexing><cassandra><updates><data-modeling><cassandra-3.0>,"<p><strong>Similar to Normal table Cassandra will delete the previous record and insert the updated category with username from the materialized view.</strong></p>

<p>In Cassandra delete create tombstone</p>

<ol>
<li>Tombstone take up space and can substantially increase the amount of storage you require.</li>
<li>Querying tables with a large number of tombstones causes performance problems and it causes Latency and heap pressure.</li>
</ol>

<p>So if you frequently update user category, then huge tombstone will be generated.</p>
",['table']
46284029,46292413,2017-09-18 16:20:44,What is the correct way to insert data into a Cassandra UDT?,"<p>Here is the type I have created,</p>

<pre><code>CREATE TYPE urs.dest (
destinations frozen&lt;list&lt;text&gt;&gt;);
</code></pre>

<p>And here is the table ,</p>

<pre><code>CREATE TABLE urs.abc (
id int,
locations map&lt;text, frozen&lt;dest&gt;&gt;,
PRIMARY KEY(id));
</code></pre>

<p>When I try to insert values from cqlsh,</p>

<p>try 1:</p>

<pre><code>insert into urs.abc (id, locations ) values (1, {'coffee': { 'abcd', 'efgh'}});
</code></pre>

<p>try 2:</p>

<pre><code>insert into urs.abc (id, locations ) values (1, {'coffee': ['abcd', 'efgh']});
</code></pre>

<p>try 3:</p>

<pre><code>insert into urs.abc (id) values (1);
update urs.abc set locations = locations + {'coffee': {'abcd','qwer'}} where id=1;
</code></pre>

<p>I'm getting the below error,</p>

<pre><code>Error from server: code=2200 [Invalid query] message=""Invalid map literal for locations: value {'abcd', 'qwer'} is not of type frozen&lt;dest&gt;""
</code></pre>

<p>Can anyone please let me know the correct way to add value to my UDT?</p>
",<cassandra><cql><cassandra-3.0>,"<p>Table creation seems fine</p>

<p>To insert to the table to <code>urs.abc</code> use this </p>

<pre><code>insert into urs.abc (id, locations ) values (1, {'coffee':{ destinations: ['abcd', 'efgh']}});
</code></pre>

<p>You are missing the field name <code>destinations</code>.</p>
",['table']
46302558,46303431,2017-09-19 14:03:00,Cassandra's poorly design of table naming convention,"<p>Let <code>a = ""03bb2997_8b7a_4359_800d_7c14e5175bc9""</code> and I decide to make it a table name of my cassandra. Hence, by using Python, </p>

<pre><code>session.execute(""""""CREATE TABLE IF NOT EXISTS ""%s"" (date date, time time, input text, predicted_result text, PRIMARY KEY(date, time));"""""" % new_modelId)
</code></pre>

<p>Take note of the double quotes between %s, without it, the cql will complain <code>SyntaxException: line 1:35 mismatched character '_' expecting '-'</code> since the table name cannot start with numeric character</p>

<p>The table is created successfully. I verified it through cqlsh. However, when I try to insert data into the table with code below: </p>

<pre><code>session.execute(""""""INSERT INTO ""%s"" (date, time, input, predicted_result) VALUES(%s, %s, %s, %s);"""""",
                        (a, str(dateTime.date()), str(dateTime.time()),
                         json.dumps(json.loads(input_json)[""0""]), json.dumps(json.loads(predicted_result_json)[""0""])))
</code></pre>

<p><code>InvalidRequest: Error from server: code=2200 [Invalid query] message=""unconfigured table '03bb2997_8b7a_4359_800d_7c14e5175bc9'""
</code></p>

<p>I tried with hardcoded table name and it works.</p>

<pre><code>session.execute(""""""INSERT INTO ""03bb2997_8b7a_4359_800d_7c14e5175bc9"" (date, time, input, predicted_result) VALUES(%s, %s, %s, %s);"""""",
                        ( str(dateTime.date()), str(dateTime.time()),
                         json.dumps(json.loads(input_json)[""0""]), json.dumps(json.loads(predicted_result_json)[""0""])))
</code></pre>

<p>I can't figure out what's wrong with Cassandra table naming. It is so confusing and frustrating.</p>
",<python><cassandra>,"<p>You cannot parameterize keyspace or table name, only the parameters on prepared statements. How you execute it here is not a prepared statement, but your arguments to execute have been confused with how you put your parentheses. You are putting a with the first arg as part of a tuple, so I think it would work to:</p>

<pre><code>session.execute(""""""INSERT INTO ""%s"" (date, time, input, predicted_result) VALUES(%s, %s, %s, %s);"""""",
                        a,
                        str(dateTime.date()),
                        str(dateTime.time()),
                        json.dumps(json.loads(input_json)[""0""]),
                        json.dumps(json.loads(predicted_result_json)[""0""])))
</code></pre>

<p>Also, you can always build string yourself as well:</p>

<pre><code>session.execute(""""""INSERT INTO ""%s"" (date, time, input, predicted_result) VALUES('%s', '%s', '%s', '%s');"""""" %
                        (a,
                        str(dateTime.date()),
                        str(dateTime.time()),
                        json.dumps(json.loads(input_json)[""0""]),
                        json.dumps(json.loads(predicted_result_json)[""0""]))))
</code></pre>

<p>Generally its good practice to have hard coded table names for security implications.</p>

<p>As an aside, are you creating tables dynamically? This will eventually cause issues. Cassandra doesn't do well if it has thousands of tables and loading schema gets slower and slower as you make alterations (uses STCS).</p>
",['table']
46340633,46342766,2017-09-21 09:44:14,Cassandra: Is partition key also used in clustering?,"<p>Let's say I have a primary key like this: <code>primary key (PK, CK)</code>. </p>

<p>Based on what I read (see refs), I think I can loosely describe the way Cassandra uses <code>PK</code> and <code>CK</code> as follows - <code>PK</code> will be used to decide which node(s) the data should go to and <code>CK</code> will be used for clustering (aka ordering) of data within that node.</p>

<p>Then, it seems <code>PK</code> is not used in clustering data within the node and that sounds wrong. What if I have a simple primary with with just <code>PK</code>? Will Cassandra only distribute data across nodes and not order data within each node since there is no clustering column?</p>

<p>refs: </p>

<ul>
<li><a href=""https://docs.datastax.com/en/cql/3.1/cql/ddl/ddl_compound_keys_c.html"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/cql/3.1/cql/ddl/ddl_compound_keys_c.html</a></li>
<li><a href=""https://stackoverflow.com/questions/24949676/difference-between-partition-key-composite-key-and-clustering-key-in-cassandra"">Difference between partition key, composite key and clustering key in Cassandra?</a></li>
</ul>
",<cassandra><datastax>,"<blockquote>
  <p>Then, it seems PK is not used in clustering data within the node and
  that sounds wrong. What if I have a simple primary with with just PK?
  Will Cassandra only distribute data across nodes and not order data
  within each node since there is no clustering column?</p>
</blockquote>

<p>Good question.  Let's try this out.  I'll create a simple table and <code>INSERT</code> some data:</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; CREATE TABLE programs 
                             (name text PRIMARY KEY, data text);
aploetz@cqlsh:stackoverflow&gt; INSERT INTO programs (name) VALUES ('Tron');
aploetz@cqlsh:stackoverflow&gt; INSERT INTO programs (name) VALUES ('Yori');
aploetz@cqlsh:stackoverflow&gt; INSERT INTO programs (name) VALUES ('Quorra');
aploetz@cqlsh:stackoverflow&gt; INSERT INTO programs (name) VALUES ('Clu');
aploetz@cqlsh:stackoverflow&gt; INSERT INTO programs (name) VALUES ('Flynn');
aploetz@cqlsh:stackoverflow&gt; INSERT INTO programs (name) VALUES ('Zuze');
</code></pre>

<p>Now, let's run a query that should answer your question:</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; SELECT name, token(name) FROM programs;

 name   | system.token(name)
--------+----------------------
  Flynn | -1059892732813900311
   Zuze |  1815531347795840810
   Yori |  2854211700591734382
 Quorra |  3079126743186967718
   Tron |  6359222509420865788
    Clu |  8304850648940574176

(6 rows)
</code></pre>

<p>As you can see, they are definitely <em>not</em> in order by <code>name</code>, which is the partition key and lone PRIMARY KEY.  But, my query runs the <code>token()</code> function on <code>name</code>, which shows the <em>hashed value of the partition key</em> (<code>name</code> in this case).  The results are ordered by that.</p>

<p>So to answer your question, Cassandra orders its partitions by the hashed value of the partition key.  Note that this order is maintained throughout the cluster, not just on a single node.  Therefore, results for an unbound query (not recommended to be run in a multi-node configuration) will be ordered by the hashed value of the partition key, regardless of the number of nodes in the cluster.</p>
",['table']
46370588,46375005,2017-09-22 17:54:17,Spark CassandraTableScanRDD KeyBy not retaining all columns,"<pre><code>CASSANDRA_TABLE has (some_other_column, itemid) as primary key.

val cassandraRdd: CassandraTableScanRDD[CassandraRow] = sparkSession.sparkContext
  .cassandraTable(cassandraKeyspace, cassandraTable)

cassandraRdd.take(10).foreach(println)
</code></pre>

<p>This cassandraRdd has all columns read from my cassandra table</p>

<pre><code>val temp1: CassandraTableScanRDD[((String), CassandraRow)] = cassandraRdd
  .select(""itemid"", ""column2"", ""column3"")
  .keyBy[(String)](""itemid"")
val temp2: CassandraTableScanRDD[((String), CassandraRow)] = cassandraRdd
  .keyBy[(String)](""itemid"")
temp1.take(10).foreach(println)
temp2.take(10).foreach(println)
</code></pre>

<p>Both temp1 and temp2 are not retaining all columns after that keyBy operation</p>

<pre><code>((988230014),CassandraRow{itemid: 988230014})
</code></pre>

<p>How can I keyBy on certain column and have CassandraRow retain all columns?</p>
",<apache-spark><cassandra><rdd><spark-cassandra-connector>,"<p>To retain partitioner and get selected rows I have to read cassandra rows something like this below</p>

<pre><code>val cassandraRdd: CassandraTableScanRDD[((String, String), (String, String, String))] = {
  sparkSession.sparkContext
    .cassandraTable[(String, String, String)](cassandraKeyspace, cassandraTable)
    .select(""some_other_column"" as ""_1"", ""itemid"" as ""_2"", ""column3"" as ""_3"", ""some_other_column"", ""itemid"")
    .keyBy[(String, String)](""some_other_column"", ""itemid"")
} 
</code></pre>
",['partitioner']
46401568,46438111,2017-09-25 09:13:41,Best practice modeling data for Cassandra databases,"<p>I'm new to Cassandra and am looking for a best practice on how to model data that has this general following structure:</p>

<p>The data is ""user"" based (per customer) , each is supplying a big data file of around 500K-2M entries (periodically updated a few times a day - sometimes full update and sometimes only deltas)</p>

<p>Each data file has certain mandatory data fields (~20 mandatory) but can add additional columns at their discretion (up to ~100). </p>

<p>The <em>additional</em> data fields are <strong>NOT</strong> necessarily the same for the different users (the names of the fields or the types of those fields)</p>

<p>Example (csv format:)</p>

<pre><code>user_id_1.csv

| column1 (unique key per user_id)  |  column2  |  column3 |   ...   |  column10  |  additionalColumn1  |  ...additionalColumn_n |
|-----------------------------------|-----------|----------|---------|------------|---------------------|------------------------|
| user_id_1_key_1                   |  value    |  value   |  value  |  value     |                ...  |  value                 |
| user_id_1_key_2                   |  ....     |  ....    |  ....   |  ....      |                ...  |  ...                   |
| ....                              |  ...      |  ...     |  ...    |  ...       |                ...  |  ...                   |
| user_id_1_key_2Million            |  ....     |  ....    |  ....   |  ....      |                ...  |  ...                   |


user_id_XXX.csv (notice that the first 10 columns are identical to the other users but the additional columns are different - both the names and their types)

|             column1 (unique key per user_id)              |  column2  |  column3 |   ...   |  column10  |  additionalColumn1 (different types than user_id_1 and others)  |  ...additional_column_x |
|-----------------------------------------------------------|-----------|----------|---------|------------|-----------------------------------------------------------------|-------------------------|
| user_id_XXX_key_1                                         |  value    |  value   |  value  |  value     |                                                            ...  |  value                  |
| user_id_XXX_key_2                                         |  ....     |  ....    |  ....   |  ....      |                                                            ...  |  ...                    |
| ....                                                      |  ...      |  ...     |  ...    |  ...       |                                                            ...  |  ...                    |
| user_id_XXX_key_500_thousand (less rows than other user)  |  ....     |  ....    |  ....   |  ....      |                                                            ...  |  ...                    |
</code></pre>

<p>Several options I have considered:</p>

<p><strong>Option 1:</strong></p>

<ol>
<li>Create a ""global"" keyspace </li>
<li>Create a big table ""data"" containing everything</li>
<li><p>Concatenate a user_id column to all of the other columns to the big table (including the non-mandatory columns). The primary key becomes user_id + ""column_1"" (column_1 is unique per user_id)</p>

<pre><code>                                 Keyspace
+--------------------------------------------------------------------------+
|                                                                          |
|                                                                          |
|                                      Data_Table                          |
|                +  +--------+-------+--------------------------+-----+    |
|                |  |        |       |                          |     |    |
|                |  +-------------------------------------------------+    |
|                |  |        |       |                          |     |    |
|    many rows   |  +-------------------------------------------------+    |
|                |  |        |       |                          |     |    |
|                |  |        |       |                          |     |    |
|                |  |        |       |                          |     |    |
|                |  |        |       |     Many columns         |     |    |
|                |  |        |       +------------------------&gt; |     |    |
|                |  |        |       |                          |     |    |
|                |  +-------------------------------------------------+    |
|                v  +-------------------------------------------------+    |
|                                                                          |
+--------------------------------------------------------------------------+
</code></pre></li>
</ol>

<p>A few things that I notice right away:</p>

<ol>
<li>The user_id repeats itself as many times as entries per user</li>
<li>The rows are very sparse for the additional columns (empty null
values) since the users don't necessarily share them</li>
<li>Number of users is relatively small so number of additional columns
is not huge (10K columns max)</li>
<li>I could compact the additional columns data per user to one column called ""meta data"" and share it per all user</li>
</ol>

<hr>

<p><strong>Option 2:</strong></p>

<p>Create Keyspace per User_id</p>

<p>Create table ""data"" per keyspace</p>

<pre><code>+-----------------------------------------------------------------------------------+
| column_1 | column_2 | ... | column_n | additional_column_1 | additional_column_n  |
+-----------------------------------------------------------------------------------+

keyspace_user1         keyspace_user2                     keyspace_user_n
+----------------+    +---------------+                  +---------------+
|                |    |               |                  |               |
|                |    |               |                  |               |
|   +-+-+--+-+   |    |    +-+--+--+  |                  |   +--+--+---+ |
|   | | |  | |   |    |    | |  |  |  |   many keyspaces |   |  |  |   | |
|   | | |  | |   |    |    | |  |  |  | +-------------&gt;  |   |  |  |   | |
|   | | |  | |   |    |    | |  |  |  |                  |   |  |  |   | |
|   | | |  | |   |    |    | |  |  |  |                  |   |  |  |   | |
|   +--------+   |    |    +-------+  |                  |   +---------+ |
+----------------+    +---------------+                  +---------------+
</code></pre>

<p>notes:</p>

<ol>
<li>Many keyspaces (keyspace per user)</li>
<li>Avoids adding ""user_id"" value per each row (I can use the key space name as the user id)</li>
<li>Very few tables per keyspace (in this example only 1 table per keyspace)</li>
</ol>

<hr>

<p><strong>Option 3:</strong></p>

<p>1) Create a global keyspace 
2) Create a table per user_id (the mandatory columns as well as their additional columns per their table)</p>

<pre><code>+---------------------------------------------------------------+
|                            Keyspace                           |
|                                                               |
|       user_1        user_2                         user_n     |
|    +--+---+--+   +--+--+--+                      +--+--+--+   |
|    |  |   |  |   |  |  |  |                      |  |  |  |   |
|    |  |   |  |   |  |  |  |                      |  |  |  |   |
|    |  |   |  |   |  |  |  |                      |  |  |  |   |
|    |  |   |  |   |  |  |  |                      |  |  |  |   |
|    |  |   |  |   |  |  |  |                      |  |  |  |   |
|    +--+---+--+   +--+--+--+                      +--+--+--+   |
|                                                               |
|                                                               |
+---------------------------------------------------------------+
</code></pre>

<p>Notes</p>

<ol>
<li>Global keyspace</li>
<li>A table per user_id (""many"" tables)</li>
<li>Avoids duplicating the user id per row</li>
</ol>

<hr>

<p>Option 4: (Does this make sense?)</p>

<p>Create a multiple keyspaces (for instance ""x"" number of keyspaces) each holding a range of tables (table per user)</p>

<pre><code>                      keyspace_1                                                                                keyspace_x
+---------------------------------------------------------------+                         +---------------------------------------------------------------+
|                                                               |                         |                                                               |
|                                                               |                         |                                                               |
|       user_1        user_2                        user_n/x    |                         |     user_n-x      user_n-x+1                       user_n     |
|    +--+---+--+   +--+--+--+                      +--+--+--+   |                         |    +--+------+   +--+--+--+                      +--+--+--+   |
|    |  |   |  |   |  |  |  |                      |  |  |  |   |        ""X"" keyspaces    |    |  |   |  |   |  |  |  |                      |  |  |  |   |
|    |  |   |  |   |  |  |  |                      |  |  |  |   | +---------------------&gt; |    |  |   |  |   |  |  |  |                      |  |  |  |   |
|    |  |   |  |   |  |  |  |                      |  |  |  |   |                         |    |  |   |  |   |  |  |  |                      |  |  |  |   |
|    |  |   |  |   |  |  |  |                      |  |  |  |   |                         |    |  |   |  |   |  |  |  |                      |  |  |  |   |
|    |  |   |  |   |  |  |  |                      |  |  |  |   |                         |    |  |   |  |   |  |  |  |                      |  |  |  |   |
|    +--+---+--+   +--+--+--+                      +--+--+--+   |                         |    +--+---+--+   +--+--+--+                      +--+--+--+   |
|                                                               |                         |                                                               |
|                                                               |                         |                                                               |
+---------------------------------------------------------------+                         +---------------------------------------------------------------+
</code></pre>

<p>Notes:</p>

<ol>
<li>Multiple keyspaces</li>
<li>Multiple tables per user</li>
<li>Requires a ""lookup"" to figure out which keyspace contains the required table</li>
</ol>

<hr>

<p><strong>Option 5:</strong></p>

<p>Split data to multiple tables and multiple keyspaces</p>

<p>Notes:
1. Requires ""joining"" information from multiple tables in some cases
2. Seems to be more complicated</p>

<hr>

<p><strong>General notes for all scenarios:</strong></p>

<ol>
<li>There are a magnitude less writes than reads</li>
<li>Many millions of reads per day</li>
<li>Traffic fluctuates per user_id - some user_ids have a lot of traffic and some user_ids have much less traffic . Would need to tune per this metric</li>
<li>Some user_ids are updated (writes) more frequently than others</li>
<li>We have multiple data centers across geographies and should sync</li>
<li>There is a long tail per primary key (some keys are accessed many times while other keys are rarely accessed)</li>
</ol>
",<database><cassandra><cql><database-normalization><scylla>,"<p>This type of integration challenge is usually solved by an <a href=""https://en.wikipedia.org/wiki/Entity%E2%80%93attribute%E2%80%93value_model"" rel=""nofollow noreferrer"">EAV</a> (Entity Attribute Value) data model in relational systems (like the one Ashrafaul demonstrates). The key consideration when considering an EAV model is an unbounded number of columns. An EAV data model may, of course, be mimicked in a CQL system like Cassandra or ScyllaDB. The EAV model lends itself nicely to writes but presents challenges when reading. You haven't really detailed your read considerations. Do you need all columns back or do you need specific columns back per user?</p>

<p><strong>Files</strong></p>

<p>Having said that, there are some further considerations inherent to Cassandra and ScyllaDB that may point you towards a unified EAV model over some of the designs you describe in your question. Both Cassandra and ScyllaDB lay out keyspaces and databases as files on disk. The number of files are basically products of number of keyspaces times number of tables. So the more keyspaces, tables or combination of the two you have, the more files you'll have on disk. This may be an issue with file descriptors and other os file juggling issues. Due to the long tail of access you mentioned it may be the case that every file is open all the time. That is not so desirable, especially when starting from a cold boot.</p>

<p><strong>[edit for clarity]</strong> All things being equal, one keyspace/table will always produce less files than many keyspace/tables. This has nothing to do with the amount of data stored or compaction strategy.</p>

<p><strong>Wide Rows</strong></p>

<p>But getting back to the data model. Ashraful's model has a primary key (userid) and another clustering key (key->column1). Due to the number of ""entries"" in each user file (500K-2M) and assuming each entry is a row comprised of avg 60 columns, what you're basically doing is creating 500k-2m * 60 avg columns rows per partition key thereby creating very large partitions. Cassandra and Scylla generally don't like very large partitions. Can they handle large partitions, sure. In practice do large partitions impact performance, yes.</p>

<p><strong>Updates or versioning</strong></p>

<p>You mention updates. The base EAV model will only represent the most recent update. There is no versioning. What you could do is add time as a clustering key to ensure that you maintain the historical values of your columns over time.</p>

<p><strong>Reads</strong></p>

<p>If you want all the columns back you could just serialize everything into a json object and put it in a single column. But I imagine that's not what you want. In the primary key (partition key) model of a key/value based system like Cassandra and Scylla, you need to know all the components of the key to get your data back. If you put <code>column1</code>, the unique row identifier, into your primary key you will need to know it in advance, likewise also the other column names if those get put in the primary key as well.</p>

<p><strong>Partitions and Composite Partition Keys</strong></p>

<p>Number of partitions dictate the parallelism of your cluster. The number of total partitions, or cardinality of partitions in your total corpus, has an affect on the utilization of your cluster hardware. More partitions = better parallelism and higher resource utilization.</p>

<p>What I might do here is modify the <code>PRIMARY KEY</code> to include <code>column1</code>. Then I would use <code>column</code> as a clustering key (which not only dictates uniqueness within a partition but also sort order - so consider this in you column naming conventions).</p>

<p>In the following table definition you would need to provide the <code>userid</code> and <code>column1</code> as equalities in your <code>WHERE</code> clause.</p>

<pre><code>CREATE TABLE data (
    userid bigint,
    column1 text,
    column text,
    value text,
    PRIMARY KEY ( (userid, column1), column )
);
</code></pre>

<p>I'd also have a separate table, maybe <code>columns_per_user</code>, that records all columns for each <code>userid</code>. Something like </p>

<pre><code>CREATE TABLE columns_per_user (
    userid bigint,
    max_columns int,
    column_names text
    PRIMARY KEY ( userid )
);
</code></pre>

<p>Where <code>max_columns</code> is the total number of columns for this user and <code>column_names</code> are the actual column names. You might also have a column for total number of entries per user, something like <code>user_entries int</code> which would basically be the number of rows in each user csv file.</p>
",['table']
46486416,46488353,2017-09-29 09:58:58,Kafka Spark Scala Cassandra Compatible Versions,"<p>I am trying to create an application using Apache Kafka,Saprk,Scala and Cassandra.
But I am facing a lot of issues in getting the right cmpatible versions of these tools.</p>

<p>Can someone please let me know which versions should I use?</p>

<p>Thanks in Advance..</p>
",<scala><apache-spark><cassandra><apache-kafka>,"<p>Here is the list of versions for libraries which we've used:</p>

<pre><code>    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
        &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;
        &lt;version&gt;2.1.1&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
        &lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;/artifactId&gt;
        &lt;version&gt;2.1.1&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
        &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;
        &lt;version&gt;0.10.2.0&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;
        &lt;artifactId&gt;kafka_2.11&lt;/artifactId&gt;
        &lt;version&gt;0.10.2.1&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.cassandra&lt;/groupId&gt;
        &lt;artifactId&gt;apache-cassandra&lt;/artifactId&gt;
        &lt;version&gt;3.10&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;com.datastax.spark&lt;/groupId&gt;
        &lt;artifactId&gt;spark-cassandra-connector_2.11&lt;/artifactId&gt;
        &lt;version&gt;2.0.2&lt;/version&gt;
    &lt;/dependency&gt;
</code></pre>

<p>The main problem which you will face with compatibility is different scala versions (2.10.* or 2.11.*). You will have to look after that and see that all of the dependencies use the same scala version. I think you can update without any doubts all versions to the latest just if you take care about the same scala versions everywhere.</p>

<p>Here is also code sample which will help you with start:</p>

<pre><code>       public static void main(String[] args) throws   InterruptedException {
           JavaStreamingContext jssc = new JavaStreamingContext(getSparkConfiguration(), Durations.seconds(5));

           JavaInputDStream&lt;ConsumerRecord&lt;String, LoggingEvent&gt;&gt; messages  =
            KafkaUtils.createDirectStream(
                    jssc,
                    LocationStrategies.PreferConsistent(),
                    ConsumerStrategies.&lt;String, LoggingEvent&gt;Subscribe(Arrays.asList(""some_topic""), getKafkaParams(""localhost:9092"", ""some_logging_group))
            );

           JavaDStream&lt;LoggingEvent&gt; loggingRecords = messages.map(
            (Function&lt;ConsumerRecord&lt;String, LoggingEvent&gt;, LoggingEvent&gt;) message -&gt; message.value()
    );

             CassandraStreamingJavaUtil.javaFunctions(loggingRecords).writerBuilder(""some_space"", ""some_table"",
                  CassandraJavaUtil.mapToRow(LoggingEvent.class)).saveToCassandra();

          jssc.start();
          jssc.awaitTermination();
}
</code></pre>

<p>Mapping in the connector is done by mapping fields in the class with table columns. </p>

<p>For setup we've used ansible and distribution versions for archives were the same as in the library dependency list.</p>
",['table']
46542447,46545114,2017-10-03 10:27:52,Cassandra system hints large partition,"<p>We are using cassandra 2.1.14. Currently large partition warning is seen on system.hints table. </p>

<p>How to make sure that system.hints table doesn't have wide partitions ?
Note that we don't want to upgrade to cassandra 3 now.</p>

<p>Is there a periodic way to clean up system.hints ?</p>

<p>Will this cause I/O spike in cassandra cluster ?</p>

<p>Log:</p>

<pre><code> Compacting large partition system/hints:
 10ad72eb-0240-4b94-b73e-eb4dc2aa759a (481568345 bytes)
</code></pre>
",<cassandra><cassandra-2.1>,"<blockquote>
  <p>How to make sure that system.hints table doesn't have wide partitions?</p>
</blockquote>

<p>There's not really much you can do about that.  <code>system.hints</code> is partitioned on <code>target_id</code> which is the hostID of the target node.  If 10000 hints build up for one node, there's really no other place for them to go.</p>

<blockquote>
  <p>Is there a periodic way to clean up system.hints?</p>
</blockquote>

<p>As mentioned above, hints should TTL after 3 hours.  This failsafe is meant to keep the <code>system.hints</code> table from getting too out of control.  But it's not at all fool-proof.</p>

<p>One way to be sure, would be to clear them out via nodetool:</p>

<pre><code>nodetool truncatehints
</code></pre>

<blockquote>
  <p>Will this cause I/O spike in cassandra cluster?</p>
</blockquote>

<p>Running <code>nodetool truncatehints</code> is fairly innocuous.  I haven't noticed a spike from running it before.</p>
",['table']
46560607,46564586,2017-10-04 08:44:45,check in one query if multiple records exist in cassandra,"<p>I have a list of Strings ""A"", ""B"", ""C"".</p>

<p>I would like to know how can I check if all these Strings exist in a Cassandra column.</p>

<p>I have two approaches I have previously used for relational databases but I recently moved to Cassandra and I don't know how to achieve this.</p>

<p>The problem is I have about 100 string that I have to check and I don't want to send 100 requests to my database. It wouldn't be wise.</p>
",<cassandra>,"<p>Interesting question... I don't know the schema you're using, but if your strings are in the only PK column (or in a composite PK where the other columns values are known at query time) then you could probably issue 100 queries without worries. The key cache will help not to hit disks, so your could get fast responses.</p>

<p>Instead, if you intend to use this for a column that is not part of any PK, you'll have hard time to figure this out unless you perform some kind of tricks, and this is all subject to some performance restrictions and/or increased code complexity anyway.</p>

<p>As an example, you could build a ""frequency"" table with the purpose described above, where you store how many times you ""saw"" each string ""A"", ""B"" etc..., and query this table when you need to retrieve the information:</p>

<pre><code>SELECT frequencies FROM freq_table WHERE pk = IN ('A', 'B', 'C');
</code></pre>

<p>Then you still need to loop over the result set and check that each record is > 0. An alternative could be to issue a <code>SELECT COUNT(*)</code> before the real query, because you know in advance how many records you should get (eg 3 in my example), but having the correct number of retrieved records could be enough (eg one counter is zero).</p>

<p>Of course you'd need to maintain this table on every insert/update/delete of your main table, raising the complexity of the solution, and of course all the <code>IN</code> clause and <code>COUNT</code> related warning applies...</p>

<p>I would probably stick with 100 queries: with a well designed table they should not be a problem, unless you have an inadequate cluster for the problem size you're dealing with.</p>
",['table']
46579495,46621797,2017-10-05 06:49:59,Unable to see all keyspaces in C* 2.1.7 after I downgraded from 3.0 to 2.1.7,"<p>I've been using Cassandra 2.1.7 and For some reason I upgraded to 3.0.12 and later realized that some dependent apps won't work with 3.0.12 and I downgraded and using C* 2.1.7 as I was using before. But Now I'm not able to see Keyspaces in C*. (Just FYI: Data directory is same in both C*yaml files)</p>

<p>Do I have to make any changes?</p>

<p>Appreciate your help. </p>
",<cassandra><upgrade><cassandra-3.0><cassandra-2.1>,"<p><strong>If you have NOT taken backup then nothing to worry as C* 3.0 won't remove older dbs after upgrade it just updates sstable related CFS and added some more CFS for compatibility.</strong></p>

<p>Here is what I've done to retain data:
Since 3.0 has completely different naming conventions for DB names, we need to carefully distinguish db from both(Older vs newer).</p>

<p>For 2.X Cassandra dbnames have the following convention for each ks:</p>

<pre><code>keyspace-ColumnFamilyName-ka-ID-Data.db
keyspace-ColumnFamilyName-ka-ID-Digest.sha1
keyspace-ColumnFamilyName-ka-ID-Filter.db
keyspace-ColumnFamilyName-ka-ID-Index.db
keyspace-ColumnFamilyName-ka-ID-Statistics.db
keyspace-ColumnFamilyName-ka-ID-Summary.db
keyspace-ColumnFamilyName-ka-ID-TOC.txt

keyspace: keyspace name
ColumnFimilyname : Name of the CF under keyspace
ka: C* Internal(Haven't explored much on this)
ID: It is incremental value I see different sets of these having different id.(looks like it is an increasing factor  when it takes snapshot, not sure though)
And the last parameter is db name
</code></pre>

<p>So when I start with 2.1.7 I read through each and every log statement in of C* daemon and found out that the sstable_actiivity file under system keyspace is not the actual one as the size of this file is very less.</p>

<pre><code>/data/system/sstable_activity-5a1ff267ace03f128563cfae6103c65e/system-sstable_activity-ka-145
</code></pre>

<p>So I tried to find the oldest file under system(keyspace directory i.e /data/system/) from snapshots and replace that with the above file. 
And the same I repeated for the ""schema_keyspaces"" table under system keyspace.</p>

<p>Now I restart cassandra daemon again, luckily I could get list of keyspaces after I run ""<strong>DESC KEYSPACES</strong>""
But I don't see list of tables when I execute ""<strong>DESC TABLES""</strong>  for my keyspace as it didn't load because files were not found by sstable_activity.</p>

<p>Now I kept repeating the same process for all other tables under ""system"" keyspace. which are as below: </p>

<pre><code>schema_keyspaces
schema_columnfamilies
local
schema_columns
schema_triggers
schema_usertypes
</code></pre>

<p>After restarting Cassandra, I was able  retrieve date that I expected for my application. </p>
",['table']
46669414,46672004,2017-10-10 14:34:28,How to store/show timestamp and double columns with 15 decimal precision?,"<p>I would like to insert experimental data into Cassandra where each data has     precision of 15 decimal places. The sample dataset is as follows:</p>

<pre><code>+------------------+-------------------+
|   Sampling_Rate  |       Value1      |
+------------------+-------------------+
| 2.48979187011719 | 0.144110783934593 |
+------------------+-------------------+
</code></pre>

<p>I would like to see the <strong>Sampling_Rate</strong> as an Epoch time (i.e. <em>1970-01-01 00:00:02.48979187011719+0000</em>), and <strong>Value1</strong> to store its full precision value.</p>

<p>For this, I inserted data with the describe table :</p>

<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE project_fvag.temp (
    sampling_rate timestamp PRIMARY KEY,
    value1 double ) WITH bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';
</code></pre>

<p>I also changed the cqlshrc file with increasing precision for both float and double. Also, changed the datetimeformat: </p>

<pre><code>datetimeformat = %Y-%m-%d %H:%M:%S.%.15f%z ;float_precision = 5 ;double_precision = 15
</code></pre>

<p>Inspite of these changes, I get the result stored as only 6 decimal places both in timestamp and value. What could be a better strategy to store/see as per my expectation?</p>
",<cassandra><timestamp><time-series><cassandra-2.0><cassandra-3.0>,"<p>For the sampling value: since you set it up as timestamp, cassandra will store it with milisecond precision. One way would be to store it as decimal.</p>

<p>The same applies to value1. Recreate your table with decimal instead of double for value1.</p>
",['table']
46796021,46892090,2017-10-17 17:39:02,NoSpamLogger.java Maximum memory usage reached Cassandra,"<p>I have a 5 node cluster of Cassandra, with ~650 GB of data on each node involving a replication factor of 3. I have recently started seeing the following error in /var/log/cassandra/system.log.</p>

<p>INFO  [ReadStage-5] 2017-10-17 17:06:07,887 NoSpamLogger.java:91 - Maximum memory usage reached (1.000GiB), cannot allocate chunk of 1.000MiB</p>

<p>I have attempted to increase the file_cache_size_in_mb, but sooner rather than later this same error catches up. I have tried to go as high as 2GB for this parameter, but to no avail.</p>

<p>When the error happens, the CPU utilisation soars and the read latencies are terribly erratic. I see this surge show up approximated every 1/2 hour. Note the timings in the list below.</p>

<p>INFO  [ReadStage-5] 2017-10-17 17:06:07,887 NoSpamLogger.java:91 - Maximum memory usage reached (1.000GiB), cannot allocate chunk of 1.000MiB
INFO  [ReadStage-36] 2017-10-17 17:36:09,807 NoSpamLogger.java:91 - Maximum memory usage reached (1.000GiB), cannot allocate chunk of 1.000MiB
INFO  [ReadStage-15] 2017-10-17 18:05:56,003 NoSpamLogger.java:91 - Maximum memory usage reached (2.000GiB), cannot allocate chunk of 1.000MiB
INFO  [ReadStage-28] 2017-10-17 18:36:01,177 NoSpamLogger.java:91 - Maximum memory usage reached (2.000GiB), cannot allocate chunk of 1.000MiB</p>

<p>Two of the tables that I have are partitioned by hour, and the partitions are large. Ex. Here are their outputs from nodetool table stats</p>

<pre><code>    Read Count: 4693453
    Read Latency: 0.36752741680805157 ms.
    Write Count: 561026
    Write Latency: 0.03742310516803143 ms.
    Pending Flushes: 0
        Table: raw_data
        SSTable count: 55
        Space used (live): 594395754275
        Space used (total): 594395754275
        Space used by snapshots (total): 0
        Off heap memory used (total): 360753372
        SSTable Compression Ratio: 0.20022598072758296
        Number of keys (estimate): 45163
        Memtable cell count: 90441
        Memtable data size: 685647925
        Memtable off heap memory used: 0
        Memtable switch count: 1
        Local read count: 0
        Local read latency: NaN ms
        Local write count: 126710
        Local write latency: 0.096 ms
        Pending flushes: 0
        Percent repaired: 52.99
        Bloom filter false positives: 167775
        Bloom filter false ratio: 0.16152
        Bloom filter space used: 264448
        Bloom filter off heap memory used: 264008
        Index summary off heap memory used: 31060
        Compression metadata off heap memory used: 360458304
        Compacted partition minimum bytes: 51
        **Compacted partition maximum bytes: 3449259151**
        Compacted partition mean bytes: 16642499
        Average live cells per slice (last five minutes): 1.0005435888450147
        Maximum live cells per slice (last five minutes): 42
        Average tombstones per slice (last five minutes): 1.0
        Maximum tombstones per slice (last five minutes): 1
        Dropped Mutations: 0



    Read Count: 4712814
    Read Latency: 0.3356051004771247 ms.
    Write Count: 643718
    Write Latency: 0.04168356951335834 ms.
    Pending Flushes: 0
        Table: customer_profile_history
        SSTable count: 20
        Space used (live): 9423364484
        Space used (total): 9423364484
        Space used by snapshots (total): 0
        Off heap memory used (total): 6560008
        SSTable Compression Ratio: 0.1744084338623116
        Number of keys (estimate): 69
        Memtable cell count: 35242
        Memtable data size: 789595302
        Memtable off heap memory used: 0
        Memtable switch count: 1
        Local read count: 2307
        Local read latency: NaN ms
        Local write count: 51772
        Local write latency: 0.076 ms
        Pending flushes: 0
        Percent repaired: 0.0
        Bloom filter false positives: 0
        Bloom filter false ratio: 0.00000
        Bloom filter space used: 384
        Bloom filter off heap memory used: 224
        Index summary off heap memory used: 400
        Compression metadata off heap memory used: 6559384
        Compacted partition minimum bytes: 20502
        **Compacted partition maximum bytes: 4139110981**
        Compacted partition mean bytes: 708736810
        Average live cells per slice (last five minutes): NaN
        Maximum live cells per slice (last five minutes): 0
        Average tombstones per slice (last five minutes): NaN
        Maximum tombstones per slice (last five minutes): 0
        Dropped Mutations: 0
</code></pre>

<p>Here goes:</p>

<pre><code>cdsdb/raw_data histograms
Percentile  SSTables     Write Latency      Read Latency    Partition Size        Cell Count
                              (micros)          (micros)           (bytes)                  
50%             0.00             61.21              0.00           1955666               642
75%             1.00             73.46              0.00          17436917              4768
95%             3.00            105.78              0.00         107964792             24601
98%             8.00            219.34              0.00         186563160             42510
99%            12.00            315.85              0.00         268650950             61214
Min             0.00              6.87              0.00                51                 0
Max            14.00           1358.10              0.00        3449259151           7007506

cdsdb/customer_profile_history histograms
Percentile  SSTables     Write Latency      Read Latency    Partition Size        Cell Count
                              (micros)          (micros)           (bytes)                  
50%             0.00             73.46              0.00         223875792             61214
75%             0.00             88.15              0.00         668489532            182785
95%             0.00            152.32              0.00        1996099046            654949
98%             0.00            785.94              0.00        3449259151           1358102
99%             0.00            943.13              0.00        3449259151           1358102
Min             0.00             24.60              0.00              5723                 4
Max             0.00           5839.59              0.00        5960319812           1955666
</code></pre>

<p>Could you please suggest a way forward to mitigate this issue?</p>
",<cassandra><cassandra-3.0>,"<p>Based on the cfhistograms output posted, the partitions are enormous. </p>

<blockquote>
  <p>95% percentile of raw_data table has partition size of 107MB and max
  of 3.44GB. 95% percentile of customer_profile_history has partition
  size of 1.99GB and max of 5.96GB.</p>
</blockquote>

<p>This clearly relates to the problem you notice every half-hour as these huge partitions are written to the sstable. The data-model has to change and based on the partition size above its better to have a partition interval as ""minute"" instead of ""hour"". So a 2GB partition would reduce to 33MB partition. </p>

<p>Recommended partition size is to keep it as close to 100MB maximum. Though theoretically we can store more than 100MB, the performance is going to suffer. Remember every read of that partition is over 100MB of data through the wire. In your case, its over 2GB and hence all the performance implications along with it.</p>
",['table']
46814323,46814930,2017-10-18 15:52:24,How to export data from Cassandra to BigQuery,"<p>I have Apache Cassandra working on 4 VMs in Google Cloud. I considered it too expensive and want to export all data to BigQuery. There are about 2 TB (60 milliards rows) in Cassandra. Any suggestions how can I do it?</p>

<p>Thanks in advance.</p>
",<apache-spark><cassandra><pyspark><google-bigquery><google-cloud-platform>,"<p>We decided to move 5 years of data from Apache Cassandra to Google BigQuery. The problem was not just transferring the data or export/import, the issue was the very old Cassandra!</p>

<p>After extensive research, we have planned the migration to export data to csv and then upload in Google Cloud Storage for importing in Big Query.</p>

<p>The pain was the way Cassandra 1.1 deal with large number of records! There is no pagination so at some point your gonna run out of something! If not mistaken, pagination is introduced since version 2.2.</p>

<p>After all my attempts to upgrade to latest version 3.4 failed I decide to try other versions and luckily the version 2.2 worked! By working I mean I were able to follow the upgrading steps to end and the data were accessible.</p>

<p>Because I could not get any support for direct upgrade and my attempts to simply upgrade to 2.2 also failed. So I had no choice but to upgrade to 2.0 and then upgrade it to 2.2. Because this is extremely delicate task I rather just forward you to official website and only then give you the summary. Please make sure you check docs.datastax.com and follow their instructions.</p>

<p>To give an overview, you are going to do these steps:</p>

<ol>
<li>Making sure all nodes are stable and there is no dead nodes.</li>
<li>Make backup (your SSTables, configurations and etc)</li>
<li><p>It is very important to successfully upgrade your SSTable before proceeding to next step. Simply use</p>

<p>nodetool upgradesstables</p></li>
<li><p>Drain the nodes using</p>

<p>nodetool drain</p></li>
<li><p>Then simply stop the node</p></li>
<li>Install the new version (I will explain fresh installation later in this document)</li>
<li>Simply do the config as your old Cassandra, start it and upgradesstables again (as in step 3) for each node.
Installing Cassandra:</li>
</ol>

<p>Edit /etc/yum.repos.d/datastax.repo</p>

<pre><code>[datastax]
name = DataStax Repo for Apache Cassandra
baseurl = https://rpm.datastax.com/community
enabled = 1
gpgcheck = 0
</code></pre>

<p>And then install and start the service:</p>

<pre><code>yum install dsc20
service cassandra start
</code></pre>

<p>Once you are upgrade to Cassandra 2+ you can export the data to csv without having pagination or crashing issue.</p>

<p>Just for the records, a few commands to get the necessary information about the data structure is as follow:</p>

<pre><code>cqlsh -u username -p password
describe tables;
describe table abcd;
describe schema;
</code></pre>

<p>And once we know the tables we want to export we just use them alongside its keyspace. First add all your commands in one file to create a batch.</p>

<pre><code>vi commands.list
</code></pre>

<p>For example a sample command to export one table:</p>

<pre><code>COPY keyspace.tablename TO '/backup/export.csv';
</code></pre>

<p>And finally run the commands from the file:</p>

<pre><code>cqlsh -u username -p password -f /backup/commands.list
</code></pre>

<p>So by now, you have exported the tables to csv file(s). All you need to do now is uploading the files to Google Cloud Storage:</p>

<pre><code>gsutil rsync /backup gs://bucket
</code></pre>

<p>Later on you can use Google API to import the csv files to Google BigQuery. You may check out the Google documentations for this in cloud.google.com</p>
",['table']
46814635,46817614,2017-10-18 16:10:13,Is Leveled Compaction Strategy still beneficial for reads when Rows Are Write-Once?,"<p>Among other cases, this <a href=""https://www.datastax.com/dev/blog/when-to-use-leveled-compaction"" rel=""nofollow noreferrer"">datastax post</a> says that <strong><em>Compaction may not be a Good Option when Rows Are Write-Once</em></strong>:</p>

<blockquote>
  <p>If your rows are always written entirely at once and are never updated, they will naturally always be contained by a single SSTable when using size-tiered compaction. Thus, there's really nothing to gain from leveled compaction.</p>
</blockquote>

<p>Also, in the talk <a href=""https://es.slideshare.net/DataStax/the-missing-manual-for-leveled-compaction-strategy-wei-deng-datastax-cassandra-summit-2016"" rel=""nofollow noreferrer"">The Missing Manual for Leveled Compaction Strategy (Wei Deng &amp; Ryan Svihla)</a> slide 30 it says that <strong><em>Where LCS fits the best</em></strong> </p>

<blockquote>
  <p>Use cases needing very consistent read performance with much higher read to write ratio</p>
  
  <p>Wide-partition data model with limited (or slow-growing) number of total partitions but <strong>a lot of updates and deletes</strong>, or fully TTL’ed dataset</p>
</blockquote>

<p>I understand that if a row is updated or deleted frequently it can end up in several SSTables, hence this will impact in read performance. From <a href=""https://www.datastax.com/dev/blog/leveled-compaction-in-apache-cassandra"" rel=""nofollow noreferrer"">Leveled Compaction in Apache Cassandra</a></p>

<blockquote>
  <p>Performance can be inconsistent because there are no guarantees as to how many sstables a row may be spread across: in the worst case, we could have columns from a given row in each sstable.</p>
</blockquote>

<p>However, in a scenario where <strong>Rows Are Write-Once</strong>, this strategy does not represent a benefit too when reading by all rows of a partition key?</p>

<p>Because if I understood correctly, with this strategy the rows with the same partition key tend to be in the same SSTable, because merges SSTables that overlaps in contrast to Size Tiered Compaction that merges SSTables with similar size.</p>
",<cassandra><datastax>,"<p>When the rows are written strictly once, there is no effect of choosing LeveledCompactionStrategy over SizeTieredCompactionStrategy, regarding read performance (there are other effects, e.g. LCS requires more IO)</p>

<p>Regarding the below comments from question</p>

<blockquote>
  <p><em>with this strategy the rows with the same partition key tend to be in
  the same SSTable, because merges SSTables that overlaps in contrast to
  Size Tiered Compaction that merges SSTables with similar size.</em></p>
</blockquote>

<p>When a row with same partition key is written exactly once, then there is no scenario of merging SSTables, as its not spread out across different SSTables in the first place. </p>

<p>When we talk in terms of update, it need not be an existing column within that row being updated. There could be scenario where we add a complete new set of clustering column along with associated columns for an already existing partition key. </p>

<p>Here is a sample table</p>

<pre><code>CREATE TABLE tablename(
   emailid text,
   sent-date date,
   column3 text,
   PRIMARY KEY (emailid,sent-date)
   )
</code></pre>

<p>Now for a given emailid (say hello@gmail.com) a single partition key, there could be inserts at two or more times with different ""sent-date"". Though they are inserts (essentially upserts) to same partition key and hence LeveledCompaction would benefit here.</p>

<p>But assume the same table with just emailid as primary key and written exactly once. Then there is no advantage irrespective of how SSTables are compacted, be it SizeTieredCompactionStrategy or LeveledCompactionStrategy, as the row always would live on only one SSTable.</p>
",['table']
46869305,46958190,2017-10-22 00:07:20,Cassandra two dimensional data modelling,"<p><strong>The Use case:</strong></p>

<p>For a game I am collecting the results of each game match. It's always Team A against Team B. Each team consists of 5 players each picking a champion and the possible outcome of a match is for one team either Won / Lost or for both teams a draw.</p>

<p>I would like to figure out the best champion combinationsI want to create win/lose/draw statistics based on the chosen champion combination of each team. In total there are ~100 champions a player can chose from. So there are many different champion combinations possible.</p>

<p>More (bonus) features:</p>

<ul>
<li>I would like to figure out how one combination performed against another specific combination (in short: what's the best combination to counter a very strong champion combination)</li>
<li>As balance changes are applied to the game it makes sense to have a possibility to select / filter stats by specific timeranges (for instance past 14 days only) - daily precision is fine for that</li>
</ul>

<p><strong>My problem:</strong></p>

<p>I wonder what's the best way to collect the statistics based on the champion combination? How would the data modelling look like?</p>

<p><strong>My idea:</strong></p>

<ol>
<li><p>Create a hash of all <code>championId</code> in a combination which would literally represent a <code>championCombinationId</code> which is a unique identifier for the champion combo a team uses.</p></li>
<li><p>Create a two dimensional table which allows tracking combination vs combination stats. Something like this:
<a href=""https://i.stack.imgur.com/pYO9N.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pYO9N.png"" alt=""enter image description here""></a></p></li>
</ol>

<p>Timeframes (daily dates) and the actual <code>championId</code>s for a <code>combinationId</code> are missing there.</p>

<p>I tried myself creating a model for the above requirements, but I am absolutely not sure about it. Nor do I know what keys I would need to specify.</p>

<pre><code>CREATE TABLE team_combination_statistics (
  combinationIdA text, // Team A
  combinationIdB text, // Team B
  championIdsA text, // An array of all champion IDs of combination A
  championIdsB text, // An array of all champion IDs of combination B
  trackingTimeFrame text, // A date?
  wins int,
  losses int,
  draws int
);
</code></pre>
",<cassandra><bigdata><data-modeling><cql><cassandra-3.0>,"<p>This question is quite long so I'll talk about different topics before suggesting my approach, be ready for a long answer:</p>

<ol>
<li>Data normalization</li>
<li>Two-dimensional tables with same value axes</li>
</ol>

<h1>Data normalization</h1>

<p>Storing total ammount of data is useful but ordering by it isn't, as the order doesn't determine if a combination is good vs another, it determines the combination that most times have won/lost vs the opposite but the total ammount of games played also matters.</p>

<p>When ordering the results, you want to order by win-ratio, draw-ratio, loose-ratio of two of the previous as the third is a linear combination.</p>

<h1>Two-dimensional tables with same value axes</h1>

<p>The problem on two-dimensional tables where both dimensions represent the same data, in this case a group of 5 champs, is that either you make a triangular table or you have data doubled as you will have to store cominationA vs combinationB and combinationB vs combinationA, being combinationX a specific group of 5 champs.</p>

<p>There are two aproaches here, using triangular tables or doubling the data manually:</p>

<h2>1. Triangular tables:</h2>

<p>You create a table where either the top right half is empty or the bottom left hand is empty. You then handle in the app which hash is A and which is B, and you may need to swap their order, as there is no duplicate data. You could for example consider alphabetical order where A &lt; B always. If you then request the data in the wrong order you would get no data. The other option would be making both A vs B and B vs A query and then joining the results (swapping the wins and looses obviously).</p>

<h2>2. Doubling the data manually:</h2>

<p>By making two inserts with reflected values (A, B, wins, draws, looses &amp; B, A, looses, draws, wins) you would duplicate the data. This lets you query in any order at the cost of using two times the space and requiring double inserts.</p>

<h2>Pros and cons:</h2>

<p>The pros of one approach are the cons of the other.</p>

<h3>Pros of triangular tables</h3>

<ul>
<li>Does not store duplicate data</li>
<li>Requires half the insert</li>
</ul>

<h3>Pros of doubling the data</h3>

<ul>
<li>The application doesn't care in which order you make the request</li>
</ul>

<p>I would probably use the triangular tables approach as the application complexity increase is not that big to be relevant, but the scalability does matter.</p>

<h1>Proposed schema</h1>

<p>Use whatever keyspace you want, I choose so from stackoverflow. Modify the replication strategy or factor as needed.</p>

<pre><code>CREATE KEYSPACE so WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 3};
</code></pre>

<h2>Champion names table</h2>

<p>The champion table will contain info about the different champions, for now it will only hold the name but you could store other things in the future.</p>

<pre><code>CREATE TABLE so.champions (
    c boolean,
    id smallint,
    name text,
    PRIMARY KEY(c, id)
) WITH comment='Champion names';
</code></pre>

<p>A <code>boolean</code> is used as the partition key as we want to store all champs in a single partition for query performance and there will be a low ammount of records (~100) we will always be using <code>c=True</code>. A <code>smallint</code> was choosen for the <code>id</code> as 2^7 = 128 was to close to the actual number of champs and to leave room for future champs without using the negative numbers.</p>

<p>When querying the champs you could get them all by doing:</p>

<pre><code>SELECT id, name FROM so.champions WHERE c=True;
</code></pre>

<p>or request a specific one by:</p>

<pre><code>SELECT name FROM so.champions WHERE c=True and id=XX;
</code></pre>

<h2>Historic match results table</h2>

<p>This table will store the results of the matches without agregating:</p>

<pre><code>CREATE TABLE so.matches (
    dt date,
    ts time,
    id XXXXXXXX,
    teams list&lt;frozen&lt;set&lt;smallint&gt;&gt;&gt;,
    winA boolean,
    winB boolean,
    PRIMARY KEY(dt, ts, id)
) WITH comment='Match results';
</code></pre>

<p>For the partition of an historic data table, and as you mentioned daily precission, <code>date</code> seems to be a nice partition key. A <code>time</code> column is used as the first clustering key for ordering reasons and to complete the timestamp, doesn't matter if these timestamp belong to the ending or finishing instant, choose one and stick with it. An additional identifier is required in the clustering key as 2 games may end in the same instant (time has nanosecond precission which would basically mean that the data lost to overlap would be quite insignificant but your data source will probably not have this precission, thus making this last key column necesary). You can use whatever type you want for this column, probably you will already have some king of identifier with the data that you can use here. You could also go for a random number, an incremental int managed by the application, or even the name of the first players as you can be sure the same player will not start/finish two games at the same second.</p>

<p>The <code>teams</code> column is the most important one: it stores the ids of the champs that were played in the game. A sequence of two elements is used, one for each team. The inner (frozen) set is for the champs id in each team, for example: <code>{1,3,5,7,9}</code>. I've tried a couple different options: <code>set&lt; frozen&lt;set&lt;smallint&gt;&gt; &gt;</code>, <code>tuple&lt; set&lt;smallint&gt;&gt; , set&lt;smallint&gt; &gt;</code> and <code>list&lt; frozen&lt;set&lt;smallint&gt;&gt; &gt;</code>. The first options doesn't store the order of the teams, so we would have no way to know who win the game. The second one doesn't accept using an index on this column and doing partial searchs through <code>CONTAINS</code> so I've opted for the third that does keep the order and allows partial searchs.</p>

<p>The other two values are two booleans representing who won the game. You could have additional columns such a <code>draw boolean</code> one but this one is not necesary or <code>duration time</code> if you want to store the length of the game (I'm not using Cassandra's <code>duration</code> type on purpouse as it is only worth when it takes months or at least days), <code>end timestamp</code>/<code>start timestamp</code> if you want to store the one that you are not using in the partition and clustering key, etc.</p>

<h3>Partial searchs</h3>

<p>It may be useful to create an index on teams so that you are allowed to query on this column:</p>

<pre><code>CREATE INDEX matchesByTeams ON so.matches( teams );
</code></pre>

<p>Then we can execute the following <code>SELECT</code> statenments:</p>

<pre><code>SELECT * FROM so.matches WHERE teams CONTAINS {1,3,5,7,9};
SELECT * FROM so.matches WHERE teams CONTAINS {1,3,5,7,9} AND dt=toDate(now());
</code></pre>

<p>The first one would select the matches in which any of the teams selected that composition and the second one will further filter it to today's matches.</p>

<h2>Stats cache table</h2>

<p>With these two tables you can hold all the info, and then request the data you need to calculate the stats involved. Once you calculate some data, you could store this info back in Cassandra as a ""cache"" in an additional table so that when a user requests some stats to be shown, you first check if they were already calculated and if they weren't calculate. This table would need to have a column for each parameter that the user can enter, for example: champion composition, starting date, final date, enemy team; and additional columns for the stats themselves.</p>

<pre><code>CREATE TABLE so.stats (
    team frozen&lt;set&lt;smallint&gt;&gt;,
    s_ts timestamp,
    e_ts timestamp,
    enemy frozen&lt;set&lt;smallint&gt;&gt;,
    win_ratio float,
    loose_ratio float,
    wins int,
    draws int,
    looses int,
    PRIMARY KEY(team, s_ts, e_ts, enemy)
) WITH comment=""Already calculated queries"";
</code></pre>

<h3>Ordered by win/loose ratios:</h3>

<p>To get the results order by ratios instead of enemy team you can use materialized views.</p>

<pre><code>CREATE MATERIALIZED VIEW so.statsByWinRatio AS
    SELECT * FROM so.stats
    WHERE team IS NOT NULL AND s_ts IS NOT NULL AND e_ts IS NOT NULL AND win_ratio IS NOT NULL AND enemy IS NOT NULL
    PRIMARY KEY(team, s_ts, e_ts, win_ratio, enemy)
    WITH comment='Allow ordering by win ratio';
</code></pre>

<p><strong>NOTE:</strong>
While I was answering I realized that introducing the concept of ""patch"" inside the DB so that the user is not allowed to determine dates but patches could be a better solution. If you are interested comment and I'll edit the answer to include the patch concept. It would mean modifying both the <code>so.historic</code> and <code>so.stats</code> tables a bit, but quite minor changes.</p>
",['table']
46885748,46886128,2017-10-23 09:15:32,Spark cassandra connector Java API with Spark 2.0,"<p>I am using the datastax spark cassandra connector with Spark 1.6.3. 
<p>Is it possible to use the Spark cassandra connector Java API with Spark 2.0+?<p>I see that the latest version of spark-cassandra-connector-java_2.11 is 1.6.0-M1.<p>Does someone know about the future of the connector's Java API?<p>Thanks,<p>
Shai</p>
",<java><apache-spark><cassandra><connector>,"<p>If you're running Spark 2.0-2.2, you can use Cassandra connector from DataStax <a href=""https://spark-packages.org/package/datastax/spark-cassandra-connector"" rel=""nofollow noreferrer"">https://spark-packages.org/package/datastax/spark-cassandra-connector</a> (which is written in Scala, but you can just consume jar).</p>

<p>Check out the compatibility table in the official repo: <a href=""https://github.com/datastax/spark-cassandra-connector#version-compatibility"" rel=""nofollow noreferrer"">https://github.com/datastax/spark-cassandra-connector#version-compatibility</a></p>
",['table']
46914892,46984578,2017-10-24 15:49:53,Cassandra materialized view partition key update performance,"<p>I am trying to update a column in base table which is a partition key in the materialized view and trying to understand its performance implications in a production environment.</p>

<p>Base Table:</p>

<pre><code>CREATE TABLE if not exists data.test
 (  foreignid    uuid,
  id           uuid,         
 kind         text,
  version      text,            
 createdon    timestamp,         
**certid**    text,
  PRIMARY KEY(foreignid,createdon,id)     );
</code></pre>

<p>Materialized view:</p>

<pre><code>CREATE MATERIALIZED VIEW if not exists data.test_by_certid 
AS  SELECT * FROM data.test  WHERE id IS NOT NULL AND foreignid 
IS NOT NULL AND createdon IS NOT NULL AND certid IS NOT NULL 
PRIMARY KEY (**certid**, foreignid, createdon, id);
</code></pre>

<p>So, certid is the new partition key in our materialized view</p>

<p>What takes place :</p>

<pre><code>1. When we first insert into the test table , usually the certids would
be empty which would be replaced by ""none"" string and inserted into
the test base table.

2.The row gets inserted into materialized view as well

3. When the user provides us with certid , the row gets updated in the test base table with the new certid

4.the action gets mirrored and the row is updated in materialized view wherein the partition key certid is getting updated from ""none""
to a new value
</code></pre>

<p>Questions:</p>

<pre><code>1.What is the perfomance implication of updating the partition key certid in the materialized view?

2.For my use case, is it better to create a new table with certid as partition key (insert only when certid in non-empty) and manually
maintain all CRUD operations to the new table or should I use MV and
let cassandra do the bookkeeping?
</code></pre>

<p>It is to be noted that performance is an important criteria since it will be used in a production environment.</p>

<p>Thanks</p>
",<cassandra><data-modeling><materialized-views><cassandra-3.0><scylla>,"<p>Updating a table for which one or more views exist is always more expensive then updating a table with no views, due to the overhead of performing a read-before-write and locking the partition to ensure concurrent updates play well with the read-before-write. You can read more about the internals of materialized views in Cassandra in <a href=""https://github.com/scylladb/scylla/wiki/Materialized-Views"" rel=""noreferrer"">ScyllaDb's wiki</a>.</p>

<p>If changing the <code>certid</code> is a one-time operation, then the performance impact shouldn't be too much of a worry. Regardless, it is always a better idea to let Cassandra deal with updating the MV because it will take care of anomalies (such as what happens when the node storing the view is partitioned away and the update is unable to propagate), and eventually ensure consistency.</p>

<p>If you are worried about performance, consider replacing Cassandra with Scylla.</p>
",['table']
46921455,46921716,2017-10-24 23:20:30,Cassandra error - Order By only supported when partition key is restricted by EQ or IN,"<p>Here is the table I'm creating, this table contains information about players that played the last mundial cup.</p>

<pre><code>CREATE TABLE players ( 
      group text, equipt text, number int, position text, name text,
      day int, month int, year int, 
      club text, liga text, capitan text,
PRIMARY key (name, day, month, year));
</code></pre>

<p>When doing the following query :  </p>

<blockquote>
  <p><strong>Obtain 5 names from the oldest players that were captain of the selection team</strong></p>
</blockquote>

<p>Here is my query:  </p>

<pre><code>SELECT name FROM players WHERE captain='YES' ORDER BY year DESC LIMIT 5;
</code></pre>

<p>And I am getting this error:</p>

<blockquote>
  <p><strong>Order By only supported when partition key is restricted by EQ or IN</strong></p>
</blockquote>

<p>I think is a problem about the table I'm creating, but I don't know how to solve it.</p>

<p>Thanks.</p>
",<select><cassandra><sql-order-by><cql>,"<p>Your table definition is incorrect for the query you're trying to run.</p>

<p>You've defined a table with partition key ""name"", clustering columns ""day"", ""month"", ""year"", and various other columns.</p>

<p>In Cassandra all SELECT queries must specify a partition key with EQ or IN. You're permitted to include some or all of the clustering columns, using the equality and inequality operators you're used to in SQL.</p>

<p>The clustering columns must be included in the order they're defined. An ORDER BY clause can only include clustering columns that aren't already specific by an EQ, again in the order they're defined.</p>

<p>For example, you can write the query </p>

<pre><code>select * from players where name = 'fiticida' and day &lt; 5 order by month desc;
</code></pre>

<p>or</p>

<pre><code>select * from players where name = 'fiticida' and day = 10 and month &gt; 2 order by month asc;
</code></pre>

<p>but not</p>

<pre><code>select * from players where name = 'fiticida' and year = 2017;
</code></pre>

<p>which doesn't include ""day"" or ""month""</p>

<p>and not</p>

<pre><code>select * from players where name = 'fiticida' and day = 5 order by year desc;
</code></pre>

<p>which doesn't include ""month"".</p>

<p><a href=""http://cassandra.apache.org/doc/latest/cql/dml.html#the-where-clause"" rel=""noreferrer"">Here</a> is the official documentation on the SELECT query.</p>

<hr>

<p>To satisfy your query, the table needs</p>

<ul>
<li>A partition key specified by EQ or IN: ""captain"" will work</li>
<li>An ORDER BY clause using the leftmost clustering column: put ""year"" to the left of ""month"" and ""day"" in your primary key definition</li>
</ul>
",['table']
46925342,46926832,2017-10-25 06:27:17,"who holds the memtables and SSTables in cassandra, the nodes or the cluster?","<p>Each node will have different memtables and SSTables or the entire cluster has certain number of these tables? And, in write operation, first it is written to the commit log and then to memtables and sstables. Is this done by the node? If not, what is the role of node in the write operation as discussed in the picture mentioned below?
<a href=""https://www.google.co.in/url?sa=i&amp;rct=j&amp;q=&amp;esrc=s&amp;source=images&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwjEmYGCkovXAhXEMo8KHeLtD48QjRwIBw&amp;url=https%3A%2F%2Fwww.guru99.com%2Fcassandra-architecture.html&amp;psig=AOvVaw0rqVl6BG9vn0TefAPCEb5t&amp;ust=1508999138916139"" rel=""nofollow noreferrer"">https://www.google.co.in/url?sa=i&amp;rct=j&amp;q=&amp;esrc=s&amp;source=images&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwjEmYGCkovXAhXEMo8KHeLtD48QjRwIBw&amp;url=https%3A%2F%2Fwww.guru99.com%2Fcassandra-architecture.html&amp;psig=AOvVaw0rqVl6BG9vn0TefAPCEb5t&amp;ust=1508999138916139</a></p>
",<cassandra><nodes>,"<p>Whenever you create a table in Cassandra , a memtable is created.
Thus a node may have many memtables.
A SStable is created when a flush is triggered.</p>

<p>See this <a href=""http://abiasforaction.net/apache-cassandra-memtable-flush/"" rel=""nofollow noreferrer"">http://abiasforaction.net/apache-cassandra-memtable-flush/</a></p>

<p>For the other question <strong>(Write path)</strong> <br>The operations are carried by node itself and the coordinator node orchestrating it <br> 
Whenever a data is inserted,it goes into the memtable and is <em>appended</em> in the commit log.Commit logs  are replayed when a node has gone down<br>
<br>
So consider you have once again flushed the data after this new insertion ,you will see 2 set(generation) of SStables . Now your partition data exists in multiple SStables.<br>
Note that SStables are immutable. Later on you may also want to read how compaction kicks in.<br><br>
<a href=""https://docs.datastax.com/en/cassandra/3.0/cassandra/operations/opsConfigureCompaction.html"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/cassandra/3.0/cassandra/operations/opsConfigureCompaction.html</a>.</p>
",['table']
47008765,47009910,2017-10-30 05:44:36,cassandra collection map indexing,"<p>I have a Cassandra table whose one column looks like this</p>

<pre><code> ""ArticlesViewed"" frozen&lt;map&lt;int, frozen&lt;list&lt;int&gt;&gt;&gt;&gt;

   and it contains data like 

       ArticlesViewed
       -----------------------------------------------
       {400: [9, 19, 11, 12], 545: [183, 44, 25, 16, 97]}
       {812: [2, 44, 41, 22], 376: [123, 14, 15, 16, 47]}
       {134: [9, 10, 11, 92], 111: [533, 14, 15, 16, 27]}
</code></pre>

<p>i want to create index on this column ,to (not allow filtering) on this column but its not allowing me to do so </p>

<pre><code> cqlsh&gt;CREATE INDEX ON user_profile(""ArticlesViewed"");

 [Invalid query] message=""Cannot create values() index on frozen column ArticlesViewed.
 Frozen collections only support full() indexes""

  Also,i want to query on the &lt;value&gt;  {400: [9, 19, 11, 12],of the column like

 select ""ArticlesViewed"" from  user_profile where ""ArticlesViewed"" =19;
</code></pre>

<p>please suggest me some ways to do this..any help would be appreciated </p>
",<indexing><cassandra><bigdata><cqlsh>,"<p><strong>You can't create index on frozen collection element</strong></p>
<p>You have to create full index on frozen collection</p>
<blockquote>
<p>Create an index on a full FROZEN collection. An FULL index can be created on a set, list, or map column of a table that doesn't have an existing index.</p>
<p>To index collection entries, you use the FULL keyword and collection name in nested parentheses</p>
</blockquote>
<p>For Example :</p>
<pre><code>CREATE INDEX on user_profile(full(articlesviewed));
</code></pre>
<p>Using the full index, if you want to query with articlesviewed you have to specified the full collection value. Since it's frozen.</p>
<p>If you have the data :</p>
<pre><code> userid | articlesviewed
--------+----------------------------------
      1 | {1: [1, 2, 3], 10: [10, 20, 30]}
</code></pre>
<p>Your query should contains the full value of articlesviewed</p>
<pre><code>SELECT * FROM user_profile WHERE articlesviewed = {1: [1, 2, 3], 10: [10, 20, 30]};
</code></pre>
",['table']
47120916,47127723,2017-11-05 11:15:28,Conceptual difference concerning column families in Cassandras data model compared to Bigtable?,"<p>I am currently trying to dig into Cassandra's data model and its relation to Bigtable, but ended up with a strong headache concerning the Column Family concept.</p>

<p>Mainly my question was asked and <a href=""https://stackoverflow.com/questions/6644474/is-the-column-family-based-data-model-of-cassandra-the-same-as-the-column-family/"">already answered</a>. However, I'm not satisfied with the answers :)</p>

<p>Firstly I've read the <a href=""https://static.googleusercontent.com/media/research.google.com/de//archive/bigtable-osdi06.pdf"" rel=""noreferrer"">Bigtable paper</a> especially concerning its data model, i.e. how data is stored. As far as I understood each table in Bigtable basically relies on a multi-dimensional sparse map with the dimensions row, column and time. The map is sorted by rows. Columns can be grouped with the name convention family:qualifier to a column family. Therefore, a single row can contain multiple column families (see the example figure in the paper).</p>

<p>Although it is stated that Cassandra relies on Bigtable data model, I read multiple times that in Cassandra a column family contains multiple rows and is to some extent comparable to a table in relational data stores. Isn't this contrary to Bigtable's approach, where a row could contain multiple column families? What comes first, the column family or row :)? Are these concepts even comparable?</p>
",<cassandra><nosql><google-cloud-bigtable><scylla>,"<p>The answer you linked to was from 6 years ago, and a lot has changed in Cassandra since. When Cassandra started out, its data model was indeed based on BigTable's. A row of data could include any number of columns, each of these columns has a name and a value. A row could have a thousand different columns, and a different row could have a thousand other columns - rows do not have to have the same columns. Such a database is called ""schema-less"", because there is no schema that each row needs to adhere to.</p>

<p>But Toto, we're not in Kansas any more - and Cassandra's model changed in focus (though not in essense) since, and I'll try to explain how and why:</p>

<p>As Cassandra matured, its developers started to realize that schema-less isn't as great as they once thought it was. Schemas are valuable in ensuring application correctness. Moreover, one doesn't normally get to 1000 columns in a single row just because there are 1000 individually-named fields in one record. Rather, the more common case is that the record actually contains 200 entries, each with 5 fields. The schema should fix these 5 fields that every one of these entries should have, and what defines each of these separate entries is called a ""clustering key"". So around the time of Cassandra 0.8, six years ago, these ideas where introduced to Cassandra as the ""CQL"" (Cassandra Query Language).</p>

<p>For example, in CQL one declares that a column-family (which was dutifully renamed ""table"") has a schema, with a known list of fields:  </p>

<pre><code>CREATE TABLE groups (
    groupname text,
    username text,
    email text,
    age int,
    PRIMARY KEY (groupname, username)
)
</code></pre>

<p>This schema says that each wide row in the table (now, in modern Cassandra, this was renamed a ""partition"") with the key ""groupname"" is a a possibly long list of users, each with username, email and age fields. The first name in the ""PRIMARY KEY"" specifier is the partition key (it determines the key of the wide rows), and the second is called the clustering key (it determines the key of the small rows that together make up the wide rows).</p>

<p>Despite the new CQL dressup, Cassandra continued to implement these new concepts using the good-old-BigTable-wide-row-without-schema implementation. For example, consider that our data has a group ""mygroup"" with two people, (john, john@somewhere.com, 27) and (joe, joe@somewhere.com, 38). Cassandra adds the following four column names->values to the wide row:</p>

<pre><code>john:email -&gt; john@somewhere.com
john:age -&gt; 27
joe:email -&gt; joe@somewhere.com
joe:age -&gt; 27
</code></pre>

<p>Note how we ended up with a wide row with 4 columns - 2 non-key fields per row (email and age), multiplied by the number of rows in the partition (2). The clustering key field ""username"" no longer appears anywhere as the value, but rather as part of the column's name! So If we have two username values ""john"" and ""joe"", We have some columns prefixed ""john"" and some columns prefixed ""joe"", and when we read the column ""joe:email"" we know this is the value of the email field of the row which has username=joe.</p>

<p>Cassandra still has this internal duality - converting the user-facing CQL rows and clustering keys into old-style wide rows. Until recently, Cassandra's on-disk format known as ""SSTables"" was still schema-less and used composite names as shown above for column names. I wrote a detailed description of the SSTable format on Scylla's site <a href=""https://github.com/scylladb/scylla/wiki/SSTables-Data-File"" rel=""noreferrer"">https://github.com/scylladb/scylla/wiki/SSTables-Data-File</a> (Scylla is a more efficient C++ re-implementation of Cassandra to which I contribute). However, column names are very inefficient in this format so Cassandra recently (in version 3.0) switched to a different file format, which for the first time, accepts clustering keys and schema-full rows as first class citizens. This was the last nail in the coffin of the schema-less Cassandra from 7 years ago. Cassandra is now schema-full, all the way.</p>
",['table']
47150623,47151760,2017-11-07 05:32:55,Table in cassandra stops responding,"<p>I have a table in cassandra with the following table structure.</p>

<pre><code>CREATE TABLE ""TagIdKeySpace"".""TagReadingsFailed"" (
""ServerUrl"" text,
""TagId"" text,
""FromTime"" timestamp,
""ToTime"" timestamp,
""AvgReading"" decimal,
""InsertTimeStamp"" timestamp,
""MaxReading"" decimal,
""MinReading"" decimal,
""Readings"" text,
PRIMARY KEY (( ""ServerUrl"", ""TagId"" ), ""FromTime"", ""ToTime"")
) WITH bloom_filter_fp_chance = 0.01
AND comment = ''
AND crc_check_chance = 1.0
AND dclocal_read_repair_chance = 0.1
AND default_time_to_live = 0
AND gc_grace_seconds = 864000
AND max_index_interval = 2048
AND memtable_flush_period_in_ms = 0
AND min_index_interval = 128
AND read_repair_chance = 0.0
AND speculative_retry = '99.0PERCENTILE'
AND caching = {
    'keys' : 'ALL',
'rows_per_partition' : 'NONE'
    }
      AND compression = {
'chunk_length_in_kb' : 64,
'class' : 'LZ4Compressor',
'enabled' : true
}
AND compaction = {
'class' : 'SizeTieredCompactionStrategy',
'max_threshold' : 32,
'min_threshold' : 4
 };
</code></pre>

<p>And I have a similar table in another  Cassandra Database in a different machine.</p>

<p>What I want to do is transfer the records from this data base to a central database, after the transfer is complete I would like to delete the records in the local database</p>

<p>For that I have used CassandraCsharpDriver (in a c# windows service application)to select records from the local table, make a web request to send the data to the other machine and then delete the records that are present in the local machine.</p>

<p>For a while it worked fine.</p>

<p>Suddenly I noticed that when I run the following query,</p>

<pre><code>select distinct (""ServerUrl"",""TagId"") from ""TagReadingsFailed""  I get an Error: Key not found in dictionary.
</code></pre>

<p>When I use The Datastax devcenter to run the same query.
It shows the Error: ReadFailure: code=1300 [Replica(s) failed to execute read] message=""Operation failed - received 0 responses and 1 failures"" </p>

<pre><code>info={'failures': 1, 'received_responses': 0, 'required_response': 1,'conistency': 'ONE'}
</code></pre>

<p>I have tried running the nodetool repair but it says that nothing to be repaired in the keyspace. </p>

<p>If I run a select query that has the both the clustring and the partitioning keys then the query executes without any errors in this table. If I run a query that selects count, distinct, * (select * from ""TagReadingsFailed"") It shows the above error.</p>

<p>I would like to know what it is that I am doing wrong</p>
",<c#><cassandra>,"<p>I am guessing the issue is with tombstone. There is huge number of tombstone while removing records for this table. Please refer to <a href=""https://stackoverflow.com/questions/47086869/ttl-in-cassandra-creating-tombstones/47099254#47099254"">this</a> for more details. Another easier way is to drop the table and create again, as you already removed all records. The second approach is to wait for compaction process to trigger (you can change related compaction parameters to make it run more frequently).</p>
",['table']
47159469,47185919,2017-11-07 13:42:10,PySpark + Cassandra: Getting distinct values of partition key,"<p>I'm trying to get the distinct values of the partition key of a cassandra table in pyspark. However, pyspark seems not to understand me and fully iterates all data (which is a lot) instead of querying the index.</p>

<p>This is the code I use, which looks pretty straightforward to me:</p>

<pre><code>from pyspark.sql import SparkSession

spark = SparkSession \
    .builder \
    .appName(""Spark! This town not big enough for the two of us."") \
    .getOrCreate()

ct = spark.read\
    .format(""org.apache.spark.sql.cassandra"")\
    .options(table=""avt_sensor_data"", keyspace=""ipe_smart_meter"")\
    .load()

all_sensors = ct.select(""machine_name"", ""sensor_name"")\
    .distinct() \
    .collect()
</code></pre>

<p>The columns ""machine_name"" and ""sensor_name"" together form the partition key (see below for the complete schema). In my opinion, this should be super-fast, and in fact, if I execute this query in cql it takes only a couple of seconds:</p>

<pre><code>select distinct machine_name,sensor_name from ipe_smart_meter.avt_sensor_data;
</code></pre>

<p>However, the spark job would take about 10 hours to complete. From what spark tells me about its plans, it looks like it really wants to iterate all the data:</p>

<pre><code>== Physical Plan ==
*HashAggregate(keys=[machine_name#0, sensor_name#1], functions=[], output=[machine_name#0, sensor_name#1])
+- Exchange hashpartitioning(machine_name#0, sensor_name#1, 200)
   +- *HashAggregate(keys=[machine_name#0, sensor_name#1], functions=[], output=[machine_name#0, sensor_name#1])
      +- *Scan org.apache.spark.sql.cassandra.CassandraSourceRelation@2ee2f21d [machine_name#0,sensor_name#1] ReadSchema: struct&lt;machine_name:string,sensor_name:string&gt;
</code></pre>

<p>I'm not an expert, but that doesn't look like ""use the cassandra index"" to me.</p>

<p>What am I doing wrong? Is there any way of telling spark to delegate the task of getting the distinct values from cassandra? Any help would be greatly appreciated!</p>

<p>If that helps, here is a schema description of the underlying cassandra table:</p>

<pre><code>CREATE KEYSPACE ipe_smart_meter WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '2'}  AND durable_writes = true;

CREATE TABLE ipe_smart_meter.avt_sensor_data (
    machine_name text,
    sensor_name text,
    ts timestamp,
    id bigint,
    value double,
    PRIMARY KEY ((machine_name, sensor_name), ts)
) WITH CLUSTERING ORDER BY (ts DESC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = '[PRODUCTION] Table for raw data from AVT smart meters.'
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.DateTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';
</code></pre>
",<apache-spark><cassandra><pyspark><spark-cassandra-connector>,"<p>It seems automatic cassandra server-side pushdown-predicate works only when selecting, filtering or ordering.</p>

<p><a href=""https://github.com/datastax/spark-cassandra-connector/blob/master/doc/14_data_frames.md"" rel=""nofollow noreferrer"">https://github.com/datastax/spark-cassandra-connector/blob/master/doc/14_data_frames.md</a></p>

<p>So, in case of your <code>distinct()</code>, spark gets all rows and then, does <code>distinct()</code>.</p>

<h2>Solution 1</h2>

<p>You say your cql <code>select distinct...</code> is already super-fast. I guess there are relatively small number of the partition keys (the combination of machine_name and sensor_name) and so many 'ts'.</p>

<p>So, most simple solution is just to use cql (for example, <a href=""https://datastax.github.io/python-driver/"" rel=""nofollow noreferrer"">cassandra-driver</a>).</p>

<h2>Solution 2</h2>

<p>Since cassandra is a query-first database, just create one more table, which has only partition keys required by your distinct query.</p>

<pre><code>CREATE TABLE ipe_smart_meter.avt_sensor_name_machine_name (
    machine_name text,
    sensor_name text,
    PRIMARY KEY ((machine_name, sensor_name))
);
</code></pre>

<p>Then, everytime you insert a row into your original table, insert the machine_name and sensor_name into the new table.
Since it has only partition keys, this is a <strong>natural distinct</strong> table for your query. Just get all rows. Maybe super-fast. No need to distinct process.</p>

<h2>Solution 3</h2>

<p>I think solution-2 is best. But if you don't want to do two inserts for one record, one more solution is to change your table and create one materialized-view table.</p>

<pre><code>CREATE TABLE ipe_smart_meter.ipe_smart_meter.avt_sensor_data (
    machine_name text,
    sensor_name text,
    ts timestamp,
    id bigint,
    value double,
    dist_hint_num smallint,
    PRIMARY KEY ((machine_name, sensor_name), ts)
) WITH CLUSTERING ORDER BY (ts DESC)
;

CREATE MATERIALIZED VIEW IF NOT EXISTS ipe_smart_meter.avt_sensor_data_mv AS
  SELECT
    machine_name
    ,sensor_name
    ,ts
    ,dist_hint_num
  FROM ipe_smart_meter.avt_sensor_data
  WHERE
    machine_name IS NOT NULL
    AND sensor_name IS NOT NULL
    AND ts IS NOT NULL
    AND dist_hint_num IS NOT NULL
  PRIMARY KEY ((dist_hint_num), machine_name, sensor_name, ts)
  WITH
  AND CLUSTERING ORDER BY (machine_name ASC, sensor_name DESC, ts DESC)
;
</code></pre>

<p>The <code>dist_hint_num</code> column is used to limit the total number of partitions for your query to iterate, and distribute records.</p>

<p>For example, from 0 to 15. Random integer <code>random.randint(0, 15)</code> or hash-based integer <code>hash_func(machine_name + sensor_name) % 16</code>is ok.
Then, when you query as follows. cassandra gets all records from only 16 partitions, which may be more efficient than your current situation.</p>

<p>But, anyway, all records have to be read and then <code>distinct()</code> (shuffle happens). Not space efficient. I think this is not a good solution.</p>

<pre><code>functools.reduce(
    lambda df, dist_hint_num: df.union(
        other=spark_session.read.format(
            'org.apache.spark.sql.cassandra',
        ).options(
            keyspace='ipe_smart_meter',
            table='avt_sensor_data_mv',
        ).load().filter(
            col('dist_hint_num') == expr(
                f'CAST({dist_hint_num} AS SMALLINT)'
            )
        ).select(
            col('machine_name'),
            col('sensor_name'),
        ),
    ),
    range(0, 16),
    spark_session.createDataFrame(
        data=(),
        schema=StructType(
            fields=(
                StructField(
                    name='machine_name',
                    dataType=StringType(),
                    nullable=False,
                ),
                StructField(
                    name='sensor_name',
                    dataType=StringType(),
                    nullable=False,
                ),
            ),
        ),
    ),
).distinct().persist().alias(
    'df_all_machine_sensor',
)
</code></pre>
",['table']
47190980,47190997,2017-11-08 22:44:00,Is there way to see active queries/requests running in Apache Cassandra?,"<p>This is the question. Not metrics like latency, active threads, etc. Pure and plain queries. Is this possible? How? Thanks.</p>
",<cassandra><python-requests><monitor>,"<p>Not currently no, just the number of requests from native transport requests in <code>nodetool tpstats</code></p>

<p><strong>Update</strong>: After <a href=""https://issues.apache.org/jira/browse/CASSANDRA-15241"" rel=""nofollow noreferrer"">CASSANDRA-15241</a> you can query a virtual table like:</p>

<pre><code>cqlsh&gt; select * from system_views.queries;

 thread_id    | duration_micros | task
--------------+-----------------+-------------------------------------
 ReadStage-10 |           16535 | SELECT * FROM basic.wide1 LIMIT 5000
 ReadStage-13 |           16535 | SELECT * FROM basic.wide1 LIMIT 5000
 ReadStage-14 |           16535 | SELECT * FROM basic.wide1 LIMIT 5000
</code></pre>
",['table']
47192548,47198301,2017-11-09 01:42:33,Is there any other way to get a cassandra hostid other than using nodetool status?,"<p>I can list complete cluster information using <code>nodetool status</code> which outputs something like this. </p>

<pre><code>Datacenter: bi
==============
Status=Up/Down|/ State=Normal/Leaving/Joining/Moving
--  Address       Load       Tokens       Owns    Host ID                               
Rack
UN  10.132.2.93   19.88 GiB  32           ?       g94eee1f-1ge1-45c3-8cfgb-643719456c4fd  us-east-1a
UN  10.221.1.140  11.64 GiB  32           ?       b38f2fb2-fcf2-4567-b0cc-1548f63f0f24  us-east-1c
</code></pre>

<p>Is there a simpler way to grab the <code>Host ID</code> from the current node (the one the terminal is ssh'ed into) without having to parse it from the <code>status</code> output?</p>

<p>I'm using datastax enterprise... in case that helps</p>
",<cassandra><datastax-enterprise><uniqueidentifier><cassandra-3.0><nodetool>,"<p>Every node saves information about itself in the system.local table. You could use cqlsh to get this information.</p>

<pre><code>cqlsh 10.132.2.93 -e 'SELECT host_id FROM system.local;'
cqlsh 10.221.1.140 -e 'SELECT host_id FROM system.local;'
</code></pre>

<p>It also saves the same type of information in system.peers table but this is only for all other nodes but itself. That's why I think it's easier to do one query for each node.</p>
",['table']
47204397,47206826,2017-11-09 14:30:20,What is the best way to do the original query when you have made your own index table in Cassandra?,"<p>Lets say that I have a table where videos are stored like this:</p>

<pre><code>CREATE TABLE videos (
    video_id UUID,
    created_date TIMESTAMP,
    description TEXT,
    title TEXT,
    user_id UUID,
    tags LIST&lt;TEXT&gt;
    PRIMARY KEY(video_id)
);
</code></pre>

<p>And I want to be able to get videos tagged with ""funny"" tag. But I want to limit each result to 20, and paginate from there.</p>

<p>So I make a tag_index table</p>

<pre><code>CREATE TABLE tag_index (
    tag TEXT,
    video_id UUID
    PRIMARY KEY (tag, video_id)
);
</code></pre>

<p>Every time I insert a new video with 3 tags, I do 4 inserts, 1 for the video, 3 for the tags. And each time a video is tagged I insert a new tag in the tag_index table.</p>

<p>But how do I do the queries?</p>

<p>Would I first do a query in the tag_index table and get 20 results of video_id, and then do another query for videos table and do an IN clause with all the video_id that I got from the first query? Or would I do 20 single select queries on the video table? That does not seem very efficient to me, or am I wrong?</p>

<p>How would I do this in the best way? I don't understand how I can used this self made index table in a good way that is best practice for Cassandra.</p>
",<cassandra>,"<p>Multi table is the correct approach.  </p>

<p>Now your secondary query <code>Select * FROM videos WHERE video_id in (1,2,3...);</code> this is also okay in the cassandra world but i would avoid it.  </p>

<p>Whats going to happen is you will query a coordinator node, this node will then figure out which nodes own the 20 videos, run a query against each of them(maybe a read repair in there, some extra queries if you ran quorum etc) and then assemble and return them to you.  This is alot of network traffic.</p>

<p>Cassandra is not great on reads, writes are cheap, it compresses data.
Instead I would move all the video data into the tag table and duplicate it in the video table to reduce the lookups.</p>

<pre><code>CREATE TABLE video_with_tags (
    tag TEXT,
    video_id UUID,
    created_date TIMESTAMP,
    description TEXT,
    title TEXT,
    user_id UUID,
    PRIMARY KEY (tag, video_id)
);
</code></pre>

<p>Now your query looks like <code>Select * from videos_with_tags WHERE tag = 'x';</code>  The data is on one node, and it will be faster.</p>

<p>You'll still maintain the videos table so if you need to do maintenance (CRUD on videos and tags) you can find the rest of your tag data etc.</p>

<p>Remember cassandra is not an RDBMS, joins aret a thing, 3NF isn't a thing.  Writing to multiple tables on an update is okay.  </p>
",['table']
47264551,47265470,2017-11-13 12:38:15,Drop column(s) on a table that has a materialized view,"<p>It seems that it's not possible to drop a column from a table when there is a defined materialized view on this table. For example, let's assume that we have this table:</p>

<pre><code>&gt; CREATE TABLE healthera.users (
&gt;   user_id timeuuid PRIMARY KEY,
&gt;   address text,
&gt;   birthday int,
&gt;   forename text,
&gt;   user_password text,
&gt;   username text
&gt; );
</code></pre>

<p>and we define the materialized view below:</p>

<pre><code>&gt; CREATE MATERIALIZED VIEW users_by_username AS
&gt;   SELECT * FROM users
&gt;   WHERE user_id IS NOT NULL AND username IS NOT NULL
&gt;   PRIMARY KEY (username, user_id);
</code></pre>

<p>Then we alter the users table and we add a column:</p>

<pre><code>&gt; ALTER TABLE users ADD last_name text;
</code></pre>

<p>When we try to drop this column or any other column(s) from the users table then we get back this error:</p>

<pre><code>&gt; ALTER TABLE users DROP last_name ;
&gt; InvalidRequest: Error from server: code=2200 [Invalid query] message=""Cannot drop column last_name, depended on by materialized views (healthera.{users_by_username})""
</code></pre>

<p>or </p>

<pre><code>&gt; ALTER TABLE users DROP forename ;
&gt; InvalidRequest: Error from server: code=2200 [Invalid query] message=""Cannot drop column forename, depended on by materialized views (healthera.{users_by_username})""
</code></pre>

<p>Is this something expected? How should we handle this? Do we need to drop the materialized view, drop the column(s) and then recreate the materialized view again? If yes, how expensive is this for cassandra?</p>
",<cassandra><datastax-enterprise><datastax-startup>,"<p>You can add a column to the base table of a MV, but you cannot drop a column even if it is not part of the PK. </p>

<p><a href=""http://www.doanduyhai.com/blog/?p=1930"" rel=""noreferrer"">http://www.doanduyhai.com/blog/?p=1930</a></p>

<p>When creating a MV for an existing table w/data, it will kick off a building process to populate the MV in the background.</p>

<p>You can check the status -</p>

<pre><code>SELECT * FROM system.views_builds_in_progress;
SELECT * FROM system.built_views;
</code></pre>
",['table']
47291069,47296232,2017-11-14 16:47:27,Cassandra read timeout because of some response size limit reached?,"<p>I encountered a strange behavior on cassandra 3.0: </p>

<p>I have the following table:</p>

<pre><code>CREATE TABLE table (
  id text,
  ts text,
  score decimal,
  type text,
  values text,
  PRIMARY KEY (id, ts)
) WITH CLUSTERING ORDER BY (ts DESC) 
</code></pre>

<p>and the following query (which returns instantly):</p>

<pre><code>SELECT * FROM keyspace.table WHERE id='someId' AND ts IN ('2017-10-15','2017-10-16','2017-10-17','2017-10-18','2017-10-19','2017-10-20','2017-10-21','2017-10-22','2017-10-23','2017-10-24','2017-10-25','2017-10-26','2017-10-27','2017-10-28','2017-10-29','2017-10-30','2017-10-31','2017-11-01','2017-11-02','2017-11-03','2017-11-04','2017-11-05','2017-11-06');
</code></pre>

<p><strong>If I add another day in the IN clause, the response never comes (even after 10 minutes!!!):</strong></p>

<p>SELECT * FROM keyspace.table WHERE id='someId' AND ts IN ('2017-10-15','2017-10-16','2017-10-17','2017-10-18','2017-10-19','2017-10-20','2017-10-21','2017-10-22','2017-10-23','2017-10-24','2017-10-25','2017-10-26','2017-10-27','2017-10-28','2017-10-29','2017-10-30','2017-10-31','2017-11-01','2017-11-02','2017-11-03','2017-11-04','2017-11-05','2017-11-06', <strong><em>'2017-11-07'</em></strong>);</p>

<p><strong>The 'values' column may have large json data. There is some flag in cassandra.yaml with some size threshold or something like this?</strong> I guess adding another day in the query reaches some limit somewhere...in cassandra system.log I didn't see anything relevant to this.</p>
",<cassandra>,"<p>If it succeeds on one node and not another while the query will work with 1 less 'in' clause I would guess this is a memory pressure issue.  To eliminate the 'query parsing problem' you can re-write your query as:</p>

<p><code>SELECT * FROM myTable WHERE id = 'x' AND ts &gt;= '2017-10-15' AND ts &lt;= '2017-11-07';
</code></p>

<p>The in clause is only truly useful if you start bucketing your data.  This is a good approach if you have hotspots or if you see 1 node with much higher load than the others. </p>

<p>To bucket your data you would want to do something like:
<code>
CREATE TABLE table (
  id text,
  ts text,
  score decimal,
  type text,
  values text,
  PRIMARY KEY ((id, ts), type)
) WITH CLUSTERING ORDER BY (type DESC)
</code>
Your data would now be partitioned by id AND day.  Your query would then become what you have now: </p>

<p><code>SELECT * FROM myTable WHERE id='x' AND ts in ('2017-01-01')</code></p>

<p>This will better distribute data on the HDDs and allow better parallelization from cassandra.  This <strong>WILL NOT</strong> fix the memory pressure issue.  To fix that you would want to move the aggregation of data from the coordinator to your application layer.  </p>

<p>This means running N <code>SELECT ... WHERE id='x' and ts = '2017-01-01';</code> queries.</p>
",['table']
47312523,47314503,2017-11-15 16:25:58,What is the best practice for implementing low cardinality searches in Cassandra,"<p>Assume that I have the following table CQL (well a fragment of the table): </p>

<pre><code>CREATE TABLE order (
  order_id UUID PRIMARY KEY,
  placed timestamp,
  status text,
)
</code></pre>

<p>Now if status could be one of PLACED, SHIPPED, or DELIVERED as an enum, I want to find all of the orders that are in PLACED status to process them. Given there are millions of orders and all orders are ultimately ending up in DELIVERED status a materialized view doesn't feel like the right solution to the problem. I am wondering what ideas there are to solve the problem of this low cardinality index without passing through the whole data set. Ideas? </p>
",<cassandra><cql><cassandra-3.0>,"<p>I would recommend a table like</p>

<pre><code>CREATE TABLE order_by_status (
  order_id UUID,
  placed timestamp,
  status text,
  PRIMARY KEY ((status), order_id)
)
</code></pre>

<p>Then you can iterate through the query to <code>SELECT * FROM order_by_status WHERE status = 'PLACED';</code>. Millions shouldnt be too much of an issue but it would be good to prevent it from getting too large by partitioning by some date window.</p>

<pre><code>CREATE TABLE order_by_status (
  order_id UUID,
  placed timestamp,
  bucket text,
  status text,
  PRIMARY KEY ((status, bucket), order_id)
)
</code></pre>

<p>Where bucket is a string generated from timestamp like <code>2017-10</code> from the YYYY-MM. You might wanna stay away from MV's for a little bit yet, it has some bugs in current version. I would also recommend against secondary indexes for this model, using a 2nd table and issuing inserts to both is going to be your best solution.</p>
",['table']
47399975,47405111,2017-11-20 19:52:31,Cassandra COPY command never stops while loads .csv file,"<p>Hello and thank you for take your time reading my issue.
I have the next issue with Cassandra cqlsh:</p>

<p>When I use the COPY command to load a .csv into my table, the command prompt never finishes the executing and loads nothing into the table if I stop it with ctrl+c.</p>

<p>Im using .csv's files from: <a href=""https://www.kaggle.com/daveianhickey/2000-16-traffic-flow-england-scotland-wales"" rel=""nofollow noreferrer"">https://www.kaggle.com/daveianhickey/2000-16-traffic-flow-england-scotland-wales</a>
specifically from ukTrafficAADF.csv.</p>

<p>I put the code below:</p>

<pre><code>CREATE TABLE first_query ( AADFYear int, RoadCategory text,
LightGoodsVehicles text, PRIMARY KEY(AADFYear, RoadCategory);
</code></pre>

<p>Im trying it:</p>

<pre><code>COPY first_query (AADFYear, RoadCategory, LightGoodsVehicles) FROM '..\ukTrafficAADF.csv' WITH DELIMITER=',' AND HEADER=TRUE;
</code></pre>

<p>This give me the error below repeatedly:</p>

<pre><code>Failed to import 5000 rows: ParseError - Invalid row length 29 should be 3,  given up without retries
</code></pre>

<p>And never finishes.
Add that the .csv file have more columns that I need, and trying the previous COPY command with the SKIPCOLS reserved word including the unused columns does the same.</p>

<p>Thanks in advance.</p>
",<csv><cassandra><copy><cqlsh>,"<p><strong>In cqlsh COPY command, All column in the csv must be present in the table schema.</strong></p>

<blockquote>
  <p>In your case your csv <code>ukTrafficAADF</code> has 29 column but in the table <code>first_query</code> has only 3 column that's why it's throwing parse error.</p>
</blockquote>

<p>So in some way you have to remove all the unused column from the csv then you can load it into cassandra table with cqlsh copy command</p>
",['table']
47412781,47418958,2017-11-21 12:08:09,Can dse cassandra audit log enabled for a particular table?,"<p>I have enabled dse cassandra audit log by following this <a href=""https://docs.datastax.com/en/datastax_enterprise/4.8/datastax_enterprise/sec/secAuditingCassandraTable.html"" rel=""nofollow noreferrer"">link</a>.</p>

<p>But, the documentation explains about configuring audit log on a keyspace. I need to enable it on a single table inside a keyspace.</p>

<p>For ex: keyspace name : test, has table table1, table2. Audit should be enabled only on table1 not table2.</p>
",<cassandra><datastax>,"<p>No, the DSE Audit Log is only configurable at the keyspace level, not the table level.</p>

<p>This is done by using the <code>included_keyspaces</code> or <code>excluded_keyspaces</code> in your DSE yaml. This is the relevant link in the docs <a href=""http://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/security/secAuditEnable.html?hl=included_keyspaces"" rel=""nofollow noreferrer"">http://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/security/secAuditEnable.html?hl=included_keyspaces</a></p>

<p>The link you shared describes how to write audit out to a DSE table (rather than to a file), not how to audit a specific table.</p>
",['table']
47442426,47465001,2017-11-22 19:23:52,DataStax Python Cassandra Driver wrongly discovering Cassandra on localhost,"<p>I have this little university project and I have developed a simple Python app  with Bokeh frontend and Cassandra backend. I have been prototyping it and developing on a single Cassandra node and then scaled up to three nodes, one native, two virtualized. Therefore the development was on localhost, then I migrated to using a Host-only network named vboxnet0 with IP addresses:</p>

<ul>
<li>192.168.56.1 for master</li>
<li>192.168.56.101/102 for slaves.</li>
</ul>

<p>Cassandra version is 3.11.1
<br>Bokeh server version is 0.12.10 (running on Tornado 4.4.3)</p>

<p><br> I have changed the code accordingly, so my app code begins with:</p>

<pre><code>from cassandra.cluster import Cluster
from cassandra.auth import PlainTextAuthProvider
from cassandra.query import dict_factory`

def pandas_factory(colnames, rows):
    return pd.DataFrame(rows, columns=colnames)
auth_provider = PlainTextAuthProvider(username='', password='')
cluster = Cluster(contact_points=['192.168.56.1'], port=9042, auth_provider=auth_provider)
session = cluster.connect()
session.row_factory = pandas_factory
session.default_fetch_size = None
</code></pre>

<p>Cassandra is <strong>not</strong> running on localhost:</p>

<pre><code>username@hostname:~&gt; cqlsh
Connection error: ('Unable to connect to any servers', {'127.0.0.1': error(111, ""Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused"")})
</code></pre>

<p>And yet Python driver somehow thinks it discovered a Cassandra host on 127.0.0.1 and tries to connect to it:</p>

<pre><code>username@hostname:~/Folder/subfolder&gt; bokeh serve Appname &gt; ~/bokeh.output
2017-11-22 19:24:49,230 Starting Bokeh server version 0.12.10 (running on Tornado 4.4.3)
2017-11-22 19:24:49,233 Bokeh app running at: http://localhost:5006/Appname
2017-11-22 19:24:49,233 Starting Bokeh server with process id: 5819
2017-11-22 19:25:03,281 Using datacenter 'datacenter1' for DCAwareRoundRobinPolicy (via host '192.168.56.1'); if incorrect, please specify a local_dc to the constructor, or limit contact points to local cluster nodes
2017-11-22 19:25:03,281 New Cassandra host &lt;Host: 127.0.0.1 datacenter1&gt; discovered
2017-11-22 19:25:03,282 Found multiple hosts with the same rpc_address (127.0.0.1). Excluding peer 192.168.56.101
2017-11-22 19:25:03,368 Failed to create connection pool for new host 127.0.0.1:
Traceback (most recent call last):
  File ""cassandra/cluster.py"", line 2343, in cassandra.cluster.Session.add_or_renew_pool.run_add_or_renew_pool (cassandra/cluster.c:44919)
  File ""cassandra/pool.py"", line 332, in cassandra.pool.HostConnection.__init__ (cassandra/pool.c:6757)
  File ""cassandra/cluster.py"", line 1119, in cassandra.cluster.Cluster.connection_factory (cassandra/cluster.c:16094)
  File ""cassandra/connection.py"", line 330, in cassandra.connection.Connection.factory (cassandra/connection.c:5963)
  File ""/usr/lib64/python3.6/site-packages/cassandra/io/asyncorereactor.py"", line 307, in __init__
    self._connect_socket()
  File ""cassandra/connection.py"", line 369, in cassandra.connection.Connection._connect_socket (cassandra/connection.c:7477)
ConnectionRefusedError: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
2017-11-22 19:25:03,403 Host 127.0.0.1 has been marked down
2017-11-22 19:25:04,406 Error attempting to reconnect to 127.0.0.1, scheduling retry in 2.0 seconds: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
2017-11-22 19:25:06,414 Error attempting to reconnect to 127.0.0.1, scheduling retry in 4.0 seconds: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
2017-11-22 19:28:14,994 Error attempting to reconnect to 127.0.0.1, scheduling retry in 8.0 seconds: [Errno 111] Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused
2017-11-22 19:28:16,489 Host 127.0.0.1 may be up; will prepare queries and open connection pool
2017-11-22 19:28:16,808 Failed to create connection pool for new host 127.0.0.1:
</code></pre>

<p><br>
<br>And then it goes on and on. On the front it looks OK, the app works and 192.168.56.1 is queried correctly, but this is just  annoying that something is not right and I don't know if it's a bug or my own fault.</p>
",<python><cassandra><bokeh>,"<p>First of all Cassandra does not use master/slave relationship. All nodes are equal in the sense that any of your nodes can act as coordinator. The coordinator is chosen based on the request and the client will pick the best coordinator to use. The coordinator will then coordinate with other nodes responsible for the data you're reading/writing and respond back to the client. The contact point that you're specifying in the client is just what it says, a contact point. It is only used to make an initial connection to the Cassandraa cluster. When that is done the client will keep a connection for each node in your Cassandra cluster (because any node is a potential coordinator for your request).</p>

<p>To answer your question. Your cassandra.yaml file is wrong.</p>

<pre><code>2017-11-22 19:25:03,282 Found multiple hosts with the same rpc_address (127.0.0.1). Excluding peer 192.168.56.101
</code></pre>

<p>You need to set rpc_address to the address of the machine. Make sure to do this on each of your nodes in the cluster. Follow these steps to make sure you're not missing any configuration: <a href=""http://cassandra.apache.org/doc/latest/getting_started/configuring.html#main-runtime-properties"" rel=""nofollow noreferrer"">http://cassandra.apache.org/doc/latest/getting_started/configuring.html#main-runtime-properties</a></p>

<p>Also make sure to set the seed to the same ip/ips for all nodes. The seed is simply just the ip of one/many nodes in the cluster that the nodes will connect to when starting up. It's recommended to have two seeds per DC and it should be the same for all nodes.</p>
",['rpc_address']
47467491,47482316,2017-11-24 06:26:44,Cassandra - Dev center not able to connect to all the 3 nodes of Cassandra cluster,"<p>I'm new to Cassandra and EC2 configuration.</p>

<p>I have configured 3 nodes in AWS EC2 instances with Cassandra 3.0 and all the three nodes are connected to each other .</p>

<p>Following things have been configured in .yaml fie.</p>

<p>Broadcast_add: Private ip ec2 add of instance
seeds : public ip add of all the three nodes.
rpc_add : blank</p>

<p>When I try to connect to this cluster from Datastax dev centre it shows only connected to one node. When individually connecting to all the 3 ip's it gets connected to all the nodes. But when connecting to cluster with 3 ip's in connection file, it connects to only one node.</p>

<p>Could any one help with this issue ?</p>

<p>Thanks
Uttkarsh</p>
",<amazon-ec2><cassandra><datastax><cassandra-3.0>,"<pre><code>open cassandra.yaml file and change the

1) listen_address        :-   private IP
2) broadcast_address     :-   blank
3) listen_on_broadcast_address:- true
4) rpc_address           :-   0.0.0.0
5) broadcast_rpc_address :-   public IP
6) seeds ip              :-   public IP for node.    

it's working finally


Thanks Utpal
</code></pre>
","['broadcast_address', 'broadcast_rpc_address', 'rpc_address', 'listen_address']"
47533923,47535530,2017-11-28 14:25:24,"Cassandra sstableloader, no data restored","<p>i've been trying to use sstableloader to restore a snapshot, and while i don't have any exception when doing so, no data is restored on my new node.</p>

<p>here is the result of <code>sstableloader -d newNodeIp keyspaceFolder</code>:</p>

<blockquote>
  <p>Established connection to initial hosts<br>
  Opening sstables and calculating sections to stream<br>
  <br />
  Summary statistics:<br>
  &nbsp;&nbsp;&nbsp;&nbsp;Connections per host    : 1<br>
  &nbsp;&nbsp;&nbsp;&nbsp;Total files transferred : 0<br>
  &nbsp;&nbsp;&nbsp;&nbsp;Total bytes transferred : 0.000KiB<br>
  &nbsp;&nbsp;&nbsp;&nbsp;Total duration          : 2637 ms<br>
  &nbsp;&nbsp;&nbsp;&nbsp;Average transfer rate   : 0.000KiB/s<br>
  &nbsp;&nbsp;&nbsp;&nbsp;Peak transfer rate      : 0.000KiB/s</p>
</blockquote>

<p>i've tried running it either from the node the backup comes from or from another remote host, the result is the same.<br>
the cassandra version is the latest stable one as of this post (3.11.1).<br>
the source and destination keyspaces have the same name.</p>

<p>the folder containing the backup is structured as such:  </p>

<blockquote>
  <p>keyspaceName/table-uuid/</p>
</blockquote>

<p>^ each of these folders containing sstable files.<br>
none of these folders contain subfolders (no backups / snapshots subfolders).<br>
the corresponding folders on the new node are similarly named (with the exception of the uuid obviously).<br>
should that be of interest, each node is a docker container.  </p>

<p>any clue as to why sstableloader wouldn't be able to restore my backup please?<br>
am i using it wrong?<br>
don't hesitate to point out any missing info.</p>
",<docker><cassandra><nosql>,"<p>You should add also table name, not only the keyspace.</p>
",['table']
47571265,47577834,2017-11-30 10:33:23,Cassandra store list of objects,"<p>I need to store a list of map in cassandra. Is that possible?
This is a json representation of my data:</p>

<pre><code>{
   ""deviceId"" : ""261e92b8-91af-40da-8ba4-c39d821472ec"",
   ""sensors"": [
   {
       ""fieldSensorId"": ""sensorID"",
       ""name"": ""sensorName"",
       ""location"": ""sensor location"",
       ""unit"": ""value units"",
       ""notes"": ""notes""
   },
   {
      ""fieldSensorId"": ""sensorID 2"",
      ""name"": ""sensorName 2"",
      ""location"": ""sensor location 2"",
      ""unit"": ""value units"",
      ""notes"": ""notes""
    }
  ]
}
</code></pre>

<p>CQL:</p>

<pre><code>CREATE TABLE device_sensors (
    device_id text,
    sensors list&lt;frozen &lt;map&lt;text,text&gt;&gt;&gt;,
    time timeuuid,
    PRIMARY KEY (device_id)
)
</code></pre>

<p>Still im not able to insert any data. What is the right way of storing such data in cassandra? Later i will need to query the sensors list
Is it maybe wiser to create a sensors table and use sensor > to reference the sensors?</p>
",<list><cassandra><storage>,"<p>I think that the problem is that you declare <code>devide_id</code> as <code>text</code> in CQL, but you have declared it<code>UUID</code> in the source code, and Spring maps it into corresponding type when trying to insert data.  Can you try to add <code>@CassandraType(type = Name.TEXT)</code> to the <code>deviceId</code> declaration.  You can also remove the <code>@Column</code> declaration - the <code>@PrimaryKeyColumn</code> should be enough.</p>

<p>Or you can change the table definition to declare <code>device_id</code>as <code>UUID</code>.</p>
",['table']
47658071,47658762,2017-12-05 16:16:05,Are Merkle trees generated from a single SSTable?,"<p>When Cassandra is doing the data integrity check, it does a validation compaction, but what does this mean exactly? My understanding is that it creates a single SSTable that will be stored temporarily (until the repair finishes), and then it generates the Merkle trees from that single created SSTable. If any of the Merkle trees leafs fails validation, then the partitions used to create that leaf (from the SSTable created during the validation compaction) will be streamed to the other node.
However, a friend told me that the Merkle trees are generated from each (previously existing) SSTable.</p>

<p>So, how many Merkle trees are generated, one or as many as SSTables?</p>
",<cassandra><merkle-tree>,"<p>The validation compaction iterates over all the sstables that are included in the range to build the merkle tree. It doesn't actually write a new sstable, but the compaction interfaces perform same type of task (iterating over data) so its reused. The compaction manager is also used for cleanup, secondary index rebuild, MV building, scrubbing, and verify processes.</p>

<p>A single merkle tree is generated. Each node of it represents a hash of all the data in a token range, each child of the node is half of its token range. The depth of the tree is dynamic, ideally the leaf represents 1 partition each but it could end up representing much more if the root node represents a wide range containing many partitions. Since the depth of the merkle tree is capped at 20 (or else it will be too large, and cause issues transferring) you generally dont want to repair a range that has much more than 2^20 or 1 million partitions in it. Can use getsplits or the size_estimates table to determine this when picking how to subdivide range for a subrange repair.</p>

<p>Worth noting, that a repair can kick off many sub repairs, each will have its own validation compaction/merkle tree/streaming session.</p>
",['table']
47712769,47715394,2017-12-08 10:47:49,Lagom persistentEntityRegistry.register doesn't work,"<p>I'm trying to configure my first Entity with Cassandra and Lagom. </p>

<p>I'm asking about how Lagom save the entity in Cassandra?</p>

<p>For example, this is my UserServiceImpl class:</p>

<pre><code>public class UserServiceImpl implements UserService {

    private final PersistentEntityRegistry persistentEntityRegistry;

    @Inject
    public UserServiceImpl(PersistentEntityRegistry persistentEntityRegistry) {
        this.persistentEntityRegistry = persistentEntityRegistry;
        persistentEntityRegistry.register(UserEntity.class);
    }

    @Override
    public ServiceCall&lt;NotUsed, String&gt; getUserInfo(String id) {
        return (request) -&gt; {
            // Look up the user entity for the given ID.
            PersistentEntityRef&lt;UserCommand&gt; ref = persistentEntityRegistry.refFor(UserEntity.class, id);
            // Ask the entity the Hello command.
            return ref.ask(new UserCommand.Hello(id, Optional.empty()));
        };
    }
}
</code></pre>

<p>So by executing:</p>

<blockquote>
  <p>persistentEntityRegistry.register(UserEntity.class);</p>
</blockquote>

<p>Should I have a user table into Cassandra? because I only have:</p>

<p><a href=""https://i.stack.imgur.com/VkRAY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VkRAY.png"" alt=""enter image description here""></a></p>

<p>I can't understand should I create the table user before starting my Lagom project or we only save the event?</p>

<p>Any help, please</p>
",<java><cassandra><lagom>,"<p>Lagom does not require a table for the entity because it's not based on an object-relational mapping or anything similar. Its persistence layer is based on the principles of Event Sourcing. </p>

<p>The messages table will contain the events that are emitted by your entity and its state is recovered each time you bring it back into memory. </p>

<p>Basically, the events are being saved in json format in the messages table. </p>

<p>We also have the concept of snapshots. The entity state may be saved as a snapshot (also using json). This happens after every 100 events and it's a small optimization to avoid to replay the events from scratch each time. </p>

<p>I'll try to explain shortly the whole mechanism.</p>

<p>A command is sent to an entity and events are persisted (that happens in the command handler). After the events are persisted, they are applied to the entity to mutate it (that happens on the event handler). </p>

<p>You restart the service and send a new command to that same entity. At that point the entity is not in memory, so Lagom will bring it into memory but before handling that new command it will replay the history of events for this entity in order to bring it back to the state it had when the system went down.</p>

<p>After that, the command is applied and new events are persisted.</p>

<p>After 100 events a snapshot of the entity will be saved and next time, when it's needed to replay that same entity, we load first the snapshot and then we apply the events that took place after the snapshot. So, in that case we don't need to replay the whole event history.</p>
",['table']
47812341,47818232,2017-12-14 11:32:55,Combine equal and smaller conditions on field of type text in Cassandra,"<p>I'm using Cassandra for storing data and I want to use EQUAL and NOT EQUALS query on it. The EQUAL (=) operator works fine. For the NOT EQUALS operator, I combine the result of &lt; and > operator on the field (I created custom secondary index on every field using SASI Index). I use SASI Index to support<br>
However, when I combine both operators = and &lt; in a query, Cassandra reject it (the result has no row). You can see the example queries below, it is easier to understand.
This is a sample table:</p>

<pre><code>CREATE TABLE test (
    id uuid,
    a int,
    b int,
    c varchar,
    d varchar,
    timestamp bigint,
    PRIMARY KEY (id)
);
</code></pre>

<p>The below queries return nothing:</p>

<pre><code>SELECT * FROM test WHERE a = 1 AND c &lt; '2' ALLOW FILTERING;
</code></pre>

<p>The below queries return correct result:</p>

<pre><code>SELECT * FROM test WHERE a &gt; 1 AND c = '2' ALLOW FILTERING;
SELECT * FROM test WHERE c &gt; '2' AND d &lt; '2' ALLOW FILTERING;
</code></pre>

<p>Also, I want to ORDER BY timestamp.
Can anyone explain why Cassandra behaves like that? And how can I design the database to support any combinations?
Thanks for help.</p>
",<database><cassandra><nosql>,"<p>I've said it before, and I'll say it again:</p>

<p><a href=""https://i.stack.imgur.com/0LF7T.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0LF7T.png"" alt=""ALLOW FILTERING is bad, bad, bad!""></a></p>

<p>And the same goes for secondary indexes.</p>

<p>Any time that you are not filtering your <code>WHERE</code> clause by the PRIMARY KEY, you are running what is known as a ""multi-key query.""  This is a known anti-pattern, because it cannot restrict itself to a partition, hence the query will need to reference multiple nodes to complete.  That is an approach that <strong>will not scale</strong>, and is therefore a bad idea to do with a <em>distributed</em> database.</p>

<p>In Cassandra you need to take a query-based modeling approach.  From what I can see, you have <code>id</code> defined as the lone PRIMARY KEY, but you're not querying by it.  Your PRIMARY KEY definition should really reflect the types of queries that you want your table to serve.</p>

<p>If that is a query you want to run, then your table definition will need to reflect that:</p>

<pre><code>CREATE TABLE test_by_a_and_c (
    id uuid,
    a int,
    b int,
    c varchar,
    d varchar,
    timestamp bigint,
    PRIMARY KEY (a,c,timestamp,id));
</code></pre>

<p>This will partition your data by <code>a</code>, and cluster it by <code>c</code>, <code>timestamp</code>, and <code>id</code>.  Using <code>c</code> as your first clustering key will allow your query to work.  Using <code>timestamp</code> as the second clustering key will allow your results to be sorted by timestamp.  <code>id</code> is added on the end to ensure uniqueness, which may or may not be necessary.  This definition will allow this query to work:</p>

<pre><code>SELECT * FROM test_by_a_and_c WHERE a = 1 AND c &lt; '2';
</code></pre>

<p>For the second query, you will need to create a <em>new</em> table which contains the same data, and adjust your PRIMARY KEY definition accordingly:</p>

<pre><code>    CREATE TABLE test_by_c_and_a (
    id uuid,
    a int,
    b int,
    c varchar,
    d varchar,
    timestamp bigint,
    PRIMARY KEY (c,a,timestamp,id));
</code></pre>

<p>This definitiuon will partition your data by <code>c</code>, and then it uses <code>a</code> as the first clustering key to satisfy your required query:</p>

<pre><code>SELECT * FROM test_by_c_and_a WHERE c = '2' AND a &gt; 1;
</code></pre>

<p>Cassandra guarantees that a partition is stored on a single node.  So by limiting your model with a good partition key, you are essentially ensuring that your query only has to go to one node to be solved, and that drastically reduces the amount of network time required.</p>

<blockquote>
  <p>Also, I want to ORDER BY timestamp. Can anyone explain why Cassandra
  behaves like that?</p>
</blockquote>

<p>Cassandra can only enforce a sort order within a partition key.  Clustering order is essentially how the data is written to disk.  When you do not specify a partition key in the <code>WHERE</code> clause, or when a query potentially covers multiple keys, the data is returned as it is read.  First, it'll be sorted by the hashed token value of the partition key.  Then, it will be by clustering key, relative to the current partition key.</p>

<p>For instance, let me take your first query table and <code>INSERT</code> some random data.  If I query without a <code>WHERE</code> clause, you can see how the hashed token value of the partition key comes into play:</p>

<p>cassdba@cqlsh:stackoverflow> SELECT token(a),a,b,c,d,timestamp FROM test_by_a_and_c ;</p>

<pre><code> system.token(a)      | a | b | c | d | timestamp
----------------------+---+---+---+---+---------------------
 -4069959284402364209 | 1 | 2 | 7 | 2 | 2017-12-14 16:33:55
 -4069959284402364209 | 1 | 1 | 8 | 1 | 2017-12-14 16:33:55
 -3248873570005575792 | 2 | 2 | 3 | 2 | 2017-12-14 16:33:55
 -3248873570005575792 | 2 | 1 | 9 | 1 | 2017-12-14 16:33:55
 -2729420104000364805 | 4 | 1 | 6 | 1 | 2017-12-14 16:33:56
 -2729420104000364805 | 4 | 2 | 7 | 2 | 2017-12-14 16:33:57
  9010454139840013625 | 3 | 2 | 4 | 2 | 2017-12-14 16:33:56
  9010454139840013625 | 3 | 1 | 5 | 1 | 2017-12-14 16:33:56

(8 rows)
</code></pre>

<p>Note that values for the partition key <code>a</code> are returned in 1,2,4,3 order because of their tokens.  Plus, the <code>timestamp</code> is really only relevant for sorting within each <code>a</code>.</p>

<blockquote>
  <p>And how can I design the database to support any combinations?</p>
</blockquote>

<p>You cannot.  For Cassandra, your queries <em>must</em> be known ahead of time. 
 Then your tables must be designed to serve those queries.  If you require a more-dynamic query model, then you'll need to use a data store that is better suited for that.</p>
",['table']
47881808,47891360,2017-12-19 07:19:55,Is Tracing On in cassandra the right choice to track the timetaken in Cassandra,"<p>When I try to execute the query on 500000 entries in a table, I could see that it is completed in 1200ms, But when I try to execute the query using TRACING ON enabled, I can see it is showing a long time in the Tracing log say 1850 ms. </p>

<p>So I would like to confirm whether the TRACING ON feature in Cassandra is a right choice for tracking time taken for executing queries? </p>
",<cassandra><benchmarking><cassandra-3.0>,"<p>There are metrics that will give you the amount of time spent on queries, you can most easily view it with <code>nodetool proxyhistograms</code> (doc) or grabbing directly from JMX. <code>TRACING ON</code> is for debugging <em>why</em> a request is slow. Its important to note that this is <strong>very</strong> expensive (and possibly adds time to query, although most tracing is async) and should be avoided outside of debugging issues. </p>

<p>You can also use <code>nodetool settraceprobability</code> to globally record some % of the queries, which you can then look at and maybe process with some tooling the events and sessions table in the <code>system_traces</code> keyspace.</p>
",['table']
47930616,47933077,2017-12-21 18:21:07,difference between metrics for cassandra_columnfamily_* cassandra_table_*,"<p>I'm trying to setup alerts for apache cassandra. using jmx exporter to expose the metrics and prometheus to pull the metrics and grafana will be display the graphs. while looking for the metrics i see it has both cassandra_columnfamily_readlatency and cassandra_table_readlatency. does both of them have any difference. can some one point me to any related documentation </p>
",<cassandra><jmx><grafana><prometheus>,"<p>The table is the new one. It used to be called columnfamily (from google bigtable paper) but was renamed. To maintain backwards compatibility with older tools the column family alias was created. Its the same metric but in some versions the alias of some of the metrics is missing (a bug) so the table metrics has more.</p>

<p>Use table to future proof app. columnfamily deprecated.</p>
",['table']
48004933,48005731,2017-12-28 09:33:56,Cassandra - What is difference between TTL at table and inserting data with TTL,"<p>I have a Cassandra 2.1 cluster where we insert data though Java with TTL as the requirement of persisting the data is 30 days.
But this causes problem as the files with old data with tombstones is kept on the disk. This results in disk space being occupied by data which is not required. Repairs take a lot of time to clear this data (upto 3 days on a single node)
Is there a better way to delete the data?</p>

<p>I have come across this on datastax</p>

<blockquote>
  <p>Cassandra allows you to set a default_time_to_live property for an entire table. Columns and rows marked with regular TTLs are processed as described above; but when a record exceeds the table-level TTL, Cassandra deletes it immediately, without tombstoning or compaction. <a href=""https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlAboutDeletes.html?hl=tombstone"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlAboutDeletes.html?hl=tombstone</a></p>
</blockquote>

<p>Will the data be deleted more efficiently if I set TTL at table level instead of setting each time while inserting.
Also, documentation is for Cassandra 3, so will I have to upgrade to newer version to get any benefits?</p>
",<cassandra><cassandra-3.0><cassandra-2.1>,"<p>Setting <code>default_time_to_live</code> applies the default ttl to all rows and columns in your table - and if no individual ttl is set (and cassandra has correct ntp time on all nodes), cassandra can easily drop those data safely. </p>

<p>But keep some things in mind: your application is still able so set a specific ttl for a single row in your table - then normal processing will apply. On top, even if the data is ttled it won't get deleted immediately - sstables are still immutable, but tombstones will be dropped during compaction. </p>

<p>What could help you really a lot - just guessing - would be an appropriate compaction strategy: </p>

<p><a href=""http://docs.datastax.com/en/archived/cassandra/3.x/cassandra/dml/dmlHowDataMaintain.html#dmlHowDataMaintain__twcs-compaction"" rel=""noreferrer"">http://docs.datastax.com/en/archived/cassandra/3.x/cassandra/dml/dmlHowDataMaintain.html#dmlHowDataMaintain__twcs-compaction</a> </p>

<p>TimeWindowCompactionStrategy (TWCS) 
Recommended for time series and expiring TTL workloads.</p>

<blockquote>
  <p>The TimeWindowCompactionStrategy (TWCS) is similar to DTCS with
  simpler settings. TWCS groups SSTables using a series of time windows.
  During compaction, TWCS applies STCS to uncompacted SSTables in the
  most recent time window. At the end of a time window, TWCS compacts
  all SSTables that fall into that time window into a single SSTable
  based on the SSTable maximum timestamp. Once the major compaction for
  a time window is completed, no further compaction of the data will
  ever occur. The process starts over with the SSTables written in the
  next time window.</p>
</blockquote>

<p>This help a lot - when choosing your time windows correctly. All data in the last compacted sstable will have roughly equal ttl values (hint: don't do out-of-order inserts or manual ttls!). Cassandra keeps the youngest ttl value in the sstable metadata and when that time has passed cassandra simply deletes the entire table as all data is now obsolete. No need for compaction. </p>

<p>How do you run your repair? Incremental? Full? Reaper? How big in terms of nodes and data is your cluster?</p>
",['table']
48049918,48050933,2018-01-01 13:07:46,How to design table about such kind of request?,"<p>I am trying to use Cassandra as a message system's db. I keep all messages into a table that keeps all required data I will query later.
Now I have a request that I have to show a user's latest 10 chat room's lastest 100 messages. The chat room need order by the lasted access time.</p>

<p>I plan to create a table that let me get lasted 10 chatroom and ordered by their access time.</p>

<p>But I can't figure out how to design table. I currently do such request by creating a table with </p>

<p>user, chatroom, access_time. And using (user, chatroom) as primary key.</p>

<p>Then I update the access_time each time I received the message.
So I may have a table like this</p>

<pre><code>user     chatroom     time
1           2          1005
2           1          1000
1           3          1003 
1           4          1004.
</code></pre>

<p>It's easy for me to get user 1's list as </p>

<pre><code>1, 2, 1005,
1, 3, 1003,
1, 4, 1004.
</code></pre>

<p>The I reorder by myself and got</p>

<pre><code>1, 3, 1003.
1, 4, 1004.
1, 2, 1005.
</code></pre>

<p>That what I need of the latest result. I need the [3, 4, 2] as the final result.
But I can't figure out a good way about how to do the sort part inside the Cassandra. 
Are there better design for the Table? 
In general, the chatroom should not be a big list. So the sorting time should not be a problem even if I sorted by myself.</p>

<p>Thank you~~</p>

<p>Eric</p>

<p>I did try the comment from Mandraenke. But My input is like:</p>

<pre><code>INSERT INTO room_access (user, when, room) VALUES ( 'demo1', toTimeStamp(now()), 'room_1');
INSERT INTO room_access (user, when, room) VALUES ( 'demo1', toTimeStamp(now()), 'room_1');
INSERT INTO room_access (user, when, room) VALUES ( 'demo1', toTimeStamp(now()), 'room_2');
INSERT INTO room_access (user, when, room) VALUES ( 'demo2', toTimeStamp(now()), 'room_4');
INSERT INTO room_access (user, when, room) VALUES ( 'demo3', toTimeStamp(now()), 'room_1');
INSERT INTO room_access (user, when, room) VALUES ( 'demo1', toTimeStamp(now()), 'room_2');
INSERT INTO room_access (user, when, room) VALUES ( 'demo2', toTimeStamp(now()), 'room_4');
INSERT INTO room_access (user, when, room) VALUES ( 'demo3', toTimeStamp(now()), 'room_3');
INSERT INTO room_access (user, when, room) VALUES ( 'demo1', toTimeStamp(now()), 'room_4');
INSERT INTO room_access (user, when, room) VALUES ( 'demo4', toTimeStamp(now()), 'room_2');
INSERT INTO room_access (user, when, room) VALUES ( 'demo5', toTimeStamp(now()), 'room_1');
INSERT INTO room_access (user, when, room) VALUES ( 'demo1', toTimeStamp(now()), 'room_3');
INSERT INTO room_access (user, when, room) VALUES ( 'demo7', toTimeStamp(now()), 'room_4');
INSERT INTO room_access (user, when, room) VALUES ( 'demo6', toTimeStamp(now()), 'room_2');
INSERT INTO room_access (user, when, room) VALUES ( 'demo8', toTimeStamp(now()), 'room_5');
INSERT INTO room_access (user, when, room) VALUES ( 'demo9', toTimeStamp(now()), 'room_1');
INSERT INTO room_access (user, when, room) VALUES ( 'demo1', toTimeStamp(now()), 'room_7');
INSERT INTO room_access (user, when, room) VALUES ( 'demo2', toTimeStamp(now()), 'room_2');
INSERT INTO room_access (user, when, room) VALUES ( 'demo3', toTimeStamp(now()), 'room_1');
INSERT INTO room_access (user, when, room) VALUES ( 'demo5', toTimeStamp(now()), 'room_3');
INSERT INTO room_access (user, when, room) VALUES ( 'demo6', toTimeStamp(now()), 'room_2');
INSERT INTO room_access (user, when, room) VALUES ( 'demo7', toTimeStamp(now()), 'room_2');
INSERT INTO room_access (user, when, room) VALUES ( 'demo2', toTimeStamp(now()), 'room_1');
INSERT INTO room_access (user, when, room) VALUES ( 'demo1', toTimeStamp(now()), 'room_1');
INSERT INTO room_access (user, when, room) VALUES ( 'demo3', toTimeStamp(now()), 'room_2');
INSERT INTO room_access (user, when, room) VALUES ( 'demo1', toTimeStamp(now()), 'room_3');
INSERT INTO room_access (user, when, room) VALUES ( 'demo7', toTimeStamp(now()), 'room_2');
INSERT INTO room_access (user, when, room) VALUES ( 'demo1', toTimeStamp(now()), 'room_1');
INSERT INTO room_access (user, when, room) VALUES ( 'demo6', toTimeStamp(now()), 'room_5');
INSERT INTO room_access (user, when, room) VALUES ( 'demo1', toTimeStamp(now()), 'room_8');
INSERT INTO room_access (user, when, room) VALUES ( 'demo2', toTimeStamp(now()), 'room_3');
INSERT INTO room_access (user, when, room) VALUES ( 'demo3', toTimeStamp(now()), 'room_2');
INSERT INTO room_access (user, when, room) VALUES ( 'demo1', toTimeStamp(now()), 'room_1');
INSERT INTO room_access (user, when, room) VALUES ( 'demo5', toTimeStamp(now()), 'room_3');
INSERT INTO room_access (user, when, room) VALUES ( 'demo6', toTimeStamp(now()), 'room_4');
INSERT INTO room_access (user, when, room) VALUES ( 'demo1', toTimeStamp(now()), 'room_1');
INSERT INTO room_access (user, when, room) VALUES ( 'demo7', toTimeStamp(now()), 'room_3');
INSERT INTO room_access (user, when, room) VALUES ( 'demo1', toTimeStamp(now()), 'room_2');
INSERT INTO room_access (user, when, room) VALUES ( 'demo2', toTimeStamp(now()), 'room_1');
INSERT INTO room_access (user, when, room) VALUES ( 'demo3', toTimeStamp(now()), 'room_2');
INSERT INTO room_access (user, when, room) VALUES ( 'demo6', toTimeStamp(now()), 'room_3');
INSERT INTO room_access (user, when, room) VALUES ( 'demo6', toTimeStamp(now()), 'room_1');
INSERT INTO room_access (user, when, room) VALUES ( 'demo2', toTimeStamp(now()), 'room_1');
INSERT INTO room_access (user, when, room) VALUES ( 'demo2', toTimeStamp(now()), 'room_1');
INSERT INTO room_access (user, when, room) VALUES ( 'demo2', toTimeStamp(now()), 'room_1');
INSERT INTO room_access (user, when, room) VALUES ( 'demo2', toTimeStamp(now()), 'room_2');
INSERT INTO room_access (user, when, room) VALUES ( 'demo2', toTimeStamp(now()), 'room_3');
INSERT INTO room_access (user, when, room) VALUES ( 'demo1', toTimeStamp(now()), 'room_1');
INSERT INTO room_access (user, when, room) VALUES ( 'demo4', toTimeStamp(now()), 'room_2');
INSERT INTO room_access (user, when, room) VALUES ( 'demo6', toTimeStamp(now()), 'room_2');
INSERT INTO room_access (user, when, room) VALUES ( 'demo2', toTimeStamp(now()), 'room_3');
INSERT INTO room_access (user, when, room) VALUES ( 'demo1', toTimeStamp(now()), 'room_2');
INSERT INTO room_access (user, when, room) VALUES ( 'demo1', toTimeStamp(now()), 'room_2');
INSERT INTO room_access (user, when, room) VALUES ( 'demo1', toTimeStamp(now()), 'room_1');
INSERT INTO room_access (user, when, room) VALUES ( 'demo1', toTimeStamp(now()), 'room_3');
INSERT INTO room_access (user, when, room) VALUES ( 'demo2', toTimeStamp(now()), 'room_2');
INSERT INTO room_access (user, when, room) VALUES ( 'demo3', toTimeStamp(now()), 'room_1');
</code></pre>

<p>The result of my query:</p>

<pre><code>select * from room_access where user='demo1';
</code></pre>

<p>will be </p>

<pre><code> user  | when                            | room
-------+---------------------------------+--------
 demo1 | 2018-01-01 18:17:27.886000+0000 | room_3
 demo1 | 2018-01-01 18:17:27.865000+0000 | room_1
 demo1 | 2018-01-01 18:17:27.850000+0000 | room_2
 demo1 | 2018-01-01 18:17:27.831000+0000 | room_2
 demo1 | 2018-01-01 18:17:27.738000+0000 | room_1
 demo1 | 2018-01-01 18:17:27.594000+0000 | room_2
 demo1 | 2018-01-01 18:17:27.556000+0000 | room_1
 demo1 | 2018-01-01 18:17:27.513000+0000 | room_1
 demo1 | 2018-01-01 18:17:27.456000+0000 | room_8
 demo1 | 2018-01-01 18:17:27.430000+0000 | room_1
 demo1 | 2018-01-01 18:17:27.404000+0000 | room_3
 demo1 | 2018-01-01 18:17:27.386000+0000 | room_1
 demo1 | 2018-01-01 18:17:27.202000+0000 | room_7
 demo1 | 2018-01-01 18:17:27.136000+0000 | room_3
 demo1 | 2018-01-01 18:17:27.088000+0000 | room_4
 demo1 | 2018-01-01 18:17:27.020000+0000 | room_2
 demo1 | 2018-01-01 18:17:26.964000+0000 | room_2
 demo1 | 2018-01-01 18:17:26.947000+0000 | room_1
 demo1 | 2018-01-01 18:14:47.267000+0000 | room_1
</code></pre>

<p>Dues to the access time modify frequently, the actual result will be more than thousands records (There are only 19 records in the sample.) but the actual result I want is</p>

<pre><code> user  | when                            | room
-------+---------------------------------+--------
 demo1 | 2018-01-01 18:17:27.886000+0000 | room_3
 demo1 | 2018-01-01 18:17:27.865000+0000 | room_1
 demo1 | 2018-01-01 18:17:27.850000+0000 | room_2
 demo1 | 2018-01-01 18:17:27.456000+0000 | room_8
 demo1 | 2018-01-01 18:17:27.202000+0000 | room_7
 demo1 | 2018-01-01 18:17:27.088000+0000 | room_4
</code></pre>

<p>It seems I still need additional filtering to get what I want.
So..Are there a simple way to do the filtering I want also? </p>
",<cassandra><cassandra-3.0>,"<p>In Cassandra you almost always denormalize data to fit your queries. For your question just track access times for example as follows:</p>

<pre><code>cqlsh:demo&gt; create table room_access ( user text, when timestamp, room text, primary key (user, when) ) WITH CLUSTERING ORDER BY (when DESC);
</code></pre>

<p>The only purpose of that table is to track last access times per user. Note the clustering column and the descending order:</p>

<p>Lets insert some data:</p>

<pre><code>cqlsh:demo&gt; INSERT INTO room_access (user, when, room) VALUES ( 'demo', toTimeStamp(now()), 'room_a');
cqlsh:demo&gt; INSERT INTO room_access (user, when, room) VALUES ( 'demo', toTimeStamp(now()), 'room_b');
cqlsh:demo&gt; INSERT INTO room_access (user, when, room) VALUES ( 'demo', toTimeStamp(now()), 'room_c');
cqlsh:demo&gt; INSERT INTO room_access (user, when, room) VALUES ( 'demo', toTimeStamp(now()), 'room_d');
cqlsh:demo&gt; INSERT INTO room_access (user, when, room) VALUES ( 'demo', toTimeStamp(now()), 'room_e');
cqlsh:demo&gt; INSERT INTO room_access (user, when, room) VALUES ( 'demo', toTimeStamp(now()), 'room_f');
cqlsh:demo&gt; INSERT INTO room_access (user, when, room) VALUES ( 'demo', toTimeStamp(now()), 'room_g');
cqlsh:demo&gt; INSERT INTO room_access (user, when, room) VALUES ( 'demo', toTimeStamp(now()), 'room_h');
cqlsh:demo&gt; INSERT INTO room_access (user, when, room) VALUES ( 'demo', toTimeStamp(now()), 'room_i');
cqlsh:demo&gt; INSERT INTO room_access (user, when, room) VALUES ( 'demo', toTimeStamp(now()), 'room_j');
cqlsh:demo&gt; INSERT INTO room_access (user, when, room) VALUES ( 'demo', toTimeStamp(now()), 'room_k');
cqlsh:demo&gt; INSERT INTO room_access (user, when, room) VALUES ( 'demo', toTimeStamp(now()), 'room_l');
cqlsh:demo&gt; INSERT INTO room_access (user, when, room) VALUES ( 'demo', toTimeStamp(now()), 'room_b');
cqlsh:demo&gt; INSERT INTO room_access (user, when, room) VALUES ( 'demo', toTimeStamp(now()), 'room_a');
</code></pre>

<p>Now you can select the last ten rooms:</p>

<pre><code>cqlsh:demo&gt; SELECT room FROM room_access WHERE user='demo' limit 10;

 room
--------
 room_a
 room_b
 room_l
 room_k
 room_j
 room_i
 room_h
 room_g
 room_f
 room_e

(10 rows)
cqlsh:demo&gt;
</code></pre>
",['table']
48055455,48056017,2018-01-02 04:03:52,"Cassandra, query the primary key, skipping the clustering column (timeuuid)","<p>Please am still new using Cassandra i have an issue which i want to solve yet have tried but i couldn't. </p>

<p>I want to get all users who like a particular post and all the posts that a particular user likes and I have a like and unlike button, when the user click like button on a post i save the information to these tables, which is working but when the user click the unlike button how do i delete that particular row from these table below</p>

<pre><code>CREATE TABLE social.post_likedby_user (
   userid bigint,
   timeuuid timestamp,
   postid bigint,
   content text,
   creation_date text,
   liked boolean,
   PRIMARY KEY (userid, timeuuid, postid)
)  WITH CLUSTERING ORDER BY (timeuuid DESC);

CREATE TABLE social.user_likeby_post (
   postid bigint,
   timeuuid timestamp,
   userid bigint,
   name text,
   username text,
   email text,
   phone text,
   birthday text,
   PRIMARY KEY (postid, timeuuid, userid)
)  WITH CLUSTERING ORDER BY (timeuuid DESC);
</code></pre>

<p>I have tried several query yet no result</p>

<pre><code>DELETE FROM post_likedby_user WHERE userid = ? AND postid = ?
DELETE FROM user_likeby_post WHERE postid = ? AND userid = ?
</code></pre>

<p>How can i query this table without knowing the timeuuid. I think the problem is the timeuuid column and i don't want to use ALLOW FILTERING since i read about it that is not good for production</p>
",<database><cassandra><nosql><datastax>,"<p>In this case we need one more table post_timeuuids:</p>

<pre><code>CREATE TABLE social.post_timeuuids (
   userid bigint,
   postid bigint,
   timeuuid timestamp,
   PRIMARY KEY (userid, postid, timeuuid)
) 
</code></pre>

<p>So to query timeuuid values you run query below:</p>

<pre><code>select timeuuid from post_timeuuids where userid = ? and postid = ?;
</code></pre>

<p>now you can use these values to run delete queries:</p>

<pre><code>DELETE FROM post_likedby_user WHERE userid = ? AND timeuuid = ? AND postid = ? 
</code></pre>

<p>OR as an alternative instead of deleting you can just delete from ""post_timeuuids"" table. On application side you should consider data in ""post_timeuuids"": if no timestamps present there it means that uses don't have ""likes"" for that particular post.</p>

<p>Article below explain why you can't run your queries for post_likedby_user without your 'timeuuid' clustering key </p>

<p><a href=""https://rollerweblogger.org/roller/entry/composite_keys_in_cassandra"" rel=""nofollow noreferrer"">https://rollerweblogger.org/roller/entry/composite_keys_in_cassandra</a></p>
",['table']
48056093,48056759,2018-01-02 05:45:40,High Availability in Cassandra,"<p>1) I have <strong>5 node cluster (172.30.56.60, 172.30.56.61, 172.30.56.62, 172.30.56.63, 172.30.56.129)</strong> </p>

<p>2) I created a keyspace with <strong>Replication Factor as 3</strong><br>
<strong>write consistency as 3</strong>, I have inserted a row in a table with the partition as '1' like below,</p>

<p>INSERT INTO user (user_id, user_name, user_phone) VALUES(1,'ram', 9003934069);</p>

<p>3) I verified the location of the data using the nodetool getendpoints utility and observed that the data is copied in three nodes 60, 129 and 62.</p>

<pre><code>./nodetool getendpoints keyspacetest user 1
172.30.56.60
172.30.36.129
172.30.56.62
</code></pre>

<p>4) Now If I bring down the node 60, Cassandra needs to transfer the existing data to <code>'1,'ram', 9003934069'</code> to the remaining node (to either 61 or 63) to maintain the RF as '3'?</p>

<p>But Cassandra is not doing that, so does it mean that If the nodes 60, 129 and 62 are down I will not be able to read / write any data under the partition '1' in the table 'user' ?</p>

<p>Ques 1 : <strong>So even If I have 5 node cluster, If the data / partiton where it resides goes down, the cluster is useless?</strong> </p>

<p>Ques 2 : If two nodes are down (Example : 60 and 129 is down) still 61,62 and 63 are up and running, but I am not able to write any data in the partition '1' with the write consistency = 3, Why it is so? Where as I am able to write the data with the write consistency = 1 so this again says the data for the partition will be available only in the predefined nodes in cluster, No possibility for repartitioning? </p>

<p>If any part of my question is not clear, Please let me know, I would like to clarify it. </p>
",<java><cassandra><cassandra-3.0><high-availability>,"<blockquote>
  <p>4) Now If I bring down the node 60, Cassandra needs to transfer the
  existing data to '1,'ram', 9003934069' to the remaining node (to
  either 61 or 63) to maintain the RF as '3'?</p>
</blockquote>

<p>That is not the way Cassandra works - replication factor 'only' declares how many copies of your data is to be stored Cassandra on disk on different nodes. Cassandra mathematically forms a ring out of your nodes. Each node is responsible for a range of so called tokens (which are basically a hash of your partition key components). Your replication factor of three means that data will be stored on the node taking care of your datas token and the next two nodes in the ring. </p>

<p>(quick googled image <a href=""https://image.slidesharecdn.com/cassandratraining-161104131405/95/cassandra-training-19-638.jpg?cb=1478265472"" rel=""nofollow noreferrer"">https://image.slidesharecdn.com/cassandratraining-161104131405/95/cassandra-training-19-638.jpg?cb=1478265472</a>) </p>

<p>Changing the ring topology is quite complex and not done automatically at all. </p>

<p>1) I have 5 node cluster (172.30.56.60, 172.30.56.61, 172.30.56.62, 172.30.56.63, 172.30.56.129)</p>

<p>2) I created a keyspace with Replication Factor as 3
write consistency as 3, I have inserted a row in a table with the partition as '1' like below,</p>

<p>INSERT INTO user (user_id, user_name, user_phone) VALUES(1,'ram', 9003934069);</p>

<p>3) I verified the location of the data using the nodetool getendpoints utility and observed that the data is copied in three nodes 60, 129 and 62.</p>

<p>./nodetool getendpoints keyspacetest user 1
172.30.56.60
172.30.36.129
172.30.56.62
4) Now If I bring down the node 60, Cassandra needs to transfer the existing data to '1,'ram', 9003934069' to the remaining node (to either 61 or 63) to maintain the RF as '3'?</p>

<p>But Cassandra is not doing that, so does it mean that If the nodes 60, 129 and 62 are down I will not be able to read / write any data under the partition '1' in the table 'user' ?</p>

<blockquote>
  <p>Ques 1 : So even If I have 5 node cluster, If the data / partiton
  where it resides goes down, the cluster is useless?</p>
</blockquote>

<p>No. On the other hand there is the consistency level - where you define how many nodes must acknowledge your write and read request before it is considered successful. Above you also took CL=3 and RF=3 - that means all nodes holding replicas have to respond and need to be online. If a single one is down your requests will fail all the time (if your cluster was bigger, say 6 nodes, chances are that the three online may be the 'right' ones for some writes). </p>

<p>But Cassandra has tuneable consistency (see the docs at <a href=""http://docs.datastax.com/en/archived/cassandra/2.0/cassandra/dml/dml_config_consistency_c.html"" rel=""nofollow noreferrer"">http://docs.datastax.com/en/archived/cassandra/2.0/cassandra/dml/dml_config_consistency_c.html</a>).</p>

<p>You could pick QUORUM for example. Then (replication factor/2)+1 nodes are needed for queries. In your case (3/2)+1=1+1=2 nodes. QUORUM is perfect if you really need consistent data as in any case at least one node participating in your request will overlap between write and read and have the latest data. Now one node can be down and every thing will still work. </p>

<p>BUT: </p>

<blockquote>
  <p>Ques 2 : If two nodes are down (Example : 60 and 129 is down) still
  61,62 and 63 are up and running, but I am not able to write any data
  in the partition '1' with the write consistency = 3, Why it is so?
  Where as I am able to write the data with the write consistency = 1 so
  this again says the data for the partition will be available only in
  the predefined nodes in cluster, No possibility for repartitioning?</p>
</blockquote>

<p>Look above - that's the explanation. CL=1 for write consistency will succeed because one node is still online and you request only one to acknowledge your write. </p>

<p>Of course replication factor is not useless at all. Writes will replicate to all nodes available even if a lower consistency level is choosen, but you will not have to wait for it on client side. If a node is down for some short period (default 3 hours) of time the coordinator will store the missed writes and replay them if the node comes up again and your data is fully replicated again. </p>

<p>If a node is down for a longer period of time it is necessary to run <code>nodetool repair</code> and let the cluster rebuild a consistent state. That should be done on a regular schedule anyway as maintenance task to keep your cluster healty - there could be missed writes because of network/load issues and there are tombstones from deletes with could be a pain. </p>

<p>And you can remove or add nodes to your cluster (if doing so, just add one at a time) and Cassandra will repartition your ring for you. </p>

<p>In case of removing an online node can stream the data on it to the others, an offline node can be removed but the data on it will not have sufficient replicas so a <code>nodetool repair</code> must be run. </p>

<p>Adding nodes will assign new token ranges to the new node and automatically stream data to your new node. But existing data is not deleted for the source nodes for you (keeps you safe), so after adding nodes <code>nodetool cleanup</code> is your friend.</p>

<p>Cassandra chooses to be A(vailable) and P(artition tolerant) from CAP theorem. 
(see <a href=""https://en.wikipedia.org/wiki/CAP_theorem"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/CAP_theorem</a>). So you can't have consistency at any time - but QUORUM will often be more than enough.</p>

<p>Keep your nodes monitored and don't be too afraid of node failure - it simply happens all the time disks die or network is lost but design your applications for it. </p>

<p>Update: It's up to the user to choose what can happen to your cluster before you are loosing data or queries. If needed you can go with higher replication factors (RF=7 and CL.QUROUM tolerates loss of 3) and/or even with multiple datacenters on different locations in case one loses an entire datacenter (which happens in real life, think of network loss). </p>

<hr>

<p>For the comment below regarding <a href=""https://www.ecyrd.com/cassandracalculator/"" rel=""nofollow noreferrer"">https://www.ecyrd.com/cassandracalculator/</a>: </p>

<p>Cluster size 3
Replication Factor 2
Write Level 2<br>
Read Level 1  </p>

<p><em>Your reads are consistent</em>: Sure, you request writes need to be ack'd by all replicas. </p>

<p><em>You can survive the loss of no nodes without impacting the application</em>: See above, RF=2 und WC=2 request that at any time all nodes need to respond to writes. So for writes your application WILL be impacted, for reads one node can be down.</p>

<p><em>You can survive the loss of 1 node without data loss</em>: as data is written to 2 replicas and you only read from one if one node is down you can still read from the other one. </p>

<p><em>You are really reading from 1 node every time</em>: RC=1 requests your read to be served by one replica - so the frist one that ack's the read will do, if one node is down that won't matter as the other one can ack your read. </p>

<p><em>You are really writing to 2 nodes every time</em>: WC=2 requests that every write will be ack'd by two replicas - which is also the number of replicas in your example. So all nodes need to be online when writing data. </p>

<p><em>Each node holds 67% of your data</em>: Just some math ;)</p>

<p>With those settings you can't survive a node loss without impact while writing to your cluster. But your data is written to disk on two replicas - so if you lose one you still have your data on the other one and recover from a dead node. </p>
",['table']
48060940,48071599,2018-01-02 12:22:27,Unable to run cqlsh(connection refused),"<p>I'm getting a connection error ""unable to connect to any server"" when I run .cqlsh command from the bin directory of my node. </p>

<p>I'm using an edited yaml file containing only the following(rest all values present in the default yaml have been omitted) :</p>

<p>cluster name, num tokens,  partitioner, data file directories,  commitlog directory, commitlog sync, commitlog sync period, saved cache directory, seed provider info,  listen address and endpoint snitch. </p>

<p>Is this error because I've not included some important parameter in the yaml like rpc address? Please help. </p>

<p>OS: RHEL 6.9
Cassandra: 3.0.14</p>
",<cassandra><cassandra-3.0>,"<ol>
<li>The cassandra yaml file can have modified values, but you should not delete the rows and make your own yaml file. And yes, rpc address is needed in yaml file.</li>
<li><p>In writing the directories like data_file_directories, you should follow the same indentation as:</p>

<pre><code> data_file_directories - 
      /path/to/access
</code></pre></li>
</ol>

<p>Cassandra is very strict at it's indentation in yaml file. I once faced an issue due to this wrong indentation in data_file_directories.</p>

<ol start=""3"">
<li>Finally, run ./cqlsh , provide ip_address if it is a remote server.</li>
<li>Check the nodetool status and confirm whether the node is up and normal.</li>
</ol>
",['data_file_directories']
48118171,48119842,2018-01-05 17:08:27,Avoiding filtering with a compound partition key in Cassandra,"<p>I am fairly new to Cassandra and currently have to following table in Cassandra:</p>

<pre><code>CREATE TABLE time_data (
id int,
secondary_id int,
timestamp timestamp,
value bigint,
PRIMARY KEY ((id, secondary_id), timestamp)
);
</code></pre>

<p>The compound partition key (with <code>secondary_id</code>) is necessary in order to not violate max partition sizes.</p>

<p>The issue I am running in to is that I would like to complete the query <code>SELECT * FROM time_data WHERE id = ?</code>.  Because the table has a compound partition key, this query requires filtering.  I realize this is a querying a lot of data and partitions, but it is necessary for the application.  For reference, <code>id</code> has relatively low cardinality and <code>secondary_id</code> has high cardinality.</p>

<p>What is the best way around this?  Should I simply allow filtering on the query?  Or is it better to create a secondary index like <code>CREATE INDEX id_idx ON time_data (id)</code>?</p>
",<cassandra><cql><cassandra-3.0>,"<p>You will need to specify full partition key on queries (ALLOW FILTERING will impact performance badly in most cases). </p>

<p>One way to go could be if you know all secondary_id (you could add a table to track them in necessary) and do the job in your application and query all (id, secondary_id) pairs and process them afterwards. This has the disadvantage of beeing more complex but the advantage that it can be done with async queries and in parallel so many nodes in your cluster participate in processing your task.</p>

<p>See also <a href=""https://www.datastax.com/dev/blog/java-driver-async-queries"" rel=""nofollow noreferrer"">https://www.datastax.com/dev/blog/java-driver-async-queries</a></p>
",['table']
48150998,48164173,2018-01-08 13:10:15,Cassandra: Data skewness,"<p>We have a 10-node Cassandra cluster. The data size looks very skewed on a few nodes. The cluster is setup using vnodes (32).</p>

<p>I am looking for suggestions on how to figure out why is there data skewness and how to balance the cluster. </p>

<p>Nodetool status output</p>

<pre><code>Datacenter: us-west-2_central
=============================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address        Load       Tokens  Owns    Host ID                               Rack
UN  172.20.111.20  832.35 GB  32      ?       a3e94d59-5897-46a5-afe7-b695b0179461  2b
UN  172.20.111.16  895.15 GB  32      ?       7e2f7726-3b98-4535-b44d-c522f7a0d4e6  2b
UN  172.20.110.17  1.39 TB    32      ?       925dc818-04d2-4dfa-a54b-6e0b4098fd76  2a
UN  172.20.111.18  861.17 GB  32      ?       acc23bfe-33ee-41b6-b78e-1d813697c9b1  2b
UN  172.20.110.19  835.67 GB  32      ?       5e2fd25a-5bbd-42c2-a627-a134c0e20c8b  2a
UN  172.20.110.13  932.74 GB  32      ?       d7534422-de99-49cb-a026-f3fa5ff8c16b  2a
UN  172.20.111.14  782.67 GB  32      ?       80802902-fd4a-4271-8dd9-d7421d823bd2  2b
UN  172.20.110.15  1.03 TB    32      ?       82535a06-30ee-4e7a-9e1e-ca6d33648ce8  2a
UN  172.20.110.10  697.58 GB  32      ?       bcf265b1-6e82-4d12-9e7c-2b769ccf1d3a  2a
UN  172.20.111.11  786.71 GB  32      ?       d34bcb42-f061-49d9-b4b1-41ae140d868b  2b
</code></pre>

<p>You can see there is a node with 1.39 TB  and other node with 1.03 TB. </p>
",<cassandra>,"<p>There may be different causes for this problem. For example your data model may be the reason for skewness. If you have a table that has small number of very wide partitions (millions of rows per partition) it may be the cause of this problem. These kind of partitions may end up in particular replicas, which may cause skewness.</p>

<p>Also, this skewness may be totally by chance. I mean, the token allocation between nodes may end up with this skewness all by chance. If this is the scenario, and if the problem gets bigger over time, you may need to replace the highly-loaded nodes with new ones. </p>

<p>Take a look at this article: <a href=""https://www.datastax.com/dev/blog/token-allocation-algorithm"" rel=""nofollow noreferrer"">https://www.datastax.com/dev/blog/token-allocation-algorithm</a></p>
",['table']
48229184,48298116,2018-01-12 15:22:18,Cassandra DB: Parsing an Instant from Cassandra's Timestamp in Json fails,"<p>Lets say I've an one-column-table like this:</p>

<pre><code>CREATE TABLE test (
id int,
time timestamp,
PRIMARY KEY(id)
);
</code></pre>

<p>Now I insert data per Json:</p>

<pre><code>INSERT INTO test JSON '{""id"":1,""time"":""2018-01-12T15:06:02.753Z""}'
</code></pre>

<p>The result would look like this:</p>

<pre><code>id        time
1         2018-01-12 15:06:02.753+0000
</code></pre>

<p>Now I select the entry again as Json:</p>

<pre><code>SELECT JSON * from test where id=1;
</code></pre>

<p>The resulting Json would look like this:</p>

<pre><code>{  
   ""id"":1,
   ""time"":""2018-01-12 15:06:02.753+0000""
}
</code></pre>

<p>If I now want to parse the field ""time"" in the json above to an Instant, it fails because of a whitespace:</p>

<pre><code>Exception in thread ""main"" java.time.format.DateTimeParseException: Text '2018-01-12 10:23:00.461Z' could not be parsed at index 10
</code></pre>

<p>I could replace the whitespace-Charakter with a 'T'-Delimiter, but is there an better way to parse an Instant?</p>
",<java><database><parsing><cassandra><timestamp>,"<p>Datastax Java driver supports table to object mapping out-of-the box. You don't have to write the code by yourself. </p>

<p>Timestamps are automatically mapped to <code>java.util.Date</code> objects by Java driver. See some related documentation below:</p>

<ul>
<li><a href=""https://docs.datastax.com/en/developer/java-driver/3.1/manual/object_mapper/"" rel=""nofollow noreferrer"">Documentation of Object mapper for Java
driver</a></li>
<li><a href=""https://docs.datastax.com/en/developer/java-driver/3.1/manual/object_mapper/creating/"" rel=""nofollow noreferrer"">Some code examples on how to map Java objects to tables</a></li>
<li><a href=""https://docs.datastax.com/en/developer/java-driver/3.1/manual/"" rel=""nofollow noreferrer"">Mapping between Java types and CQL
types</a></li>
</ul>
",['table']
48233469,48251673,2018-01-12 20:21:24,Cassandra coordinator read timeout count operation vs select all,"<p>I have 4 node dse 5.1.2 Cassandra cluster on avg each node has 3 gb data.i am trying to query on that data. I know i should not do the below queries as it has bad effects in cassandra. But what i observed is </p>

<blockquote>
  <p>select * works , but select count(*) fails </p>
</blockquote>

<p>on the same table with coordinator timeout.when both perform same operation in back ground while reading , why is the difference.
my cluster is 100% repaired and no tombstones found in that table. i also increased read request timeout in cqlsh command.</p>

<p>I am getting the below error for query </p>

<pre><code>select count(*) from xxx.xxxx;

ReadTimeout: Error from server: code=1200 [Coordinator node timed out waiting for replica nodes' responses] message=""Operation timed out - received only 0 responses."" info={'received_responses': 0, 'required_responses': 1, 'consistency': 'ONE'}
</code></pre>
",<cassandra><datastax-enterprise><datastax-java-driver><cassandra-3.0><cqlsh>,"<p>If you are running those commands via <code>cqlsh</code> then they are not strictly the same. </p>

<p><code>cqlsh</code> has a default limit of 10000 lines so unless you have changed that limit that is all you are going to get with your <code>select *</code>. </p>

<p>The <code>select count(*)</code> is going to do a full table scan on all nodes to get the count, hence the timeout.</p>
",['table']
48257958,48271228,2018-01-15 06:31:20,What is the best way to clear unused Cassandra directories,"<p>Why cassandra's gc didn't delete unused directories of column family during compaction? How can I delete them safely?</p>

<p>I have a 5 nodes Cassandra cluster:</p>

<pre><code># nodetool status
Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address      Load       Tokens       Owns (effective)  Host ID                               Rack
UN  10.97.18.21  5.13 GiB   256          60.4%             8a6828d8-db43-4722-82fd-dd37ec1c25a1  rack1
UN  10.97.18.23  7.53 GiB   256          60.4%             adb18dfd-3cef-4ae3-9766-1e3f17d68588  rack1
UN  10.97.18.22  8.3 GiB    256          62.8%             1d6c453a-e3fb-4b3b-b7c1-689e7c8fbbbb  rack1
UN  10.97.18.25  5.1 GiB    256          60.1%             c8e4a4dc-4a05-4bac-b4d2-669fae9282b0  rack1
UN  10.97.18.24  7.97 GiB   256          56.3%             f2732a23-b70a-41a5-aaaa-1be95002ee8a  rack1
</code></pre>

<p>I have a keyspace 'loan_products' with only one column family 'events':</p>

<pre><code>[cqlsh 5.0.1 | Cassandra 3.11.1 | CQL spec 3.4.4 | Native protocol v4]
Use HELP for help.
cqlsh&gt; 
cqlsh&gt; DESCRIBE KEYSPACE loan_products ;

CREATE KEYSPACE loan_products WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '3'}  AND durable_writes = true;

CREATE TABLE loan_products.events (
    persistence_id text,
    partition_nr bigint,
    sequence_nr bigint,
    timestamp timeuuid,
    timebucket text,
    event blob,
    event_manifest text,
    message blob,
    meta blob,
    meta_ser_id int,
    meta_ser_manifest text,
    ser_id int,
    ser_manifest text,
    tag1 text,
    tag2 text,
    tag3 text,
    used boolean static,
    writer_uuid text,
    PRIMARY KEY ((persistence_id, partition_nr), sequence_nr, timestamp, timebucket)
) WITH CLUSTERING ORDER BY (sequence_nr ASC, timestamp ASC, timebucket ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';
</code></pre>

<p>I have no snapshots at all:</p>

<pre><code># nodetool listsnapshots
Snapshot Details: 
There are no snapshots
</code></pre>

<p>Column family has default <strong>gc_grace_seconds = 864000</strong> (10 days), so gc had to remove tombstones etc., but they are still exist on filesystem. Parallel-ssh shows:</p>

<pre><code>[1] 11:50:34 [SUCCESS] 10.97.18.21
total 20
drwxr-xr-x. 4 cassandra cassandra 4096 дек 21 13:01 events-a83b3be0e61711e7a2863103117dd196
drwxr-xr-x. 4 cassandra cassandra 4096 дек 21 13:02 events-bbedb500e61c11e7a2863103117dd196
drwxr-xr-x. 4 cassandra cassandra 4096 дек 21 19:08 events-48c2b750e61d11e7a2863103117dd196
drwxr-xr-x. 4 cassandra cassandra 4096 дек 21 19:19 events-16c0b670e65011e7a2863103117dd196
drwxr-xr-x. 3 cassandra cassandra 4096 янв 15 11:46 events-c156cc40e65111e7a2863103117dd196

[2] 11:50:34 [SUCCESS] 10.97.18.22
total 20
drwxr-xr-x. 4 cassandra cassandra 4096 дек 21 13:00 events-a83b3be0e61711e7a2863103117dd196
drwxr-xr-x. 4 cassandra cassandra 4096 дек 21 13:01 events-bbedb500e61c11e7a2863103117dd196
drwxr-xr-x. 4 cassandra cassandra 4096 дек 21 19:08 events-48c2b750e61d11e7a2863103117dd196
drwxr-xr-x. 4 cassandra cassandra 4096 дек 21 19:19 events-16c0b670e65011e7a2863103117dd196
drwxr-xr-x. 3 cassandra cassandra 4096 янв 15 11:49 events-c156cc40e65111e7a2863103117dd196

[3] 11:50:34 [SUCCESS] 10.97.18.23
total 20
drwxr-xr-x. 4 cassandra cassandra 4096 дек 21 13:00 events-a83b3be0e61711e7a2863103117dd196
drwxr-xr-x. 4 cassandra cassandra 4096 дек 21 13:01 events-bbedb500e61c11e7a2863103117dd196
drwxr-xr-x. 4 cassandra cassandra 4096 дек 21 19:07 events-48c2b750e61d11e7a2863103117dd196
drwxr-xr-x. 4 cassandra cassandra 4096 дек 21 19:19 events-16c0b670e65011e7a2863103117dd196
drwxr-xr-x. 3 cassandra cassandra 4096 янв 15 11:48 events-c156cc40e65111e7a2863103117dd196

[4] 11:50:34 [SUCCESS] 10.97.18.25
total 20
drwxr-xr-x. 3 cassandra cassandra 4096 янв  9 15:08 events-a83b3be0e61711e7a2863103117dd196
drwxr-xr-x. 3 cassandra cassandra 4096 янв  9 15:08 events-bbedb500e61c11e7a2863103117dd196
drwxr-xr-x. 3 cassandra cassandra 4096 янв  9 15:08 events-48c2b750e61d11e7a2863103117dd196
drwxr-xr-x. 3 cassandra cassandra 4096 янв  9 15:08 events-16c0b670e65011e7a2863103117dd196
drwxr-xr-x. 3 cassandra cassandra 4096 янв 15 11:45 events-c156cc40e65111e7a2863103117dd196

[5] 11:50:34 [SUCCESS] 10.97.18.24
total 20
drwxr-xr-x. 4 cassandra cassandra 4096 дек 21 13:00 events-a83b3be0e61711e7a2863103117dd196
drwxr-xr-x. 4 cassandra cassandra 4096 дек 21 13:01 events-bbedb500e61c11e7a2863103117dd196
drwxr-xr-x. 4 cassandra cassandra 4096 дек 21 19:08 events-48c2b750e61d11e7a2863103117dd196
drwxr-xr-x. 4 cassandra cassandra 4096 дек 21 19:19 events-16c0b670e65011e7a2863103117dd196
drwxr-xr-x. 3 cassandra cassandra 4096 янв 15 11:50 events-c156cc40e65111e7a2863103117dd196
</code></pre>

<p>As i see only one directory with id <strong>c156cc40e65111e7a2863103117dd196</strong> is in use, last update was on January 15</p>
",<database><cassandra><cassandra-3.0>,"<p>By default Cassandra takes a snapshot whenever a column family is dropped. This is by design to protect accidental truncation (deletion of all records in a table) or accidental drop of that table. The parameter in Cassandra.yaml controlling this is auto_snapshot </p>

<blockquote>
  <p>Whether or not a snapshot is taken of the data before keyspace truncation
  or dropping of column families. The STRONGLY advised default of true
  should be used to provide data safety. If you set this flag to false, you will
  lose data on truncation or drop. 
  <strong>auto_snapshot: true</strong></p>
</blockquote>

<p>So based on the screenshot you have shown, looks like the ""events"" table was dropped atleast 4 times and recreated. So the proper way to clean this up would be to first figure out the correct UUID used by Cassandra for a given table in keyspace. In your case, the query would be</p>

<pre><code>select id from system_schema.tables where keyspace_name = 'loan_products' and table_name = 'events' ;
</code></pre>

<p>Now remove the other table directories manually by ""rm -rf"" for the UUID's that doesn't correspond in the above output.</p>

<p>Also the reason ""nodetool listsnapshots"" isn't giving any snapshots, because the active table doesn't have any. But if you go to any of the other 4 ""events"" table directory and do a ""ls -ltr"" you should be able to find snapshot directories inside them, which were created as the table was dropped.</p>
","['auto_snapshot', 'table']"
48294755,48305302,2018-01-17 06:21:48,Read fail with consistency one,"<p>I've got 3 nodes; 2 in datacenter 1 (node 1 and node 2) and 1 in datacenter 2 (node 3). Replication strategy: Network Topology, dc1:2, dc2: 1. </p>

<p>Initially I keep one of the nodes in dc1 off (node 2) and write 100 000 entries with consistency 2 (via c++ program). After writing, I shut down the node in datacenter 2 (node 3) and turn on node 2. </p>

<p>Now, if I try to read those 100 000 entries I had written (again via c++ program) with consistency set as ONE, I'm not able to read all those 100 000 entries i.e. I'm able to read only some of the entries. As I run the program again and again, my program fetches more and more entries.</p>

<p>I was expecting that since one of the 2 nodes which are up contains all the 100 000 entries, therefore, the read program should fetch all the entries in the first execution when the set consistency is ONE.</p>

<p>Is this related to read repair? I'm thinking that because the read repair is happening in the background, that is why, the node is not able to respond to all the queries? But nowhere could I find anything regarding this behavior.</p>
",<cassandra><cassandra-3.0>,"<p>Let's run through the scenario. </p>

<p>During the write of 100K rows (DC1) Node1 and (DC2) Node3 took all the writes. As it was happening Node1 also might have taken hints for Node2 (DC1) for default 3 hours and then stop doing that. </p>

<p>Once Node2 comes back up online, unless a repair was run - it takes a bit to catch up through replay of hints. If the node was down for more than 3 hours, repair becomes mandatory. </p>

<p>During the reads, it can technically reach to any node in the cluster based on the loadbalancy policy used by driver. Unless specified to do ""DCAwareRoundRobinPolicy"", the read request might even reach any of the DC (DC1 or DC2 in this case). Since the consistency requested is ""ONE"", practically any ALIVE node can respond - NODE1 &amp; NODE2 (DC1) in this case. So NODE2 may not even have all data and it can still respond with NULL value and thats why you received empty data sometimes and correct data some other time.</p>

<p>With consistency ""ONE"" read repair doesn't even happen, as there no other node to compare it with. Here is the <a href=""http://docs.datastax.com/en/cassandra/2.1/cassandra/operations/opsRepairNodesReadRepair.html"" rel=""nofollow noreferrer"">documentation</a> on it . Even in case of consistency ""local_quorum"" or ""quorum"" there is a <strong>read_repair_chance</strong> set at the table level which is default to 0.1. Which means only 10% of reads will trigger read_repair. This is to save performance by not triggering every time.  Think about it, if read repair can bring the table entirely consistent across nodes, then why does ""nodetool repair"" even exist?</p>

<p>To avoid this situation, whenever the node comes back up online its best practice to do a ""nodetool repair"" or run queries with consistency ""local_quorum"" to get consistent data back.</p>

<p>Also remember, consistency ""ONE"" is comparable to uncommitted read (dirty read) in the world of RDBMS (WITH UR). So expect to see unexpected data.</p>
",['table']
48334204,48656177,2018-01-19 04:39:22,How to prevent decrease of Dataflow read parallelism from Cassandra,"<p>You can read about my setup <a href=""https://stackoverflow.com/questions/48090668/how-to-increase-dataflow-read-parallelism-from-cassandra"">here</a>. I solved problems, described there, but I have new one.</p>

<p>I am reading the data from 3 tables. I have problem with one (the largest) table. I've read a lot of data from the table with great rate ~300000 rows/s, but after ~10 hours (and when reading from other two tables finished) it decreased to ~20000 rows/s. And after 24 hours it's not finished yet.</p>

<p>There are a lot of suspicious lines in the log:</p>

<pre><code>I  Proposing dynamic split of work unit cybrmt;2018-01-17_22_54_11-12138573770170126316;3251780906818434621 at {""fractionConsumed"":0.5} 
I  Rejecting split request because custom reader returned null residual source. 
I  Proposing dynamic split of work unit cybrmt;2018-01-17_22_54_11-12138573770170126316;3251780906818434621 at {""fractionConsumed"":0.5} 
I  Rejecting split request because custom reader returned null residual source. 
I  Proposing dynamic split of work unit cybrmt;2018-01-17_22_54_11-12138573770170126316;3251780906818434621 at {""fractionConsumed"":0.5} 
I  Rejecting split request because custom reader returned null residual source. 
I  Proposing dynamic split of work unit cybrmt;2018-01-17_22_54_11-12138573770170126316;3251780906818434621 at {""fractionConsumed"":0.5} 
I  Rejecting split request because custom reader returned null residual source. 
I  Proposing dynamic split of work unit cybrmt;2018-01-17_22_54_11-12138573770170126316;3251780906818434621 at {""fractionConsumed"":0.5} 
I  Rejecting split request because custom reader returned null residual source. 
I  Proposing dynamic split of work unit cybrmt;2018-01-17_22_54_11-12138573770170126316;3251780906818434621 at {""fractionConsumed"":0.5} 
I  Rejecting split request because custom reader returned null residual source. 
I  Proposing dynamic split of work unit cybrmt;2018-01-17_22_54_11-12138573770170126316;3251780906818434621 at {""fractionConsumed"":0.5} 
I  Rejecting split request because custom reader returned null residual source.
</code></pre>

<h1>UPD</h1>

<p>The job ended with exception:</p>

<pre><code>(f000632be487340d): Workflow failed. Causes: (844d65bb40eb132b): S14:Read from Cassa table/Read(CassandraSource)+Transform to KV by id+CoGroupByKey id/MakeUnionTable0+CoGroupByKey id/GroupByKey/Reify+CoGroupByKey id/GroupByKey/Write failed., (c07ceebe5d95f668): A work item was attempted 4 times without success. Each time the worker eventually lost contact with the service. The work item was attempted on: 
  starterpipeline-sosenko19-01172254-4260-harness-wrdk,
  starterpipeline-sosenko19-01172254-4260-harness-xrkd,
  starterpipeline-sosenko19-01172254-4260-harness-hvfd,
  starterpipeline-sosenko19-01172254-4260-harness-0pf5
</code></pre>

<h2>About CoGroupByKey</h2>

<p>There are two tables. One have ~2 billion rows, each with unique key (1 row per key). Second have ~20 billion rows with less or equal 10 rows per key.  </p>

<h2>Graph of the pipeline</h2>

<p><a href=""https://i.stack.imgur.com/8OUpo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8OUpo.png"" alt=""enter image description here""></a></p>

<p>Here is what inside <code>CoGroupByKey match_id</code> block:</p>

<p><a href=""https://i.stack.imgur.com/l4aQS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/l4aQS.png"" alt=""enter image description here""></a></p>

<h2>Code of the pipeline</h2>

<pre><code>// Create pipeline
Pipeline p = Pipeline.create(PipelineOptionsFactory.fromArgs(args).withValidation().create());

// Read data from Cassandra table opendota_player_match_by_account_id2
PCollection&lt;OpendotaPlayerMatch&gt; player_matches = p.apply(""Read from Cassa table opendota_player_match_by_account_id2"", CassandraIO.&lt;OpendotaPlayerMatch&gt;read()
        .withHosts(Arrays.asList(""10.132.9.101"", ""10.132.9.102"", ""10.132.9.103"", ""10.132.9.104"")).withPort(9042)
        .withKeyspace(""cybermates"").withTable(CASSA_OPENDOTA_PLAYER_MATCH_BY_ACCOUNT_ID_TABLE_NAME)
        .withEntity(OpendotaPlayerMatch.class).withCoder(SerializableCoder.of(OpendotaPlayerMatch.class))
        .withConsistencyLevel(CASSA_CONSISTENCY_LEVEL));

// Transform player_matches to KV by match_id
PCollection&lt;KV&lt;Long, OpendotaPlayerMatch&gt;&gt; opendota_player_matches_by_match_id = player_matches
        .apply(""Transform player_matches to KV by match_id"", ParDo.of(new DoFn&lt;OpendotaPlayerMatch, KV&lt;Long, OpendotaPlayerMatch&gt;&gt;() {
            @ProcessElement
            public void processElement(ProcessContext c) {
                // LOG.info(c.element().match_id.toString());
                c.output(KV.of(c.element().match_id, c.element()));
            }
        }));

// Read data from Cassandra table opendota_match
PCollection&lt;OpendotaMatch&gt; opendota_matches = p.apply(""Read from Cassa table opendota_match"", CassandraIO.&lt;OpendotaMatch&gt;read()
        .withHosts(Arrays.asList(""10.132.9.101"", ""10.132.9.102"", ""10.132.9.103"", ""10.132.9.104"")).withPort(9042)
        .withKeyspace(""cybermates"").withTable(CASSA_OPENDOTA_MATCH_TABLE_NAME).withEntity(OpendotaMatch.class)
        .withCoder(SerializableCoder.of(OpendotaMatch.class))
        .withConsistencyLevel(CASSA_CONSISTENCY_LEVEL));

// Read data from Cassandra table match
PCollection&lt;OpendotaMatch&gt; matches = p.apply(""Read from Cassa table match"", CassandraIO.&lt;Match&gt;read()
        .withHosts(Arrays.asList(""10.132.9.101"", ""10.132.9.102"", ""10.132.9.103"", ""10.132.9.104"")).withPort(9042)
        .withKeyspace(""cybermates"").withTable(CASSA_MATCH_TABLE_NAME).withEntity(Match.class)
        .withCoder(SerializableCoder.of(Match.class))
        .withConsistencyLevel(CASSA_CONSISTENCY_LEVEL))
        .apply(""Adopt match for uniform structure"", ParDo.of(new DoFn&lt;Match, OpendotaMatch&gt;() {
            @ProcessElement
            public void processElement(ProcessContext c) {
                // LOG.info(c.element().match_id.toString());
                OpendotaMatch m = new OpendotaMatch();

                // opendota_match and  match tables have slightly different schema. I've cut out conversion here because it's large and dummy

                c.output(m);
            }
        }));


// Union match and opendota_match
PCollectionList&lt;OpendotaMatch&gt; matches_collections = PCollectionList.of(opendota_matches).and(matches);
PCollection&lt;OpendotaMatch&gt; all_matches = matches_collections.apply(""Union match and opendota_match"", Flatten.&lt;OpendotaMatch&gt;pCollections());

// Transform matches to KV by match_id
PCollection&lt;KV&lt;Long, OpendotaMatch&gt;&gt; matches_by_match_id = all_matches
        .apply(""Transform matches to KV by match_id"", ParDo.of(new DoFn&lt;OpendotaMatch, KV&lt;Long, OpendotaMatch&gt;&gt;() {
            @ProcessElement
            public void processElement(ProcessContext c) {
                // LOG.info(c.element().players.toString());
                c.output(KV.of(c.element().match_id, c.element()));
            }
        }));

// CoGroupByKey match_id
// Replicate data
final TupleTag&lt;OpendotaPlayerMatch&gt; player_match_tag = new TupleTag&lt;OpendotaPlayerMatch&gt;();
final TupleTag&lt;OpendotaMatch&gt; match_tag = new TupleTag&lt;OpendotaMatch&gt;();
PCollection&lt;KV&lt;Long, PMandM&gt;&gt; joined_matches = KeyedPCollectionTuple
        .of(player_match_tag, opendota_player_matches_by_match_id).and(match_tag, matches_by_match_id)
        .apply(""CoGroupByKey match_id"", CoGroupByKey.&lt;Long&gt;create())
        .apply(""Replicate data"", ParDo.of(new DoFn&lt;KV&lt;Long, CoGbkResult&gt;, KV&lt;Long, PMandM&gt;&gt;() {
            @ProcessElement
            public void processElement(ProcessContext c) {
                try {
                    OpendotaMatch m = c.element().getValue().getAll(match_tag).iterator().next();
                    Iterable&lt;OpendotaPlayerMatch&gt; pms = c.element().getValue().getAll(player_match_tag);
                    for (OpendotaPlayerMatch pm : pms) {
                        if (0 &lt;= pm.account_id &amp;&amp; pm.account_id &lt; MAX_UINT) {
                            for (OpendotaPlayerMatch pm2 : pms) {                                   
                                c.output(KV.of(pm.account_id, new PMandM(pm2, m)));
                            }
                        }
                    }
                } catch (NoSuchElementException e) {
                    LOG.error(c.element().getValue().getAll(player_match_tag).iterator().next().match_id.toString() + "" "" + e.toString());
                }
            }
        }));        


// Transform to byte array
// Write to BQ
joined_matches
        .apply(""Transform to byte array, Write to BQ"", BigQueryIO.&lt;KV&lt;Long, PMandM&gt;&gt;write().to(new DynamicDestinations&lt;KV&lt;Long, PMandM&gt;, String&gt;() {
            public String getDestination(ValueInSingleWindow&lt;KV&lt;Long, PMandM&gt;&gt; element) {
                return element.getValue().getKey().toString();
            }

            public TableDestination getTable(String account_id_str) {
                return new TableDestination(""cybrmt:"" + BQ_DATASET_NAME + "".player_match_"" + account_id_str,
                        ""Table for user "" + account_id_str);
            }

            public TableSchema getSchema(String account_id_str) {
                List&lt;TableFieldSchema&gt; fields = new ArrayList&lt;&gt;();
                fields.add(new TableFieldSchema().setName(""value"").setType(""BYTES""));
                return new TableSchema().setFields(fields);
            }
        }).withFormatFunction(new SerializableFunction&lt;KV&lt;Long, PMandM&gt;, TableRow&gt;() {
            public TableRow apply(KV&lt;Long, PMandM&gt; element) {
                OpendotaPlayerMatch pm = element.getValue().pm;                     
                OpendotaMatch m = element.getValue().m;
                TableRow tr = new TableRow();
                ByteBuffer bb = ByteBuffer.allocate(114);

                // I've cut out transform to byte buffer here because it's large and dummy

                tr.set(""value"", bb.array());
                return tr;                      
            }
        }));

p.run();
</code></pre>

<h1>UPD2. I've tried to read the problem table alone</h1>

<p>I've tried to read the problem table from above alone. Pipeline contains CassandraIO.Read transform and dummy ParDo transform with some logging output. And now it behaves like the full pipeline. There is one (I believe last) split that cannot be done:</p>

<pre><code>I  Proposing dynamic split of work unit cybrmt;2018-01-20_21_28_01-3451798636786921663;1617811313034836533 at {""fractionConsumed"":0.5} 
I  Rejecting split request because custom reader returned null residual source. 
</code></pre>

<p>Here is the graph of the pipeline:</p>

<p><a href=""https://i.stack.imgur.com/xWGl4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xWGl4.png"" alt=""enter image description here""></a></p>

<p>And here is the code:</p>

<pre><code>// Create pipeline
Pipeline p = Pipeline.create(PipelineOptionsFactory.fromArgs(args).withValidation().create());

// Read data from Cassandra table opendota_player_match_by_account_id2
PCollection&lt;OpendotaPlayerMatch&gt; player_matches = p.apply(""Read from Cassa table opendota_player_match_by_account_id2"", CassandraIO.&lt;OpendotaPlayerMatch&gt;read()
        .withHosts(Arrays.asList(""10.132.9.101"", ""10.132.9.102"", ""10.132.9.103"", ""10.132.9.104"")).withPort(9042)
        .withKeyspace(""cybermates"").withTable(CASSA_OPENDOTA_PLAYER_MATCH_BY_ACCOUNT_ID_TABLE_NAME)
        .withEntity(OpendotaPlayerMatch.class).withCoder(SerializableCoder.of(OpendotaPlayerMatch.class))
        .withConsistencyLevel(CASSA_CONSISTENCY_LEVEL));

// Print my matches
player_matches.apply(""Print my matches"", ParDo.of(new DoFn&lt;OpendotaPlayerMatch, Long&gt;() {
            @ProcessElement
            public void processElement(ProcessContext c) {
                if (c.element().account_id == 114688838) {
                    LOG.info(c.element().match_id.toString());
                    c.output(c.element().match_id);
                }
            }
        }));

p.run();
</code></pre>

<h1>UPD3</h1>

<p>The small pipeline (CassandraIO.Read and ParDo) successfully finished in 23 hours. First 4 hours there were max number of workers (40) and great read speed (~300000 rows/s). After that number of workers autoscaled to 1 as well as read speed to ~15000 rows/s. Here is the graph:</p>

<p><a href=""https://i.stack.imgur.com/prnPT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/prnPT.png"" alt=""enter image description here""></a></p>

<p>And here is the log end:</p>

<pre><code>I  Proposing dynamic split of work unit cybrmt;2018-01-20_21_28_01-3451798636786921663;1617811313034836533 at {""fractionConsumed"":0.5} 
I  Rejecting split request because custom reader returned null residual source. 
I  Proposing dynamic split of work unit cybrmt;2018-01-20_21_28_01-3451798636786921663;1617811313034836533 at {""fractionConsumed"":0.5} 
I  Rejecting split request because custom reader returned null residual source. 
I  Success processing work item cybrmt;2018-01-20_21_28_01-3451798636786921663;1617811313034836533 
I  Finished processing stage s01 with 0 errors in 75268.681 seconds 
</code></pre>
",<java><cassandra><google-cloud-platform><google-cloud-dataflow><apache-beam>,"<p>I've finally used @jkff advise and read the data from a table with different partition key that is distributed more evenly (there were actually two tables with the same data but different partition keys in my data schema). </p>
",['table']
48347733,48358066,2018-01-19 18:50:51,Cassandra UDT TYPE remove a field name,"<p>I have a UDT type in Cassandra. I want to ALTER this type to remove the country field.  I don't find any delete or remove field for ALTER TYPE documentation.  <a href=""https://docs.datastax.com/en/cql/3.3/cql/cql_reference/cqlAlterType.html"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/cql/3.3/cql/cql_reference/cqlAlterType.html</a></p>

<pre><code>create type bank_payment (
      account_number text,
      name text,
      city text,
      country text,
      key text
);
</code></pre>

<p>Please help sharing ALTER command to remove one field from above UDT.</p>
",<cassandra><cql><cassandra-3.0>,"<p>Cassandra UDT doesn't store any actual data and its the table where this UDT referred to has actual data. So go ahead and drop the UDT &amp; recreate it with the correct definition that you want. Remember the serialization of old sstables that had this UDT (with additional column) will be different from the new one.</p>

<p>So if possible add a new version name to this UDT as you recreate, say </p>

<blockquote>
  <p>bank_payment_v2</p>
</blockquote>
",['table']
48366032,48369676,2018-01-21 10:55:24,Cassandra: Undefined column name permissions,"<p>Cassandra throws an exception: </p>

<p><code>com.datastax.driver.core.exceptions.InvalidQueryException: Undefined column name permissions at com.datastax.driver.core.exceptions.InvalidQueryException.copy (InvalidQueryException.java:50) at com.datastax.driver.core.DriverThrowables.propagateCause (DriverThrowables.java:37) at com.datastax.driver.core.AbstractSession.prepare (AbstractSession.java:100) at org.accells.connection.CassandraSession.prepare(CassandraSession.java:120)</code> </p>

<p>Unfortunately I can't find any info about this in the net. Please, advice.
The line that throwing this exception is cassandra code :</p>

<pre><code>/**
 * Prepares the provided query string.
 *
 * @param query the CQL query string to prepare
 * @return the prepared statement corresponding to {@code query}.
 * @throws NoHostAvailableException if no host in the cluster can be
 *                                  contacted successfully to prepare this query.
 */
PreparedStatement prepare(String query);
</code></pre>

<p>The query provided is SQL valid query.</p>
",<java><cassandra><cassandra-3.0>,"<p>This error is telling you that the column <code>permissions</code> does not exist on the table you are trying to query it from.</p>

<p>It comes from here:</p>

<pre><code>ColumnDefinition def = cfm.getColumnDefinition(id);
if (def == null)
    throw new InvalidRequestException(String.format(""Undefined column name %s"", toString()));
return def;
</code></pre>

<p>in <code>ColumnDefinition.java</code> (I haven't linked to the code because I don't know what version you are running.</p>
",['table']
48414931,48419238,2018-01-24 04:41:11,Am I violating the data modelling rule in Cassandra?,"<p>I understand that we should not create 'N' number of partition under a single table because in this case, it tries to query from N number of nodes where the partitions are available. </p>

<p>(Modifying the example for understanding and security)</p>

<p>If I have a table like 'user'</p>

<pre><code>CREATE TABLE user(
   user_id int PRIMARY KEY,
   user_name text,
   user_phone varint
   );
</code></pre>

<p>where user_id is unique.</p>

<p>Example - To get all the users from the table, I use the query :</p>

<pre><code>select * from user;  
</code></pre>

<p>So which means It goes to all the nodes where the partitions for the 'user_id' are available. Since I used the user_id as partition / primary key here, It will be scattered to all the nodes based on the partition_id.</p>

<p>Is it fine? Or Is there a better way to design this in Cassandra?</p>

<p><strong>Edited :</strong></p>

<p>By Keeping a single partition as 'uniquekey' and sorted by user_name will have the advantage that uniquekey will make a single partition. Is it the better design compare to the above one?</p>

<pre><code>CREATE TABLE user(
   user_id int,
   user_name text,
   user_phone varint,
   primary key ('uniquekey', user_name));



select * from user where user_id = 'uniquekey';
</code></pre>
",<cassandra><data-modeling><cassandra-3.0>,"<p>A fundamental table design rule in Cassandra is called <strong>Query-Driven</strong>, which means you usually understand what are you trying to query on before you make the table schema. </p>

<p>If you just want to simply return all the rows (select * ) in the database (which is not a common use case for Cassandra since Cassandra aims to store very, very large amount of data), whatever you designed is fine. But Cassandra might not be the best choice in this case. </p>

<p>How to ensure a good table design in Cassandra?
Ref: <a href=""https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling"" rel=""noreferrer"">Basic Rules of Cassandra Data Modeling</a></p>
",['table']
48481676,48482505,2018-01-27 23:37:13,cassandra get all where sth contains sth,"<p>I am currently writing a spring application. I am searching a way to do a request in cassandra with which I can get all entries by sth as example:</p>

<p>I have the following table:</p>

<pre><code> username | entityid
----------+--------------------------------------
 pascalku | 55f23680-0201-11e8-9971-4516c86aec2e
</code></pre>

<p>When I am now search for Pascal i wanna get this result.</p>

<p>I tested much cql commandlines but nothing works.</p>

<p>Hope somebody got an idea or know how to do this. Thanks for reading!</p>

<p>EDIT:</p>

<p>My resolver_names:</p>

<pre><code>CREATE TABLE resolver_names (
    username text PRIMARY KEY,
    entityid uuid
) WITH bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';
</code></pre>
",<java><spring><cassandra><cql>,"<p>As <code>username</code> is primary key, there is no way to query with <code>LIKE</code> operation in cassandra. Kindly note that cassandra is NoSQL database, it's not like relational database, in which you can run such kind of query like <code>select * from table where comlum1 like '%pascal%'</code>. I would recommend you to go through free data modeling course <a href=""https://academy.datastax.com/resources/ds220-data-modeling"" rel=""nofollow noreferrer"">here</a> for better understanding. In short, if you want to run such kind of queries in your application with cassandra, there might be something wrong when data/table is modelled.</p>

<p>If it's not primary key but just normal column, LIKE queries can be achieved using a SSTable Attached Secondary Index (SASI) in cassandra 3.4 onward. Please refer to <a href=""https://docs.datastax.com/en/dse/5.1/cql/cql/cql_using/useSASIIndex.html"" rel=""nofollow noreferrer"">this</a> for more details.</p>

<p>There are some frameworks, which can be added on top of cassandra to achieve the same (e.g. Spark SQL, etc). However, the performance might not be as good as built-in query, and it's not recommended to so do.</p>
",['table']
48519373,48520918,2018-01-30 10:39:22,Too many columns in Cassandra,"<p>I have 20 columns in a table in Cassandra. Will there be a performance impact in performing </p>

<pre><code>select * from table where partitionKey = 'test';
</code></pre>

<p>I am not able to understand from this link,</p>

<p><a href=""https://wiki.apache.org/cassandra/CassandraLimitations"" rel=""nofollow noreferrer"">https://wiki.apache.org/cassandra/CassandraLimitations</a></p>

<p>1) What will be the consequence of having too many columns (say 20) in the Cassandra tables?</p>
",<cassandra><cassandra-3.0>,"<p>Unless you have a lot of rows on the partition, I don't see an impact with having 20 columns. As stated in the documentation that you linked:</p>

<blockquote>
  <p>The maximum number of cells (rows x columns) in a single partition is 2 billion.</p>
</blockquote>

<p>So, unless you are expecting to have more than 100 million rows in a single partition, I don't see why 20 columns would be an issue. Keep in mind that Cassandra is a column family store. This designation means that Cassandra can store a large number of columns per partition.</p>

<p>Having said that, I would personally recommend not to go over 100 MB per partition. It might bring you problems in the future with streaming during repairs.</p>

<p>===============================</p>

<p>To answer to your comment. Keep in mind that partitions and rows are 2 different things in Cassandra. A partition is only equal to a row if there's no clustering columns. For instance, take a look at this table creation and the values we insert, and then look at the sstabledump:</p>

<pre><code>create TABLE tt2 ( foo int , bar int , mar int , PRIMARY KEY (foo , bar )) ;
insert INTO tt2 (foo , bar , mar ) VALUES ( 1, 2, 3) ;
insert INTO tt2 (foo , bar , mar ) VALUES ( 1, 3, 4) ;
</code></pre>

<p>sstabledump:</p>

<pre><code>./cassandra/tools/bin/sstabledump ~/cassandra/data/data/tk/tt2-1386f69005bd11e89c0bbfb5c1157523/mc-1-big-Data.db 
[
  {
    ""partition"" : {
      ""key"" : [ ""1"" ],
      ""position"" : 0
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 32,
        ""clustering"" : [ ""2"" ],
        ""liveness_info"" : { ""tstamp"" : ""2018-01-30T12:57:36.362483Z"" },
        ""cells"" : [
          { ""name"" : ""mar"", ""value"" : 3 }
        ]
      },
      {
        ""type"" : ""row"",
        ""position"" : 32,
        ""clustering"" : [ ""3"" ],
        ""liveness_info"" : { ""tstamp"" : ""2018-01-30T12:58:03.538482Z"" },
        ""cells"" : [
          { ""name"" : ""mar"", ""value"" : 4 }
        ]
      }
    ]
  }
]
</code></pre>

<p>Also, if you use the <code>-d</code> option, it might make it easier for you to see the internal representation. As you can see, for the same partition, we have 2 distinct rows:</p>

<pre><code>./cassandra/tools/bin/sstabledump -d ~/cassandra/data/data/tk/tt2-1386f69005bd11e89c0bbfb5c1157523/mc-1-big-Data.db 
[1]@0 Row[info=[ts=1517317056362483] ]: 2 | [mar=3 ts=1517317056362483]
[1]@32 Row[info=[ts=1517317083538482] ]: 3 | [mar=4 ts=1517317083538482]
</code></pre>
",['table']
48537712,48554817,2018-01-31 08:32:26,Ensure previous version of a cell is removed after 10 minutes,"<p>In Cassandra, I want to update a row to remove some sensitive data once the row has been processed.
A row has the following process.</p>

<ol>
<li>Insert the record</li>
<li>Process the record (update it)</li>
<li>Set the row to processed and remove the sensitive data from one column of the row</li>
</ol>

<p>I am aware that the update are not actually updating the data on disk by design of Cassandra. However, I want to ensure that after a not too long period the data is actually removed from disk. There is no row removed from this table explicitly (with a CQL statement) only insert and update statements.</p>

<p>From what I understood, I have to use a relatively short <code>gc_grace_period</code>, like 10 minutes. </p>

<p>Could you tell me if this configuration would work? What is the impact of such strategy?</p>

<p>I am using Cassandra 3.11.1 and the table has a TTL of one day. About 100k to 1M records are inserted by day in the table.</p>
",<cassandra><cassandra-3.0>,"<p>Let me answer this two part question :-</p>

<p>gc_grace_seconds is the time Cassandra has to wait before it can clean up the SSTables that have tombstone data (caused by TTL/Deletes). So here in your case, the table has a TTL of 1 day and by default gc_grace_seconds is 864000 (seconds) = 10days. Which means that the data that has expired in a day waits for another 10days (by default) before getting cleaned up.</p>

<p>The reason default gc_grace_seconds is high is to ensure that during explicit deletes, if any node in the cluster was down, the deletes (tombstones) get propagated as the node comes back up. In other words to avoid zombie data.</p>

<p>In your case, since there isn't any explicit deletes and only tombstones its safe to have a smaller value for gc_grace_seconds say 90000 (25hours).</p>

<p><em>Another riskier option, is to set gc_grace_seconds to zero if guaranteed that the application would never do explicit deletes and rely on TTL only. Setting it to zero has the advantage of no tombstones in the system. Data gets purged as soon as its TTLed</em> </p>

<p>Second Part of the question :</p>

<p>In order to expire the column within 10 minutes of processing, we can set column level TTL as follows. Below I'm suggesting a shorter gc_grace_seconds along with TWCS which would help to evict this row within 10 + 1min and wouldn't cause tombstone pressure.</p>

<p>Update CQL to set column level TTL</p>

<pre><code>UPDATE test USING TTL 600 
  SET status = 'PROCESSED' 
  WHERE primary_key = ? ;
</code></pre>

<p>Additionally, regarding the table compaction strategy :-</p>

<p>I'm assuming the rows are being processed sequentially (or in other words this table being treated as a queue). So cleaner way to handle this situation would be to use ""Time Window Compaction Strategy"". Generally its recommended to keep the number of TimeWindow slices to less than 50 slices.</p>

<p>The command would be</p>

<pre><code>CREATE TABLE test (
........
) WITH 
    AND gc_grace_seconds = 60
    AND default_time_to_live = 86400
    AND compaction = {'compaction_window_size': '30', 
                      'compaction_window_unit': 'MINUTES', 
                      'class': 'org.apache.cassandra.db.compaction.TimeWindowCompactionStrategy'}
</code></pre>

<p>This setup would give us the following guarantees :</p>

<ul>
<li>Data older than 30 Minutes stops being compacted, lowering I/O consumption. Queries targeting rows within a 30 minute time range will mostly hit a limited number of SSTables in case compaction is up to date</li>
<li>Using TTL inserts, tombstones get purged by file deletion (in this case, shortly after 1 days and 1 minute after the original write)</li>
<li>Data sent out of their original time window through hints or repairs are compacted with SSTables of the current window only, preventing write amplification</li>
<li>Maximum compaction overhead on disk is 50% of the last created bucket</li>
<li>Disk space usage growth is easily predictable</li>
</ul>

<p>A nice read on <a href=""http://thelastpickle.com/blog/2016/12/08/TWCS-part1.html"" rel=""nofollow noreferrer"">TWCS</a>.</p>
",['table']
48545341,48546909,2018-01-31 15:07:05,Cassandra query timeout,"<p>We are pulling data from around 20-25 industrial motor sensors and data is being stored in cassandra database.Cassandra as of now is running in a single node.</p>

<p>Below is the table structure</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>CREATE TABLE cisonpremdemo.machine_data (
    id uuid PRIMARY KEY,
    data_temperature bigint,
    data_current bigint,
    data_timestamp timestamp,
    deviceid text,
    
) WITH bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND default_time_to_live = 7884000
    AND gc_grace_seconds = 100;
	
CREATE INDEX deviceid_idx ON db.machine_data (deviceid);
CREATE INDEX data_timestamp_idx ON db.machine_data (data_timestamp);</code></pre>
</div>
</div>
</p>

<p>Data is being collected in this table for couple of months say at every 5 seconds for almost 24 hours so there is pretty huge volume of data.</p>

<p>I am trying to execute a date range based query using java and dotnet and in both cases i am getting time out errors (Cassandra failure during read query at consistency LocalOne (0 replica(s) responded over 1 required))</p>

<p>Query works fine if i give limit of 100 otherwise it fails anything above than that.Some of the things i have tried...</p>

<p>1) increased query time out.
2) reduced gc_grace_seconds to 100 (temporarily) to eliminate any tombstones.</p>

<p>Query used</p>

<pre><code>SELECT data_temperature AS ""DATA_TEMP"",data_current AS ""DATA_CURRENT"" FROM machine_data 
WHERE DATA_TIMESTAMP&gt;=1517402474699 
AND DATA_TIMESTAMP&lt;=1517402774699 
AND DEVICEID='BP_100' ALLOW FILTERING;
</code></pre>

<p>Not sure if i the table structure (primary key) is of a wrong choice. should it be both deviceid and timestamp ??</p>
",<cassandra><cassandra-jdbc>,"<p>The secondary indexes will almost surely fail. They should have ""not to low, not to high"" cardinality (which depends on # of nodes in ring). Its very hard to get right and you should really just avoid using it unless have strong need and the data fits (cross table consistency not possible with a denormalized table).</p>

<p>Another thing you should never use is <code>allow filtering</code>, thats there pretty much just for debugging/development and large spark job kinda things that are reading entire dataset. Its horribly expensive and will almost always result in timeouts long term.</p>

<p>Instead you should create new tables and also break them up by time so the partitions do not get too large. ie</p>

<pre><code>CREATE TABLE cisonpremdemo.machine_data_by_time (
    id uuid PRIMARY KEY,
    data_temperature bigint,
    data_current bigint,
    data_timestamp timestamp,
    yymm text,
    deviceid text,
    PRIMARY KEY ((deviceid, yymm), data_timestamp)
) WITH CLUSTERING ORDER BY (data_timestamp DESC);
</code></pre>

<p>When you insert your data, write to both. You should essentially create a table for each kind of request you have, so the data is in the format you need it. Do not model your table around how the data looks. If you do not need direct message lookups by uuid, do not make the <code>machine_data</code> table like you have above at all since thats not how you are querying it.</p>
",['table']
48548191,48566442,2018-01-31 17:33:57,Ordering by username in Cassandra,"<p>Let's say I have this table:</p>

<pre><code>CREATE TABLE ""users"" (
    username text,
    created_at timeuuid,
    email text,
    firstname text,
    groups list&lt;text&gt;,
    is_active boolean,
    lastname text,
    ""password"" text,
    roles list&lt;text&gt;,
    PRIMARY KEY (username, created_at)
) 
</code></pre>

<p>I want to order users by username, which is not possible as ordering is only possible via the clustering column. How can I order my users by username?</p>

<p>I need to query users by username, so that is the reason, why username is the indexing column.</p>

<p>What is the right approach here?</p>
",<cassandra><sql-order-by><clustering-key>,"<p>If you absolutely must have the username sorted, and return all usernames in one query then you will need to create another table for this effect:</p>

<pre><code>CREATE TABLE ""users"" (
field text,
value text,
PRIMARY KEY (field, value)
)
</code></pre>

<p>Unfortunately, this will put all the usernames in just one partition, but it's the only way of keeping them sorted. On the other hand, you could expand the table to store different values that you need to retrieve in the same way. So for instance, the partition field=""username"" would have all the usernames, but you could create another partition field=""Surname"" to store all the usernames sorted.</p>

<p>Cassandra is NoSQL, so duplication of data can be expected.</p>
",['table']
48560390,48565375,2018-02-01 10:23:37,Connect remote scylla db server shows error,"<p>I have installed scylla-db in google cloud servers.</p>

<p><strong>Steps i have followed:</strong></p>

<pre><code>sudo yum install epel-release

sudo curl -o /etc/yum.repos.d/scylla.repo -L http://repositories.scylladb.com/scylla/repo/a2a0ba89d456770dfdc1cd70325e3291/centos/scylladb-2.0.repo

sudo yum install scylla

sudo scylla_setup

(Given yes to ""verify supportable version"" , "" verify packages"" , ""core dump"", "" fstim ssd ""
For remaining : Given NO)

IN  file :/etc/scylla.d/io.conf

SEASTAR_IO=""--max-io-requests=12 --num-io-queues=1""
( edited this file manually )

sudo systemctl start scylla-server
</code></pre>

<p>It shows: Cannot able to read yaml file. Then google it and downgraded the yaml-cpp version to 0.5.1 from 0.5.3 version.
<em>then 
scylla-server started running .</em></p>

<pre><code>[root@scylla ~]# nodetool status
Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address    Load       Tokens       Owns    Host ID                               Rack
UN  127.0.0.1  208.69 KB  256          ?       888e91da-9385-4c61-8417-dd59c1a979b8  rack1
Note: Non-system keyspaces don't have the same replication settings, effective ownership information is meaningless


[root@scylla ~]# cat /etc/scylla/scylla.yaml | grep seeds:
          - seeds: ""127.0.0.1""
[root@scylla ~]# cat /etc/scylla/scylla.yaml | grep rpc_address:
rpc_address: localhost
#broadcast_rpc_address: 
[root@scylla ~]# cat /etc/scylla/scylla.yaml | grep listen_address:
listen_address: localhost

[root@scylla ~]# cqlsh
Connected to Test Cluster at 127.0.0.1:9042.
[cqlsh 5.0.1 | Cassandra 3.0.8 | CQL spec 3.3.1 | Native protocol v4]
Use HELP for help.
cqlsh&gt; exit



[root@scylla ~]# netstat -tupln | grep LISTEN
tcp        0      0 127.0.0.1:10000         0.0.0.0:*               LISTEN      6387/scylla         
tcp        0      0 127.0.0.1:9042          0.0.0.0:*               LISTEN      6387/scylla         
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1105/sshd           
tcp        0      0 127.0.0.1:7000          0.0.0.0:*               LISTEN      6387/scylla         
tcp        0      0 127.0.0.1:25            0.0.0.0:*               LISTEN      1119/master         
tcp        0      0 0.0.0.0:9180            0.0.0.0:*               LISTEN      6387/scylla         
tcp        0      0 127.0.0.1:9160          0.0.0.0:*               LISTEN      6387/scylla         
tcp6       0      0 :::80                   :::*                    LISTEN      5217/httpd          
tcp6       0      0 :::22                   :::*                    LISTEN      1105/sshd           
tcp6       0      0 :::35063                :::*                    LISTEN      6412/scylla-jmx     
tcp6       0      0 ::1:25                  :::*                    LISTEN      1119/master         
tcp6       0      0 127.0.0.1:7199          :::*                    LISTEN      6412/scylla-jmx     
</code></pre>

<p>scylla-server is running.</p>

<p>Same setup was done another server 
Server Name scylla-db-1</p>

<p>I need to connect to the server scylla ( IP: xx.xx.xxx) from this server.</p>

<p>when i execute the below :</p>

<pre><code>[root@scylla-db-1 ~]# cqlsh xx.xx.xxx
Connection error: ('Unable to connect to any servers', {'xx.xx.xxx': error(111, ""Tried connecting to [('xx.xx.xxx', 9042)]. Last error: Connection refused"")})
</code></pre>

<p><strong>How to connect the remote server from this server?</strong></p>

<p>Also 
while checking the <a href=""http://xx.xx.xxx:10000"" rel=""nofollow noreferrer"">http://xx.xx.xxx:10000</a>  and <a href=""http://xx.xx.xxx:10000/ui"" rel=""nofollow noreferrer"">http://xx.xx.xxx:10000/ui</a> in the browser , I m getting problem loading page.</p>

<p>Note :</p>

<blockquote>
  <p>I have done editing the /etc/scylla.d/io.conf file for assigning the
  max-io-requests manually </p>
</blockquote>
",<cassandra><scylla>,"<p>Port 10000 is the rest api for scylla and is usually left bounded to the 127.0.0.1 - thats why you can not access it</p>

<p>To gain access via cql you need to edit the /etc/scylla/scylla.yaml file and set the rpc_address </p>

<p>Please follow the instructions for configuring scylla for a cluster deployment: single dc <a href=""http://docs.scylladb.com/procedures/create_cluster/"" rel=""noreferrer"">http://docs.scylladb.com/procedures/create_cluster/</a> or multi dc <a href=""http://docs.scylladb.com/procedures/create_cluster_multidc/"" rel=""noreferrer"">http://docs.scylladb.com/procedures/create_cluster_multidc/</a>.</p>
","['rpc_address', 'dc']"
48565728,48566771,2018-02-01 14:59:09,GeoMesa: Cassandra table with composite key,"<p>Is it possible to create a Cassandra table with GeoMesa specifying keys (ie - a composite key)? I have a spark job that writes to Cassandra and a composite key is necessary for the output table. I would now like to create/write that same table somehow through the GeoMesa api instead of directly to Cassandra. The format is like this:</p>

<pre><code>CREATE TABLE IF NOT EXISTS mykeyspace.testcompkey (pkey1 text, ckey1 int, attr1 int, attr2 int, minlat decimal, minlong decimal, maxlat decimal, maxlong decimal, updatetime text, PRIMARY KEY((pkey1), ckey1) )
</code></pre>

<p>Is this possible? You can see also in the create table statement that I have a partition key and a clustering key. From what I have read, I believe Geoserver does support both Simple and Complex features. I am just wondering if that support also maps over into the realm of Cassandra with GeoMesa?</p>

<p>Thank you</p>
",<cassandra><geoserver><geomesa>,"<p>GeoMesa does use composite partition and clustering keys for Cassandra tables, but the keys are not configurable by the user - they are designed to facilitate spatial/temporal/attribute CQL queries.</p>

<p>Keys can be seen in the index table implementations <a href=""https://github.com/locationtech/geomesa/tree/geomesa_2.11-1.3.5/geomesa-cassandra/geomesa-cassandra-datastore/src/main/scala/org/locationtech/geomesa/cassandra/index"" rel=""nofollow noreferrer"">here</a>. The <code>columns</code> field (for example <a href=""https://github.com/locationtech/geomesa/blob/geomesa_2.11-1.3.5/geomesa-cassandra/geomesa-cassandra-datastore/src/main/scala/org/locationtech/geomesa/cassandra/index/CassandraZ3Layout.scala#L20-L25"" rel=""nofollow noreferrer"">here</a>) defines the primary keys. Columns with <code>partition = true</code> are used for partitioning, the rest are used for clustering.</p>
",['table']
48568454,48568570,2018-02-01 17:24:05,Parallelism in reading Oracle data from using Spark 1.6.2 JDBC,"<p>We have around 40 million records for table.</p>

<p>How to choose the values of <code>lowerBound</code>, <code>upperBound</code>, and <code>numPartitions</code> while reading data from Oracle using Spark 1.6.2.</p>

<p>I have partition key.
we have datastax cluster - 3 nodes
                         - 18  cores each
                         - 27 GB for each    </p>

<p>Thanks in advance.</p>
",<oracle><scala><apache-spark><cassandra>,"<p>The simplest heuristic is to calculate min and max of the column values. In SQL (normal JDBC) you can run:</p>

<pre><code>select min(column) as lowerBound, max(column) as upperBound from table;
</code></pre>

<p>And set lowerBound and upperBound to the result of query.</p>

<p>But, it's not always so simple. Sometimes column has very skewed distribution - i.e., <code>SomeFunnyID</code> may have 100x values from range [0..100] and then 100.000.000x values higher than 100000. Then you reading will be very skewed and, because of that, slower. </p>

<p>In such cases I recommend to set <code>dbtable</code> parameter to <code>select mod(s.someFunnyID, partitionCount) as partition_key, s.* from table s</code> and then query set <code>lowerBound</code> to <code>0</code> and <code>""upperBound""</code> to <code>partitionCount</code></p>

<pre><code>val partitionCount = // here choose partition count
val df = spark.read.jdbc(...)
    .option(""dbtable"", s""select mod(s.someFunnyID, $partitionCount) as partition_key, s.* from table s"")
    .option(""lowerBound"", ""0"")
    .option(""upperBound"", partitionCount)
    .option(""partitionColumn"", ""partition_key"")
    .load()
</code></pre>

<p>For partitionCount, I have only one simple heuristic: number of executors * executor cores</p>

<p>Be aware, that each time you must benchmark you configuration to see if it's correct in your case</p>
",['table']
48575944,48576742,2018-02-02 04:58:20,Normalization in Cassandra for the specified use case?,"<p>I have a device table (say 'device' table) which has the static fields with current statistics and I have another table (say 'devicestat' table) which has the statistics of that device collected for every one minute and sorted by timestamp like below.</p>

<p><strong><em>Example :</em></strong> </p>

<pre><code>CREATE TABLE device(
   ""partitionId"" text,
   ""deviceId"" text,
   ""name"" text,
   ""totalMemoryInMB"" bigint,
   ""totalCpu"" int,
   ""currentUsedMemoryInMB"" bigint,
   ""totalStorageInMB"" bigint,
   ""currentUsedCpu"" int,
   ""ipAddress"" text,
    primary key (""partitionId"",""deviceId""));


CREATE TABLE devicestat(
   ""deviceId"" text,
   ""timestamp"" timestamp,
   ""totalMemoryInMB"" bigint,
   ""totalCpu"" int,
   ""usedMemoryInMB"" bigint,
   ""totalStorageInMB"" bigint,
   ""usedCpu"" int
    primary key (""deviceId"",""timestamp""));
</code></pre>

<p>where, </p>

<pre><code>currentUsedMemoryInMB &amp; currentUsedCpu =&gt; Hold the most recent statistics

usedMemoryInMB &amp; usedCpu =&gt; Hold the most and also old statistics based on time stamp.
</code></pre>

<p>Could somebody suggest me the correct approach for the following concept?</p>

<p>So whenever I need static data with the most recent statistics I read from <strong><code>device</code></strong> table, Whenever I need history of device staistical data I read from the <strong><code>devicestat</code></strong> table</p>

<p>This looks fine for me, But only problem is I need to write the statitics in both table, In case of <strong><code>devicestat</code></strong> table It will be a new entry based on timestamp but In case of <strong><code>device</code></strong> table, we will just update the statistics. What is your thought on this, Does this need to be maintained in only the single stat table or Is it fine to update the most recent stat in device table too.</p>
",<cassandra><cassandra-3.0>,"<p>in Cassandra the common approach is to have a table(ColumnFamily) per query. And denormalization is also a good practice in Cassandra. So it's ok to keep 2 column families in this case. </p>

<p>Another way to get the latest stat from devicestat table is make data be DESC sorted by timestamp:</p>

<pre><code>CREATE TABLE devicestat(
   ""deviceId"" text,
   ""timestamp"" timestamp,
   ""totalMemoryInMB"" bigint,
   ""totalCpu"" int,
   ""usedMemoryInMB"" bigint,
   ""totalStorageInMB"" bigint,
   ""usedCpu"" int
    primary key (""deviceId"",""timestamp""))
WITH CLUSTERING ORDER BY (timestamp DESC);
</code></pre>

<p>so you can query with <code>limit 1</code> when you know deviceId</p>

<pre><code>select * from devicestat where deviceId = 'someId' limit 1;
</code></pre>

<p>But if you want to list last stat of devices by partitionId then your approach with updating device table with latest stat is correct</p>
",['table']
48608390,48616848,2018-02-04 13:09:41,"Yaml change not detected,Exception encountered during startup: Invalid yaml: file:/etc/cassandra/cassandra.yaml","<p>I have changed Cassandra configuration file</p>

<pre><code>cat /etc/cassandra/cassandra.yaml | grep -n 'seed'
416:seed_provider:
423:          # seeds is actually a comma-delimited list of addresses.
425:          - seeds:""84.208.89.132,192.168.0.23,192.168.0.25,192.168.0.28""
</code></pre>

<p>and also cluster name</p>

<pre><code>10:cluster_name: 'Petter Cluster'
</code></pre>

<p>I am surprised to see what the system.log shows</p>

<pre><code>INFO  [main] 2018-01-27 17:20:51,343 YamlConfigurationLoader.java:89 - Configuration location: file:/etc/cassandra/cassandra.yaml
ERROR [main] 2018-01-27 17:20:51,427 CassandraDaemon.java:706 - Exception encountered during startup: Invalid yaml: file:/etc/cassandra/cassandra.yaml
 Error: while parsing a block mapping; expected &lt;block end&gt;, but found FlowEntry;  in 'reader', line 425, column 34:
              - seeds: ""192.168.0.13"",""192.168.0.23"",""192.168.0.25"","" ... 
                                     ^
INFO  [main] 2018-02-03 20:35:48,528 YamlConfigurationLoader.java:89 - Configuration location: file:/etc/cassandra/cassandra.yaml
ERROR [main] 2018-02-03 20:35:48,844 CassandraDaemon.java:706 - Exception encountered during startup: Invalid yaml: file:/etc/cassandra/cassandra.yaml
 Error: null; Can't construct a java object for tag:yaml.org,2002:org.apache.cassandra.config.Config; exception=Cannot create property=seed_provider for JavaBean=org.apache.cassandra.config.Config@551bdc27; java.lang.reflect.InvocationTargetException;  in 'reader', line 10, column 1:
    cluster_name: 'Test Cluster'
    ^
INFO  [main] 2018-02-03 20:39:08,311 YamlConfigurationLoader.java:89 - Configuration location: file:/etc/cassandra/cassandra.yaml
ERROR [main] 2018-02-03 20:39:08,647 CassandraDaemon.java:706 - Exception encountered during startup: Invalid yaml: file:/etc/cassandra/cassandra.yaml
 Error: null; Can't construct a java object for tag:yaml.org,2002:org.apache.cassandra.config.Config; exception=Cannot create property=seed_provider for JavaBean=org.apache.cassandra.config.Config@551bdc27; java.lang.reflect.InvocationTargetException;  in 'reader', line 10, column 1:
    cluster_name: 'Test Cluster'
</code></pre>

<p>How to fix this?How to initialize system after the changes?</p>
",<java><cassandra><yaml>,"<p>It seems you have got into a issue with Cluster name,it is supposed be changed on all the nodes if you willing to change it. </p>

<p>Here are instruction to change Cluster name :
1. Log into cqlsh
2. cqlsh> UPDATE system.local SET cluster_name = 'Petter Cluster' where key='local';  (You need to issue this command on each of the nodes where you would like to change the cluster name. )
system.local gets changed only locally
3. cqlsh> exit;
4. $ nodetool flush system
5. edit cassandra.yaml cluster name to YOUR_CLUSTER_NAME.
6. Restart cassandra.</p>

<p>Please check this link as well:
<a href=""https://surbhinosqldba.wordpress.com/2015/07/23/how-to-rename-modify-cassandra-cluster-name/"" rel=""nofollow noreferrer"">https://surbhinosqldba.wordpress.com/2015/07/23/how-to-rename-modify-cassandra-cluster-name/</a></p>
",['cluster_name']
48636172,48659155,2018-02-06 05:40:53,Does nodetool for cassandra only gather data for a single node or for the entire cluster?,"<p>I have a 19-node Cassandra cluster for our internal service. If I log into a node using nodetool and run commands like tablestats, etc, does that gather stats just for that particular node or for the entire cluster?</p>
",<cassandra><nodetool>,"<p><code>nodetool</code> utility for cassandra gather for entire cluster, not a single node. 
For example, if you run command like-</p>

<blockquote>
  <p>command:</p>
</blockquote>

<pre><code>nodetool tablestats musicdb.artist
</code></pre>

<blockquote>
  <p>result:</p>
</blockquote>

<pre><code>Keyspace: musicdb
Read Count: 0
Read Latency: NaN ms.
Write Count: 0
Write Latency: NaN ms.
Pending Flushes: 0
    Table: artist
    SSTable count: 1
    Space used (live): 62073
    Space used (total): 62073
    Space used by snapshots (total): 0
    Off heap memory used (total): 1400
    SSTable Compression Ratio: 0.27975344141453456
    Number of keys (estimate): 1000
    Memtable cell count: 0
    Memtable data size: 0
    Memtable off heap memory used: 0
    Memtable switch count: 0
    Local read count: 0
    Local read latency: NaN ms
    Local write count: 0
    Local write latency: NaN ms
    Pending flushes: 0
    Bloom filter false positives: 0
    Bloom filter false ratio: 0.00000
    Bloom filter space used: 1264
    Bloom filter off heap memory used: 1256
    Index summary off heap memory used: 128
    Compression metadata off heap memory used: 16
    Compacted partition minimum bytes: 104
    Compacted partition maximum bytes: 149
    Compacted partition mean bytes: 149
    Average live cells per slice (last five minutes): 0.0
    Maximum live cells per slice (last five minutes): 0
    Average tombstones per slice (last five minutes): 0.0
    Maximum tombstones per slice (last five minutes): 0
</code></pre>

<hr>

<p>Status of the table <code>artist</code> belongs to keyspace <code>musicdb</code> above is from the <strong>entire cluster</strong>.</p>
",['table']
48707291,48769275,2018-02-09 13:45:22,Cassandra how to query to get the most recent message,"<p>Please i need a help am new to Cassandra. I have a table called conversation,
user1 is my partition key while time is my clustering key<br></p>

<p>How can i query this table to get all users, user1 have established conversation with, including the most recent message.</p>

<pre>
CREATE TABLE conversation (
  user1 text,
  conversationId text,
  message text,
  user2,
  time,
  PRIMARY KEY(user1, time) 
) WITH CLUSTERING ORDER BY (time DESC)

.............................................................
| User1 | ConversationId |    Message    | User 2  |  Time  |
|.......|................|...............|.........|........|
| Bobby | 100 - 101      | Hello         | Chris   |12:10pm |
| Bobby | 100 - 101      | U there?      | Chris   |12:11pm |
| Bobby | 100 - 102      | Am here       | Dan     |12:12pm |
| Bobby | 100 - 102      | Hello Dan     | Dan     |12:13pm |
| Bobby | 100 - 103      | Am coming     | Sam     |12:14pm |
| Bobby | 100 - 103      | Hello sam     | Sam     |12:15pm |

This should be my output after query.

.............................................................
| User1 | ConversationId |    Message    | User 2  |  Time  |
|.......|................|...............|.........|........|
| Bobby | 100 - 103      | Hello sam     | Sam     |12:15pm |
| Bobby | 100 - 102      | Hello Dan     | Dan     |12:13pm |
| Bobby | 100 - 101      | U there?      | Chris   |12:11pm |



</pre>
",<database><cassandra><nosql><datastax><cql>,"<p>You cannot do that in Cassandra with your existing table. But you could create a new table for this purpose, and insert all records in that table too.</p>

<pre><code>CREATE TABLE conversation_lastmessage (
  user1 text,
  conversationId text,
  message text,
  user2 text,
  time timestamp,
  PRIMARY KEY ((user1), user2));
</code></pre>

<p>Assuming that you insert your data in order of time, you will get the latest messages of user1 with every correspondent.</p>

<pre><code>SELECT * FROM conversation_lastmessage WHERE user1 = 'user1';
</code></pre>
",['table']
48729254,48732871,2018-02-11 07:17:17,High Availability & Performance consideration with secondary index in Cassandra,"<p>I have a <strong><em>Set up with</em></strong>: 5 Cassandra node cluster with RF =3, I performed a secondary index for a column in the table 'user',</p>

<p>1) As per my study on Secondary Index using the link: <a href=""https://www.datastax.com/dev/blog/cassandra-native-secondary-index-deep-dive"" rel=""nofollow noreferrer"">https://www.datastax.com/dev/blog/cassandra-native-secondary-index-deep-dive</a> I understood that secondary indexes will be stored in the local node. Does it mean that in the five node cluster only in one node the secondary index will be available? If not in the RF =3 for <strong>user</strong> table, In how many nodes the Secondary Index table will be available?</p>

<p>2) How does the following two query differ in execution? </p>

<pre><code>   CREATE TABLE user(
    user_group int PRIMARY KEY,
    user_name text,
    user_phone varint
   );

  CREATE INDEX username_idx ON user (user_name);
</code></pre>

<p>In this table setup, </p>

<p><strong>Query 1</strong> :  SELECT * FROM user WHERE user_name = 'test';</p>

<p><strong>Query 2</strong> :  SELECT * FROM user WHERE  user_group = 1 AND user_name = 'test';</p>

<p>How many nodes (In the 5 node cluster) will the above two queries pass through for execution and How the two queries differ in performance?</p>

<p><strong>Edited :</strong></p>

<p>Say I have a table like below,</p>

<pre><code>CREATE TABLE nodestat (
    uniqueId text,
    totalCapacity int,
    physicalUsage int,
    flashMode text,
    timestamp timestamp,
    primary key (uniqueId, timestamp)) 
    with clustering order by (timestamp desc);

CREATE CUSTOM INDEX nodeIp_idx ON nodestat(flashMode)
</code></pre>

<p><strong>Query 3</strong> :  select * from nodestat where uniqueId = 'test' AND flashMode = 'yes'</p>

<p>So In this case, I always have only one partition in the table, so How does the secondary index search differ compare to the secondary index without partition key? How efficient is it? </p>
",<indexing><cassandra><cassandra-3.0>,"<p>Regd your Question 1:</p>

<p><em>Does it mean that in the five node cluster only in one node the secondary index will be available?</em> </p>

<p>The secondary index is available in every node of the cluster, built upon the data in that node and its just local to that node. That is, its aware of only the primary keys in that particular node. You can imagine the secondary index to be a lookup table with references to primary keys on that node.</p>

<p>So every node builds its own secondary index (in your case all 5), but unaware of each others references.</p>

<p><em>If not in the RF =3 for user table, In how many nodes the Secondary Index table will be available?</em></p>

<p>There is no replication factor for secondary indexes, since its local to every node. Since your data is already being replicated RF = 3, your secondary indexes in every node will have that indexed.</p>

<p>Regd your Question 2:</p>

<pre><code>Query 1 : SELECT * FROM user WHERE user_name = 'test';
</code></pre>

<p>This query is going to perform a scatter gather on all nodes in the cluster. Since the secondary indexes are local to each node, every node (in your case all 5) has to execute the query -> perform a secondary index lookup to figure out the partition key -> then fetch the actual results back to coordinator.</p>

<p>As the table grows bigger, the query often results in timeout. In extreme cases it can bring down the node (just like ""select *"" without partition key). <strong>Hence secondary indexes and this type of query (without partition key) in general are discouraged in Cassandra and better to avoid them</strong></p>

<pre><code>Query 2 : SELECT * FROM user WHERE user_group = 1 AND user_name = 'test';
</code></pre>

<p>This query will perform better compared to the previous one, as it has filter on partition key. In the table definition above there is no clustering column, so this query would just filter on primary key as there is only one row per partition. Hence there isn't much improvement with secondary index. Overall its not a scatter gather type of query and hence perform much better.  </p>

<p><strong>edited to explain query3</strong></p>

<pre><code>Query 3 : select * from nodestat where uniqueId = 'test' AND flashMode = 'yes'
</code></pre>

<p>In this query the secondary index is used in conjunction with partition key. This secondary index would help in case of 1000s of clustering columns exists for a given partition key and we want to quickly narrow down on the resultset. Remember the secondary index stores the entire primary key (partition key + clustering column reference). So in case of a wide partition, this secondary index proves useful when used alongside a partition key.</p>

<p>For example in your case, say there is only one partition uniqueId = 'test'. But within that partition 'test', say there are 10000 different timestamp values (clustering column). So potentially there could be 10000 different values for ""flashMode"". This secondary index will help narrow down to the ""flashMode"" column with value ""yes"" within the partition 'test' amongst that 10000 matches.</p>
",['table']
48731286,48733064,2018-02-11 12:05:19,Representing a super column in Cassandra CQL,"<p>Currently, I have the following schema for one of my tables:</p>

<pre><code>id
name
...
country
  - id
  - name
  - city
    - id
    - name
</code></pre>

<p>I was looking around the Cassandra documentation, and I cannot find any clear examples or demonstrations as to how I would represent my super columns in a column family. The code I have is as follows:</p>

<pre><code>CREATE COLUMNFAMILY table (
    id varint,
    name varchar,
    &lt;&lt;regular columns omitted&gt;&gt;
    country ..?,
    PRIMARY KEY = (id)
);
</code></pre>
",<cassandra><nosql><cql><super-columns>,"<p>You can create a user-defined type to attach multiple data fields to a column.</p>

<p>For example, in your case</p>

<pre><code>country
  - id
  - name
  - city
    - id
    - name
</code></pre>

<p>Can be represented in a UDT as</p>

<pre><code>CREATE TYPE mykeyspace.countryudt (
  id uuid,
  name text,
  city map&lt;uuid, text&gt;
);
</code></pre>

<p>Now the table definition will look like,</p>

<pre><code>CREATE COLUMNFAMILY table (
    id varint,
    name varchar,
    &lt;&lt;regular columns omitted&gt;&gt;
    country frozen &lt;countryudt&gt;,
    PRIMARY KEY = (id)
);
</code></pre>

<p>Additional reference for UDT <a href=""https://docs.datastax.com/en/cql/3.1/cql/cql_using/cqlUseUDT.html"" rel=""nofollow noreferrer"">here</a>.</p>
",['table']
48732843,48732954,2018-02-11 14:58:49,Secondary index relating to Replication Factor,"<p>I am using the Secondary index for one of the column in Cassandra table.,</p>

<p>Say I have a <strong>5 node cluster</strong> (192.168.1.1, 192.168.1.2, 192.168.1.3, 192.168.1.4, 192.168.1.5) with the Keyspace <strong>replication factor as '3'</strong> and considering the following table,</p>

<pre><code>CREATE TABLE nodestat (
    uniqueId text,
    totalCapacity int,
    physicalUsage int,
    flashMode text,
    timestamp timestamp,
    primary key (uniqueId, timestamp)) 
    with clustering order by (timestamp desc);
</code></pre>

<p>In this, I have the value of uniqueId as '<strong>test</strong>', which means I just have <strong>only one partition named 'test'</strong>. </p>

<p>When I perform the getEndPoints, I could see that the data resides in only 3 nodes.</p>

<pre><code>./nodetool getendpoints keyspacename nodestat test
</code></pre>

<p>192.168.1.1
192.168.1.2
192.168.1.3</p>

<p>So my partition data is available in 3 nodes, I did the secondary index on one of the columns,</p>

<pre><code>CREATE CUSTOM INDEX nodeIp_idx ON nodestat(flashMode)
</code></pre>

<p>So now when I perform </p>

<pre><code>select * from nodestat where uniqueId = 'test' AND flashMode = 'yes'
</code></pre>

<p><strong>How many nodes will it go to collect the data?</strong> </p>
",<cassandra><cassandra-3.0>,"<pre><code>select * from nodestat where uniqueId = 'test' AND flashMode = 'yes'
</code></pre>

<p>Based on this query, you are using partition key along with a secondary index. Hence it will behave like a normal query based on the chosen consistency level. That is if ""local_one"" only one node will be enough to respond and if ""local_quorum"" a quorum of nodes in that dc will have to respond. Secondary index will further assist to narrow down the resultset.</p>

<p>Remember secondary index are local to data in every node of that cluster and hence present in all nodes of the cluster. Additional reference <a href=""https://stackoverflow.com/questions/48729254/high-availability-on-secondary-index-in-cassandra"">here</a>.</p>

<p><strong>In short, there is no direct correlation of Replication factor to Secondary index.</strong></p>
",['dc']
48734670,48735864,2018-02-11 18:03:00,SASI index in Cassandra and How it differs from normal indexing,"<p>I started using <strong><em>SASI indexing</em></strong> and used the following setup,</p>

<pre><code>CREATE TABLE employee (
    id int,
    lastname text,
    firstname text,
    dateofbirth date,
    PRIMARY KEY (id, lastname, firstname)
) WITH CLUSTERING ORDER BY (lastname ASC, firstname ASC));

CREATE CUSTOM INDEX employee_firstname_idx ON employee (firstname) USING 'org.apache.cassandra.index.sasi.SASIIndex' WITH OPTIONS = {'mode': 'CONTAINS', 'analyzer_class': 'org.apache.cassandra.index.sasi.analyzer.StandardAnalyzer', 'case_sensitive': 'false'};
</code></pre>

<p>I perform the following query,</p>

<pre><code>SELECT * FROM employee WHERE firstname like '%s';
</code></pre>

<p>As per my study, It seems the same as normal secondary indexing in Cassandra, Except providing the <strong><em>LIKE</em></strong> search, </p>

<p>1) Could somebody explain how it differs from normal secondary index in Cassandra?<br>
2) What are the best configurations like mode, analyzer_class and case_sensitive - Any recommended documentation for this?</p>
",<cassandra><cassandra-3.0>,"<p><em>1) Could somebody explain how it differs from normal secondary index in Cassandra?</em></p>

<p>Normal secondary index is essentially another lookup table comprising secondary index columns &amp; primary key. Hence it has its own set of sstable files (disk), memtable (memory) and write overhead (cpu). </p>

<p>SASI was an improvement open sourced (contributed by Apple) to Cassandra community. This index gets created for every SSTable being flushed to disk and doesn't maintain a separate table. Hence less disk usage, no separate memtable/bloom filter/partition index (less memory) and minimal overhead. </p>

<p><em>2) What are the best configurations like mode, analyzer_class and case_sensitive - Any recommended documentation for this?</em></p>

<p>Configuration depends on your use case :-</p>

<p>Essentially there are three modes</p>

<ol>
<li>PREFIX - Used to serve LIKE queries based on prefix of indexed column</li>
<li>CONTAINS - Used to serve LIKE queries based on whether the search term exists in the indexed column</li>
<li>SPARSE - Used to index data that is sparse (every term/column value has less than 5 matching keys). For example range queries that span large timestamps.</li>
</ol>

<p>Analyzer_class : Analyzers can be specified that will analyze the text in the specified column. </p>

<ol>
<li>The <strong>NonTokenizingAnalyzer</strong> is used for cases where the text is not analyzed, but case normalization or sensitivity is required. </li>
<li>The <strong>StandardAnalyzer</strong> is used for analysis that involves stemming, case normalization, case sensitivity, skipping common words like ""and"" and ""the"", and localization of the language used to complete the analysis</li>
</ol>

<p>case_sensitive : As name implies, whether the indexed column should be searched case insensitive. Applicable values are </p>

<ol>
<li>True</li>
<li>False</li>
</ol>

<p>Detailed documentation reference <a href=""https://docs.datastax.com/en/dse/5.1/cql/cql/cql_using/useSASIIndexConcept.html"" rel=""noreferrer"">here</a> and detailed blog post on <a href=""http://www.doanduyhai.com/blog/?p=2058#sasi_perf_benchmarks"" rel=""noreferrer"">performance</a>.</p>
",['table']
48792385,48792627,2018-02-14 16:53:28,Selecting from multiple tables in Cassandra CQL,"<p>So I have two tables in the query I am using:</p>

<pre><code>SELECT
    R.dst_ap, B.name
FROM airports as A, airports as B, routes as R
WHERE R.src_ap = A.iata
AND R.dst_ap = B.iata;
</code></pre>

<p>However it is throwing the error:</p>

<pre><code>mismatched input 'as' expecting EOF (..., B.name    FROM airports [as] A...)
</code></pre>

<p>Is there anyway I can do what I am attempting to do (which is how it works relationally) in Cassandra CQL?</p>
",<cassandra><nosql><cql><cql3><cqlsh>,"<p>The short answer, is that <strong>there are no joins in Cassandra</strong>.  Period.  So using SQL-based JOIN syntax will yield an error similar to what you posted above.</p>

<p>The idea with Cassandra (or any distributed database) is to ensure that your queries can be served by a single node (cutting down on network time).  There really isn't a way to guarantee that data from different tables could be queried from a single node.  For this reason, distributed joins are typically seen as an anti-pattern.  To that end, Cassandra simply doesn't allow them.</p>

<p>In Cassandra you need to take a query-based modeling approach.  So you <em>could</em> solve this by building a table from your post-join result set, consisting of desired combinations of <code>dst_ap</code> and <code>name</code>.  You would have to find an appropriate way to partition this table, but ultimately you would want to build it based on A) the result set you expect to see and B) the properties you expect to filter on in your WHERE clause.</p>
",['table']
48793844,48799416,2018-02-14 18:22:19,Selecting data based on specific column in user-defined type,"<p>So I have the following columnfamily with its respective types:</p>

<pre><code>CREATE TYPE subudt (
    id varint,
    -- snipped --
);

CREATE TYPE udt (
    name text,
    -- snipped --
    subudt frozen &lt;subudt&gt;
);

CREATE COLUMNFAMILY tablename (
    id varint,
    -- snipped --
    udt frozen &lt;udt&gt;,
    PRIMARY KEY (id)
); 
</code></pre>

<p>How can I perform a select query on the <code>name</code> field in the <code>udt</code> type? I was looking around and it seems that you cannot use <code>CREATE INDEX</code> on the udt fields, but only on the entire user defined type itself.</p>
",<cassandra><nosql><cql><cqlsh>,"<p>Ideally you model the data in Cassandra based on queries it will serve. Querying for specific fields within UDT, defeats the purpose of having them combined in the first place.</p>

<p>Having secondary indexes will depend on what type of query its trying to solve. The performance varies depending on different query structures explained <a href=""https://stackoverflow.com/questions/48729254/high-availability-performance-consideration-with-secondary-index-in-cassandra/48732871#48732871"">here</a>.</p>

<p>In short, you can't create Indexes on specific fields and ideally should look at modeling the data different. Its absolutely okay to duplicate the data to serve different query patterns. Say maintaining a whole new table to serve queries based on ""names"" is common.</p>
",['table']
48839541,48842875,2018-02-17 08:48:01,How can i see an individual cluster latency?,"<p>How can i see an individual cluster latency, number of node up/down, rea/write performance of a cluster in cassandra? Does cassandra stores all these information somewere?</p>
",<cassandra><cql>,"<p>I would suggest looking at the nodetool utility for this information.</p>

<pre><code>nodetool ring
</code></pre>

<p>will give you the up/down status of the cluster and</p>

<pre><code>nodetool tablehistograms
</code></pre>

<p>will give you table read/write latencies. Nodetool has many other commands for looking at the cluster or node internals. </p>

<p>There is a lot of information about it <a href=""https://docs.datastax.com/en/cassandra/3.0/cassandra/tools/toolsNodetool.html"" rel=""nofollow noreferrer"">here</a>.</p>
",['table']
48849615,48850850,2018-02-18 07:59:25,Data Model design approach for one use case in Cassandra,"<p>I am in need of the best approach for the following use case,</p>

<p>I have 'Device' table (Only one Partition Id : 'Device') and I have another table 'DeviceStatistics' (Partition Id : 'deviceId' so that this table will have as many partition as number of devices) which means for every device there will be a statistics collected for every minute.</p>

<pre><code>CREATE TABLE device(
   ""partitionId"" text,""name"" text,""deviceId"" text, ..., primary key (""partitionId"",""name"",""deviceId""));
</code></pre>

<p>where partitionId - it is a constant ('device')</p>

<pre><code>CREATE TABLE deviceStatistics (    
""deviceId"" text,     
""timestamp"" timestamp, ...,
primary key (""deviceId"",""timestamp"")) with clustering order by (""timestamp"" DESC);
</code></pre>

<p>where 'deviceId' - it is the partition key and under each partition will have the list of timestamp entries </p>

<p>Till this it is fine, Because I just need the following queries,</p>

<pre><code>1) select * from device where partitionId = 'device'
   - which list all the devices available. 
2) select * from deviceStatistics where deviceId = 'deviceId_1'
   - which list all the device statistics for a deviceId
3) select * from deviceStatistics where deviceId = 'deviceId_1' LIMIT 1
   - which gets the most recent statistics for a deviceId 
</code></pre>

<p><strong>Now I need the solution for the following use case</strong>,</p>

<p>I need to collect the cluster level statistics which means I need to collect all the device statistics for the timestamp,</p>

<p>(i.e) If the deviceStatistics for 4 devices are available for the timestamp then I need to collect all the four statistics for a timestamp and add in device group level. </p>

<p>which means my DeviceGroupstatistics is the aggregation of all the device statistics for the timestamp. </p>

<p>Now the problem is, Since I have 'deviceId' as the partitionId for the deviceStatistics table, I need to perform this query (select * from deviceStatistics where deviceId = 'deviceId' LIMIT 1) for all the deviceIds.
So Lets say I have 1000 devices, then I need to trigger this query for all the 1000 devices for every minute. </p>

<p>Is there a better design for this? </p>
",<cassandra><cassandra-3.0>,"<p>I would recommend to have separate table where timestamp will be the partition key, and device ID is clustering key. Granularity of timestamp may depend on your application - for example, drop seconds &amp; round to minutes, or something like.</p>

<p>You can implement storing the data from your application (preferred), or you can use materialized views (but they are experimental, and not always recommended to use).</p>
",['table']
48876392,48876752,2018-02-20 00:56:15,If not MaterializedViews and not secondary indices then what else is the recommended way to query data in cassandra,"<p>I have some data in Cassandra. Say </p>

<pre><code>create table MyTable {
    id text PRIMARY KEY,
    data text,
    updated_on timestamp
}
</code></pre>

<p>My application in addition to querying this data by primary key id, needs to query it by updated_on timestamp as well. To fulfil the query by time use case I have tried the following.</p>

<pre><code>create table MyTable {
    id text PRIMARY KEY,
    data text,
    updated_on timestamp,
    updated_on_minute timestamp
}
</code></pre>

<ol>
<li><p>Secondary index on the updated_on_minute field. As I understand, secondary indexes are not recommended for high cardinality cases (which is my case, because I could have a lot of data at the same minute mark). Moreover I have data that gets frequently updated, which means the updated_on_minute will keep revving.</p></li>
<li><p>MaterializedView with updated_on_minute as the partition key and a id as the clustering key. I am on version 3.9 of cassandra and had just begun using these, but alas I find these release notes for 3.11x (<a href=""https://github.com/apache/cassandra/blob/cassandra-3.11/NEWS.txt"" rel=""nofollow noreferrer"">https://github.com/apache/cassandra/blob/cassandra-3.11/NEWS.txt</a>), which declare them purely experimental and not meant for production clusters.</p></li>
</ol>

<p>So then what are my options? Do I just need to maintain my own tables to track data that comes in timewise? Would love some input on this.</p>

<p>Thanks in advance.</p>
",<cassandra><cassandra-3.0>,"<p>As always have been the case, create additional table to query by a different partition key.</p>

<p>In your case the table would be </p>

<pre><code>create table MyTable_by_timestamp {
    id text,
    data text,
    updated_on timestamp,
    Primary key(updated_on, id)
}
</code></pre>

<p>Write to both tables mytable_by_timetamp and mytable_by_id. Use the corresponding table to READ from based on the partition key either updated_on or id.</p>

<p>It’s absolutely fine to duplicate data based on the use case (query) it’s trying solve.</p>

<p><strong>Edited:</strong></p>

<p>In case there is a fear about huge partition, you can always bucket into smaller partitions. For example the table above could be broken down into</p>

<pre><code>create table MyTable_by_timestamp {
    id text,
    data text,
    updated_on timestamp,
    updated_min timestamp,
    Primary key(updated_min, id)
}
</code></pre>

<p>Here I have chosen every minute as the bucket size. Depending on how many updates you receive, you can change it to seconds (updated_sec) to reduce the partition size further. </p>
",['table']
48894065,49048141,2018-02-20 20:56:19,Cassandra phantom-dsl derived column is missing in create database generated queries,"<p>I have following table definition</p>

<pre><code>import com.outworkers.phantom.builder.primitives.Primitive
import com.outworkers.phantom.dsl._

abstract class DST[V, P &lt;: TSP[V], T &lt;: DST[V, P, T]] extends Table[T, P] {
  object entityKey extends StringColumn with PartitionKey {
    override lazy val name = ""entity_key""
  }

 abstract class entityValue(implicit ev: Primitive[V]) extends PrimitiveColumn[V] {
    override lazy val name = ""entity_value""
  }
</code></pre>

<p>In concrete table sub class</p>

<pre><code>abstract class SDST[P &lt;: TSP[String]] extends DST[String, P, SDST[P]] {
  override def tableName: String = ""\""SDS\""""

  object entityValue extends entityValue
}
</code></pre>

<p>Database class</p>

<pre><code>class TestDatabase(override val connector: CassandraConnection) extends Database[TestDatabase](connector) {
object SDST extends SDST[SDSR] with connector.Connector {
    override def fromRow(r: Row): SDSR=
      SDSR(entityKey(r), entityValue(r))
 }
}
</code></pre>

<p>The create table query generated by phantom-dsl looks like below</p>

<p>database.create()</p>

<pre><code>c.o.phantom Executing query: CREATE TABLE IF NOT EXISTS test.""SDS"" (entity_key text,PRIMARY KEY (entity_key))
</code></pre>

<p>As you can see derived column is missing from the create table DDL.</p>

<p>Please let me know if I am missing something in the implementation.</p>

<p>Omitted class definitions like SDSR and TSP are simple case classes. </p>

<p>Thanks</p>
",<cassandra><cassandra-3.0><phantom-dsl>,"<p>Phantom doesn't currently support table to table inheritance. The reasons behind that decision are complexities inherent in the Macro API that we rely on to power the DSL.</p>

<p>This feature is planned for a future release, but until that stage we do not expect this to work, as the table helper macro does not read columns that are inherited basically.</p>
",['table']
48894321,48894863,2018-02-20 21:15:21,Cassandra: denormalization and paging,"<p>I'm trying to understand and to get familiar with cassandra data models.
This article explains some basic modeling rules:</p>

<p><a href=""https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/"" rel=""nofollow noreferrer"">https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/</a></p>

<p>Option 3 shows an denormalized data model:</p>

<p><a href=""https://i.stack.imgur.com/NicyF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NicyF.png"" alt=""enter image description here""></a></p>

<p>Am I getting the things right, that the ""user_by_item"" table has the following structure?</p>

<pre><code>CREATE TABLE ""user_by_item"" (
    item_id int,
    users list&lt;User&gt;
    PRIMARY KEY (item_id)
)
</code></pre>

<p>If yes: it is clear that I can get all users by item_id with one query. But there is no possibility to page through the user list then.</p>

<p>Did I understand the table structure right and how are lists of items managed then, especially if they can get very large?</p>
",<database><database-design><cassandra><denormalization>,"<p>First of all, that article is 6 years old.  For its time, it was a great article, but Cassandra has changed <em>significantly</em> since then.  Case in point, collections didn't exist in Cassandra 1.1, which I <em>think</em> was the most-recent version at the time of this writing.</p>

<blockquote>
  <p>Am I getting the things right, that the ""user_by_item"" table has the following structure?</p>
</blockquote>

<p>Yes, I think you are understanding it.  Using item_id as a single <code>PRIMARY KEY</code> on users_by_item, while storing users as a collection is one way that you could do this.  But, it limits your query flexibility to pulling all of the users back at once.</p>

<p>Probably the most <em>query-friendly</em> way to build that query table, is with a clustering key on <code>user_id</code>:</p>

<pre><code>CREATE TABLE user_by_item (
  item_id int,
  user_id int,
  email text,
  name text,
  PRIMARY KEY ((item_id),user_id)
);
</code></pre>

<p>This way, I can query for all users tied to item 111:</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; SELECT * FROM user_by_item WHERE item_id=111;

 item_id | user_id | email   | name
---------+---------+---------+------
     111 |     123 | jp@ebay |  Jay
     111 |     456 | jd@ebay | John

(2 rows)
</code></pre>

<p>And I can also query just Jay, if I know his <code>user_id</code>:</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; SELECT * FROM user_by_item WHERE item_id=111
                               AND user_id=123;

 item_id | user_id | email   | name
---------+---------+---------+------
     111 |     123 | jp@ebay |  Jay

(1 rows)
</code></pre>

<p>This gives me a bit more query flexibility, while also storing all of the user data by <code>item_id</code>.</p>

<p><strong>Pro tips:</strong></p>

<ul>
<li>Don't encapsulate your table name in double quotes unless you have to.  It forces Cassandra to maintain its case, but can make data retrieval a frustrating experience later on.</li>
<li>When modeling for Cassandra, it is a common practice to use natural keys like <code>name</code> = ""Jay.""  The whole point of a surrogate key like <code>_id</code>, was that something could be referenced from a main table without risking it being misspelled every time it was needed/stored.  In Cassandra we don't have things like foreign keys, so natural keys help you cut out some unnecessary columns.</li>
<li>Primary keys in Cassandra cannot change.  So the exception to the above rule, is if a primary key value is predicted to change (Jay legally changes his <code>name</code>, for instance) then using a surrogate key becomes a good idea.</li>
</ul>
",['table']
48939815,48944282,2018-02-23 01:28:01,Create nested case class instance from a DataFrame,"<p>I have this two case classes:</p>

<pre><code>case class Inline_response_200(
  nodeid: Option[String],
  data: Option[List[ReadingsByEpoch_data]]
)
</code></pre>

<p>and </p>

<pre><code>case class ReadingsByEpoch_data(
  timestamp: Option[Int],
  value: Option[String]
)
</code></pre>

<p>And I have a Cassandra table that has data like <code>nodeid|timestamp|value</code>. Basically, each <code>nodeid</code> has multiple <code>timestamp</code>-<code>value</code> pairs.</p>

<p>All I want to do is create instances of <code>Inline_response_200</code> with their proper List of <code>ReadingsByEpoch_data</code> so Jackson can serialize them properly to Json.    </p>

<p>I've tried</p>

<pre><code>val res = sc.cassandraTable[Inline_response_200](""test"", ""taghistory"").limit(100).collect()
</code></pre>

<p>But I get this error</p>

<blockquote>
  <p>java.lang.IllegalArgumentException: Failed to map constructor parameter data in com.wordnik.client.model.Inline_response_200 to a column of test.taghistory</p>
</blockquote>

<p>Makes total sense because there is no column <code>data</code> in my Cassandra table. But then how can I create the instances correctly?</p>

<p>Cassandra table looks like this:</p>

<pre><code>CREATE TABLE test.taghistory (
nodeid text,
timestamp text,
value text,
PRIMARY KEY (nodeid, timestamp)
) WITH CLUSTERING ORDER BY (timestamp DESC)
</code></pre>

<p><strong>EDIT</strong><br>
As per Alex Ott's suggestion:</p>

<pre><code>val grouped = data.groupByKey.map {
  case (k, v) =&gt;
    Inline_response_200(k.getString(0), v.map(x =&gt; ReadingsByEpoch_data(x.getInt(1), x.getString(2))).toList)
}
grouped.collect().toList
</code></pre>

<p>I'm close but not there yet. This gives me the format I expect, however its creating one instance of <code>Inline_response_200</code> per record:  </p>

<pre><code>[{""nodeid"":""Tag3"",""data"":[{""timestamp"":1519411780,""value"":""80.0""}]},{""nodeid"":""Tag3"",""data"":[{""timestamp"":1519411776,""value"":""76.0""}]}]  
</code></pre>

<p>In this example I need to have one nodeid key, and an array of two timestamp-value pairs, like this:  </p>

<pre><code>[{""nodeid"":""Tag3"",""data"":[{""timestamp"":1519411780,""value"":""80.0""},{""timestamp"":1519411776,""value"":""76.0""}]}]`  
</code></pre>

<p>Maybe I'm grouping the wrong way?</p>
",<scala><apache-spark><cassandra><spark-dataframe><case-class>,"<p>If you have data like <code>nodeid|timestamp|value</code> in your DB (yes, according to schema), you can't directly map it into structure that you created. Read data from table as pair RDD:</p>

<pre><code>val data = sc.cassandraTable[(String,String,Option[String])](""test"", ""taghistory"")
     .select(""nodeid"",""timestamp"",""value"").keyBy[String](""nodeid"")
</code></pre>

<p>and then transform it into structure that you need by using <code>groupByKey</code> on that pair RDD &amp; transforming into <code>Inline_response_200</code> class that you need, like this:</p>

<pre><code>val grouped = data.groupByKey.map{case (k,v) =&gt; Inline_response_200(k,
       v.map(x =&gt; ReadingsByEpoch_data(x._2, x._3)).toList)}
grouped.collect 
</code></pre>
",['table']
48949878,48954203,2018-02-23 14:17:43,Python Cassandra Driver: encoding issue during insertion,"<p>I'm developing a simple python module that reads data from a tsv file and load them into a Cassandra keyspace table.</p>

<p>I started by looking at the examples given by Datastax and everything seemed to be ok, so at that point I began to code.</p>

<p>The program reads data from the tsv file correctly, it translates them into a list of rows and I verified that every element of each row has the right type for the destination column. But when I try to insert a raw into a table the terminal says: </p>

<blockquote>
  <p>AttributeError: 'float' object has no attribute 'encode'  </p>
</blockquote>

<p>This is the code:</p>

<pre><code>#Upload data to Cassandra DB (cassandra_df is a Pandas dataframe)
session.set_keyspace(data_ks)
cassandra_df_list = cassandra_df.values.tolist()

query = ""INSERT INTO table_str (rowid,a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z,aa,ab,ac,ad,ae,af,ag,ah,ai,aj,ak,al,am,an,ao,ap,aq,ar,as,at,au,av,aw,ax,ay,az,ba,bb,bc,bd) VALUES (uuid(),?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)""
prepared = session.prepare(query)

for row in cassandra_df_list:

    prepared.bind(row)
    session.execute(prepared)

cluster.shutdown()
</code></pre>

<p>I made a lot of changes in order to solve the problem, but I got new issues or the same with 'int' instead of 'float'. I also read other questions here and tried to use str(row) and repr(row) in prepared.bind(), but I got other errors.</p>

<p>I'm new to Python and I'm not able to find other solutions, what would you do?</p>

<p>Thanks in advance!</p>

<p><strong>Edit</strong>
Sorry, I forgot to give details about the DB table. Here is the creation statement:</p>

<pre><code>CREATE TABLE prova.table_str (
rowid uuid PRIMARY KEY,
a text,
aa text,
ab text,
ac text,
ad text,
ae text,
af text,
ag text,
ah text,
ai text,
aj double,
ak double,
al double,
am text,
an double,
ao double,
ap double,
aq double,
ar double,
as double,
at double,
au double,
av double,
aw double,
ax double,
ay double,
az double,
b text,
ba double,
bb text,
bc text,
bd text,
c text,
d text,
e int,
f text,
g text,
h text,
i text,
j text,
k double,
l int,
m text,
n double,
o int,
p int,
q text,
r text,
s text,
t text,
u text,
v int,
w text,
x text,
y text,
z text
</code></pre>

<p>)</p>
",<python><cassandra><datastax><datastax-python-driver>,"<p>You didn't share your schema or a stack trace, but I'll guess that the dataframe has numeric types, and your Cassandra table has a bunch of string columns. I'll outline three possible resolutions:</p>

<p>1.) Make the table types match your data so the bind encoding works.</p>

<p>2.) Convert your parameters to the same types as your schema. For example, if they're all strings:</p>

<pre><code>prepared.bind(str(c) for c in row)
</code></pre>

<p>3.) Use simple statements instead of preparing. In this case you would replace the <code>?</code> bind markers with <code>%s</code> and let the driver use string interpolation of the parameters.</p>

<pre><code>query = ""INSERT INTO table_str (rowid,a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z,aa,ab,ac,ad,ae,af,ag,ah,ai,aj,ak,al,am,an,ao,ap,aq,ar,as,at,au,av,aw,ax,ay,az,ba,bb,bc,bd) VALUES (uuid(),%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)""
for row in cassandra_df_list:
    session.execute(query, row)
</code></pre>
",['table']
48974287,48977288,2018-02-25 14:00:15,Cassandra Materialized views impact,"<ol>
<li>Would like to know impact of mv on base table. Does it slows down base table?  When does it starts writing to mv like does it write to base table and mv same time?</li>
<li>If I have CL of local_quorum and RF=3 does the client has to wait till it write to mv to get the ack.</li>
<li>what kind of lock involved in base table and mv does it impacts the latencies on base table</li>
</ol>
",<cassandra>,"<p>Materialized Views are considered <a href=""https://www.mail-archive.com/user@cassandra.apache.org/msg54073.html"" rel=""noreferrer"">experimental</a>. The next patch releases of 3.0, 3.11, and 4.0 will include CASSANDRA-13959, which will log warnings when materialized views are created, and introduce a yaml setting that will allow operators to disable their creation. <strong>So better avoid using them.</strong></p>

<p>As the original modeling lessons says, duplicate the data into a different table for querying by a different partition key.</p>

<p>But anyways to answer your original questions</p>

<p><em>1. Would like to know impact of mv on base table. Does it slows down base table? When does it starts writing to mv like does it write to base table and mv same time?</em></p>

<p>With a materialized view, there is an overhead of read before write. Every write to the base table, involves a read from base table about the corresponding partition key in MV. Then further it writes to the MV with a log based approach, as to make sure the write when applied to base table gets committed in MV as well. So writes will be slower for a table with MV.</p>

<p><em>2) if I have CL of local_quorum and RF=3 does the client has to wait till it write to mv to get the ack.</em></p>

<p>The client will not wait for MV writes, as its handled separately by Cassandra with a log based write to MV from the base table. The consistency guarantee is still applicable only for the base table.</p>

<p><em>3) what kind of lock involved in base table and mv does it impacts the latencies on base table</em></p>

<p>Instead of locking, Cassandra uses batchlog to guarantee the writes happen to MV from base table.</p>

<p>For further <a href=""https://www.datastax.com/dev/blog/materialized-view-performance-in-cassandra-3-x"" rel=""noreferrer"">reference</a> on performance impacts on MV.  </p>
",['table']
49034929,49035720,2018-02-28 17:04:14,Clustering order does not work with compound partition key,"<p>With the following table definition:</p>

<pre><code>CREATE TABLE device_by_create_date (
    year int,
    comm_nr text,
    created_at timestamp,
    PRIMARY KEY ((year, comm_nr), created_at)
) WITH CLUSTERING ORDER BY (created_at DESC)
</code></pre>

<p><code>Comm_nr</code> is a unique identifier.</p>

<p>I would expect to see data ordered by <code>created_at</code> column, which is not the case when I add data.</p>

<p><strong>Example entries:</strong></p>

<p><a href=""https://i.stack.imgur.com/pnv8J.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pnv8J.jpg"" alt=""""></a></p>

<p><strong>Table CQL:</strong></p>

<p><a href=""https://i.stack.imgur.com/pekGe.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pekGe.jpg"" alt=""""></a></p>

<p>How can I issue <code>select * from table;</code> queries, which return data ordered by the <code>created_at</code> row?</p>
",<sorting><cassandra><cassandra-3.0><compound-key>,"<p><strong>TLDR</strong>: You need to create a new table.</p>

<p>Your partition key is <code>(year, comm_nr)</code>.  You're created_at key is ordered but it is ordered WITHIN that partition key.  A query where <code>SELECT * FROM table WHERE year=x AND comm_nr=y;</code> will be ordered by created_at.</p>

<p>Additionally if instead of <code>(year, comm_nr), created_at</code> your key was instead <code>year, comm_r, created_at</code> even if your create table syntax only specifiied created_at as the having a clustering order, it would be created as <code>WITH CLUSTERING ORDER BY (comm_nr DESC, created_at DESC)</code>.  Data is sorted within SSTables by key from left to right.  </p>

<p>The way to do this in true nosql fashion is to create a separate table where your key is instead <code>year, created_at, comm_nr</code>.  You would write to both on user creation, but if you needed the answer for who created their account first you would instead query the new table.</p>
",['table']
49061398,49067290,2018-03-02 01:29:33,"Cassandra Geolocation, to index or not to index?","<p>My goal is to be able to write a query such that I can find all of the rows in a table between a certain radius of a lat and long.</p>

<p>So a query like this:</p>

<pre><code>SELECT * FROM some_table WHERE lat &gt; someVariableMinLat AND 
    lat &lt; someVariableMaxLat AND
    lng &gt; someVariableMinLng AND lng &lt; someVariableMaxLng;
</code></pre>

<p>along those lines.</p>

<p>Now, my thought is of course these should be an index, and I just wanted to confirm that, and related reading or info would be great, thank you!</p>
",<database><database-design><cassandra>,"<p>Your query requires <code>ALLOW FILTERING</code> to run, assuming you've set <strong>lat</strong> and <strong>lng</strong> as secondary indices.</p>

<p>Since you're interested in related readings and information, I would gladly shere my little knowledge with you. let me start with <em>Allow Filtering</em>. You've created a rather complex query that <em>(1)</em> uses <strong>&lt;</strong> and <strong>></strong> instead of <strong>=</strong> <em>(2)</em> on more than one non-primary-key column.</p>

<p>What <em>Allow Filtering</em> does is that it will query a database first, and then it applies <strong>some</strong> part of your conditions on it. Therefore, it's far from efficient if performance is your concern.</p>

<p>Speaking of performance, it's important to note that a column that tends to have more distinct values is not a good candidate to be set as a secondary index. You may find out more about this topic <a href=""https://docs.datastax.com/en/cql/3.3/cql/cql_using/useWhenIndex.html"" rel=""nofollow noreferrer"">here</a>.</p>

<p>How would I do that?</p>

<p>I'm not sure about your requirements. But you could consider using <a href=""https://en.wikipedia.org/wiki/Geohash"" rel=""nofollow noreferrer"">Geohash</a>. Geohash is the encoded form of both longitude and latitude. It can get pretty precise as well. By using geohash strings, you can play a tradeoff game between the length of your geohash in characters and their precision <em>(the lengthier the string, the more pricise they become)</em>. Perhaps you may set the geohash as your index column which implies that the lengthier the geohash, the more distinct values the column would have. You may even consider setting it as the primary key to take the performace to a higher level.</p>

<p><em>Or maybe, you could set two primary keys. One, to keep short geohash, and another one to keep the longer hash for the same location if you want different level of precision :)</em></p>
",['precision']
49067862,49070127,2018-03-02 11:05:35,how do I check if a column exists in cassandra table in java?,"<p>I want to check if a column exists in cassandra table in java,and then perform an action if it exists.How do I do that?</p>
",<java><cassandra><cassandra-3.0><cassandra-2.0><datastax-java-driver>,"<p>You can get table definition via <a href=""https://docs.datastax.com/en/developer/java-driver/3.4/manual/metadata/"" rel=""nofollow noreferrer"">Metadata class</a>.  Something like:</p>

<pre><code>Column column = cluster.getMetadata().getKeyspace(""ks-name"")
   .getTable(""table-name"").getColumn(""column-name"");
if (column != null) {
   // do your stuff
}
</code></pre>
",['table']
49103434,49103548,2018-03-05 04:20:00,Which node a replica is stored in the cassandra ring?,"<p>The data is stored on the ring based on rowkeys, but the replica's row key is the same with the original, then the replica would be stored in the same node in the ring. How does cassandra decide where to store the replica? </p>
",<cassandra>,"<p>Data partitioner determines coordinator node for each key. The coordinator node is the fist replica for a key which is also called primary replica. If replication factor is N, a key's coordinator node replicates it to other N-1 replicas.</p>

<p>In <strong>SimpleStrategy</strong>, successor nodes or the nodes on the ring immediate following in clockwise direction to the coordinator node are selected as  replicas. </p>

<p>This link may help you: <a href=""http://distributeddatastore.blogspot.com/2015/08/cassandra-replication.html"" rel=""nofollow noreferrer"">http://distributeddatastore.blogspot.com/2015/08/cassandra-replication.html</a></p>

<p>You can use <code>nodetool getendpoints</code> to get the node_ip of the replicas. For example:</p>

<pre><code>nodetool getendpoints &lt;keyspace_name&gt; &lt;table_name&gt; &lt;partition_key&gt;
</code></pre>

<blockquote>
  <p>Sample:</p>
</blockquote>

<pre><code>nodetool getendpoints musicdb artist 1
</code></pre>

<blockquote>
  <p>Result:</p>
</blockquote>

<pre><code>192.168.122.13
192.168.122.14
192.168.122.12
</code></pre>
",['partitioner']
49108809,50508046,2018-03-05 10:54:19,How to insert Pandas DataFrame into Cassandra?,"<p>I have a dataframe as below:</p>

<pre><code>df

date        time       open   high   low   last
01-01-2017  11:00:00   37      45     36    42
01-01-2017  11:23:00   36      43     33    38
01-01-2017  12:00:00   45      55     35    43

....
</code></pre>

<p>I want to write it into cassandra. It's kind of bulk upload after processing on data in python.</p>

<p>The schema for cassandra is as below:</p>

<pre><code>CREATE TABLE ks.table1(date text, time text, open float, high float, low 
                       float, last float, PRIMARY KEY(date, time))
</code></pre>

<p>To insert single row into cassandra we can use cassandra-driver in python but I couldn't find any details about uploading an entire dataframe.</p>

<pre><code>from cassandra.cluster import Cluster

session.execute(
    """"""
    INSERT INTO ks.table1 (date,time,open,high,low,last)
    VALUES (01-01-2017, 11:00:00, 37, 45, 36, 42)
    """""")
</code></pre>

<p>P.S:  The similar <a href=""https://stackoverflow.com/questions/46280412/insert-pandas-dataframe-into-cassandra-table"">question</a> have been asked earlier, but doesn't have answer to my question. </p>
",<python><pandas><cassandra>,"<p>Even i was facing this problem but i figured out that even while uploading Millions of rows(19 Million to be exact) into Cassandra its didn't take much time.</p>

<p>Coming to your problem,you can use <a href=""https://www.datastax.com/dev/blog/using-the-cassandra-bulk-loader-updated"" rel=""noreferrer"">cassandra Bulk LOADER</a> 
to get your job done.</p>

<p>EDIT 1: </p>

<p>You can use prepared statements to help uplaod data into cassandra table while iterating through the dataFrame.</p>

<pre><code>    from cassandra.cluster import Cluster
    cluster = Cluster(ip_address)
    session = cluster.connect(keyspace_name)
    query = ""INSERT INTO data(date,time,open,high,low,last) VALUES (?,?,?,?,?,?)""
    prepared = session.prepare(query)
</code></pre>

<p>""?"" is used to input variables</p>

<pre><code>    for item in dataFrame:
        session.execute(prepared, (item.date_value,item.time_value,item.open_value,item.high_value,item.low_value,item.last_value))
</code></pre>

<h1>or</h1>

<pre><code>    for item in dataFrame:
        session.execute(prepared, (item[0],item[1],item[2],item[3],item[4],item[5]))
</code></pre>

<p>What i mean is that use for loop to extract data and upload using session.execute().</p>

<p>for more info on <a href=""https://docs.datastax.com/en/drivers/python/3.2/api/cassandra/query.html"" rel=""noreferrer"">prepared statements</a></p>

<p>Hope this helps..  </p>
",['table']
49133853,49145545,2018-03-06 15:00:48,Cassandra has two primary key and use the second,"<p>I created the next table in Cassandra</p>

<pre><code>create table yyy (
 index1 int,
 index2 int,
 name text,
 primary key ((index1, index2), name)
)
</code></pre>

<p>when i search only by index1, perfect. Its fine!</p>

<pre><code>select * from yyy where index1 ...
</code></pre>

<p>Buy i cant search by index2</p>

<p>ReadFailure: Error from server: code=1300 [Replica(s) failed to execute read] message=""Operation failed - received 0 responses and 1 failures"" info={'failures': 1, 'received_responses': 0, 'required_responses': 1, 'consistency': 'ONE'}</p>

<p>How can I search by index2?</p>
",<cassandra><cassandra-3.0>,"<p>Cassandra is much less flexible on how data can be queried than a relational database. </p>

<p>What you have called <code>index1</code> and <code>index2</code> are actually together called a composite partition key in Cassandra. In order to use a WHERE clause, you must always provide the entire partition key.</p>

<p>For example, these queries will succeed:</p>

<p><code>SELECT * FROM yyy WHERE index1 = 1 AND index2 = 2;</code></p>

<p><code>SELECT * FROM yyy WHERE index1 = 1 AND index2 = 2 AND name = 'fred';</code></p>

<p>But these queries will all fail:</p>

<p><code>SELECT * FROM yyy WHERE index1 = 1;</code></p>

<p><code>SELECT * FROM yyy WHERE index2 = 2;</code></p>

<p><code>SELECT * FROM yyy WHERE index1 = 1 AND name = 'fred';</code></p>

<p>This is because the partition key is used by Cassandra to determine which node the partition will be stored on. If you only supply half of the key, Cassandra cannot tell which node contains the data.</p>

<p>When data modelling for Cassandra you generally denormalize tables if certain queries are required that cannot be served by a single table. For example, if you needed to query on index2 alone, you might make a second table with this schema:</p>

<pre><code>create table zzz (
 index1 int,
 index2 int,
 name text,
 primary key ((index2), index1, name)
)
</code></pre>

<p>Now you can execute <code>SELECT * FROM zzz WHERE index2 = 2;</code></p>

<p>Note that in this case you'll also need to ensure that your <code>index2</code> partitions will not get too large.</p>

<p>There are also some other feature to allow for more flexible querying, although you should fully understand their limitations before using them, as they can lead to significant performance or operational issues when not used as intended:</p>

<ul>
<li><a href=""http://cassandra.apache.org/doc/latest/cql/indexes.html"" rel=""noreferrer"">Secondary Indexes</a></li>
<li><a href=""http://cassandra.apache.org/doc/latest/cql/dml.html#allow-filtering"" rel=""noreferrer"">ALLOW FILTERING</a></li>
</ul>

<p>I highly suggest you read up on Cassandra data modelling best practices if you're new to Cassandra, as it's not as straightforward as it is with relational databases. There are a lot of good blogs and resources on this out there - google is your friend!</p>
",['table']
49166676,49193219,2018-03-08 06:23:57,Create a duplicate of an existing Keyspace in Cassandra (with a new name),"<p>I have a keyspace in a <code>Cassandra</code> database. It contains a number of tables and these tables contain data. For TDD purpose, I need to create an exact copy of the keyspace. I guess it is a single line of code to execute in python. Any ideas?</p>
",<python><python-2.7><cassandra>,"<p>If I understand correctly, you want to ""copy"" a keyspace, in the same cluster? There is no build in CQLSH command to achieve that, yet it's actually simple. Let's assume your source keyspace is keyspace_source, and your dest keyspace is keyspace_dest</p>

<p>a. extract the schema of your keyspace:</p>

<pre><code>cqlsh -e ""DESCRIBE KEYSPACE keyspace_source"" &gt; keyspace_source.txt
</code></pre>

<p>b. edit keyspace_source.txt to change the name of the keyspace to keyspace_dest</p>

<p>c. apply the schema with:</p>

<pre><code>cqlsh -f 'keyspace_source.txt'
</code></pre>

<p>d. on each node, create a snapshot (at the same time) with</p>

<pre><code>nodetool snapshot -t copy keyspace_source
</code></pre>

<p>At this exact moment, any write that goes to a table of keyspace_source will not be part of keyspace_dest. The snapshot directory will be named copy, in this example</p>

<p>e. On each nodes, move all the snapshot files of the tables to the new keyspace (repeat for each table in the keyspace):</p>

<pre><code>mv /var/lib/cassandra/data/data/keyspace_source/table_source1-*/snapshot/copy/* /var/lib/cassandra/data/data/keyspace_dest/table_dest1-*/
</code></pre>

<p>f. Finally, instruct cassandra to refresh the SSTables with (repeat for each table in the keyspace):</p>

<pre><code>nodetool refresh keyspace_dest table_dest1
</code></pre>

<p>Optionally, you might want to repair the keyspace keyspace_dest. That's just because it is impossible to run a snapshot at an exact same time on all the nodes.</p>
",['table']
49204346,49204389,2018-03-10 01:11:05,how to save output from cassandra table using spark,"<p>I want to save the output/rows read from <code>cassandra</code> table to a file in either csv or json format. Using, Spark 1.6.3:</p>

<pre><code>scala&gt;val results.sqlContext.sql(""select * from myks.mytable"")
scala&gt;val.write.option(""header"",""true"").save(""/tmp/xx.csv"") -- writes to cfs:// filesystem
</code></pre>

<p>I am not able to find an option to write to the OS as <code>csv</code> or <code>json</code> format file.</p>

<p>Appreciate any help!</p>
",<scala><cassandra><apache-spark-sql><cassandra-3.0>,"<p>Use the spark Cassandra connector to read data from a Cassandra table into spark</p>

<p><a href=""https://github.com/datastax/spark-cassandra-connector/blob/master/doc/0_quick_start.md"" rel=""nofollow noreferrer"">https://github.com/datastax/spark-cassandra-connector/blob/master/doc/0_quick_start.md</a></p>
",['table']
49274714,49275834,2018-03-14 10:02:16,How to take advantage of Cassandra partitioner using DataFrames?,"<p>According to <a href=""https://github.com/datastax/spark-cassandra-connector/blob/master/doc/16_partitioning.md"" rel=""nofollow noreferrer"">documentation</a>, Cassandra Partitioner can help to reduce shuffles improving overall performance. To take advantage of partitioner I should use <code>keyBy</code> method. Given table:</p>

<pre><code>CREATE TABLE data_storage.dummy (
id text,
value bigint,
PRIMARY KEY (id)
) 
</code></pre>

<p>I can query a table using RDD API and DataFrame API</p>

<pre><code>  val keySpace = ""data_storage""
  val table = ""dummy""

  //option 1
  private val df: DataFrame = session.read.format(""org.apache.spark.sql.cassandra"")
    .option(""keyspace"", keySpace)
    .option(""table"", table)
    .load
  println(df.rdd.partitioner) //prints None

  //option 2
  val rdd = session.sparkContext.cassandraTable(keySpace, table).keyBy(""id"")
  println(rdd.partitioner) //prints Some(CassandraPartitioner)
</code></pre>

<p>Is there any way to pass information to DataFrame reader about how data should be queried (something like <code>keyBy()</code> method for DataFrame)</p>
",<scala><apache-spark><cassandra><spark-dataframe><spark-cassandra-connector>,"<p>You don't need to specify partitioner in case of DataFrame. You just need to make sure <code>pushdown</code> is set to <code>true</code> for the Cassandra DataFrame.
Check this doc <a href=""https://github.com/datastax/spark-cassandra-connector/blob/master/doc/14_data_frames.md#automatic--predicate-pushdown-and-column-pruning"" rel=""nofollow noreferrer"">Automatic Predicate Pushdown and Column Pruning</a>.</p>
",['partitioner']
49277673,49287960,2018-03-14 12:21:25,How do I transfer my cassandra data to pyspark using QueryCassandra and ExecutePySpark Nifi Processors?,"<p>I'm just querying cassandra table using querycassandra processor but what I'm not understanding is how do I pass my Json output file into ExecutePyspark processor as a Input file and later on I need to pass my Spark output data to Hive. Please help me on this, Thanks.</p>

<p>My Query Cassandra Properties:</p>

<p><a href=""https://i.stack.imgur.com/10YZ5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/10YZ5.png"" alt=""enter image description here""></a></p>

<p>Pyspark Properties:
<a href=""https://i.stack.imgur.com/X1s2l.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/X1s2l.png"" alt=""enter image description here""></a></p>
",<apache-spark><cassandra><pyspark><apache-nifi><kylo>,"<p>Consider this flow that uses 4 processors as below:</p>

<p><a href=""https://nifi.apache.org/docs/nifi-docs/components/org.apache.nifi/nifi-cassandra-nar/1.5.0/org.apache.nifi.processors.cassandra.QueryCassandra/index.html"" rel=""noreferrer"">QueryCassandra</a> -> <a href=""https://nifi.apache.org/docs/nifi-docs/components/org.apache.nifi/nifi-update-attribute-nar/1.5.0/org.apache.nifi.processors.attributes.UpdateAttribute/additionalDetails.html"" rel=""noreferrer"">UpdateAttribute</a> -> <a href=""https://nifi.apache.org/docs/nifi-docs/components/org.apache.nifi/nifi-standard-nar/1.5.0/org.apache.nifi.processors.standard.PutFile/index.html"" rel=""noreferrer"">PutFile</a> -> <a href=""https://github.com/Teradata/kylo/blob/master/integrations/nifi/nifi-nar-bundles/nifi-spark-bundle/nifi-spark-processors/src/main/java/com/thinkbiganalytics/nifi/pyspark/core/ExecutePySpark.java"" rel=""noreferrer"">ExecutePySpark</a></p>

<p><strong>Step 1</strong>: <code>QueryCassandra</code> processor: Execute a CQL on Cassandra and output the result in a flow file.</p>

<p><strong>Step 2</strong>: <code>UpdateAttribute</code> processor: Assign the property <code>filename</code> a value containing name for a temporary file on disk that will contain the query results. Use <a href=""https://nifi.apache.org/docs/nifi-docs/html/expression-language-guide.html"" rel=""noreferrer"">NiFi expression language</a> for generating the file name so that it will be different for each run. Create a property <code>result_directory</code> and assign a value for a folder on disk that NiFi has write permissions to.</p>

<ul>
<li>property: <code>filename</code></li>
<li><p>value: <code>cassandra_result_${now():toNumber()}</code></p></li>
<li><p>property: <code>result_directory</code></p></li>
<li>value: <code>/tmp</code></li>
</ul>

<p><a href=""https://i.stack.imgur.com/b2ubS.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/b2ubS.png"" alt=""enter image description here""></a></p>

<p><strong>Step 3</strong>: <code>PutFile</code> processor: Configure the <code>Directory</code> property with the value <code>${result_directory}</code> populated in Step 2.</p>

<p><a href=""https://i.stack.imgur.com/mxAp1.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/mxAp1.png"" alt=""enter image description here""></a></p>

<p><strong>Step 4</strong>: <code>ExecutePySpark</code> processor: Pass the filename with its location as an argument to the PySpark application via the <code>PySpark App Args</code> processor property. The application can then have code to read data from the file on disk, process it and write to Hive.</p>

<ul>
<li>property: <code>PySpark App Args</code> </li>
<li>value: <code>${result_directory}/${filename}</code></li>
</ul>

<p><a href=""https://i.stack.imgur.com/BMyyD.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/BMyyD.png"" alt=""enter image description here""></a></p>

<p>Additionally, you could configure more attributes in Step 2 (UpdateAttribute) that could be then passed as arguments in Step 4 (ExecutePySpark) and considered by the PySpark application in writing to Hive (for example, the Hive database and table name).</p>
",['table']
49327502,49344924,2018-03-16 18:35:19,How to store array of objects in Cassandra,"<p>In cassandra DB I am planning to store an array-of-object. What is the best way to do that. object mapping to Data mapping with model class in java.</p>

<pre><code>Class Test{
   @Column(name = ""id"")
   int id,
   @Column(name = ""name"")
   String name,
   Address[] address

 class Address{
   String add1,
   String city,
   String state 
  }
}
</code></pre>

<p>Should I put all(id, name, add1, city, state) in one table by adding columns to same keyspace with add1, city, state also? or add new table for address
Or 
any other options..</p>

<p>I have tried to add <code>TYPE</code> But throwing error as: ""Error from server: code=2200 <code>[Invalid query] message=""A user type cannot contain non-frozen UDTs</code>""
From the error and type syntax I have used keyword '<code>frozen</code>', but not luck. Altering table also gives similar Error something like : ""<code>mismatched input 'frozen' expecting EOF</code>""</p>

<p>Also,
What if I have to save column of type <code>'String[ ]'</code> As it is not custom type like Address[]. it is of String or text.? Do we need to just add alter statement? if so how it looks like</p>
",<java><cassandra><arrayofstring>,"<p>For your case, first, you need to create a UDT(user defined type) in Cassandra.</p>

<pre><code>Create TYPE address(
   add1 text,
   city text,
   state text
);
</code></pre>

<p>Then create a table including this UDT.</p>

<pre><code>Create table Test(
   id int,
   name text,
   address list&lt;frozen&lt;address&gt;&gt;,
   primary key(id)
);
</code></pre>

<p>If you want to know more about UTD and the usages, visit following links:</p>

<ul>
<li><a href=""https://docs.datastax.com/en/cql/3.1/cql/cql_using/cqlUseUDT.html"" rel=""noreferrer"">Using a user-defined type</a></li>
<li><a href=""https://docs.datastax.com/en/cql/3.1/cql/cql_reference/collection_type_r.html"" rel=""noreferrer"">Collection type (List, Set, Map)</a> </li>
</ul>

<p><strong>EDIT</strong>:</p>

<blockquote>
  <p>Also, What if I have to save column of type 'String[ ]' As it is not custom type like Address[]. it is of String or text.? Do we need to just add alter statement? if so how it looks like</p>
</blockquote>

<p><strong>Answer</strong>: <code>Alter table test add stringarr list&lt;text&gt;</code> Check this links to get more idea about cassandra data types: <a href=""https://docs.datastax.com/en/cql/3.1/cql/cql_reference/cql_data_types_c.html"" rel=""noreferrer"">CQL data types</a></p>
",['table']
49354953,49355803,2018-03-19 02:25:47,data-at-rest encryption for NoSQL,"<p>Prototyping a project with Mongo &amp; Spring Boot and thinking it does a lot of what I want. However, I really need to have encrypted data-at-rest, which would seem to indicate I have to purchase the enterprise version.  Since I don't have a budget yet, I am wondering if there is another alternative that people have found useful? I think DynamoDB can be used in a local &amp; test environment.  Or it viable to encrypt the data at the application level and still have great performance for my CRUD operations?</p>
",<mongodb><cassandra><spring-data-jpa><amazon-dynamodb>,"<p>I've done application level encryption with DynamoDB before with some success. My issues where not really with DynamoDB but with the encryption in the application. </p>

<p>First, encryption/decryption is very expensive. I had to increase the number of servers I was using by over double just to handle the extra CPU load. Your milage may very. In my case, I was using Node.js and the servers suddenly switched from being I/O bound to being CPU bound.</p>

<p>Second, doing encryption/decryption application side adds a lot of complexity to your app. You will almost certainly need to parallelize the encryption/decryption to minimize the added latency that it will cause. Also, you will need to figure out a secure way of sharing the keys.</p>

<p>Last, application level encryption will make some DynamoDB operations unavailable to you. For example, <a href=""https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_Condition.html"" rel=""nofollow noreferrer"">conditions</a> probably won't make sense anymore for encrypted values.</p>

<p>Long story short, I wouldn't recommend application level encryption regardless of the database.</p>

<p>DynamoDB now supports what they call Server-Side Encryption at Rest. Personally I think that name is a little confusing but from their perspective, your application is the client and DynamoDB is the server.</p>

<blockquote>
  <p>Amazon DynamoDB encryption at rest helps you secure your application
  data in Amazon DynamoDB tables further using AWS-managed encryption
  keys stored in AWS Key Management Service (KMS). Encryption at rest is
  fully transparent to the user with all DynamoDB queries working
  seamlessly on encrypted data. With this new capability, it has never
  been easier to use DynamoDB for security-sensitive applications with
  strict encryption compliance and regulatory requirements.</p>
</blockquote>

<p><a href=""https://aws.amazon.com/blogs/aws/new-encryption-at-rest-for-dynamodb/"" rel=""nofollow noreferrer"">Blog post about DynamoDB encryption at rest</a></p>

<blockquote>
  <p>You simply enable encryption when you create a new table and DynamoDB
  takes care of the rest. Your data (tables, local secondary indexes,
  and global secondary indexes) will be encrypted using AES-256 and a
  service-default AWS Key Management Service (KMS) key. The encryption
  adds no storage overhead and is completely transparent; you can
  insert, query, scan, and delete items as before. The team did not
  observe any changes in latency after enabling encryption and running
  several different workloads on an encrypted DynamoDB table.</p>
</blockquote>
",['table']
49356986,49360627,2018-03-19 06:29:48,Cassandra Batch statement-Multiple tables,"<p>I want to use batch statement to delete a row from 3 tables in my database to ensure atomicity. The partition key is going to be the same in all the 3 tables. In all the examples that I read about batch statements, all the queries were for a single table? In my case, is it a good idea to use batch statements? Or, should I avoid it?</p>

<p>I'm using Cassandra-3.11.2 and I execute my queries using the C++ driver.</p>
",<cassandra><cassandra-3.0>,"<p>Yes, you can use batch to ensure atomicity. Single partition batches are faster (same table and same partition key) but only for a limited number of partitions (in your case three) it is okay. But don't use it for performance optimization (Ex: reduce of multiple requests). If you need atomicity you can use it.</p>

<p>You can check below links:</p>

<p><a href=""https://stackoverflow.com/questions/42929928/cassandra-batch-query-performance-on-tables-having-different-partition-keys/42946757#42946757"">Cassandra batch query performance on tables having different partition keys</a><br>
<a href=""https://stackoverflow.com/questions/42930498/cassandra-batch-query-vs-single-insert-performance/42947125#42947125"">Cassandra batch query vs single insert performance</a><br>
<a href=""https://stackoverflow.com/questions/39121092/how-single-parition-batch-in-cassandra-function-for-multiple-column-update"">How single parition batch in cassandra function for multiple column update?</a></p>

<p><strong>EDITED</strong></p>

<blockquote>
  <blockquote>
    <p>In my case, the tables are different but the partition key is the same in all 3 tables. So is this a special case of single partition batch or is it something entirely different. </p>
  </blockquote>
</blockquote>

<p>For different tables partitions are also different. So this is a multi partition batch. <strong>LOGGED</strong> batches are used to ensure atomicity for different partitions (different tables or different partition keys). <strong>UNLOGGED</strong> batches are used to ensure atomicity and isolation for single partition batch. If you use <strong>UNLOGGED</strong> batch for multi partition batch atomicity will not be ensured. Default is <strong>LOGGED</strong> batch. For single partition batch default is <strong>UNLOGGED</strong>. Cause single partition batch is considered as single row mutation. For single row update, there is no need of using <strong>LOGGED</strong> batch. To know about <strong>LOGGED</strong> or <strong>UNLOGGED</strong> batch, I have shared a link below.</p>

<blockquote>
  <p>Multi partition batches should only be used to achieve atomicity for a few writes on different tables. Apart from this they should be avoided because they’re too expensive.</p>
  
  <p>Single partition batches can be used to achieve atomicity and isolation. They’re not much more expensive than normal writes.</p>
</blockquote>

<p>But you can use multi partition <strong>LOGGED</strong> batch as partitions are limited.</p>

<p>A very useful Doc in Batch and all the details are provided. If you read this, all the confusions will be cleared.</p>

<p><a href=""https://inoio.de/blog/2016/01/13/cassandra-to-batch-or-not-to-batch/"" rel=""noreferrer"">Cassandra - to BATCH or not to BATCH</a></p>

<p><strong>Partition Key tokens vs row partition</strong></p>

<p>Table partitions and partition key tokens are different. Partition key is used to decide which node the data resides. For same row key partition tokens are same thus resides in the same node. For different partition key or same key different tables they are different row mutation. You cannot get data with one query for different partition keys or from different tables even if for the same key. Coordinator nodes have to treat it as different request or mutation and request the actual data from replicated nodes separately. It's the internal structure of how C* stores data.</p>

<blockquote>
  <p>Every table even has it's own directory structure making it clear that a partition from one table will never interact with the partition of another. </p>
</blockquote>

<p><a href=""https://stackoverflow.com/questions/36700859/does-the-same-partition-key-in-different-cassandra-tables-add-up-to-cell-theoret"">Does the same partition key in different cassandra tables add up to cell theoretical limit?</a></p>

<p>To know details how C* maps data check this link:</p>

<p><a href=""https://www.slideshare.net/DataStax/understanding-how-cql3-maps-to-cassandras-internal-data-structure"" rel=""noreferrer"">Understanding How CQL3 Maps to Cassandra's Internal Data Structure </a></p>
",['table']
49393722,49394516,2018-03-20 20:40:51,Cassandra: dropping keyspace with tables containing numerous user defined types,"<p>I am in a situation where I would like to drop a keyspace which can also <em>safely</em> delete all the contained tables' various user defined types. </p>

<p>The official document of CQL Cassandra states: </p>

<blockquote>
  <p>""Immediate, irreversible removal of the keyspace, including objects
  such as tables, functions, and data it contains. ""</p>
</blockquote>

<p>However, from the document I cannot make out if it also highlights removal of user defined types. Could someone confirm this?</p>
",<cassandra><cassandra-3.0>,"<p>The DROP KEYSPACE command drops the keyspace and all objects that are part of that keyspace: data, tables/colum families, user defined types, indexes.</p>

<p>Before the actual drop,  snapshot of the keyspace is taken. This can be enabled/disabled using auto_snapshot parameter in cassandra.yaml. Default value for this parameter is true.</p>
",['auto_snapshot']
49538476,49543028,2018-03-28 15:27:05,Search on two indexed field in Cassandra,"<p>I have an log table in below,</p>

<pre><code>CREATE TABLE log (    
""date"" text,     
""timestamp"" timestamp, 
""eventId"" text, 
""message"" text,
""module"" text,
""userId"" text,
""ovirtEventId"" text, 
""category"" text, 
primary key (""date"",""timestamp"",""eventId"")) with clustering order by (""timestamp"" DESC);
</code></pre>

<p>It is partitioned based on date so it is scaling perfectly,</p>

<p>I did the indexing on the following fields,</p>

<pre><code>CREATE CUSTOM INDEX module_idx ON log (""module"") USING 'org.apache.cassandra.index.sasi.SASIIndex' WITH OPTIONS = {'mode': 'CONTAINS', 'analyzer_class': 'org.apache.cassandra.index.sasi.analyzer.NonTokenizingAnalyzer', 'case_sensitive': 'false'};

CREATE CUSTOM INDEX user_idx ON log (""userId"")  USING 'org.apache.cassandra.index.sasi.SASIIndex' WITH OPTIONS = {'mode': 'CONTAINS', 'analyzer_class': 'org.apache.cassandra.index.sasi.analyzer.NonTokenizingAnalyzer', 'case_sensitive': 'false'};

CREATE CUSTOM INDEX message_idx ON log (""message"")  USING 'org.apache.cassandra.index.sasi.SASIIndex' WITH OPTIONS = {'mode': 'CONTAINS', 'analyzer_class': 'org.apache.cassandra.index.sasi.analyzer.NonTokenizingAnalyzer', 'case_sensitive': 'false'};

CREATE CUSTOM INDEX event_category_idx ON log (""category"")  USING 'org.apache.cassandra.index.sasi.SASIIndex' WITH OPTIONS = {'mode': 'CONTAINS', 'analyzer_class': 'org.apache.cassandra.index.sasi.analyzer.NonTokenizingAnalyzer', 'case_sensitive': 'false'};
</code></pre>

<p>The following query works,</p>

<pre><code>SELECT * FROM log WHERE date = '20180223' AND ""message"" LIKE '%This%';
</code></pre>

<p>But when I try to query on two indexed field, It fails,</p>

<pre><code>SELECT * FROM log WHERE date = '20180223' AND ""message"" LIKE '%This%' AND module LIKE 'test';
</code></pre>

<p>Is there a way to search on two indexed field, please help me in this.</p>
",<cassandra><cassandra-3.0>,"<p>Like's and scanning over secondary indexes are not cassandra's strong points.  </p>

<p>You may instead want to run cassandra as your storage engine and have solr or elastic search infront of it.</p>

<p>So with the warning out of the way, you may be able to do something like this: </p>

<pre><code>CREATE TABLE log_idx (
""date"" text, 
""timestamp"" timestamp, 
""eventId"" text, 
""message"" text,
""module"" text,
""userId"" text,
""ovirtEventId"" text, 
""category"" text, 
primary key (module, date), timestamp, eventId) with clustering order by (""timestamp"" DESC);


CREATE CUSTOM INDEX message_log_idx ON log (""message"")  USING 'org.apache.cassandra.index.sasi.SASIIndex' WITH OPTIONS = {'mode': 'CONTAINS', 'analyzer_class': 'org.apache.cassandra.index.sasi.analyzer.NonTokenizingAnalyzer', 'case_sensitive': 'false'};

SELECT * FROM log_idx WHERE module='x' AND date='y' AND message LIKE '%z';
</code></pre>

<p>This isn't exactly what you want as we've made the consession that your module can no longer be in a LIKE clause and instead must be hardcoded.  You would would fall back on your previous log table if you wanted to just scan over messages across all modules.</p>

<p>Alternatively you could filter at the application layer.</p>
",['table']
49556740,49557386,2018-03-29 12:59:58,Cassandra 2.2.11 add new map column from text column,"<p>Let's say I have table with 2 columns
<code>primary key</code>: id - type <code>varchar</code>
and <code>non-primary-key</code>: data - type <code>text</code></p>

<p>Data column consist only of <code>json</code> values for example like:</p>

<pre><code>{
""name"":""John"",
""age"":30
}
</code></pre>

<p>I know that i can not alter this column to <code>map</code> type but maybe i can add new <code>map</code> column with values from <code>data</code> column or maybe you have some other idea?</p>

<p>What can i do about it ? I want to get <code>map</code> column in this table with values from <code>data</code></p>
",<cassandra><alter><cqlsh>,"<p>You might want to make use of the CQL <a href=""https://docs.datastax.com/en/cql/3.3/cql/cql_reference/cqlshCopy.html"" rel=""nofollow noreferrer"">COPY</a> command to export all your data to a CSV file. </p>

<p>Then alter your table and create a new column of type <code>map</code>. </p>

<p>Convert the exported data to another file containing <code>UPDATE</code> statements where you only update the newly created column with values converted from JSON to a <code>map</code>. For conversion use a tool or language of your choice (be it bash, python, perl or whatever). </p>

<p>BTW be aware, that with <code>map</code> you specify what data type is your map's key and what data type is your map's value. So you will most probably be limited to use strings only if you want to be generic, i.e. a <code>map&lt;text, text&gt;</code>. Consider whether this is appropriate for your use case.</p>
",['table']
49644528,49646210,2018-04-04 06:49:42,Tombstone vs nodetool and repair,"<p>I inserted 10K entries in a table in Cassandra which has the TTL of 1 minute under the single partition. </p>

<p>After the successful insert, I tried to read all the data from a single partition but it throws an error like below,</p>

<pre><code>WARN  [ReadStage-2] 2018-04-04 11:39:44,833 ReadCommand.java:533 - Read 0 live rows and 100001 tombstone cells for query SELECT * FROM qcs.job LIMIT 100 (see tombstone_warn_threshold)
DEBUG [Native-Transport-Requests-1] 2018-04-04 11:39:44,834 ReadCallback.java:132 - Failed; received 0 of 1 responses
ERROR [ReadStage-2] 2018-04-04 11:39:44,836 StorageProxy.java:1906 - Scanned over 100001 tombstones during query 'SELECT * FROM qcs.job LIMIT 100' (last scanned row partion key was ((job), 2018-04-04 11:19+0530, 1, jobType1522820944168, jobId1522820944168)); query aborted
</code></pre>

<p>I understand tombstone is an marking in the sstable not the actual delete.</p>

<p>So I performed the <strong>compaction</strong> and <strong>repair</strong> using <strong>nodetool</strong> </p>

<p>Even after that when I read the data from the table, It throws the same error in log file. </p>

<p>1) How to handle this scenario? </p>

<p>2) Could some explain why this scenario happened and Why not the compaction and repair didn't solve this issue? </p>
",<cassandra><cassandra-3.0>,"<p>Tombstones are really deleted after period specified by <code>gc_grace_seconds</code> setting of the table (it's 10 days by default).  This is done to make sure that any node that was down at time of deletion will pickup these changes after recover.  Here are the blog posts that discuss this in great details: <a href=""http://thelastpickle.com/blog/2016/07/27/about-deletes-and-tombstones.html"" rel=""nofollow noreferrer"">from thelastpickle (recommended)</a>, <a href=""https://opencredo.com/cassandra-tombstones-common-issues/"" rel=""nofollow noreferrer"">1</a>, <a href=""https://www.beyondthelines.net/databases/cassandra-tombstones/"" rel=""nofollow noreferrer"">2</a>, and <a href=""https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlAboutDeletes.html"" rel=""nofollow noreferrer"">DSE documentation</a> or <a href=""http://cassandra.apache.org/doc/latest/operating/compaction.html?highlight=gc_grace_seconds"" rel=""nofollow noreferrer"">Cassandra documentation</a>.</p>

<p>You can set the <code>gc_grace_seconds</code> option on the individual table to lower value to remove deleted data faster, but this should be done only for tables with TTLed data.  You may also need to tweak <code>tombstone_threshold</code> &amp; <code>tombstone_compaction_interval</code> table options to perform compactions faster.  See <a href=""https://docs.datastax.com/en/dse/5.1/cql/cql/cql_reference/cql_commands/cqlCreateTable.html"" rel=""nofollow noreferrer"">this document</a> or <a href=""http://cassandra.apache.org/doc/latest/operating/compaction.html"" rel=""nofollow noreferrer"">this document</a> for description of these options.</p>
",['table']
49653634,49678555,2018-04-04 14:27:03,Can username and password be passed in connection url for cassandra,"<p>I want to ask if we can pass username and password in database connection url
Example:</p>

<pre><code>jdbc:cassandra:keyspace=keyspace1;host=host;port=port;user=user;password=password;
</code></pre>

<p>I'm using cdata drivers for apache cassandra.
And if yes, how can i create a user with password whom I'll via the connection url?
And no, the documentation on datastax is not helping me out.</p>
",<cassandra><database-connection><cdata>,"<p>In short, yes. Set the <em>AuthScheme</em> Connection property to 'Basic' and set the <em>User</em> and <em>Password</em> connection properties, in addition to the other necessary properties:</p>

<pre><code>jdbc:cassandra:AuthSchem=BASIC;User=&lt;username&gt;;Password=&lt;password&gt;;...
</code></pre>

<p>@Aaron offers the solution for creating the first user: <a href=""https://stackoverflow.com/questions/22213786/how-do-you-create-the-first-user-in-cassandra-db"">How do you create the first user in Cassandra DB</a></p>

<p>From the <a href=""http://cdn.cdata.com/help/RCC/jdbc/pg_connectionj.htm"" rel=""nofollow noreferrer"">online CData JDBC Driver for Cassandra help</a>:</p>

<blockquote>
  <p>The driver supports Basic authentication with login credentials and
  the additional authentication features of DataStax Enterprise (DSE)
  Cassandra. The following sections detail connection properties your
  authentication method may require.</p>
  
  <p>You need to set <em>AuthScheme</em> to the value corresponding to the
  authenticator configured for your system. You specify the
  authenticator in the authenticator property in the cassandra.yaml
  file. This file is typically found in <code>/etc/dse/cassandra</code>. or through
  the DSE Unified Authenticator on DSE Cassandra.</p>
  
  <p><strong>Basic Authentication</strong></p>
  
  <p>Basic authentication is supported through Cassandra's built-in default
  PasswordAuthenticator.</p>
  
  <ul>
  <li>Set the AuthScheme property to 'BASIC' and set the <em>User</em> and <em>Password</em>    properties. </li>
  <li>In the cassandra.yaml file, set the authenticator    property to 'PasswordAuthenticator'.</li>
  </ul>
  
  <p><strong>Kerberos Authentication</strong></p>
  
  <p>Kerberos authentication is supported through DataStax Enterprise
  Unified Authentication.</p>
  
  <ul>
  <li>Set the <em>AuthScheme</em> property to 'KERBEROS' and set the <em>User</em> and <em>Password</em> properties.</li>
  <li>Set the KerberosKDC, KerberosRealm, and KerberosSPN properties.</li>
  <li>In the cassandra.yaml file, set the authenticator property to ""com.datastax.bdp.cassandra.auth.DseAuthenticator"".</li>
  <li>Modify the authentication_options section in the dse.yaml file, specifying the default_schema and other_schemas properties as
  'kerberos'.</li>
  <li>Modify the kerberos_options section in the dse.yaml file, specifying the keytab, service_principle, http_principle and qop
  properties</li>
  </ul>
  
  <p><strong>LDAP Authentication</strong></p>
  
  <p>LDAP authentication is supported through DataStax Enterprise Unified
  Authentication.</p>
  
  <ul>
  <li>Set the <em>AuthScheme</em> property to 'LDAP' and set the User and Password properties.</li>
  <li>In the cassandra.yaml file, set the authenticator property to ""com.datastax.bdp.cassandra.auth.DseAuthenticator"".</li>
  <li>Modify the authentication_options section in the dse.yaml file, specifying the default_schema and other_schemas properties as 'ldap'.</li>
  <li>Modify the ldap_options section in the dse.yaml file, specifying the server_host, server_port, search_dn, search_password,
  user_search_base, and user_search_filter properties</li>
  </ul>
  
  <p><strong>Using PKI</strong></p>
  
  <p>You can specify a client certificate to authenticate the driver with
  <em>SSLClientCert</em>, <em>SSLClientCertType</em>, <em>SSLClientCertSubject</em>, and <em>SSLClientCertPassword</em>.</p>
</blockquote>
",['authenticator']
49673484,49674304,2018-04-05 13:22:36,Cassandar DB : How to display output of describe tables in ascendant or descendant order?,"<p>I am trying to execute this cql command to get list of tables in cassandra:</p>

<p><code>cqlsh:myspace&gt; describe tables;</code> </p>

<p>I am getting output like this:</p>

<pre><code>site                    vehicle                  vehicle_brand         
vehicle_color           employee                 project  
...
... 
</code></pre>

<p>I have many tables.</p>

<p>Is it possible to show list of tables displayed in ascendant or descendant order ? </p>

<p>If yes, how ? </p>
",<cassandra><cql><cassandra-3.0><cqlsh>,"<p>You can do this by querying <code>system_schema.tables</code> table with CQL query like this:</p>

<pre><code>select table_name from system_schema.tables 
   where keyspace_name = 'myspace' order by table_name desc;
</code></pre>
",['table']
49678078,49678329,2018-04-05 17:20:56,Which Database would be the best option to use?,"<p>Actually I'm designing a social media app using mongodb and it's almost ready to release . But today I was checking some posts about Cassandra vs MongoDB and in those posts it was mentioned that for heavy writes Cassandra should be used instead of MongoDB.</p>

<p>So I'm confused should I migrate to Cassandra from MongoDB or should I release my app in MongoDB itself.</p>

<p>Please do suggest what would be the best db for my case. I have messaging, picture uploading, video uploading (in future) in my app.</p>
",<android><mongodb><cassandra>,"<p>While both are NoSQL databases, they are very different. Looking at a few key characteristics you can determine which one is better for your application (although I will say that if your application is so close to release, does it make sense to delay in order to redesign the database portion? Just asking.)</p>

<ul>
<li>Objects: do you need flexibility in your data model? Mongo uses an
expressive object model that includes indexing the property of an
object. Cassandra uses a more traditional table structure with rows
and columns of a specific data type.</li>
<li><p>Secondary Indexes: do you need secondary indexes and a more flexible
query model? MongoDB has full support of secondary indexes. Cassandra
has cursory support for secondary indexes, queries are more
traditional.</p></li>
<li><p>High Availability: Do you need 100% uptime? With MongoDB's single
master model you will have downtime if the master is down and while
the slaves elect a new master. Cassandra supports multiple masters
and can achieve 100% uptime for writes.</p></li>
<li><p>Query Language support: do you need query language support? MongoDB
has no support for a query language, whereas Cassandra supports CQL
that is very similar to SQL.</p></li>
<li>High Write Throughput: if you have a large number of concurrent
writes happening, then Cassandra has the high write throughput
advantage.</li>
</ul>
",['table']
49684138,49809338,2018-04-06 02:02:03,Cassandra - Grouping By ID and Ordering by Date,"<p>A portion of my application consists of a discussion board: there are threads, posts, and categories. Threads are grouped by category, and posts are grouped by threads. I'm having a problem coming up with a model / query that will allow the selection threads by category with a descending ordering of their last post.</p>

<p>Category</p>

<pre><code>CREATE TABLE keyspace.categories (
    id ascii PRIMARY KEY,
    description text,
    name text,
    ...
);
</code></pre>

<p>Thread</p>

<pre><code>CREATE TABLE keyspace.threads (
    id ascii PRIMARY KEY,
    category_id ascii,
    content text,
    ...
);
</code></pre>

<p>Post</p>

<pre><code>CREATE TABLE keyspace.posts (
    thread_id ascii,
    created_at timestamp,
    id ascii,
    content text,
    ...
    PRIMARY KEY (thread_id, created_at, id)
);
</code></pre>

<p>I initially thought about putting the last post's ""created at"" time as a clustering key on the thread table, but that's impossible as it changes with each post.</p>

<p>I then thought about creating an intermediate table that is written to every time a post is created. This solves the immutability issue with the first approach, but the problem is that it will contain multiple values per thread and I have not been able to figure out a partition / clustering order that would support grouping by thread and ordering by date.</p>

<p>For example, the following would allow me to group by thread, but not order by date:</p>

<pre><code>CREATE TABLE last_post_for_category (
    category_id ascii,
    thread_id ascii,
    created_at timestamp,
    PRIMARY KEY ((category_id), thread_id, created_at)
) WITH CLUSTERING ORDER BY (thread_id DESC, created_at DESC);

SELECT thread_id FROM last_post_for_category WHERE category_id = 'category' GROUP BY thread_id, created_at;
</code></pre>

<p>And the following would allow me to order by date, but not group by thread:</p>

<pre><code>CREATE TABLE keyspace.last_post_for_category (
    category_id ascii,
    thread_id ascii,
    created_at timestamp,
    PRIMARY KEY ((category_id), created_at, thread_id)
) WITH CLUSTERING ORDER BY (created_at DESC, thread_id DESC);

SELECT thread_id FROM last_post_for_category WHERE category_id = 'category' GROUP BY created_at, thread_id;
</code></pre>

<p>I'm not able to do a <code>distinct</code> on <code>(category_id, thread_id)</code> either as I know nothing about thread IDs at the point in which this query is executed.</p>

<p>Does anyone have any idea on how I can best represent this ordering?</p>
",<database><cassandra><nosql><data-modeling><cql>,"<p>First of all, I recommend that you use the datatype <code>datetime</code> rather than <code>timestamp</code>, since it will make it easy for you to modify it or set a default value. This is just a recommendation.</p>
<p><strong>Suggested solution:</strong></p>
<p>Add the attribute <code>last_post</code> to the table <code>threads</code> to save the time of last added post in each thread.<br />
When a thread is first created, the <code>last_post</code> value should equal a very old date (because no posts in that thread yet).</p>
<p>After tha, create a trigger so that whenever a post is inserted in <code>posts</code>, the trigger updates the <code>last_post</code> value of the corresponding thread. The trigger can be added like this:</p>
<pre><code>CREATE TRIGGER triggerName ON posts
FOR INSERT
AS
declare @post_time datetime;
declare @thread_id int;
select @post_time=i.created_at from inserted i;
select @thread_id=i.thread_id from inserted i;

update threads set lastpost = @post_time where id=@thread_id  
GO
</code></pre>
<p>Last step will be a direct query to select threads by category sorted by <code>last_post</code>, just like this:</p>
<pre><code>select * from threads where category_id = 'theCategoryYouWant' order by lastpost asc /*or desc as you like*/  
</code></pre>
<p><strong>Note:</strong> if you want <code>created_at</code> to be updated when the post is edited, you will need to add a similar trigger to update the <code>last_post</code> attribute of the corresponding thread.</p>
",['table']
49720493,49724461,2018-04-08 17:09:58,Cassandra: how to define a table with auto uuid key?,"<p>I know that cqlsh has uuid() function I can use doing INSERT using cqlsh. But I want to do INSERTs (or creating data from app with driver) without generating uuid on client side.</p>
",<cassandra>,"<p>You can use uuid() in your queries outside of cqlsh which would generate them on the coordinator. You dont need to specify them, ie:</p>

<pre><code>session.execute(""INSERT INTO blah (id, value) VALUES (now(), 'bob')"");

# id being a timeuuid type, or can use uuid() for a random
</code></pre>

<p>That said it can actually be better to do them on client side. If you do them client side your inserts are idempotent. If you have the function providing the key to be generated on coordinator you cannot retry it on a different coordinator if there are write timeouts. Write timeouts may or may not have been applied so its better if you can just retry.</p>

<p>Theres no concern of collisions by the way. Particularly if you use type 1 uuid's, its impossible for them to collide. Even with >10,000 a millisecond a host (which is the precision of timeuuids, since its number of 100 nanoseconds since creation of Gregorian calendar), most libraries are monotonically increasing and will just progress into future milliseconds ensuring you will never have duplicates.</p>
",['precision']
49741745,49742794,2018-04-09 21:09:45,How to select data in Cassandra either by ID or date?,"<p>I have a very simple data table. But after reading a lot of examples in the internet, I am still more and more confused how to solve the following scenario:</p>

<p><strong>1) The Table</strong></p>

<p>My data table looks like this (without defining the primayr key, as this is my understanding problem):</p>

<pre><code>CREATE TABLE documents (
    uid text,
    created text,
    data text
}
</code></pre>

<p>Now my goal is to have to different ways to select data.</p>

<p><strong>2) Select by the UID:</strong></p>

<pre><code>SELECT * FROM documents
    WHERE uid = ‘xxxx-yyyyy-zzzz’
</code></pre>

<p><strong>3) Select by a date limit</strong></p>

<pre><code>SELECT * FROM documents
    WHERE created &gt;= ‘2015-06-05’
</code></pre>

<p>So my question is: </p>

<p>What should my table definition in Cassandra look like, so that I can perform these selections?</p>
",<cassandra>,"<p>To achieve both queries, you would need two tables.
First one would look like:</p>

<pre><code>CREATE TABLE documents (
    uid text,
    created text,
    data text,
    PRIMARY KEY (uid));
</code></pre>

<p>and you retrieve your data with: <code>SELECT * FROM documents WHERE uid='xxxx-yyyy-zzzzz'</code> Of course, uid must be unique. You might want to consider the uuid data type (instead of text)</p>

<p>Second one is more delicate. If you set your partition to the full date, you won't be able to do a range query, as range query is only available on the clustering column. So you need to find the sweet spot for your partition key in order to:</p>

<ol>
<li>make sure a single partition won't be too large (max 100MB,
otherwise you will run into trouble)</li>
<li>satisfy your query requirements.</li>
</ol>

<p>As an example:</p>

<pre><code>CREATE TABLE documents_by_date (
    year int,
    month int,
    day int,
    uid text,
    data text,
    PRIMARY KEY ((year, month), day, uid);
</code></pre>

<p>This works fine if within a day, you don't have too many documents (so your partition don't grow too much). And this allows you to create queries such as: <code>SELECT * FROM documents_by_date WHERE year=2018 and month=12 and day&gt;=6 and day&lt;=24;</code> If you need to issue a range query across multiple months, you will need to issue multiple queries.
If your partition is too large due to the <code>data</code> field, you will need to remove it from documents_by_date. And use <code>documents</code> table to retrieve the data, given the uid you retreived from <code>documents_by_date</code>.
If your partition is still too large, you will need to add <code>hour</code> in the partition key of <code>documents_by_date</code>.</p>

<p>So overall, it's not a straightforward request, and you will need to find the right balance for yourself when defining your partition key.</p>

<p>If latency is not a huge concern, an alternative would be to use the stratio lucene cassandra plugin, and index your date.</p>
",['table']
49747222,49767192,2018-04-10 06:54:46,Possibility of using 2 different snitches in the Multiple Data Centers cluster in cassandra,"<p>We have a multiple data centers cluster spanning across our own network and AWS.</p>

<p>We are currently using GossipingPropertFileSnitch snitch across our network.</p>

<p>Is it possible to use GossipingPropertFileSnitch on datacenter in our private network and Ec2MultiRegionSnitch in AWS ? </p>
",<cassandra><cassandra-3.0>,"<p>You can continue to use <strong>GossipingPropertyFileSnitch</strong> in the AWS cluster as well. Just remember to automate or setup proper values for dc and rack in ""cassandra-rackdc.properties"" on the AWS nodes (just like you did in your own datacenter nodes).</p>

<p>Ec2MultiRegionSnitch gets this information free for you. Its rack and DC aware. But bad part about it is that the discovery of the nodes mandates public ip/internet and then future communications happen through private ips.</p>

<p>In short GossipingPropertyFileSnitch continue to work great.</p>
","['rack', 'dc']"
49749573,49767856,2018-04-10 09:04:27,Cassandra Partition vs NoSql Partition,"<p>I've understood difference b/w Cassandra Partition key, Composite key, Clustering key. But not finding enough information to understand how partition is handled in cassandra.<br/>
In cassandra, range of partition keys are stored on a node like a partition/shard. Is my understanding is correct or not..?<br/>
Is each partition key has different file(at the system level) in DB..? If so, won't the reads be slower..?<br/>
If each partition key is not having different file in DB. How it's handled..?</p>
",<cassandra><nosql><cassandra-3.0>,"<p>Data is stored in Cassandra in wide rows called partitions. Each row has a partition key used for identifying that partition. For distributing the data across the cluster, Cassandra is using partitioners which are basically computing hashes of the partition key and the data is distributed across the cluster based on these values. The default partitioner in Cassandra is Murmur3Partitioner.</p>

<p>At OS level, the data is stored in sstables files. A partition can be spread across many sstables. That's why you also need compaction, which is the process of consolidating those sstables, so your partitions won't be spread across a lot of sstables. Reducing the number of sstables a partitions is spread across, will also improve read time. It's worth noting that sstables are immutable.</p>

<p>I suggest reading <a href=""https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlDatabaseInternalsTOC.html"" rel=""nofollow noreferrer"">this</a>, especially ""How Cassandra reads and writes data"".</p>
",['partitioner']
49769643,49772074,2018-04-11 08:01:42,Cassandra read process,"<p>Say, I have a table, with 4 columns. I write some data in it. If I try to read the data, the procedure goes like <a href=""https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlAboutReads.html"" rel=""nofollow noreferrer"">this</a>. I want to understand a specific scenario, in which, all the columns(of the row which I'm trying to read) are present in the memtable. Will SSTables, be checked for data for such a row? I think, that in this case, there's no need to check the SSTables as obviously the data present in the memtable will be the latest copy. Therefore, reads in such cases, should be faster as compared to those when memtable either doesn't have the row, or contains only partial data.</p>

<p>I created a table(user_data), and entered some data which resulted in the creation of 2 SSTables. After this, I inserted a new row. I checked in the data directory and made sure that the SSTable count was still 2. This means that the new data which I entered is lying in the Memtable. I set the 'tracing on' in cqlsh and then selected the same row. Given below is the output:</p>

<pre><code>Tracing session: de2e8ce0-cf1e-11e6-9318-a131a78ce29a

 activity                                                                                     | timestamp                  | source        | source_elapsed | client
----------------------------------------------------------------------------------------------+----------------------------+---------------+----------------+---------------
                                                                           Execute CQL3 query | 2016-12-31 11:33:36.494000 | 172.16.129.67 |              0 | 172.16.129.67
 Parsing select address,age from user_data where name='Kishan'; [Native-Transport-Requests-1] | 2016-12-31 11:33:36.495000 | 172.16.129.67 |            182 | 172.16.129.67
                                            Preparing statement [Native-Transport-Requests-1] | 2016-12-31 11:33:36.495000 | 172.16.129.67 |            340 | 172.16.129.67
                                  Executing single-partition query on user_data [ReadStage-2] | 2016-12-31 11:33:36.495000 | 172.16.129.67 |            693 | 172.16.129.67
                                                   Acquiring sstable references [ReadStage-2] | 2016-12-31 11:33:36.495000 | 172.16.129.67 |            765 | 172.16.129.67
                                                      Merging memtable contents [ReadStage-2] | 2016-12-31 11:33:36.495000 | 172.16.129.67 |            821 | 172.16.129.67
                                         Read 1 live rows and 0 tombstone cells [ReadStage-2] | 2016-12-31 11:33:36.495000 | 172.16.129.67 |           1028 | 172.16.129.67
                                                                             Request complete | 2016-12-31 11:33:36.495225 | 172.16.129.67 |           1225 | 172.16.129.67
</code></pre>

<p>I don't understand the meaning of ""Acquiring sstable references"" here. As the complete data was lying in the Memtable, therefore, as I understand, there's no need to check the SSTables. So, what exactly are these references for?</p>
",<cassandra><cassandra-3.0>,"<blockquote>
  <p>all the columns(of the row which I'm trying to read) are present in the memtable.Will SSTables, be checked for data for such a row?</p>
</blockquote>

<p>In this particular case, <strong><em>it will also check sstable data along memtable parallaly.</em></strong></p>

<p>It will only go to sstable (actually first in <strong>row-cache</strong>, then <strong>bloom filter</strong> and then <strong>sstable</strong>), for that column, which is not present in memtable.</p>

<p><strong>Edit:</strong></p>

<p>To understand more about how read process are working here lets dive into the cassandra source. Let's start from the trace log and we will walk through the steps line by line:</p>

<p>Let's start from here:</p>

<blockquote>
  <p><code>Executing single-partition query on user_data [ReadStage-2]</code></p>
</blockquote>

<p>Your select query is a single partition row query which is obvious. Cassandra just needs to read data from a single partition. Let's jump to the corresponding method and java-doc here, is self-explained: </p>

<pre><code>/**
 * Queries both memtable and sstables to fetch the result of this query.
 * &lt;p&gt;
 * Please note that this method:
 *   1) does not check the row cache.
 *   2) does not apply the query limit, nor the row filter (and so ignore 2ndary indexes).
 *      Those are applied in {@link ReadCommand#executeLocally}.
 *   3) does not record some of the read metrics (latency, scanned cells histograms) nor
 *      throws TombstoneOverwhelmingException.
 * It is publicly exposed because there is a few places where that is exactly what we want,
 * but it should be used only where you know you don't need thoses things.
 * &lt;p&gt;
 * Also note that one must have created a {@code ReadExecutionController} on the queried table and we require it as
 * a parameter to enforce that fact, even though it's not explicitlly used by the method.
 */
public UnfilteredRowIterator queryMemtableAndDisk(ColumnFamilyStore cfs, ReadExecutionController executionController)
{
    assert executionController != null &amp;&amp; executionController.validForReadOn(cfs);
    Tracing.trace(""Executing single-partition query on {}"", cfs.name);

    return queryMemtableAndDiskInternal(cfs);
}
</code></pre>

<hr>

<p>From the avobe step we've found that for your query it will call <code>queryMemtableAndDiskInternal(cfs);</code> this method:</p>

<pre><code>private UnfilteredRowIterator queryMemtableAndDiskInternal(ColumnFamilyStore cfs)
    {
        /*
         * We have 2 main strategies:
         *   1) We query memtables and sstables simulateneously. This is our most generic strategy and the one we use
         *      unless we have a names filter that we know we can optimize futher.
         *   2) If we have a name filter (so we query specific rows), we can make a bet: that all column for all queried row
         *      will have data in the most recent sstable(s), thus saving us from reading older ones. This does imply we
         *      have a way to guarantee we have all the data for what is queried, which is only possible for name queries
         *      and if we have neither non-frozen collections/UDTs nor counters (indeed, for a non-frozen collection or UDT,
         *      we can't guarantee an older sstable won't have some elements that weren't in the most recent sstables,
         *      and counters are intrinsically a collection of shards and so have the same problem).
         */
        if (clusteringIndexFilter() instanceof ClusteringIndexNamesFilter &amp;&amp; !queriesMulticellType())
            return queryMemtableAndSSTablesInTimestampOrder(cfs, (ClusteringIndexNamesFilter)clusteringIndexFilter());
        ...
        ...
</code></pre>

<hr>

<p>Here we've found our answer from this comment: </p>

<p><strong><code>We have 2 main strategies:
                1) We query memtables and sstables simulateneously. This is our most generic strategy and the one we use........</code></strong></p>

<p>Cassandra is simultaniously querying on memtables and sstables.</p>

<p>After that if we jump into the <code>queryMemtableAndSSTablesInTimestampOrder</code> method we've found:</p>

<pre><code>/**
 * Do a read by querying the memtable(s) first, and then each relevant sstables sequentially by order of the sstable
 * max timestamp.
 *
 * This is used for names query in the hope of only having to query the 1 or 2 most recent query and then knowing nothing
 * more recent could be in the older sstables (which we can only guarantee if we know exactly which row we queries, and if
 * no collection or counters are included).
 * This method assumes the filter is a {@code ClusteringIndexNamesFilter}.
 */
private UnfilteredRowIterator queryMemtableAndSSTablesInTimestampOrder(ColumnFamilyStore cfs, ClusteringIndexNamesFilter filter)
{
    Tracing.trace(""Acquiring sstable references"");
    ColumnFamilyStore.ViewFragment view = cfs.select(View.select(SSTableSet.LIVE, partitionKey()));

    ImmutableBTreePartition result = null;

    Tracing.trace(""Merging memtable contents"");
    .... // then it also looks into sstable on timestamp order.
</code></pre>

<hr>

<p>From the above portion we've already found our last two tracing logs:</p>

<blockquote>
  <p><code>Acquiring sstable references [ReadStage-2]</code></p>
  
  <p><code>Merging memtable contents [ReadStage-2]</code></p>
</blockquote>

<hr>

<p>Hope this helps.</p>

<p>Related links:
<a href=""https://github.com/apache/cassandra/blob/0db88242c66d3a7193a9ad836f9a515b3ac7f9fa/src/java/org/apache/cassandra/db/SinglePartitionReadCommand.java"" rel=""nofollow noreferrer"">Source: SinglePartitionReadCommand.java</a></p>
",['table']
49824457,49829078,2018-04-13 20:11:36,Secondary index on for low cardinality clustering column,"<p>Using Cassandra as db:</p>

<p>Say we have this schema
<code>primary_key((id1),id2,type)</code> with <code>index</code> on <code>type</code>, because we want to query by <code>id1</code> and <code>id2</code>.</p>

<p>Does query like
<code>SELECT * FROM my_table WHERE id1=xxx AND type='some type'</code> 
going to perform well?</p>

<p>I wonder if we have to create and manage another table for this situation? </p>
",<cassandra>,"<p>The way you are planning to use secondary index is ideal (which is rare). Here is why:</p>

<ul>
<li>you specify the partition key (id1) in your query. This ensures that
only the relevant partition (node) will be queried, instead of
hitting all the nodes in the cluster (which is not scalable)</li>
<li>You are (presumably) indexing an attribute of low cardinality (I can imagine you have maybe a few hundred types?), which is the sweet spot when using secondary indexes.</li>
</ul>

<p>Overall, your data model should perform well and scale. Yet, if you look for optimal performances, I would suggest you use an additional table ((id1), type, id2).</p>

<p>Finale note: if you have a limited number of type, you might consider using solely ((id1), type, id2) as a single table. When querying by id1-id2, just issue a few parallel queries against the possible value of type.</p>

<p>The final decision needs to take into account your target latency, the disk usage (duplicating table with a different primary key is sometimes too expensive), and the frequency of each of your queries.</p>
",['table']
50032482,50032650,2018-04-25 22:58:33,Filtering by using map value in cql Cassandra,"<p>I'm trying to select records by filtering value on a map column.</p>

<pre><code>name (text) | last (text) | languages(map&lt;text:text&gt;)
john | stith | {12:English, 123:Spanish}
Jane | Doe | {34:Italian, 123:Spanish}
</code></pre>

<p>I'm trying select records have only have Italian as a value. but on the documentation only shows how to get records from the by the key. </p>

<p>by filtering by Italian I should get Jane Doe on the example above. How can I accomplish my filtering?</p>
",<collections><cassandra><filtering><cql>,"<p>you need to create an index on value of the map.
Assuming an index on map value is created, filter the data using a value in the map</p>

<pre><code>SELECT * FROM table WHERE languages CONTAINS 'Italian';
</code></pre>

<p>To create an index on the values</p>

<pre><code>CREATE INDEX mymapvalues ON tableName(languages);
</code></pre>

<p>see here for more info
<a href=""https://docs.datastax.com/en/cql/3.1/cql/ddl/ddlIndexColl.html"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/cql/3.1/cql/ddl/ddlIndexColl.html</a></p>
",['table']
50055638,50060436,2018-04-27 05:13:05,TTL vs default_time_to_live which one is better and why?,"<p>Requirement is simple: we have to create a table which will have only 24 hours of data. 
We have two options</p>

<ol>
<li>Defile TTL with each insert </li>
<li>Make table property default_time_to_live for 24 hours. </li>
</ol>

<p>I have general idea about both the things but internally which one will be helpful to deal with tombstones? or both will generate same amount of tombstones? Which one will be better and why any reference link will be appreciated. </p>
",<cassandra><datastax-enterprise><ttl>,"<p>If a table has <code>default_time_to_live</code> on it then rows that exceed this time limit are deleted immediately without tombstones being written. This will not affect rows / columns that have an explicit TTL set on them. These will be tombstoned. </p>

<p>If you go down the TTL route then you should consider setting the <code>gc_grace_seconds</code> property on the table to something less than the default (10 days). Particularly if you are looking at a 24 hour TTL.</p>

<p>References:</p>

<p><a href=""https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlAboutDeletes.html"" rel=""noreferrer"" title=""How is data deleted"">How data is deleted</a> &lt;-- Good background</p>

<p><a href=""https://docs.datastax.com/en/cql/3.3/cql/cql_reference/cqlCreateTable.html#tabProp"" rel=""noreferrer"">CREATE TABLE properties</a> &lt;-- Table property reference</p>

<p><a href=""http://thelastpickle.com/blog/2016/07/27/about-deletes-and-tombstones.html"" rel=""noreferrer"">About Deletes and Tombstones in Cassandra</a> &lt;-- Everything you ever wanted to know about deletes and tombstones</p>
",['table']
50084180,50088450,2018-04-29 07:03:30,Cassandra prefix search,"<p>I have table 
CREATE TABLE site(url text PRIMARY KEY, count int)
with data</p>

<p>com.google 5</p>

<p>com.google.subdomain 10</p>

<p>Does Cassandra support select all google sites (with subdomain) in single query?</p>
",<cassandra>,"<p>Cassandra has <a href=""https://docs.datastax.com/en/dse/5.1/cql/cql/cql_using/useSASIIndexConcept.html"" rel=""nofollow noreferrer"">SASI index type</a> that allows effective indexing of the text for prefix search &amp; contains.  But it couldn't be used to index the partition key like in your case. One potential workaround could be to put the copy of the same data (or only domain part) into the table as ordinary column, and index that column. (This <a href=""http://www.doanduyhai.com/blog/?p=2058"" rel=""nofollow noreferrer"">blog post</a> has very detailed description of internals of SASI indices).</p>

<p>If you'll do this operation very often, then maybe you'll need to re-model data, and for example, use table of following structure:</p>

<pre><code>create table site(
   ps text,
   url text,
   count int,
   primary key (ps, url));
</code></pre>

<p>where <code>ps</code> is <a href=""https://publicsuffix.org/"" rel=""nofollow noreferrer"">public suffix</a> of domain.  But this will depend on do you count the individual URLs, or only domains, otherwise you'll get too wide rows for sites like google, facebook, etc.</p>
",['table']
50224203,50225546,2018-05-08 00:35:32,Configure cassandra 3.11.2 to accept remote connection on ec2,"<p>Hey guys i tried many of the solutions posted on different sites and stackoverflow too, but none of them worked for me.
I have eximented with the following cassandra.yaml paramters-</p>

<ol>
<li>rpc_address</li>
<li>rpc_broadcast_address</li>
<li>listen_address</li>
</ol>

<p>Most of the solutions are out dated, little help is highly appreciated</p>
",<cassandra><cassandra-3.0>,"<p>As discussed in chat, there was a typo while adding private ip address for listen_address in the cassandra.yaml. Due to this error, Cassandra never got started and hence no output for the following commands.</p>

<pre><code>nodetool status
  - Failed to connect to '127.0.0.1:7199' - ConnectException: 'Connection refused (Connection refused)

netstat -an | grep 9042 
  - returns nothing
</code></pre>

<p>As always, the first place to look at the errors is cassandra system log. And it clearly indicated the following problem.</p>

<pre><code>ERROR [main] 2018-05-08 02:01:26,541 CassandraDaemon.java:708 - Exception encountered during startup: Invalid yaml: file:/etc/cassandra/cassan$ 
Error: while scanning a simple key; could not found expected ':'; in 'reader', line 678, column 1: 
# Set rpc_address OR rpc_interfa ...
</code></pre>

<p>After fixing the "":"" error in cassandra.yaml file, Cassandra came up smoothly. </p>
","['rpc_address', 'listen_address']"
50260061,50268243,2018-05-09 18:38:27,Saving a DateTime to Cassandra Date column,"<p>Cassandra .NET driver documentation is unbelievably poor, I'm trying to scrap together something functional but I waste so much time trying to change code from Java documents that I found.</p>

<p>I am trying to write data to a simple table using Cassandra driver. The table already exists and there's date inside. I have created a mapping and added some columns. Here's a cut-off version to demonstrate:</p>

<pre><code>For&lt;Profile&gt;().TableName(""profiles"")
    .PartitionKey(p =&gt; p.IntegerId)
    .Column(p =&gt; p.IntegerId, cm =&gt; cm.WithName(""profileid""))
    .Column(p =&gt; p.BirthDate, cm =&gt; cm.WithName(""dateofbirth""))
</code></pre>

<p>There are more columns and tables but that this the important part.</p>

<p>Then saving is done from a simple generic method:</p>

<pre><code>public async Task&lt;bool&gt; Add&lt;T&gt;(T item) where T : EntityBase, new()
{
    await _mapper.InsertIfNotExistsAsync(item);
}
</code></pre>

<p>Again there's more code there, but the relevant parts are here. What's important is that I'm using InsertIfNotExists and using generic method that works with a base entity.</p>

<p><code>dateofbirth</code> column in Cassandra is of type Date.
When I run the Insert method I get exception that the length of Date should be 4 bytes instead of 8 (I assume I need to cut off the time part of the DateTime).</p>

<p>I tried using WithType on the mapping and creating a TypeSerializer similar to what was described at <a href=""https://stackoverflow.com/questions/43064133/how-to-insert-java-util-date-values-into-cassandra-date-type-column-using-spring"">this question</a>, but had no luck.
Anyone has a working code that saves this type (and possibly other types) to Cassandra?</p>

<p>Here's the code for the date codec that was adapted from the internet, and how it was used, it might be (very) wrong:</p>

<pre><code>public class DateCodec : TypeSerializer&lt;DateTime&gt;
{
    private static TypeSerializer&lt;LocalDate&gt; _innerSerializer;

    public DateCodec(TypeSerializer&lt;LocalDate&gt; serializer)
    {
        _innerSerializer = serializer;
        TypeInfo = new CustomColumnInfo(""LocalDate"");
    }

    public override IColumnInfo TypeInfo { get; }

    public override DateTime Deserialize(ushort protocolVersion, byte[] buffer, int offset, int length, IColumnInfo typeInfo)
    {
        var result = _innerSerializer.Deserialize(protocolVersion, buffer, offset, length, typeInfo);
        return new DateTime(result.Year, result.Month, result.Day);
    }

    public override ColumnTypeCode CqlType { get; }

    public override byte[] Serialize(ushort protocolVersion, DateTime value)
    {
        return _innerSerializer.Serialize(protocolVersion, new LocalDate(value.Year, value.Month, value.Day));
    }
}
</code></pre>

<p>Usage:</p>

<pre><code>TypeSerializerDefinitions definitions = new TypeSerializerDefinitions();
definitions.Define(new DateCodec(TypeSerializer.PrimitiveLocalDateSerializer));

var cluster = Cluster.Builder()
    .AddContactPoints(...)
    .WithCredentials(...)
    .WithTypeSerializers(definitions)
    .Build();
</code></pre>
",<c#><cassandra>,"<p>C# driver uses <code>LocalDate</code> class to represent <code>date</code> from Cassandra, so either need to change your declaration of <code>dateofbirth</code> to use it, or develop corresponding codec.</p>

<p>You can check the documentation on date and time representation for the C# driver: <a href=""https://docs.datastax.com/en/developer/csharp-driver/3.5/features/datatypes/datetime/"" rel=""noreferrer"">https://docs.datastax.com/en/developer/csharp-driver/3.5/features/datatypes/datetime/</a></p>

<p>Update after update of the question with code sample:</p>

<p>Define table &amp; insert sample data:</p>

<pre><code>cqlsh&gt; create table test.dt(id int primary key, d date);
cqlsh&gt; insert into test.dt(id, d) values(1, '2018-05-17');
cqlsh&gt; insert into test.dt(id, d) values(2, '2018-05-16');
cqlsh&gt; insert into test.dt(id, d) values(3, '2018-05-15');
</code></pre>

<p>Following converted works for me:</p>

<pre><code>public class DateCodec : TypeSerializer&lt;DateTime&gt;
{
    private static readonly TypeSerializer&lt;LocalDate&gt; serializer = 
         TypeSerializer.PrimitiveLocalDateSerializer;

    public override ColumnTypeCode CqlType
    {
        get { return ColumnTypeCode.Date; }
    }

    public DateCodec() { }

    public override DateTime Deserialize(ushort protocolVersion, byte[] buffer, 
         int offset, int length, IColumnInfo typeInfo)
    {
        var result = serializer.Deserialize(protocolVersion, buffer,
                offset, length, typeInfo);
        return new DateTime(result.Year, result.Month, result.Day);
    }

    public override byte[] Serialize(ushort protocolVersion, DateTime value)
    {
        return serializer.Serialize(protocolVersion, 
            new LocalDate(value.Year, value.Month, value.Day));
    }
}
</code></pre>

<p>Main program:</p>

<pre><code>TypeSerializerDefinitions definitions = new TypeSerializerDefinitions();
definitions.Define(new DateCodec());

var cluster = Cluster.Builder()
         .AddContactPoints(""localhost"")
         .WithTypeSerializers(definitions)
         .Build();
var session = cluster.Connect();
var rs = session.Execute(""SELECT * FROM test.dt"");
foreach (var row in rs)
{
    var id = row.GetValue&lt;int&gt;(""id"");
    var date = row.GetValue&lt;DateTime&gt;(""d"");
    Console.WriteLine(""id="" + id + "", date="" + date);
}

var pq = session.Prepare(""insert into test.dt(id, d) values(?, ?);"");
var bound = pq.Bind(10, new DateTime(2018, 04, 01));
session.Execute(bound);
</code></pre>

<p>Gives as result following:</p>

<pre><code>id=1, date=5/17/18 12:00:00 AM
id=2, date=5/16/18 12:00:00 AM
id=3, date=5/15/18 12:00:00 AM
</code></pre>

<p>And checking from <code>cqlsh</code>:</p>

<pre><code>cqlsh&gt; SELECT * from test.dt ;

 id | d
----+------------
 10 | 2018-04-01
  1 | 2018-05-17
  2 | 2018-05-16
  3 | 2018-05-15
</code></pre>
",['table']
50326818,50328739,2018-05-14 09:14:06,How to specify column length on table creation in cql?,"<p>so in SQL I can do the following to specify needed length:</p>

<pre><code>CREATE TABLE ... name VARCHAR(**255**) ...
</code></pre>

<p>Is there some way to do it in CQL? In documentation there is no word about it (or maybe I'm missing something important), so I think it's not possible, but I also didn't find confirmation of it. And what is the maximum length of varchar type in CQL?</p>
",<cassandra><cql><cql3><cqlsh>,"<p>In Cassandra VARCHAR is just an alias for TEXT data type which stores UTF-8 encoded string. The max size of a string is limit4ed by the max size for a string in Java which is 2147483647 characters (2^31 - 1). You can't set a max string length in table definition for string datatypes.</p>
",['table']
50364251,50364554,2018-05-16 06:56:52,InvalidQueryException despite of correct query,"<p>I am using SpringBoot connceted with Hibernate and Cassandra Database. I made couple of methods using ResultSet and everything works perfect till now. I create another method, create query and then ResultSet.</p>

<pre><code>String queryString = query.toString().replace(""?"", dayList.toString());
ResultSet rS = dataSource.executeQuery(queryString);
</code></pre>

<p>It throws me:</p>

<pre><code>com.datastax.driver.core.exceptions.InvalidQueryException: No keyspace has been specified. USE a keyspace, or explicitly specify keyspace.tablename
</code></pre>

<p>Query is correct. When I execute query in database it returns me proper data.</p>

<p>It is wierd because I use same implementation in previous method and it works. </p>

<p>Here is my query:</p>

<pre><code>SELECT * FROM object_action_statistics WHERE day IN ('2018-04-29','2018-04-30') AND action_id=14 AND timestamp_from&gt;=1525099500073 AND timestamp_from&lt;1525120897000 ALLOW FILTERING
</code></pre>
",<java><cassandra><resultset>,"<p>Correct query should be like this:
<code>
SELECT * FROM KEYSPACE_NAME.object_action_statistics WHERE day IN ('2018-04-29','2018-04-30') AND action_id=14 AND timestamp_from&gt;=1525099500073 AND timestamp_from&lt;1525120897000 ALLOW FILTERING
</code>
I guess you forgot to put keyspace name ahead of table name.</p>
",['table']
50367999,50370878,2018-05-16 10:07:15,How to order room entity by last reply time in a chat application?,"<p>I am designing a chat application's database schema in Apache Cassandra and I couldn't wrap my head around this.</p>

<p>Here's my schema so far:</p>

<pre><code>CREATE TABLE users(
  user_id bigint,
  nickname text,
  email text,
  chat_rooms set&lt;timeuuid&gt;,
  PRIMARY KEY(user_id)
);

CREATE TABLE rooms(
  room_id timeuuid,
  creation_date timestamp,
  creator_id bigint,
  participants set&lt;bigint&gt;,
  PRIMARY KEY(room_id)
);￼

CREATE TABLE messages(
  message_id timeuuid,
  room_id timeuuid,
  author_id bigint,
  time_bucket int,
  content text,
  PRIMARY KEY((room_id, time_bucket), message_id)
) WITH CLUSTERING ORDER BY (message_id DESC);
</code></pre>

<p>I would like to get a list of room by a user's id ordered by last reply time, similar to Facebook Messenger and Telegram.</p>

<p>I was thinking adding a new column <code>last_reply_time</code> to <code>rooms</code>, use it as clustering key and update that when there is new message in the room. However it is impossible to update clustering key value in Cassandra. How should I go about modelling this?</p>

<p>I have looked at the <a href=""https://academy.datastax.com/resources/getting-started-killrchat-example-data-model-messaging"" rel=""nofollow noreferrer"">KillrChat example</a> and <a href=""https://blog.discordapp.com/how-discord-stores-billions-of-messages-7fa6ec7ee4c7"" rel=""nofollow noreferrer"">Discord's wonderful piece</a> on their Cassandra implementation but they did not mention anything related to my issue.</p>

<p>Thanks in advance!</p>
",<cassandra><cql>,"<p>You could have a table like this</p>

<pre><code>create table test (
   creator_id bigint ,
   room_id timeuuid ,
   last_reply_time timestamp,
   primary KEY ((creator_id), room_id))
with CLUSTERING ORDER BY
   (room_id ASC );
</code></pre>

<p>and you could insert into it all last_reply_time and select your data with</p>

<pre><code>select * from test where creator_id = ? and room_id = ?
</code></pre>

<p>You will do only inserts that will update the same entry in the database, since you will have the same creator_id and room_id.</p>
",['table']
50385629,50391654,2018-05-17 07:23:33,Cannot drop column in Cassandra 3.11,"<p>Is it not possible to drop column on Cassandra 3.11? </p>

<p>We recently migrated from Cassandra 3.7 to 3.11, and while checking the compatibility we found that Alter Table table_name drop column_name does not work if the table has a materialized view (even though the column is not a part of any Materialized view).</p>

<p>This seems to be a bit odd as our DataModel has not changed even a single bit and it was possible on 3.7.</p>

<p>Certain other threads on SO suggested to drop materialized view first, then drop column and finally recreate your materialized views.</p>

<p>Even there seems to be no information regarding this in Cassandra Release notes for versions greater than 3.7.</p>

<p>Any idea if this is a bug on Cassandra?</p>
",<cassandra>,"<p>It's no longer possible to drop a column from a table with a materialized view.</p>

<p>This is in the 3.11.1 <a href=""https://git1-us-west.apache.org/repos/asf?p=cassandra.git;a=blob_plain;f=NEWS.txt;hb=refs/tags/cassandra-3.11.1"" rel=""nofollow noreferrer"">release notes</a>: ""Cassandra will no longer allow dropping columns on tables with Materialized Views.""</p>

<p>If you want to see the <a href=""https://github.com/apache/cassandra/commit/e6fb8302848bc43888b0a742a9b0abce09872c45#diff-43b2d530031e2f7ad4286bd05fed4ca0L222"" rel=""nofollow noreferrer"">related code change, it is here</a>.</p>
",['table']
50405435,50407289,2018-05-18 06:53:30,DSE SearchAnalytics with Scala error,"<p>By refering to <a href=""https://docs.datastax.com/en/dse/6.0/dse-dev/datastax_enterprise/analytics/dseSearchAnalyticsOverview.html"" rel=""nofollow noreferrer"">this</a> link , I tried to query cassandra table in spark Dataframe</p>

<pre><code>val spark = SparkSession
          .builder()
          .appName(""CassandraSpark"")
          .config(""spark.cassandra.connection.host"", ""127.0.0.1"")
          .config(""spark.cassandra.connection.port"", ""9042"")
          .master(""local[2]"")
          .getOrCreate();
</code></pre>

<p>The node which I'm using is SearchAnalytics node
With using this spark session , i tried sql query</p>

<pre><code>val ss = spark.sql(""select * from killr_video.videos where solr_query = '{\""q\"":\""video_id:1\""}'"")
</code></pre>

<p>Search indexing is already enabled on that table.</p>

<p>After running the program , here is the error i am getting</p>

<pre><code>Exception in thread ""main"" org.apache.spark.sql.AnalysisException: Table or view not found: `killr_video`.`videos`; line 1 pos 14;
</code></pre>

<p>'Project [*]
+- 'UnresolvedRelation <code>killr_video</code>.<code>videos</code></p>

<pre><code>at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:82)
at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:78)
at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)
at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)
at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)
at scala.collection.immutable.List.foreach(List.scala:392)
at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)
at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:78)
at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)
at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)
at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)
at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:623)
at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:691)
</code></pre>

<p>How can i get Cassandra data into Spark?</p>
",<apache-spark><cassandra><apache-spark-sql><datastax-enterprise><spark-cassandra-connector>,"<p>From this error message it looks like that you're running your code using the standalone Spark, not via DSE Analytics (via <code>dse spark-submit</code>, or <code>dse spark</code>).</p>

<p>In this case you need to register tables - <a href=""https://docs.datastax.com/en/dse/6.0/dse-dev/datastax_enterprise/spark/byosGeneratingSparkSqlSchema.html"" rel=""nofollow noreferrer"">DSE documentation describes</a> how to do it for all tables, using <code>dse client-tool</code> &amp; <code>spark-sql</code>:</p>

<pre><code>dse client-tool --use-server-config spark sql-schema --all &gt; output.sql
spark-sql --jars byos-5.1.jar -f  output.sql
</code></pre>

<p>For my example, it looks like following:</p>

<pre><code>USE test;
CREATE TABLE t122
       USING org.apache.spark.sql.cassandra
       OPTIONS (
                keyspace ""test"",
                table ""t122"",
                pushdown ""true"");
</code></pre>

<p>Here is an example of <code>solr_query</code> that just works out of box if I run it in the spark-shell started with <code>dse spark</code>:</p>

<pre><code>scala&gt; val ss = spark.sql(""select * from test.t122 where solr_query='{\""q\"":\""t:t2\""}'"").show
+---+----------+---+
| id|solr_query|  t|
+---+----------+---+
|  2|      null| t2|
+---+----------+---+
</code></pre>

<p>To make your life easier, it's better to use DSE Analytics, not the <a href=""https://docs.datastax.com/en/dse/6.0/dse-dev/datastax_enterprise/spark/byosIntro.html"" rel=""nofollow noreferrer"">bring your own spark</a>.</p>
",['table']
50414372,50414845,2018-05-18 15:11:51,Cassandra: no viable alternative at input 'IF',"<p>This query: </p>

<p><code>UPDATE jdtestbysentence.""sparseSupplement"" SET uuid = 2b22da9c-58a6-11e8-ae82-2d3e941502e8 WHERE a_uid = ""1849"" IF EXISTS</code></p>

<p>gives this error:</p>

<blockquote>
  <p>no viable alternative at input 'IF' (...=
  2b22da9c-58a6-11e8-ae82-2d3e941502e8 WHERE a_uid = [""184]9"" IF...)</p>
</blockquote>

<p>I am fairly new to Cassandra.
Can someone advise?</p>
",<cassandra><cql>,"<pre><code>UPDATE jdtestbysentence.""sparseSupplement""
SET uuid = 2b22da9c-58a6-11e8-ae82-2d3e941502e8 WHERE a_uid = ""1849"" IF EXISTS
</code></pre>

<p>Ok, so I created your table on my local like this:</p>

<pre><code>CREATE TABLE ""sparseSupplement"" (uuid UUID, a_uid TEXT PRIMARY KEY);
</code></pre>

<p>I ran your CQL UPDATE, and sure enough I got the same error message.  Essentially, there is some confusion around the use of quotes here.  Double quotes are only to be used when enforcing case on a table or column name.  When setting or checking the value of a TEXT (<code>a_uid</code>) you should use single quotes around 1849:</p>

<pre><code>cassdba@cqlsh:stackoverflow&gt; UPDATE ""sparseSupplement""
    SET uuid = 2b22da9c-58a6-11e8-ae82-2d3e941502e8
    WHERE a_uid = '1849' IF EXISTS;

 [applied]
-----------
     False
</code></pre>

<p>Pro-tip: Also, I would caution you against using double-quotes like that.  Unless you absolutely need it to match case to a Java class, it's just going to make it more difficult to work with that table.  Kind of like it did here.</p>
",['table']
50449964,50451088,2018-05-21 13:26:07,Cassandra Native Transport Request tuning,"<p>As per the study to scale the Native Transport Request, I have configured the following parameters and the application seems to scale good, But I don't understand the following,</p>

<p>1) I have configured <code>native_transport_max_threads: 256</code> As per my understanding it provides the concurrent request handling, so It should be equal to the total number of cores right, Why is it default to 128? </p>

<p>2) I set this value as <code>-Dcassandra.max_queued_native_transport_requests=5192</code> What is the problem in increasing these values?</p>
",<cassandra><cassandra-3.0>,"<p>1) Its the maximum concurrent requests it can coordinate, not necessarily the limit to the number of requests it do. This coordination includes things like waiting for the replicas (determined by consistency level) to return the data. This is not active work so theres no reason to limit by number of cores.</p>

<p>2) Back pressure to your application pushing more than your coordinator is configured to handle at once is being applied in memory of your coordinator. The cost here is heap pressure and memory available to the system along with the time sitting in queue added to your latency.</p>

<p>Per your <a href=""https://stackoverflow.com/questions/50432926/native-transport-requests-in-cassandra"">other question</a>, I think you may be getting too focused on the NTR stage when the problem is likely in your data model/queries. If increasing that queue didn't help its probably not the cause. Typically the only scenario when the queued NTRs is the issue is when you slam a LOT of tiny queries at once (usually more than a single client can make as theres a 1024 default limit per host by default). Thats pretty much the only scenario that increasing the queue limit to smooth out the spikes helps. If it doesn't help then use proxyhistograms/tablehistograms/tablestats to narrow down the table and query causing the pressure. If its not obvious it may be a GC related issue or both.</p>
",['table']
50461406,50461600,2018-05-22 06:46:32,CassandraRepository with String returning value in query method,"<p>I have a Spring application which requests data from a Cassandra data base. Usually, I specify convenient POJOs that I'm using in the query methods of the repository class as data model return result.</p>

<p>But in my current case, <strong>I don't want a mapping into a specific model object. I want the query result as raw JSON string (or whatever as <code>Map</code>).</strong></p>

<p>According to following documentation I can request raw JSON by using <code>JSON</code> keyword in the CQL query: <a href=""https://docs.datastax.com/en/cql/3.3/cql/cql_using/useQueryJSON.html"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/cql/3.3/cql/cql_using/useQueryJSON.html</a></p>

<p>And theoretically, Spring Data JPA should support simple types as query results: <a href=""https://www.petrikainulainen.net/programming/spring-framework/spring-data-jpa-tutorial-introduction-to-query-methods/"" rel=""nofollow noreferrer"">https://www.petrikainulainen.net/programming/spring-framework/spring-data-jpa-tutorial-introduction-to-query-methods/</a></p>

<pre><code>@Repository public interface DataRepository extends CassandraRepository&lt;String&gt; {

  @Query(
     ""SELECT JSON uid""
         + "", version""
         + "", timestamp""
         + "", message""
         + "" FROM message_data WHERE uid=?0 ALLOW FILTERING"")
  Optional&lt;String&gt; findById(UUID id);
}
</code></pre>

<p>But finally, I get a mapping error on Spring application's start up:</p>

<pre><code>org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'dataController' defined in URL [jar:file:/app.jar!/BOOT-INF/classes!/.../DataController.class]: Unsatisfied dependency expressed through constructor parameter 0; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'dataRepository': Invocation of init method failed; nested exception is org.springframework.data.mapping.model.MappingException: Could not lookup mapping metadata for domain class java.lang.String
    at org.springframework.beans.factory.support.ConstructorResolver.createArgumentArray(ConstructorResolver.java:749) ~[spring-beans-4.3.13.RELEASE.jar!/:4.3.13.RELEASE]
    ...
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'dataRepository': Invocation of init method failed; nested exception is org.springframework.data.mapping.model.MappingException: Could not lookup mapping metadata for domain class java.lang.String
    at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1628) ~[spring-beans-4.3.13.RELEASE.jar!/:4.3.13.RELEASE]
    ...
Caused by: org.springframework.data.mapping.model.MappingException: Could not lookup mapping metadata for domain class java.lang.String
    at org.springframework.data.cassandra.repository.support.CassandraRepositoryFactory.getEntityInformation(CassandraRepositoryFactory.java:104) ~[spring-data-cassandra-1.5.9.RELEASE.jar!/:na]
</code></pre>

<p>What am I missing here?</p>

<p><strong>SOLUTION:</strong></p>

<p>According to the accepted answer following code snippet did the trick:</p>

<pre><code>import com.datastax.driver.core.querybuilder.QueryBuilder;
import com.datastax.driver.core.querybuilder.Select;

...
@Autowired
private CassandraOperations cassandraTemplate;
...

private Optional&lt;String&gt; findById(final UUID id) {
  final Select select = QueryBuilder.select().json().from(""message_data"");
  select.where(QueryBuilder.eq(...))
        .and(QueryBuilder.eq(""uid"", QueryBuilder.raw(id.toString())));

  return Optional.ofNullable(cassandraTemplate.selectOne(select, String.class));
}

private void insert(final MessageEntity entity) {
  cassandraTemplate.insert(entity);
}
</code></pre>
",<java><json><spring><cassandra><spring-data>,"<p>Your <code>DataRepository</code> is extending <code>CassandraRepository&lt;String&gt;</code> which is a problem.</p>

<p>The <code>CassandraRepository</code> declaration goes like <code>CassandraRepository&lt;T,ID&gt;</code> where </p>

<pre><code>T - the domain type the repository manages
ID - the type of the id of the entity the repository manages
</code></pre>

<p>In your case <code>T</code> would be <code>Entity/Table</code> class you are using for <code>message_data</code> table and <code>ID</code> would be <code>UUID</code>.</p>

<p>If you need to <strong>run the query without Repository</strong>, use <code>cassandraTemplate</code>:</p>

<pre><code>@Autowired
private CassandraOperations cassandraTemplate;
</code></pre>
",['table']
50521935,50522478,2018-05-25 05:14:59,How to recover data deleted using truncate in Cassandra,"<p>I have accidentally truncated a table in Cassandra. I would like to know whether there is any tool available to restore data in it. Any help would be appreciated. Thanks.</p>
",<cassandra><restore><truncate><cassandra-3.0>,"<blockquote>
<p>auto_snapshot  (Default: true)</p>
<p>Whether Cassandra takes a snapshot of
the data before truncating a keyspace or dropping a table. To prevent
data loss, DataStax strongly advises using the default setting. If you
set auto_snapshot to false, data loss occurs on truncation or drop.</p>
</blockquote>
<p>(taken from <a href=""https://docs.datastax.com/en/cassandra/3.0/cassandra/configuration/configCassandra_yaml.html"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/cassandra/3.0/cassandra/configuration/configCassandra_yaml.html</a>)</p>
<p>If you are in luck, look into the snapshot directories for your old sstables. For restoring they &quot;only&quot; need to be copied back to their original locations.</p>
<p>Here is what happens:</p>
<pre><code>./data1/demokeyspace/demo-0d1a38b05fe211e8875d13cbb58d64f2
./data1/demokeyspace/demo-0d1a38b05fe211e8875d13cbb58d64f2/backups
./data1/demokeyspace/demo-0d1a38b05fe211e8875d13cbb58d64f2/mc-1-big-Data.db
./data1/demokeyspace/demo-0d1a38b05fe211e8875d13cbb58d64f2/mc-1-big-Index.db
./data1/demokeyspace/demo-0d1a38b05fe211e8875d13cbb58d64f2/mc-1-big-Filter.db
./data1/demokeyspace/demo-0d1a38b05fe211e8875d13cbb58d64f2/mc-1-big-Summary.db
./data1/demokeyspace/demo-0d1a38b05fe211e8875d13cbb58d64f2/mc-1-big-Digest.crc32
./data1/demokeyspace/demo-0d1a38b05fe211e8875d13cbb58d64f2/mc-1-big-CompressionInfo.db
./data1/demokeyspace/demo-0d1a38b05fe211e8875d13cbb58d64f2/mc-1-big-Statistics.db
./data1/demokeyspace/demo-0d1a38b05fe211e8875d13cbb58d64f2/mc-1-big-TOC.txt
</code></pre>
<p>Then I issued <code>TRUNCATE demokeyspace.demo</code> - after that it is something like this:</p>
<pre><code>./data1/demokeyspace/demo-0d1a38b05fe211e8875d13cbb58d64f2
./data1/demokeyspace/demo-0d1a38b05fe211e8875d13cbb58d64f2/backups
./data1/demokeyspace/demo-0d1a38b05fe211e8875d13cbb58d64f2/snapshots
./data1/demokeyspace/demo-0d1a38b05fe211e8875d13cbb58d64f2/snapshots/truncated-1527228644868-demo
./data1/demokeyspace/demo-0d1a38b05fe211e8875d13cbb58d64f2/snapshots/truncated-1527228644868-demo/mc-1-big-Summary.db
./data1/demokeyspace/demo-0d1a38b05fe211e8875d13cbb58d64f2/snapshots/truncated-1527228644868-demo/mc-1-big-TOC.txt
./data1/demokeyspace/demo-0d1a38b05fe211e8875d13cbb58d64f2/snapshots/truncated-1527228644868-demo/mc-1-big-Digest.crc32
./data1/demokeyspace/demo-0d1a38b05fe211e8875d13cbb58d64f2/snapshots/truncated-1527228644868-demo/mc-1-big-Filter.db
./data1/demokeyspace/demo-0d1a38b05fe211e8875d13cbb58d64f2/snapshots/truncated-1527228644868-demo/mc-1-big-CompressionInfo.db
./data1/demokeyspace/demo-0d1a38b05fe211e8875d13cbb58d64f2/snapshots/truncated-1527228644868-demo/mc-1-big-Index.db
./data1/demokeyspace/demo-0d1a38b05fe211e8875d13cbb58d64f2/snapshots/truncated-1527228644868-demo/mc-1-big-Data.db
./data1/demokeyspace/demo-0d1a38b05fe211e8875d13cbb58d64f2/snapshots/truncated-1527228644868-demo/mc-1-big-Statistics.db
</code></pre>
<p>Simply copy those files back. But keep in mind that you need to do this on all nodes and run <code>nodetool refresh demokeyspace demo</code> afterwards to reread the tables (of course with your keyspace and columnfamily).</p>
",['auto_snapshot']
50534094,50555662,2018-05-25 17:16:42,Cassandra client driver timeout parameters,"<p>I observed the following timeout parameters in Cassandra driver,</p>

<pre><code>counter_write_request_timeout_in_ms: 5000
range_request_timeout_in_ms: 10000
request_timeout_in_ms: 10000
</code></pre>

<p>1) Could somebody explain what are these timeout in Cassandra driver?<br>
2) What is the difference between request and read / write timeout?</p>
",<cassandra><datastax><cassandra-3.0>,"<p>There is a list <a href=""https://docs.datastax.com/en/cassandra/2.1/cassandra/configuration/configCassandra_yaml_r.html#configCassandra_yaml_r__PerformanceTuningProps"" rel=""nofollow noreferrer"">here</a> regarding the most important timeout parameters in Cassandra, but to also explain your question here:</p>

<ul>
<li><p><strong>range_request_timeout_in_ms</strong>:
The time that the coordinator waits for sequential or index scans to complete.</p></li>
<li><p><strong>counter_write_request_timeout_in_ms</strong> 
The time that the coordinator waits for counter writes to complete. 
<em>e.g: writing a table with counter column (counter is a special column for storing a number that is changed in increments.)</em> <a href=""https://issues.apache.org/jira/browse/CASSANDRA-8878?focusedCommentId=14346748&amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-14346748"" rel=""nofollow noreferrer"">Here</a> you can find a good explanation on why the counter write needs separate timeout parameter.</p></li>
<li><p><strong>read_request_timeout_in_ms</strong>:
The time that the coordinator waits for read operations to complete.</p></li>
<li><p><strong>write_request_timeout_in_ms</strong>:
The time that the coordinator waits for write operations to complete.</p></li>
<li><p><strong>request_timeout_in_ms</strong>:
The default time for other, miscellaneous operations.</p></li>
</ul>

<p>Note:</p>

<p>1) <strong>Coordinator:</strong> <em>A node that receives a client query; it facilitates communication between all replica nodes responsible for the query (contacting at least n replica nodes to satisfy the query’s consistency level) and prepares and returns a result to the client.</em></p>

<p>2) The timeout parameters are defined per node base (in cassandra.yaml) and not per client based.</p>
",['table']
50577558,50579659,2018-05-29 06:21:25,tombstone_failure_threshold is maintained per table or for whole cassanandra cluster,"<p>The <code>tombstone_failure_threshold</code> default value is 100000. But is this value for each table or for the whole cluster? I am confused. If one table exceeds this value, will range queries in other tables also be aborted? Please explain.</p>
",<cassandra><tombstone>,"<p>This parameter is set cluster-wide, but it's applied to <em>individual queries</em> - when query is running, and during data processing it finds that number of tombstones are greater than this parameter, then it will abort execution of this query...</p>

<p>It could be a situation, when you performed deletion of data, but most of deletion happened only in some partitions, so some queries on this table may work, and some could be aborted.</p>
",['table']
50641737,50644324,2018-06-01 10:46:54,Find Partition key of the SSTables,"<p>In the <code>nodetool getsstables</code> documentation write about partition key of sstables. How could I find it?</p>

<p><a href=""https://docs.datastax.com/en/cassandra/3.0/cassandra/tools/toolsGetSstables.html"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/cassandra/3.0/cassandra/tools/toolsGetSstables.html</a></p>
",<cassandra>,"<p>Your partition key can be found in the table description. </p>

<p>If you have access to the Cassandra shell, you could do the following to find the table description, for example (my keyspace is <code>animals</code> and my table is <code>types</code>:</p>

<pre><code>cqlsh&gt; desc animals.types;

CREATE TABLE animals.types (
    id uuid PRIMARY KEY,
    color text,
    type text
)
</code></pre>

<p>The partition key is <code>id</code> in this simple table.</p>

<p>Example id:</p>

<pre><code>cqlsh&gt; select * from animals.types;
 id                                   | color   | type
 14fc1189-cb57-4e83-baca-ee178e605af1 |  orange |  giraffe 
</code></pre>

<p>Use that in getsstables like the following:</p>

<pre><code># nodetool getsstables animals types 14fc1189-cb57-4e83-baca-ee178e605af1
/var/lib/cassandra/data/animals/types-6ef0da00659911e88803df4f403979ef/mc-1-big-Data.db
</code></pre>
",['table']
50676010,50682209,2018-06-04 08:09:09,Which one is better to use TTL or Delete in Cassandra?,"<p>I want to  remove records from Cassandra cluster after a particular time.
So what Should I use TTL or manually delete?</p>
",<cassandra><datastax-enterprise><cassandra-3.0>,"<p>It depends on your data model.  The fortunate answer, is that due to their predictable nature, you <em>can</em> build your data model to accommodate TTLs.</p>

<p>Let's say I build the following table to track user requests to a REST service, for example.  Suppose that I really only care about the last week's worth of data, so I'll set a TTL of 604800 seconds (7 days).  So the query I need to support is basically this (querying transactions for user 'Bob' for the prior 7 days):</p>

<pre><code>SELECT * FROM rest_transactions_by_user 
  WHERE username='Bob' AND transaction_time &gt; '2018-05-28 13:41';
</code></pre>

<p>To support that query, I'll build this table:</p>

<pre><code>CREATE TABLE rest_transactions_by_user (
  username TEXT,
  transaction_time TIMESTAMP,
  service_name TEXT,
  HTTP_result BIGINT,
  PRIMARY KEY (username,transaction_time))
  WITH CLUSTERING ORDER BY (transaction_time DESC)
  AND gc_grace_seconds = 864000      
  AND default_time_to_live = 604800;
</code></pre>

<p>A few things to note:</p>

<ul>
<li>I am leaving <code>gc_grace_seconds</code> at the default of 864000 (ten days).  This will ensure that the TTL tombstones will have adequate time to be propagated throughout the cluster.</li>
<li>Rows will TTL at 7 days (as mentioned above).  After that, they become tombstones for an additional 10 days.</li>
<li>I am clustering by <code>transaction_time</code> in DESCending order.  This puts the rows I care about (the ones that haven't TTL'd) at the ""top"" of my partition (sequentially).</li>
<li>By querying for a <code>transaction_time</code> of the prior 7 days, I am ignoring anything older than that.  As my TTL tombstones will exist for 10 days afterward, they will be at the ""bottom"" of my partition.</li>
</ul>

<p>In this way, limiting my query to the last 7 days ensures that <em>Cassandra will <strong>never</strong> have to deal with the tombstones, <strong>as my query will never find them</em></strong>.  So in this case, I <em>have</em> built a data model where a TTL is ""better"" than a random delete.</p>
",['table']
50790887,50793030,2018-06-11 05:17:52,DSE CQL Query for Solr Suggestor,"<p>I am using DSE 5.0.1 version. Earlier we used facet query to show search suggestions. For performance reasons , looking for other alternatives to get suggestions and found solr search suggester component. But I couldn't find examples where suggester component is used from a CQL query. Its possible right?Can anyone help me on this.
Thanks in advance.</p>
",<solr><cassandra><datastax-enterprise><autosuggest><search-suggestion>,"<p>Yes, it's possible and relatively easy - you just need to understand how to map XML that you want to put into generated <code>solrconfig.xml</code> into JSON that is used for configuration.</p>

<p>For example, we want to configure suggestor to suggest on the data from field <code>title</code>, and use additional weights from the <code>rating</code> field. As per <a href=""https://lucene.apache.org/solr/guide/6_6/suggester.html"" rel=""nofollow noreferrer"">Solr documentation</a> the XML piece should look following way:</p>

<pre><code>  &lt;searchComponent class=""solr.SuggestComponent"" name=""suggest""&gt;
    &lt;lst name=""suggester""&gt;
      &lt;str name=""name""&gt;titleSuggester&lt;/str&gt;
      &lt;str name=""lookupImpl""&gt;AnalyzingInfixLookupFactory&lt;/str&gt;
      &lt;str name=""dictionaryImpl""&gt;DocumentDictionaryFactory&lt;/str&gt;
      &lt;str name=""suggestAnalyzerFieldType""&gt;TextField&lt;/str&gt;
      &lt;str name=""field""&gt;title&lt;/str&gt;
      &lt;str name=""weightField""&gt;rating&lt;/str&gt;
      &lt;str name=""buildOnCommit""&gt;false&lt;/str&gt;
      &lt;str name=""exactMatchFirst""&gt;true&lt;/str&gt;
      &lt;str name=""contextField""&gt;country&lt;/str&gt;
    &lt;/lst&gt;
  &lt;/searchComponent&gt;
  &lt;requestHandler class=""solr.SearchHandler"" name=""/suggest""&gt;
    &lt;arr name=""components""&gt;
      &lt;str&gt;suggest&lt;/str&gt;
    &lt;/arr&gt;
    &lt;lst name=""defaults""&gt;
      &lt;str name=""suggest""&gt;true&lt;/str&gt;
      &lt;str name=""suggest.count""&gt;10&lt;/str&gt;
    &lt;/lst&gt;
  &lt;/requestHandler&gt;
</code></pre>

<p>In CQL, it will be converted </p>

<pre><code>ALTER SEARCH INDEX CONFIG ON table ADD 
  searchComponent[@name='suggest',@class='solr.SuggestComponent'] 
  WITH  $$ {""suggester"":[{""name"":""titleSuggester""}, 
   {""lookupImpl"":""AnalyzingInfixLookupFactory""}, 
   {""dictionaryImpl"":""DocumentDictionaryFactory""},
   {""suggestAnalyzerFieldType"":""TextField""}, 
   {""field"":""title""}, {""weightField"":""rating""}, 
   {""buildOnCommit"":""false""}, {""exactMatchFirst"":""true""},
   {""contextField"":""country""}]} $$;

ALTER SEARCH INDEX CONFIG ON table ADD 
  requestHandler[@name='/suggest',@class='solr.SearchHandler'] 
  WITH  $$ {""defaults"":[{""suggest"":""true""}, 
    {""suggest.count"":""10""}],""components"":[""suggest""]} $$;
</code></pre>

<p>After that you need not to forget to execute:</p>

<pre><code>RELOAD SEARCH INDEX ON table;
</code></pre>

<p>And your suggestor will work.  In my example, the index for suggestor should be build explicitly because inventory doesn't change very often.  This is done via HTTP call like this:</p>

<pre><code>curl 'http://localhost:8983/solr/keyspace.table/suggest?suggest=true&amp;suggest.dictionary=titleSuggester&amp;suggest.q=Wat&amp;suggest.cfq=US&amp;wt=json&amp;suggest.build=true&amp;suggest.reload=true'
</code></pre>

<p>But you can control this by setting <code>buildOnCommit</code> to true. Or you can configure it to build suggestor index on start, etc. - see Solr's documentation.</p>

<p>Full example is <a href=""https://github.com/alexott/datastax-bootcamp-project/blob/master/cql/inventory.cql#L41"" rel=""nofollow noreferrer"">here</a> - this is an example of the e-commerce application.</p>
",['table']
50797496,50797891,2018-06-11 12:14:40,Optimal Cassandra Schema for Time-Series,"<p>So I am storing user events in Cassandra and am looking for the right key'ing for the table.</p>

<p><code>CREATE TABLE user_events (
    user text,
    timestamp timestamp,
    ip text,
    event text,
    content text,
    service text,
    PRIMARY KEY (user, timestamp)
) WITH CLUSTERING ORDER BY (timestamp DESC)
    AND compaction = { 'class' : 'DateTieredCompactionStrategy' };
</code></p>

<p>I know there is a limit to a single partition ( I think ~1B ). I do not plan on deleting data as it gets older. Would I need to also key this by month or something? eg:</p>

<p><code>PRIMARY KEY((user, month) timestamp)</code></p>

<p>Or if there is a more optimal way or storing events for time-series data.</p>
",<database-design><cassandra><time-series><cql>,"<p>Don't use DateTiered, use TimeWindow. Second you should write as you expect to read (ex: List all the SELECT queries you want, and then model after that). But avoid large partitions.</p>

<p>There are several ways of avoid big partitions if you want to look for user events based on time.</p>

<ol>
<li>Key by date as you said.</li>
<li>Separate events of different time in different tables (ex: One table per month)</li>
</ol>

<p>The second way has the advantage of segregating data and allow you to move/store/change settings as you go instead of, in the future if you need to change something, you have to deal with a massive dataset. Also, if you ever plan to delete in the future (let's say, GDPR), you avoid tombstones as you drop the full tables. </p>
",['table']
50923469,50933324,2018-06-19 08:04:27,uneven data size on cassandra nodes,"<p>I am struggling to understand why my Cassandra nodes have uneven data size.</p>

<p>I have a cluster of three nodes. According to <code>nodetool ring</code>, each node owns 33.33%. Still disk space usages are uneven.</p>

<pre><code>Node1: 4.7 GB (DC: logg_2, RAC: RAC1)
Node2: 13.9 GB (DC: logg_2, RAC:RAC2)
Node3: 9.3 GB (DC: logg_2, RAC:RAC1)
</code></pre>

<p>There is only one keysapce.</p>

<pre><code>keyspace_definition: |
 CREATE KEYSPACE stresscql_cass_logg WITH replication = { 'class': 'NetworkTopologyStrategy', 'logg_2' : 2, 'logg_1' : 1};
</code></pre>

<p>And there is only one table named <code>blogposts</code>.</p>

<pre><code>table_definition: |
  CREATE TABLE blogposts (
        domain text,
        published_date timeuuid,
        url text,
        author text,
        title text,
        body text,
        PRIMARY KEY(domain, published_date)
  ) WITH CLUSTERING ORDER BY (published_date DESC)
    AND compaction = { 'class':'LeveledCompactionStrategy' }
    AND comment='A table to hold blog posts'
</code></pre>

<p>Please help me to understand it why each node has uneven datasize.</p>
",<cassandra><cassandra-2.0>,"<p>Ownership is how much data is owned by the node.</p>
<blockquote>
<p>The percentage of the data owned by the node per datacenter times the
replication factor. For example, a node can own 33% of the ring, but
show 100% if the replication factor is 3.</p>
<p>Attention: If your cluster uses keyspaces having different replication
strategies or replication factors, specify a keyspace when you run
nodetool status to get meaningful ownship information.</p>
</blockquote>
<p>More information can be found here:
<a href=""https://docs.datastax.com/en/cassandra/2.1/cassandra/tools/toolsStatus.html#toolsStatus__description"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/cassandra/2.1/cassandra/tools/toolsStatus.html#toolsStatus__description</a></p>
<blockquote>
<p>NetworkTopologyStrategy places replicas in the same datacenter by walking the ring clockwise until reaching the first node in another rack.</p>
<p>NetworkTopologyStrategy attempts to place replicas on distinct racks because nodes in the same rack (or similar physical grouping) often fail at the same time due to power, cooling, or network issues.</p>
</blockquote>
<p>Because you only have two racks(RAC1 and RAC2), you are placing node 1 and node 3's replicas in node 2, which is why it is bigger.</p>
<p><a href=""https://docs.datastax.com/en/cassandra/3.0/cassandra/architecture/archDataDistributeReplication.html"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/cassandra/3.0/cassandra/architecture/archDataDistributeReplication.html</a></p>
",['rack']
50992427,50992654,2018-06-22 16:49:16,Alter Keyspace on cassandra 3.11 production cluster to switch to NetworkTopologyStrategy,"<p>I have a cassandra 3.11 production cluster with 15 nodes. Each node has ~500GB total with replication factor 3. Unfortunately the cluster is setup with Replication 'SimpleStrategy'. I am switching it to 'NetworkTopologyStrategy'. I am looking to understand the caveats of doing so on a production cluster. What should I expect?</p>
",<cassandra><cassandra-3.0>,"<p>Switching from m<code>SimpleStrategy</code> to <code>NetworkTopologyStrategy</code> in a single data center configuration is very simple.  The only caveat of which I would warn, is to make sure you spell the data center name correctly.  Failure to do so will cause operations to fail.</p>

<p>One way to ensure that you use the right data center, is to query it from <code>system.local</code>.</p>

<pre><code>cassdba@cqlsh&gt; SELECT data_center FROM system.local;

 data_center
-------------
 west_dc

(1 rows)
</code></pre>

<p>Then adjust your keyspace to replicate to that DC:</p>

<pre><code>ALTER KEYSPACE stackoverflow WITH replication = {'class': 'NetworkTopologyStrategy',
    'west_dc': '3'};
</code></pre>

<p>Now for <em>multiple</em> data centers, you'll want to make sure that you specify your new data center names correctly, AND that you run a repair (on all nodes) when you're done.  This is because <code>SimpleStrategy</code> treats all nodes as a single data center, regardless of their actual DC definition.  So you could have 2 replicas in one DC, and only 1 in another.</p>

<p>I have changed RFs for keyspaces on-the-fly several times.  Usually, there are no issues.  But it's a good idea to run <code>nodetool describecluster</code> when you're done, just to make sure all nodes have schema agreement.</p>

<p><strong>Pro-tip:</strong> For future googlers, there is NO BENEFIT to creating keyspaces using <code>SimpleStrategy</code>.  All it does, is put you in a position where you have to fix it later.  In fact, I would argue that <code>SimpleStrategy</code> should <strong>NEVER BE USED.</strong></p>

<blockquote>
  <p>so when will the data movement commence? In my case since I have specific rack ids now, so I expect my replicas to switch nodes upon this alter keyspace action.</p>
</blockquote>

<p>This alone will not cause any adjustments of token range responsibility. If you already have a RF of 3 and so does your new DC definition, you won't need to run a repair, so nothing will stream.</p>

<blockquote>
  <p>I have a 15 nodes cluster which is divided into 5 racks. So each rack has 3 nodes belonging to it. Since I previously have replication factor 3 and SimpleStrategy, more than 1 replica could have belonged to the same rack. Whereas NetworkStrategy guarantees that no two replicas will belong to the same rack. So shouldn't this cause data to move?</p>
</blockquote>

<p>In that case, if you run a repair your secondary or ternary replicas may find a new home. But your primaries will stay the same.</p>

<blockquote>
  <p>So are you saying that nothing changes until I run a repair?</p>
</blockquote>

<p>Correct.</p>
",['rack']
50995145,51048374,2018-06-22 20:25:11,"How to bind a value to TTL in INSERT, Cassandra C++ driver","<p>I'm using prepared statements in the Cassandra Datastax C++ Driver. How do I bind an integer-value to the ""USING TTL ?"" part of a prepared statement?</p>

<p>My statement would be something like</p>

<pre><code>INSERT INTO table (column1, column2, column3)  VALUES (?, ?, ?) USING TTL ?
</code></pre>

<p>In other words, If I'm using the position to bind to TTL, what is its position? (In this example, is it 4?) If I'm using bind by column name, what is its column name?</p>

<p>It looks like this can be done in CQL, but I couldn't find any documentation about the C++ driver API for doing this.</p>
",<c++><cassandra>,"<p>In Cassandra CQL 2.0 you can have:</p>

<blockquote>
  <p>Cassandra 1.2 doesn't allow you to use a bind marker for the TIMESTAMP and TTL properties of update statements, nor for the LIMIT property of SELECT statements. This is now fixed and you can for instance prepare statements like:</p>
</blockquote>

<pre><code>SELECT * FROM myTable LIMIT ?;
UPDATE myTable USING TTL ? SET v = 2 WHERE k = 'foo';
</code></pre>

<p>See their <a href=""https://www.datastax.com/dev/blog/cql-in-cassandra-2-0"" rel=""nofollow noreferrer"">blog</a> for more. </p>

<p><strong>Edit:</strong></p>

<p>I found <a href=""http://www.eandbsoftware.org/geek/wp-content/uploads/2016/02/cppDriver22.pdf"" rel=""nofollow noreferrer"">this pdf</a> and it tells you more:</p>

<p><strong>Bound parameters:</strong></p>

<blockquote>
  <p>The driver supports two kinds of bound parameters: by marker and by name.
  Binding parameters
  The ? marker is used to denote the bind variables in a query string. This is used for both regular
  and prepared parameterized queries. In addition to adding the bind marker to your query string,
  your application must also provide the number of bind variables to 
  cass_statement_new() when constructing a new statement. If a query doesn’t require any bind variables then 0 can be used. The cass_statement_bind_*()
   functions are then used to bind values to the statement’s variables.
  Bind variables can be bound by the marker index or by name.</p>
</blockquote>

<p><strong>Bind by marker index example</strong></p>

<pre><code>CassString query = cass_string_init(""SELECT * FROM table1 WHERE column1
 = ?"");
/* Create a statement with a single parameter */
CassStatement* statement = cass_statement_new(query, 1);
cass_statement_bind_string(statement, 0, cass_string_init(""abc""));
/* Execute statement */
cass_statement_free(statement);
</code></pre>

<p><strong>Bind by marker name example</strong></p>

<blockquote>
  <p>Variables can only be bound by name for prepared statements. This limitation exists because query
  metadata provided by Cassandra is required to map the variable name to the variable’s marker index.</p>
</blockquote>

<pre><code>/* Prepare statement */
/* The prepared query allocates the correct number of parameters
 automatically */
CassStatement* statement = cass_prepared_bind(prepared);
/* The parameter can now be bound by name */
cass_statement_bind_string_by_name(statement, ""column1"",
 cass_string_init(""abc""));
/* Execute statement */
cass_statement_free(statement);
</code></pre>

<p><strong>To answer your question you can use bind by index (works at least for sure):</strong></p>

<pre><code>CassString query = cass_string_init(""INSERT INTO table (column1, column2, column3)  VALUES (?, ?, ?) USING TTL ?"");
/* Create a statement with a single parameter */
CassStatement* statement = cass_statement_new(query, 4); // Bind 4 variables.
cass_statement_bind_string(statement, 0, cass_string_init(""abc"")); // Bind abc to first column.
cass_statement_bind_string(statement, 1, cass_string_init(""bcd"")); // Bind bcd to second column.
cass_statement_bind_string(statement, 2, cass_string_init(""cde"")); // Bind cde to third column.
cass_statement_bind_string(statement, 3, cass_string_init(50)); // Bind 50 to TTL.   
/* Execute statement */
cass_statement_free(statement);
</code></pre>

<p><strong>Edit:</strong></p>

<p>See <a href=""https://docs.datastax.com/en/cql/3.3/cql/cql_using/useExpireExample.html"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/cql/3.3/cql/cql_using/useExpireExample.html</a> where you see that in case of INSERT we have USING TTL as last part of query, as seen above. </p>
",['table']
51028972,51036212,2018-06-25 17:32:10,how to convert Scala List of bytes to blob,"<p>My schema uses a <code>blob</code> type</p>

<pre><code>    id uuid PRIMARY KEY,
    a  text,
    d  text,
    h  list&lt;text&gt;,
    i list&lt;blob&gt;,
    p  text,
    t  set&lt;text&gt;,
    title text
</code></pre>

<p>I send a <code>json</code> fromm client which gets converted into a case class and then I use the case class in QueryBuilder. In <code>json</code>, I am sending a <code>string</code> for the property which will be stored as a <code>blob</code> and I want to convert the <code>string</code> into <code>blob</code> when inserting into Cassandra. But I don't know how to do so.</p>

<p><code>json</code> is <code>PracticeQuestion(Some(2b01be60-6210-4449-ad40-6b97297d69f2),d,List(d),List(List(1)),d,Set(d),d,d)</code></p>

<p><code>case class</code> is </p>

<pre><code>case class Data (id: Option[UUID],
                              d : String, //text in cassandra
                              h : List[String], //list &lt;text&gt;
                              i : List[Seq[Byte]], //list&lt;blob&gt;. 
                              p : String,//string
                              t : Set[String], //set&lt;text&gt;
                              title : String, // text
                              a:String ) //text
</code></pre>

<p>The <code>QueryBuilder</code> code is</p>

<pre><code>def  insertValues(tableName:String, model:PracticeQuestion):Insert = {
    QueryBuilder.insertInto(tableName).value(""id"",model.id.get)
      .value(""a"",model.a)
      .value(""d"",model.d)
      .value(""h"",seqAsJavaList(model.h)) 
      .value(""image"",seqAsJavaList(model.image)) //this probably isn't correct.
      .value(""p"",model.p)
      .value(""t"",setAsJavaSet(model.t))
      .value(""title"",model.title)
      .ifNotExists();       }
</code></pre>

<p>When I run my code, I get the following error <code>Errorcom.datastax.driver.core.exceptions.InvalidTypeException: Value 4 of type class scala.collection.convert.Wrappers$SeqWrapper does not correspond to any CQL3 type</code> </p>
",<scala><cassandra><playframework-2.6>,"<p>First thing you can do is to change your table scheme. Because Cassandra doesn’t support data type Seq[List[Bytes]]. </p>

<p>If that’s not possible you can try the following. </p>

<p>The problem is that Cassandra doesn’t support the <code>List[Byte]</code> so it has the data type blob which actually accept the hexadecimal value. As you have the Array of bytes and you need to set it as value for your 4th field, I would suggest you to use BoundStatements which has the method <code>setList(yourList)</code> this way you can Set Value for your preparedStatement. </p>

<p>You can do something like this. </p>

<pre><code>BoundStatement bound = ps1.bind()

bound.setList(yourList[Byte])

session.execute(bound)
</code></pre>

<p>You can refer to this: </p>

<blockquote>
  <p><a href=""https://docs.datastax.com/en/developer/java-driver/3.0/manual/statements/prepared/"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/developer/java-driver/3.0/manual/statements/prepared/</a></p>
</blockquote>
",['table']
51052556,51055251,2018-06-26 23:17:13,Counting columns that are not PK - Cassandra,"<p>So I'm running with the problem of not being able to select a count of rows in my table. </p>

<p>My problem is: ""Get the most reserved book in a library""</p>

<p>The model I created:</p>

<pre><code>CREATE TABLE library_reservations (
     book_isbn bigint,
     book_title text,
     book_publicationdate date,
     reservation_date date,
     reservation_addinfo text,
     PRIMARY KEY(book_isbn)
);
</code></pre>

<p>The query:</p>

<pre><code>SELECT book_isbn, book_title, COUNT(reservation_date) 
FROM library_reservations 
GROUP BY book_isbn;
</code></pre>

<p>I really feel like I got my modeling wrong. The table was an adaptation of PostgreSQL tables. So how can I properly get the count of reservations for each book? What shoud be my PK in this case?</p>
",<database-design><cassandra><column-oriented>,"<p>Unfortunately Cassandra does not have a GROUP BY function.</p>

<p>A possible solution would be to maintain a second table with a counter.</p>

<p><a href=""https://docs.datastax.com/en/cql/3.3/cql/cql_using/useCountersConcept.html"" rel=""nofollow noreferrer"">Here</a> you can find some more info regarding counters.</p>

<p>In your particular case, when a new reservation is made you will have to update also the counter table info.</p>

<p>Another thing to have in mind is that tables in Cassandra are designed to satisfy a certain query.</p>
",['table']
51090449,51096281,2018-06-28 20:00:26,Storing counter inside cassandra collection,"<p>I want to store aggregation data from my sensor, here's my schema plan for cassandra table</p>

<p><strong>UPDATED</strong></p>

<pre><code>CREATE TABLE every_second_aggregate_signature(
    device_id text,
    year int,
    month int,
    day int,
    hour int,
    minute int,
    second int,
    signature map&lt;text,counter&gt;,
    PRIMARY KEY ((device_id, year, month, day, hour, minute, second))
)WITH CLUSTERING ORDER BY (year DESC, month DESC, day DESC, hour DESC, minute DESC, second DESC);
</code></pre>

<p>The signature data is in form of increment value and dynamic value, ex.</p>

<pre><code>ts1 - {""4123sad"" : 80, ""djas10"" : 99}
ts2 - {""4123sad"" : 83, ""djas10"" : 103}
ts3 - {""4123sad"" : 87, ""djas10"" : 198, ""asfac9"" : 281}
ts4 - {""4123sad"" : 89, ""djas10"" : 201, ""asfac9"" : 540, ""asd81"" : 12}
</code></pre>

<p>The problem is I knew that cassandra didn't support counter inside the collection. Are there any alternative approach or solution to this problem ? Thanks for your help </p>
",<cassandra>,"<p>The only alternative approach here is to move the ""key"" of the map into primary key of the table. Right now you're trying to model like this (as I understood):</p>

<pre><code>create table counters (
  ts timestamp primary key,
  counters map&lt;text, counter&gt;
);
</code></pre>

<p>Then you'll need to change it to following:</p>

<pre><code>create table counters (
  ts timestamp,
  key text,
  counter counter,
  primary key (ts, key)
);
</code></pre>

<p>And to select all values that should go into map, you simply do</p>

<pre><code>select ts, key, counter from counters where ts = 'some value';
</code></pre>

<p>it will return you every pair of key/counter for given <code>ts</code> in the separate rows, so you'll need to have a code that merge them into map...</p>
",['table']
51114269,51116931,2018-06-30 11:00:15,SSTable folder naming convention,"<p>I just noticed that my newly created sstable folder has a combination of numbers and letters attached. For my table ""tweets"" it looks like:</p>

<pre><code>/var/lib/cassandra/data/twitter/tweets-a6da23906d8211e8a057ffb9a095df5c
</code></pre>

<p>on the disk. Does anybody know what this attached hash is?</p>

<p>Thanks! 
Christian</p>
",<hash><cassandra><directory><naming>,"<p>The folder name consists of table name, and table ID that is generated anew every time when the table is created - this is done to prevent race condition when table created, dropped, created, etc.</p>
",['table']
51131562,51135815,2018-07-02 08:04:42,Cassandra: efficient way to compare stats?,"<p>I want to compare stats by hour between today and yesterday. 
My structure looks like that</p>

<pre><code>CREATE TABLE stats0( id UUID,hour_0 INT, hour_1 INT,[all hours], hour_23 INT,total BIGINT, PRIMARY KEY (id));

CREATE TABLE stats1( id UUID,hour_0 INT, hour_1 INT,[all hours], hour_23 INT,total BIGINT, PRIMARY KEY (id));

CREATE TABLE today( id UUID, today INT, PRIMARY KEY(id))
</code></pre>

<p>This is the example.</p>

<p>DAY 1:</p>

<ul>
<li>today set to 0.</li>
<li>stats load in table stats0</li>
</ul>

<p>DAY 2:</p>

<ul>
<li>today set to 1.</li>
<li>stats of today load in stats1 and compare with stats0</li>
</ul>

<p>DAY 3:</p>

<ul>
<li>truncate stats0</li>
<li>today set to 0.</li>
<li>stats of today load in stats0 and compare with stats1</li>
</ul>

<p>DAY 4:</p>

<ul>
<li>truncate stats1</li>
<li>today set to 1.</li>
<li>stats of today load in stats1 and compare with stats0</li>
</ul>

<p>And continue like that.
I compare hours of stats0 and stats1 during the day.</p>

<p>Is there a way to do it more efficiently?</p>
",<database><cassandra>,"<p>@nasubik To avoid renaming the tables every day, which is one inefficiency here, you could create a structure where the date is in the table, like this:</p>

<p>CREATE TABLE stats( id UUID, date, hour_0 INT, hour_1 INT,[all hours], hour_23 INT,total BIGINT, PRIMARY KEY (id));</p>

<p>I would test this as well; this turns the table sideways and performance will depend on how you're loading your data into the table:</p>

<p>CREATE TABLE stats( id UUID, date, hour (0-23) INT, the_stat_value INT, PRIMARY KEY (id));</p>
",['table']
51143313,51213411,2018-07-02 20:27:24,Cassandra data persistence with and without flushing the node,"<p>Does Cassandra not persist the tables to disk if I don't do <code>nodetool flush</code>
I created the table and keyspace using CQL but I only saw the *-data.db file when I did a flush.
Without it, the data will be lost?</p>
",<cassandra><cassandra-3.0>,"<p>When data is written to a node it is first written to the commitlog on disk, and then it is written to a memtable in RAM. </p>

<p>Cassandra will flush the memtable from RAM and write it to an SSTable on disk when its data footprint exceeds the memtable_cleanup_threshold. Only when this happens will the commitlog be cleared. Nodetool flush converts all memtables to SSTables, even if the memtable_cleanup_threshold is not met.</p>

<p>So the answer is no, your data will not be lost. Eventually the memtable size will meet the threshold, and be written to an SSTable on the disk. </p>
",['memtable_cleanup_threshold']
51192289,51193762,2018-07-05 13:14:36,Converting time to 12 hour format in Cassandra,"<p>I am using Cassandra 3.11.2. 
Cassandra is showing date and time in 24 hour format.How do i change this to  standard 12 hour format?</p>
",<cassandra><cqlsh>,"<p>The printing of time in <code>cqlsh</code> is controlled by <code>time_format</code> setting in the <code>[ui]</code> section of the <code>~/.cassandra/cqlshrc</code>.  The format string corresponds to the Python's <code>time.strftime</code> <a href=""https://docs.python.org/2/library/time.html#time.strftime"" rel=""nofollow noreferrer"">format</a>. So you need to change to something like this (<code>%I</code> - 12 hours format, <code>%p</code> - AM/PM indicator):</p>

<pre><code>[ui]
time_format = %Y-%m-%d %I:%M:%S %p 
</code></pre>

<p>Example:</p>

<pre><code>cqlsh&gt; create table test.tm(id int primary key, tm timestamp);
cqlsh&gt; insert into test.tm(id,tm) values(1, '2018-06-18 18:30:55Z');
cqlsh&gt; SELECT * from test.tm;

 id | tm
----+------------------------
  1 | 2018-06-18 08:30:55 PM    

(1 rows)
</code></pre>
",['table']
51235895,51239605,2018-07-08 20:50:03,Inserting data from spark to cassandra : how to verify everything is ok,"<p>I'm trying to insert data from a csv file into Cassandra using pyspark.</p>

<p>Here is the code :</p>

<p>I read the data :</p>

<pre><code>    df =spark.read.format(""csv"") \
        .option(""header"",""true"") \
        .option(""inferSchema"",""true"") \
        .option(""nullValue"",""NA"") \
        .option(""timestampFormat"",""ddMMMyyyy:HH:mm:ss"") \
        .option(""quote"", ""\"""") \
        .option(""delimiter"", "";"") \
        .option(""mode"",""failfast"") \
        .load(""gs://tidy-centaur-b1/data/PRESCRIPTIONS_ANO.csv"")
</code></pre>

<p>Edit : i put the whole code to show the unique key</p>

<pre><code>    dfi = df.withColumn(""id"", F.monotonically_increasing_id()) \
        .withColumnRenamed(""CHAIPRAT"", ""chaiprat"") \
        .withColumnRenamed(""PRE_PRE_DTD"", ""pre_pre_dtd"") \
        .withColumnRenamed(""NbMol"", ""nbmol"") \
        .withColumnRenamed(""NumAno"", ""numano"")


    dfi.createOrReplaceTempView(""prescription"")
</code></pre>

<p>I count the rows and save the data into cassandra</p>

<pre><code>    dfi.count()
    &gt; 4169826

    dfi.write.format(""org.apache.spark.sql.cassandra"") \
        .mode(""overwrite"") \
        .option(""confirm.truncate"",""true"") \
        .option(""spark.cassandra.connection.host"",""10.142.0.4"") \
        .option(""spark.cassandra.connection.port"",""9042"") \
        .option(""keyspace"",""uasb03"") \
        .option(""table"",""prescription"") \
        .save()
</code></pre>

<p>Now I read the data from cassandra and count the rows.</p>

<pre><code>    presc = sql.read \
        .format(""org.apache.spark.sql.cassandra"") \
        .option(""spark.cassandra.connection.host"",""10.142.0.4"") \
        .option(""spark.cassandra.connection.port"",""9042"") \
        .load(table=""prescription"", keyspace=""uasb03"")

    presc.count()
    &gt; 2148762
</code></pre>

<p>Only the half from the first counting.</p>

<p>I don't find anything in the log files that shows something went wrong.
Does anyone have a clue ?</p>

<p>Edit : I tried to change all the timeout values in cassandra.yaml but presc.count remains the same</p>

<p>Edit here is the cassandra table description</p>

<pre><code>    cqlsh:uasb03&gt; desc prescription;

    CREATE TABLE uasb03.prescription (
        id int PRIMARY KEY,
        chaiprat int,
        nbmol int,
        numano int,
        pre_pre_dtd timestamp
    ) WITH bloom_filter_fp_chance = 0.01
        AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
        AND comment = ''
        AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
        AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
        AND crc_check_chance = 1.0
        AND dclocal_read_repair_chance = 0.1
        AND default_time_to_live = 0
        AND gc_grace_seconds = 864000
        AND max_index_interval = 2048
        AND memtable_flush_period_in_ms = 0
        AND min_index_interval = 128
        AND read_repair_chance = 0.0
        AND speculative_retry = '99PERCENTILE';
</code></pre>

<p>To perform my verification, I also wrote the output in a csv file and i got</p>

<pre><code>    chaiprat;pre_pre_dtd;nbmol;numano;id
    29100476;03Feb2017:00:00:00;5;378369;8589934592
    29100476;24Feb2017:00:00:00;1;378369;8589934593
    29100476;27Feb2017:00:00:00;2;378369;8589934594
</code></pre>

<p>the id is bigger than an int.</p>
",<apache-spark><cassandra><pyspark><spark-cassandra-connector>,"<p>The most probable cause for this is that your data don't have really unique row identifier that could become the partition key, so when you're store the data, some values are overwritten. You can fix this by explicitly creating the table with correct partition key &amp; clustering columns before saving the data. This could be done via <code>createCassandraTable</code> call on your data frame (see <a href=""https://github.com/datastax/spark-cassandra-connector/blob/master/doc/14_data_frames.md#creating-a-new-cassandra-table-from-a-dataset-schema"" rel=""nofollow noreferrer"">docs</a>), something like this:</p>

<pre><code>createCassandraTable(
  ""uasb03"", ""prescription"", 
  partitionKeyColumns = Some(Seq(""columnA"")), 
  clusteringKeyColumns = Some(Seq(""columnB"")))
</code></pre>
",['table']
51274364,51277258,2018-07-10 21:49:32,How to view all tables in CQL and CQLSH?,"<p>I'm trying to explore the database and want to see all tables that exist there. What's the command that's equivalent to <code>SHOW TABLES;</code> in SQL?</p>
",<cassandra><cql><cqlsh>,"<p>to get all table names : <code>DESC tables;</code></p>

<p>to get detailed view of a table : <code>DESC table &lt;table_name&gt;;</code></p>

<p>to get detailed view of all tables in a keyspace : <code>DESC keyspace &lt;keyspace_name&gt;;</code></p>
",['table']
51292105,51292489,2018-07-11 18:29:06,Custom column type in cassandra: List of arrays,"<p>How can I define a table schema for a custom column type, as described below?</p>

<p>I did look at the <a href=""https://docs.datastax.com/en/cql/3.3/cql/cql_using/useCreateUDT.html"" rel=""nofollow noreferrer"">Datastax documentation</a> for UDTs on frozen types. But, I am not able to apply that to my java code. What changes are required for Cassandra TYPE node, so that I can serialize/deserialize easily?. It should store List of Double arrays in column node.</p>

<pre><code>static class Testf {
        String id;
        String name;
        List&lt;Double[]&gt; nodes;
    }
</code></pre>

<p>Table schema:</p>

<pre><code>CREATE TABLE IF NOT EXISTS myks.testf(
id text,
name text,
nodes list&lt;FROZEN&lt;node&gt;&gt;,
PRIMARY KEY (id) );

CREATE TYPE myks.node (
     node map&lt;double&gt;
);
</code></pre>
",<java><cassandra><datastax-java-driver><user-defined-types>,"<p>The easiest way will be to use <a href=""https://docs.datastax.com/en/developer/java-driver/3.5/manual/object_mapper/"" rel=""nofollow noreferrer"">ObjectMapper</a> from Java driver.  You can add necessary annotations for your class, and then map class to Cassandra table &amp; back. But you'll need to create a separate class to match your <code>node</code> UDT.</p>
",['table']
51322715,51323351,2018-07-13 10:07:37,Cassandra order by timestemp desc,"<p>I just begin study cassandra.</p>

<p>It was a table and queries.</p>

<pre><code>    CREATE TABLE finance.tickdata(
    id_symbol  int, 
    ts         timestamp,
    bid        double,
    ask        double,
    PRIMARY KEY(id_symbol,ts)
);
</code></pre>

<p>And query is successful,</p>

<pre><code>select ts,ask,bid 
  from finance.tickdata 
 where id_symbol=3 
 order by ts desc;
</code></pre>

<p>Next it was decision move id_symbol in table name, new table(s) scripts.</p>

<pre><code>CREATE TABLE IF NOT EXISTS mts_src.ticks_3(
    ts         timestamp PRIMARY KEY,
    bid        double,
    ask        double
);
</code></pre>

<p>And now query fails, </p>

<pre><code>select * from mts_src.ticks_3 order by ts desc
</code></pre>

<p>I read from docs, that I need use and filter (WHERE) by primary key (partition key),
but technically my both examples same. Why cassandra so restricted in this aspect?</p>

<p>And one more question, It is good idea in general? move id_symbol in table name - 
potentially it can be 1000 of unique id_symbol and a lot of data for each. Separate this data on individual tables look like good idea!? But I lose order by possibility, that is so necessary for me to take fresh data by each symbol_id. </p>

<p>Thanks. </p>
",<cassandra>,"<p>You can't sort on the partition key, you can sort only on clustering columns inside the single partition. So you need to model your data accordingly. But you need to be very careful not to create very large partitions (when using <code>ticker_id</code> as partition key, for example). In this case you may need to create a composite keys, like, <code>ticker_id</code> + year, or month, depending on how often you're inserting the data.</p>

<p>Regarding the table per ticker, that's not very good idea, because every table has overhead, it will lead to increased resource consumption. 200 tables is already high number, and 500 is almost ""hard limit""</p>
",['table']
51336768,51346674,2018-07-14 08:37:46,Optimal way to insert highly-duplicated data into Cassandra,"<p>I have a set-like table: 
It consists of 2 primary columns and a dummy boolean non-primary column.
The table is replicated.
I write massively into this table and very often the entry already exists in the database.
Deletion of entries happens due to TTL and sometimes (not so often) due to DELETE queries.</p>

<p>What is the most performant way to write values into this table?</p>

<p>First option:</p>

<p>Just blindly write values.</p>

<p>Second option:</p>

<p>Check if the value already exists and write only if it is missing.</p>

<p>The second approach requires one more lookup before each write but saves database capacity because it doesn't propagate unnecessary writes to the other replicas.</p>
",<cassandra>,"<p>I would go with option 1, and then tune the compaction strategies. Option 2 will add much more load to the cluster, as reads are always slower than writes, and if in your case inserts happen when previous data still in memtable, then they will be directly overwritten (so you may consider to tune memtable as well).</p>

<p>If you have high read/write ration, you can go with leveled compaction - it could be more optimized for this use case. If ratio isn't very high, leave the default compaction strategy.</p>

<p>But in any case you'll need to tune compaction:</p>

<ol>
<li>decrease <code>gc_grace_period</code> to acceptable value, depending on how fast you can bring back nodes that are down;</li>
<li>change table options like <code>tombstone_compaction_interval</code> (<a href=""https://docs.datastax.com/en/cql/3.3/cql/cql_reference/cqlCreateTable.html#compactSubprop__compactionSubpropertiesSTCS"" rel=""nofollow noreferrer"">doc</a>), and maybe <code>unchecked_tombstone_compaction</code>;</li>
<li>You may also tune things like, <code>concurrent_compactors</code> &amp; <code>compaction_throughput_mb_per_sec</code> to perform more aggressive compactions.</li>
</ol>
",['table']
51427404,51427450,2018-07-19 16:10:52,cassandra @table dynamically change name,"<p>I have multiple consumers for an API who post similar data into my API. My API needs to consume this data and persist the data into cassandra tables identified by consumer name. Eg. consumername_tablename</p>

<p>My spring boot entity is annotated with @Table which doesn't let me change the table name dynamically. Most recommendations online suggest that its not something we should try and change. </p>

<p>But in my scenario identifying all consumers and creating table in advance doesnt sound right. In future I want to be able to add consumers to my API seamlessly.</p>

<p>I want to use a variable passed in my API call as the prefix for my cassandra table names. Is this something I can achieve?</p>
",<spring-boot><cassandra><spring-data>,"<p>For starters: You cannot change annotations without recompiling- they are baked into the compiled class file. This is not the right approach.</p>

<p>Why not put everything in one table and make consumer part of the key? This should give you identical functionality without any of the hassle.</p>
",['table']
51479124,51612990,2018-07-23 12:43:17,Extending Cassandra cluster with datacenter in China (CGF),"<p>I need to extend my cluster with a new datacenter to be present in China mainland, behind the Great Firewall. Currently I have datacenters in the US and Europe - so the cluster already matches to the requirements of the <a href=""https://www.datastax.com/dev/blog/multi-datacenter-replication"" rel=""noreferrer"">Geographical Location Scenario</a>.</p>

<p>At this point I have the chinese infrastructure ready for Cassandra, but the network statistics from the past few days are bit troublesome and I am a bit afraid: <strong><em>if and how this can effect my current cluster</em></strong> and will be the new datacenter functional at all?</p>

<p>My actual questions regarding this are:</p>

<ul>
<li>How does Cassandra handle huge packet-loss during replication? (occasionally up to 40%)</li>
<li>How does it effect the cluster when the network connection between two datacenters are really bad (only few kilobits/sec and latency as above) for hours?

<ul>
<li>Will the chinese dc considered as dead? Or Cassandra will still try to use the limited bandwidth? </li>
<li>Can this cause any problem on the non-chinese datacenters? e.g. they get slow, which results in client request timeouts.</li>
</ul></li>
<li>Is it possible to enforce somehow, that only one of my non-chinese datacenter communicates with the chinese one? Or should I trust that Cassandra will handle this? (trying to avoid to possible harm all my datacenters)</li>
<li>Is there any way to fasten up the initial data replication (<code>nodetool rebuild</code>), because with the current speed it would take weeks to replicate our current data.</li>
</ul>

<p>Any suggestion or remark is welcomed, thanks!</p>
",<cassandra><cassandra-3.0>,"<blockquote>
  <p>How does Cassandra handle huge packet-loss during replication? (occasionally up to 40%)</p>
</blockquote>

<p>Usually packet loss will cause a large number of read repairs. In some cases it can cause requests to fail depending on replication factor and consistency. Also, be prepared to have very costly repairs which will create a lot of tiny SSTables and a substansial amount of IO.</p>

<p>I would suggest to run a test on a development requirement to see the actual behavior in your system. There are plenty of <a href=""https://wiki.linuxfoundation.org/networking/netem"" rel=""nofollow noreferrer"">tools</a> to simulate bad network.</p>

<blockquote>
  <p>How does it effect the cluster when the network connection between two datacenters are really bad (only few kilobits/sec and latency as above) for hours? Will the chinese dc considered as dead? Or Cassandra will still try to use the limited bandwidth? Can this cause any problem on the non-chinese datacenters?</p>
</blockquote>

<p>It largely depends on <em>how bad</em> and what consistency level/replication factor you are running with. In some cases it will just cause rather high latency between clusters. However, if the connection is bad enough that the nodes will start marking the other as down - Then you are looking at issues in all datacenters. Your existing datacenters will struggle with performance caused by requests timing out. This will in turn cause requests to be held longer in memory which can lead to GC. (It can cause a number of other issues in your other cluster as well)</p>

<p>The threshold on how sensitive the failure detector is can be adjusted and fine tuned to suit your use case. phi_convict_threshold is a setting that can decrease the likelihood of a node being marked as down. You can find more about that <a href=""https://docs.datastax.com/en/cassandra/2.1/cassandra/architecture/architectureDataDistributeFailDetect_c.html"" rel=""nofollow noreferrer"">here</a>. If you find that sweet spot where your nodes are not marked down due to being unresponsive, you can have Cassandra leverage what little it has to work with.</p>

<blockquote>
  <p>Is it possible to enforce somehow, that only one of my non-chinese datacenter communicates with the chinese one? Or should I trust that Cassandra will handle this? (trying to avoid to possible harm all my datacenters)</p>
</blockquote>

<p>There is not really a way to tell Cassandra to limit what datacenters to speak to. You are kind of stuck with communicating between the datacenters you include in your replication factor.</p>

<blockquote>
  <p>Is there any way to fasten up the initial data replication (nodetool rebuild), because with the current speed it would take weeks to replicate our current data.</p>
</blockquote>

<p>I would recommend against the solution of using sstableloader for it functions very similar as rebuild does and requires a snapshot to operate. If network is what is causing the slow speed, then changing the way of streaming is not going to make much difference.</p>

<p>In my opinion, the first thing to do would be to measure where the bottleneck is for your system. If the slow network is really the bottleneck, one could add more nodes to stream from more sources at the same time but ultimately you will still be hampered by the slow network connection.</p>
","['phi_convict_threshold', 'dc']"
51511267,51516446,2018-07-25 05:18:09,Cassandra table design for user chat,"<p>I want to create table in Cassandra for user chats, I end up doing this </p>

<pre><code>CREATE TABLE sample.user_messages (
    user_id INT,
    second_user_id INT,
    id TIMEUUID,
    author_id INT,
    message TEXT,
    PRIMARY KEY ((user_id), second_user_id, id)
) WITH CLUSTERING ORDER BY (second_user_id ASC, id DESC);
</code></pre>

<p>I have two type of query</p>

<ol>
<li><p>get chats between two users that this table design satisfy <code>... where user_id=100 and second_user_id=200</code></p></li>
<li><p><strong>get all chats of a specific user that this table design not good for</strong> and I don't have any idea what to do, for this should I use two queries, 1- <code>... where user_id=100'</code> 2- ... <code>where second_user_id=100</code> which second query is not good, also is there any way where I can use only one query</p></li>
</ol>
",<database><database-design><cassandra><nosql>,"<p>Your table allows you to get all chats by user_id, so you can just insert data twice into this table but change user's ids for the second insert.</p>

<p>Put message for the first user: </p>

<pre><code>UPDATE user_messages SET .... second_user_id = 200 WHERE user_id = 100;
</code></pre>

<p>and put the same message for the second user:</p>

<pre><code>UPDATE user_messages SET .... second_user_id = 100 WHERE user_id = 200;
</code></pre>

<p>Now you can get all chats for each user:</p>

<pre><code>Select * from user_messages where user_id = 100;
Select * from user_messages where user_id = 200;
</code></pre>

<p>Get chat between two users:</p>

<pre><code>Select * from user_messages where user_id = 100 and second_user_id = 200;
</code></pre>

<p>or vice versa: </p>

<pre><code>Select * from user_messages where user_id = 200 and second_user_id = 100;
</code></pre>

<p>This approach will duplicate data but for Cassandra it is a common way to pay for read speed.</p>

<p><strong>[Edited]</strong> Large partitions issue</p>

<p>If you expect too many messages per user you should choose another partition key rather user_id. For example, you can use a composite partition key which will consist of user_id and day, in this case each partition will contain messages only for one day, but you will have separate partition for each day. This technique is usually called ""bucketing"", <a href=""http://thelastpickle.com/blog/2017/08/02/time-series-data-modeling-massive-scale.html"" rel=""nofollow noreferrer"">some example of bucketing</a></p>
",['table']
51524102,51536702,2018-07-25 16:53:50,Does running scrub on a table joins SSTables?,"<p>My table has time window compaction strategy enabled (TWCS), for some reason I have a lot of SStables just with tombstones. </p>

<p>When I run a manual compaction on a single sstable, it does not get removed. If I run a reapir it will join all the sstables on a single one, which breaks TWCS.</p>

<p>According to the documentation on the nodetool scrub command:</p>

<blockquote>
  <p>Scrub automatically discards broken data and removes any tombstoned rows that have exceeded gc_grace period of the table.</p>
</blockquote>

<p>Will this join all the sstables?</p>
",<cassandra><nodetool>,"<p>Short answer: the scrub is not joining the sstables.</p>

<p>Long answer: keep reading.</p>

<p>I have checked the code in Cassandra 3.11.2, but the code is similar on 3.0 and 2.2.</p>

<p>The sstables are scrubbed in parallel, using the compaction threads, each thread scrubbing one sstable.</p>

<p>As you can see in <a href=""https://github.com/apache/cassandra/blob/1d506f9d09c880ff2b2693e3e27fa58c02ecf398/src/java/org/apache/cassandra/db/ColumnFamilyStore.java#L1527"" rel=""nofollow noreferrer"">ColumnFamilyStore.java</a> the scrub command is ran using the <a href=""https://github.com/apache/cassandra/blob/1d506f9d09c880ff2b2693e3e27fa58c02ecf398/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L377"" rel=""nofollow noreferrer"">CompactionManager</a> threads.</p>

<p>The interesting function to inspect is <a href=""https://github.com/apache/cassandra/blob/1d506f9d09c880ff2b2693e3e27fa58c02ecf398/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L296"" rel=""nofollow noreferrer"">parallelAllSSTableOperation</a>. All live sstables (excluding the ones marked as suspects - for instance because of some exceptions during compaction) belonging to the table are marked as compacting, all compactions running on that table are <a href=""https://github.com/apache/cassandra/blob/1d506f9d09c880ff2b2693e3e27fa58c02ecf398/src/java/org/apache/cassandra/db/ColumnFamilyStore.java#L2259"" rel=""nofollow noreferrer"">paused</a> and the operation is executed against <a href=""https://github.com/apache/cassandra/blob/1d506f9d09c880ff2b2693e3e27fa58c02ecf398/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L313"" rel=""nofollow noreferrer"">each sstable</a>, in parallel.</p>

<p>In the case of scrub, the operation is <a href=""https://github.com/apache/cassandra/blob/1d506f9d09c880ff2b2693e3e27fa58c02ecf398/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L980"" rel=""nofollow noreferrer"">scrubOne</a> which calls the <a href=""https://github.com/apache/cassandra/blob/1d506f9d09c880ff2b2693e3e27fa58c02ecf398/src/java/org/apache/cassandra/db/compaction/Scrubber.java#L156"" rel=""nofollow noreferrer"">Scrubber.scrub()</a>. This one obsoletes the old sstable and creates a new sstable that contains the live rows.</p>

<p>At the end of parallelAllSSTableOperation the list of sstables marked as compacting should be empty and the operation is successfull. No join of sstables is performed.</p>

<p>So, you can see that the scrub tool is invasive: it obsoletes old sstables, discarding tombstones and keeping the live rows in new sstables.</p>

<p>I hope this helps and I didn't miss anything :).</p>
",['table']
51525836,51527427,2018-07-25 18:52:05,Cassandra migrates value from text to list of text,"<p>I try to update my table in Cassandra database. Right now I have table which looks like </p>

<pre><code>foo(id, email) - email text type
</code></pre>

<p>I want to update the table to something like </p>

<pre><code>foo(id, emails) - emails list of text type
</code></pre>

<p>I added new column</p>

<pre><code>ALTER TABLE foo ADD emails set &lt;text&gt;;
</code></pre>

<p>but I don't know how to migrate value from email column to emails.</p>
",<list><text><cassandra>,"<p>I am not very familiar with your domain, however you have to tell Cassandra how to create the new list of emails. Take a look at <a href=""https://docs.datastax.com/en/cql/3.1/cql/cql_reference/copy_r.html"" rel=""nofollow noreferrer"">COPY</a> if you don't have too many records. You can </p>

<ol>
<li>Add a new column</li>
<li>Use copy to export your existing data using <code>COPY TO</code></li>
<li>Apply your transformations on the exported data</li>
<li>Use <code>COPY FROM</code> to upload your new data. </li>
</ol>

<p>If you have to do this update in real time while your table is growing, this solution won't work. </p>
",['table']
51552792,51553195,2018-07-27 07:21:45,Cassandra Apache query,"<p>I have a problems with a table in cassandra. Below is what I did:</p>
<p><code>CREATE KEYSPACE tfm WITH REPLICATION = {'class': 'SimpleStrategy', 'replication_factor': 1 };</code></p>
<p>I'm working in one machine.</p>
<p><code>CREATE TABLE tfm.foehis(hocpny text, hocol text,honumr text,holinh text,hodtto text,hotour text,hoclic text, hooe  text,hotpac text,hodtac text,hohrac text,hodesf text,hocdan text,hocdrs text,hocdsl text, hoobs text,hotdsc text,honrac text,holinr text,housca text,hodtea text,hohrea text,housea text,hodtcl text,hohrcl text,houscl text,hodtrc text,hohrrc text,housrc text,hodtra text,hohrra text,housra text,hodtcm text,hohrcm text,houscm text,hodtua text,hohrua text,houser text, PRIMARY KEY((hooe,hodtac,hohrac),hoclic));</code></p>
<p>Until this point everything is OK. But when I try to do some select queries, I get warnings and errors:</p>
<p><code>cqlsh&gt; select count(*) from tfm.foehis;</code></p>
<p><code>count</code></p>
<p><code>-------</code></p>
<p><code> 56980</code></p>
<p><code>(1 rows)</code></p>
<p><code>Warnings :</code>
<code>Aggregation query used without partition key</code></p>
<p><code>Read 100 live rows and 1055 tombstone cells for query SELECT * FROM tfm.foehis LIMIT 100 (see tombstone_warn_threshold)</code></p>
<p><code>Read 100 live rows and 1066 tombstone cells for query SELECT * FROM tfm.foehis WHERE token(hooe, hodtac, hohrac) &gt;= token(1045161613, 20180502, 2304) LIMIT 100 (see tombstone_warn_threshold)</code></p>
<h3>And</h3>
<p><code>cqlsh&gt; select count(*) from tfm.foehis where hoclic=1011;</code></p>
<p><code>InvalidRequest: Error from server: code=2200 [Invalid query] message=&quot;Invalid INTEGER constant (1011) for &quot;hoclic&quot; of type text&quot;</code></p>
<p><code>cqlsh&gt;  select count(*) from tfm.foehis where hotpac=ANOE;</code></p>
<p><code>SyntaxException: line 1:49 no viable alternative at input ';' (...from tfm.foehis where hotpac=[ANOE];) </code></p>
<p>I supposed that the problems is in the definition of table, but I don't know where the problems is.</p>
",<cassandra>,"<p>Actually your issue is in the queries. Since all your columns are text you need to use simple quotes around values.</p>

<p>Also, according to your table definition, the partition key is formed  by hooe,hodtac,hohrac columns which means that all your queries must include this columns with exact values (=). hoclic will be the clustering column and on this one you will be able to use other operators and ordering.</p>

<p>Also, have in mind that running queries without the partition key is not recommended in Cassandra (like your select) since this will trigger a full cluster scan and you can run in all sorts of problems (for instance, garbage collection issues).</p>

<p>I would recommend some basic reading: <a href=""https://www.datastax.com/dev/blog/the-most-important-thing-to-know-in-cassandra-data-modeling-the-primary-key"" rel=""nofollow noreferrer"">https://www.datastax.com/dev/blog/the-most-important-thing-to-know-in-cassandra-data-modeling-the-primary-key</a> and <a href=""https://docs.datastax.com/en/cql/3.3/index.html"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/cql/3.3/index.html</a> </p>
",['table']
51577843,51595300,2018-07-29 06:30:51,How to find last and first entry in cassandra (date is part of partition key),"<p>Is it possible to find first and last entry in Cassandra database if my partition key contains text date as a part of partition key to avoid large partitions?</p>

<pre><code>CREATE TABLE trades (
    stockexchange text,
    symbol text,
    ts timestamp,
    date text,
    tid text,
    price decimal,
    side text,
    size decimal,
    PRIMARY KEY ((stockexchange, symbol, date), ts, tid)
) WITH CLUSTERING ORDER BY (ts ASC, tid ASC)
</code></pre>
",<cassandra><partitioning><cassandra-3.0>,"<p>The one solution is - to create the second table and store separately.
only: stockexchange, symbol, timestamp</p>

<p>This gives you ability to find the first and last timestamp by your key (stockexchange:symbol)</p>

<p>Please pay attention, that you have to store the data in the same moment and Cassandra is not ACID database type.</p>

<pre><code>CREATE TABLE trades (
    stockexchange text,
    symbol text,
    ts timestamp,
    date text,
    tid text,
    price decimal,
    side text,
    size decimal,
    PRIMARY KEY ((stockexchange, symbol, date), ts, tid)
) WITH CLUSTERING ORDER BY (ts ASC, tid ASC)

CREATE TABLE trades_timestampts (
stockexchange text,
symbol text,
tid text,
ts timestamp,
PRIMARY KEY ((stockexchange, symbol), ts, tid)) WITH CLUSTERING ORDER BY (ts asc, tid asc);
</code></pre>
",['table']
51604961,51606429,2018-07-31 03:15:24,How to get size of cassandra list type column's items,"<p>cassandra table</p>

<pre><code> seq | keywords            | timestamp
-----+---------------------+---------------
 100 | ['apple', 'banana'] | 1488637750836
</code></pre>

<p>wanted result</p>

<pre><code> seq | keyword_size
-----+--------------
 100 | 2
</code></pre>

<p>query </p>

<pre><code>select
    seq,
    something(keywords) as keyword_size
from
    t
where
    seq = 100
</code></pre>

<p>Is there something like function for count column items?</p>
",<list><cassandra><count>,"<p><strong>There is no builtin method to do this</strong>  </p>

<p>What you can do is get the keywords using a query and get the size using Java or Other programming language from your application backend.</p>

<p>Or Cassandra 2.2 and later allows users to define functions (UDT) that can be applied to data stored in a table as part of a query resul. You can try the below UDF</p>

<pre><code>CREATE OR REPLACE FUNCTION lsizeof(data list&lt;text&gt;) CALLED ON NULL INPUT RETURNS int LANGUAGE java AS 'return data.size();';
</code></pre>

<p>Now you have the UTF function <code>lsizeof</code>, here is how you can get the size of your list using a query like below</p>

<pre><code>SELECT seq, lsizeof(keywords) as keyword_size FROM test_list;
</code></pre>

<p>Output : </p>

<pre><code> seq | keyword_size
-----+--------------
 100 |            2
</code></pre>

<p><em>Note : UDFs (user defined functions) are disabled by default, you can enable it by setting enable_user_defined_functions: true on cassandra.yaml</em></p>
",['table']
51635049,51635721,2018-08-01 13:50:06,run a bulk update query in cassandra on 1 column,"<p>we have a scenario where a table in cassandra which has over million records and we want execute a bulk update on a column(basically set the column value to null in entire table).</p>

<p>is there a way to do so since below query won't work in CQL</p>

<pre><code>UPDATE TABLE_NAME SET COL1=NULL WHERE PRIMARY_KEY IN(SELECT PRIMARY_KEY FROM TABLE_NAME );
</code></pre>

<p>P.S - the column is not a primary key or a cluster key.</p>
",<cassandra><cql3><cqlsh>,"<p>There has been a similar question the other days regarding <a href=""https://stackoverflow.com/questions/51555961/deleting-column-in-cassandra-for-large-dataset"">Deleting a column in cassandra for a large dataset</a>...I suggest also reading the section Dropping a column from the <a href=""https://docs.datastax.com/en/cql/3.1/cql/cql_reference/alter_table_r.html"" rel=""nofollow noreferrer"">Alter table documentation</a>.</p>

<p>One solution in this case might be dropping the column and re-adding it since</p>

<blockquote>
  <p>If you drop a column then re-add it, Cassandra does not restore the
  values written before the column was dropped. A subsequent SELECT on
  this column does not return the dropped data.</p>
</blockquote>

<p>I would test this on a test system beforehand and I would check if the tombstones have been removed.</p>
",['table']
51650770,51652169,2018-08-02 10:00:39,nodetool snapshot takes schema snapshot (backup) too?,"<p><a href=""http://cassandra.apache.org/doc/latest/tools/nodetool/snapshot.html"" rel=""nofollow noreferrer"">Cassandra doc</a> mentions that ""nodetool snapshot"" command takes snapshot of table data. However, I am also able to see schema.cql and manifest.json file in my snapshot directory where all snapshot files are generated.</p>

<p>Is this expected behavior? Also can I use this schema.cql file to restore the schema if needed?</p>

<p>My cassandra version</p>

<pre><code>cqlsh&gt; show version
[cqlsh 5.0.1 | Cassandra 3.0.9 | CQL spec 3.4.0 | Native protocol v4]

&gt;nodetool version
ReleaseVersion: 3.0.9
</code></pre>

<p>EDIT:</p>

<ol>
<li>Is it mandatory to use cql file from snapshot while restoring data? Suppose I have create table cql stored somewhere else. Can I use that? 
I performed some tests. When I re-created table using cql from snapshot, ID in table name remains same ""employee-<strong>42a71380966111e8870f97a01282a56a</strong>"". However when I re-created table using my original cql, ID in table name changed. Can this be a problem and that's why we should use cql from snapshot?
Note-: When I restored data from snapshot, it loaded fine in both above cases</li>
<li>This cql file is for table. Can we get cql from snapshot to create keyspace?</li>
<li>Does cql file gets generated only for user defined table? I can't see cql file getting generated for system tables..</li>
</ol>
",<cassandra><cassandra-3.0>,"<p>Yes, these files are necessary for restoring this particular table. And <code>schema.cql</code> captures the structure of table on the time of the snapshot because you need to restore snapshot to table with the same structure.</p>

<p>You can find <a href=""https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/tools/nodetool/toolsSnapShot.html#toolsSnapShot__description"" rel=""nofollow noreferrer"">more detailed description in the DataStax documentation</a>.</p>

<p>Update after addition of more questions:</p>

<ol>
<li>Presence of schema in snapshot makes life easier - quite often the schema evolve, and you can use non-snapshot schema if you guarantee that schema will match to data in snapshot;</li>
<li><code>nodetool snapshot</code> generates only table's schemas</li>
<li>It's better not to mess-up with system tables...</li>
</ol>

<p>Here is <a href=""https://support.datastax.com/hc/en-us/articles/115001593706-Manual-Backup-and-Restore-with-Point-in-time-and-table-level-restore-"" rel=""nofollow noreferrer"">detailed knowledge base article</a> from DataStax support about backup/restore.</p>
",['table']
51702419,51706543,2018-08-06 07:19:23,Cassandra CQLEngine Allow Filtering,"<p>I'm using Python Cassandra Cqlengine extension. I create many-to-many table but I receive error in user_applications model query filtering process. I'm readed different resource  for this problem, but I did not fully understand this problem. </p>

<p>Sources: 
<a href=""https://ohioedge.com/2017/07/05/cassandra-primary-key-partitioning-key-clustering-key-a-simple-explanation/"" rel=""nofollow noreferrer"">https://ohioedge.com/2017/07/05/cassandra-primary-key-partitioning-key-clustering-key-a-simple-explanation/</a></p>

<p><a href=""https://stackoverflow.com/questions/42576201/cassandra-allow-filtering"">Cassandra Allow filtering</a></p>

<p><a href=""https://stackoverflow.com/questions/41364262/is-allow-filtering-in-cassandra-for-following-query-efficient"">Is ALLOW FILTERING in Cassandra for following query efficient?</a></p>

<p>Database Model:</p>

<pre><code>class UserApplications(BaseModel):
    __table_name__ = ""user_applications""

    user_id = columns.UUID(required=True, primary_key=True, index=True)
    application_id = columns.UUID(required=True, primary_key=True, index=True)
    membership_id = columns.UUID(required=True, primary_key=True, index=True)
</code></pre>

<p>Error Message:</p>

<blockquote>
  <p>Cannot execute this query as it might involve data filtering and thus may have unpredictable performance. If you want to execute this query despite the performance unpredictability, use ALLOW FILTERING""</p>
</blockquote>

<p>Python CQLEngine Code:</p>

<pre><code>q = UserApplications.filter(membership_id=r.membership_id,
                                    user_id=r.user_id,
                                    application_id=r.application_id)
</code></pre>

<p>CQLEngine SQL Statements:</p>

<pre><code>SELECT ""id"", ""status"", ""created_date"", ""update_date"" FROM db.user_applications WHERE ""membership_id"" = %(0)s AND ""user_id"" = %(1)s AND ""application_id"" = %(2)s LIMIT 10000
</code></pre>

<p>Describe Table Result:</p>

<pre><code>CREATE TABLE db.user_applications (
    id uuid,
    user_id uuid,
    application_id uuid,
    membership_id uuid,
    created_date timestamp,
    status int,
    update_date timestamp,
    PRIMARY KEY (id, user_id, application_id, membership_id)
) WITH CLUSTERING ORDER BY (user_id ASC, application_id ASC, membership_id ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';
CREATE INDEX user_applications_membership_id_idx ON db.user_applications (membership_id);
</code></pre>

<p>Waiting your helps.</p>
",<database><cassandra><cql><cassandra-3.0><cqlengine>,"<p>The reason you are getting this error is that you are not adding <code>ALLOW FILTERING</code> flag to your query, if you add <code>ALLOW FILTERING</code> to the end of your query it should work.</p>

<p>Using <code>ALLOW FILTERING</code> in Cassandra queries actually allows cassandra to filter out some rows after loading them (maybe after it loads all rows from a table). For example in the case of your query the only way Cassandra can execute this query is by retrieving all the rows from the table UserApplications and then by filtering out the ones which do not have the requested value for the each of the columns your are restricting.</p>

<p>Using <code>ALLOW FILTERING</code> can have unpredictable performance outcomes and the actual performance depends on data distribution inside your table. If your table contains for example a 1 million rows and 95% of them have the requested value for the columns your are specifying the query will still be relatively efficient and you should use ALLOW FILTERING. On the other hand, if your table contains 1 million rows and only 2 rows contain the requested values , your query is extremely inefficient. Cassandra will load 999, 998 rows for nothing. In general if your queries require adding <code>ALLOW FILTERING</code> then probably you should rethink about your schema or add secondary indexes for the columns you are querying often.</p>

<p>In your case I suggest making columns membership_id, user_id, application_id as a composite partition key. If you do so you will no longer need to filter out any rows after loading because all rows having the same values for the three column will reside on the same partition (in the same physical node), and you should provide the three values in the query (you are already doing so in the query you added in the question). Here is the way you can do so:</p>

<pre><code>CREATE TABLE db.user_applications (
    user_id uuid,
    application_id uuid,
    membership_id uuid,
    created_date timestamp,
    status int,
    update_date timestamp,
    PRIMARY KEY ((user_id, application_id, membership_id))
);
</code></pre>
",['table']
51744943,51747769,2018-08-08 10:56:12,Is there a way to effectively count rows of a very huge partition in Cassandra?,"<p>I have very huge Cassandra table containing over 1 billion records. My primary key forms like this: ""<code>(partition_id, cluster_id1, cluster_id2)</code>"". Now for several particular partition_id, I have too many records that I can't run row count on these partition keys without timeout exception raised.</p>

<p>What I ran in cqlsh is:</p>

<p><code>SELECT count(*) FROM relation WHERE partition_id='some_huge_partition';</code></p>

<p>I got this exception: </p>

<blockquote>
  <p>ReadTimeout: Error from server: code=1200 [Coordinator node timed out waiting for replica nodes' responses] message=""Operation timed out - received only 0 responses."" info={'received_responses': 0, 'required_responses': 1, 'consistency': 'ONE'}</p>
</blockquote>

<p>I tried to set <code>--connect-timeout</code> and <code>--request-timeout</code>, no luck. I counted same data in ElasticSearch, the row count is approximately 30 million (the same partition). </p>

<p>My Cassandra is 3.11.2 and CQLSH is 5.0.1. The Cassandra cluster contains 3 nodes and each has more 1T HDD(fairly old servers, more than 8 years).</p>

<p>So in short, my questions are:</p>

<ol>
<li>How can I count it? is it even possible to count a huge partition in Cassandra?</li>
<li>Can I use COPY TO command with partition key as it's filter, so I can count it in the exported CSV file?</li>
<li>Is there a way I can monitor the insert process before any partition getting too huge?</li>
</ol>

<p>Big thanks advanced. </p>
",<cassandra><cqlsh>,"<p>Yes, working with large partitions is difficult with Cassandra.  There really isn't a good way to monitor particular partition sizes, although Cassandra will warn about writing large partitions in your <code>system.log</code>.  Unbound partition growth is something you need to address during the creation of your table, and it involves adding an additional (usually time based) partition key derived from understanding your business use case.</p>

<p>The answer here, is that you <em>may</em> be able to export the data in the partition using the <code>COPY</code> command.  To keep it from timing out, you'll want to use the <code>PAGESIZE</code> and <code>PAGETIMEOUT</code> options, kind of like this:</p>

<pre><code>COPY products TO '/home/aploetz/products.txt'
  WITH DELIMITER='|' AND HEADER=true
  AND PAGETIMEOUT=40 AND PAGESIZE=20;
</code></pre>

<p>That will export the <code>products</code> table to a pipe-delimited file, with a header, at a page size of 20 rows at a time and with a 40 second timeout for each page fetch.</p>

<p>If you still get timeouts, try decreasing <code>PAGESIZE</code> and/or increasing <code>PAGETIMEOUT</code>.</p>
",['table']
51768737,51824588,2018-08-09 13:45:26,Can a commitlog created several hours ago apply to a cassandra cluster?,"<p>I have a cassandra cluster running in the kubernetes environment, in a namespace, say test1, and I want to test the restore function. So I did a snapshot in the test1 cassandra, moved the snapshot to another node, from these data started a cassandra cluster in another namespace, say test2. The problem was, test2 cassandra cluster replaced test1 cluster totally, Customer's data that should write to the test1 cassandra cluster had written to the test2 cassandra cluster. </p>

<p>An hour later, I noticed this, stopped test2 cassandra cluster, and restarted test1 cassandra cluster, although it has come back to work shortly, but some data was lost. </p>

<p>After a while, I noticed there was some commitlog at that period in the test2 cassandra node, and want to recover these data. Can I just stop the test1 cassandra cluster, put these commitlog files into the commitlog directory of test1 cassandra node, then start cassandra, let cassandra to replay these commitlog ?</p>
",<cassandra>,"<blockquote>
  <p>Commitlogs from one node can’t be played on a different node or
  cluster. They are transactions specific to the node they came from.</p>
</blockquote>

<p><a href=""https://support.datastax.com/hc/en-us/articles/115001593706-Manual-Backup-and-Restore-with-Point-in-time-and-table-level-restore-"" rel=""nofollow noreferrer"">source</a> - read the notes (""Important note: A point-in-time restore requires a cluster restart for the commitlog replay to run"" and ""Some Helpful Notes for Planning"")</p>

<p>Later update:</p>

<p>I'm not sure what you mean by ""test2 cassandra cluster replaced test1 cluster totally"". My assumption is that you restored everything, including system keyspaces.</p>

<p>If you did this, yes, then applying the commit logs might work since besides the IP's and the hostnames, the cluster is sort of the same.</p>

<p>If you look into the CommitLogReader code, you will see that a mutation is considered invalid if an <a href=""https://github.com/apache/cassandra/blob/4864ccf61811f0469dfc77fc83c5313bb77213ee/src/java/org/apache/cassandra/db/commitlog/CommitLogReader.java#L437"" rel=""nofollow noreferrer"">UnknownTableException</a> is thrown (basically if the id of the table is not the same between the commit log and the system keyspace).</p>

<p>I did a similar test on ccm and successfully replayed the commit logs after I changed the id of the table both on file system and in system_schema.tables. </p>

<p>From my perspective your cluster is pretty messed up. Although you could do this and it might work, I think you will always have a high risk of corrupt data.</p>

<p>So, since in the datastax documentation (which we could consider the base documentation for Cassandra) is stated that this operation is supported I am not recommending this.</p>
",['table']
51775404,51782007,2018-08-09 20:36:42,Cassandra is missing data when loading a csv with cassandra-loader,"<p>I use Cassandra 3.11.3 with two nodes on Ubuntu 16.04.
The keyspace and table I will use here are:</p>

<pre><code>## Create a keyspace
CREATE KEYSPACE sto
WITH REPLICATION = { 
'class' : 'SimpleStrategy', 
'replication_factor' : 1 
} ;
## Create a table
CREATE TABLE sto.cartespuce_numligne_date (
numcarteserie text,
codetypetitre int,
typetransaction int,
heuretransaction float,
numservice int,
numligne text,
direction text,
heureligne float,
numjour text,
numarret text,
numbus int,
date date,
PRIMARY KEY (numligne, date) 
) WITH CLUSTERING ORDER BY (date DESC);
</code></pre>

<p>I upload a small dataset of 50,000 rows to this table</p>

<pre><code>numligne,date,codetypetitre,direction,heureligne,heuretransaction,numarret,numbus,numcarteserie,numjour,numservice,typetransaction
33,2017-12-07,144,Nord,13.88,15.27,2190,808,1229320749340288,1,268,2
749,2017-12-08,144,Nord,6.93,7.35,1459,507,1229320749340288,1,548,1
</code></pre>

<p>using cassandra-loader
    <a href=""https://github.com/brianmhess/cassandra-loader"" rel=""nofollow noreferrer"">https://github.com/brianmhess/cassandra-loader</a></p>

<p>I could use the CQL copy, but this is a preliminary test for further loadings where I will need cassandra-loader.</p>

<p>I load the csv file data.csv:</p>

<pre><code>cassandra-loader -f data.csv -host my-ip-address -schema ""sto.cartespuce_numligne_date(numligne,date,codetypetitre,direction,heureligne,heuretransaction,numarret,numbus,numcarteserie,numjour,numservice,typetransaction)""
</code></pre>

<p>The processing runs smoothly, it ends with the following log:</p>

<pre><code>*** DONE: data.csv  number of lines processed: 50000 (50000 inserted)
</code></pre>

<p>But when I count the rows with CQL:</p>

<pre><code>cqlsh&gt; SELECT COUNT(*) FROM sto.cartespuce_numligne_date;

count
-------
9877
</code></pre>

<p>comparing particular cases, it is clear that data is missing in the database. I see no difference between the data stored and the data missed.</p>

<p>How can I loose 80% of my data?</p>
",<cassandra>,"<p>The primary key of your table is numligne, date.</p>

<p>Since the data in your csv file is not unique according to the same primary key, even if you do inserts, cassandra just updates those entries.</p>

<p>To give you an example if at line 43 you have the combination 33,2017-12-07,...this will be inserted. If at line 2000 you have the same combination, when this insert will be run, Cassandra will actually do an update, since that key is already in the database.</p>

<p>Both INSERT and UPDATE operations are <a href=""https://docs.datastax.com/en/glossary/doc/glossary/gloss_upsert.html"" rel=""nofollow noreferrer"">upsert</a> operation. Some further reading about <a href=""https://docs.datastax.com/en/cql/3.3/cql/cql_reference/cqlInsert.html"" rel=""nofollow noreferrer"">INSERT</a> and <a href=""https://docs.datastax.com/en/cql/3.3/cql/cql_reference/cqlUpdate.html"" rel=""nofollow noreferrer"">UPDATE</a> commands.</p>

<p>In order to avoid this you could define another primary key so each line would have a unique key or you could write your own loader that would insert using IF NOT EXISTS so it inserts rows only if they don't exist (see the link for INSERT command, paragraph Inserting a row only if it does not already exist).</p>

<p>Cassandra provides its own <a href=""https://docs.datastax.com/en/cql/3.3/cql/cql_reference/cqlshCopy.html"" rel=""nofollow noreferrer"">COPY</a> command, but</p>

<blockquote>
  <p>The process verifies the PRIMARY KEY and updates existing records.</p>
</blockquote>

<p>After checking the code of the tool that you are using, I can see that the <a href=""https://github.com/brianmhess/cassandra-loader/blob/2ccfc0d25566ab57ea28e2c7bfa724c33b44e11b/src/main/java/com/datastax/loader/CqlDelimParser.java#L313"" rel=""nofollow noreferrer"">INSERT command being used</a> there is not using IF NOT EXISTS so it will also update if the key already exists.</p>
",['table']
51783314,51783711,2018-08-10 09:28:10,"Cassandra: If TRUNCATE table and restore backup for only one node, will I loose data?","<p>Suppose I have [3 nodes - 1 datacenter - 1 cluster] cassandra setup.</p>

<p>A keysapce with replication factor = 2</p>

<p>I am taking regular snapshots and incremental backups for all nodes.</p>

<p>One of my 3 node goes completely down with whatever reason and I want to restore backup. </p>

<p><a href=""https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/operations/opsBackupSnapshotRestore.html"" rel=""nofollow noreferrer"">Cassandra(datastax) documentation</a> suggests to usually TRUNCATE table before restoring.</p>

<p>Question:
As I am only going to restore backup on one node, is TRUNCATE necessary? Because truncate will delete that table's data from ALL nodes as per my understanding. <a href=""https://docs.datastax.com/en/dse/5.1/cql/cql/cql_reference/cql_commands/cqlTruncate.html"" rel=""nofollow noreferrer"">TRUNCATE Doc</a></p>

<p>So if I truncate table and restore backup only on one node, then wouldn't I loose data for that table which was stored on other nodes too?</p>
",<cassandra>,"<p>First of all, <strong>in your scenario</strong>, you might not want to restore a backup at all. Since you have replication factor = 2, your data is still on one other node of the original three. Therefore, you could remove the node that went completely down and add it again. Cassandra will automatically get it up to speed and stream the data to it.</p>

<p>Alternatively or complementary, you can stream the data files from the backup into your cluster with SSTableLoader.</p>

<p><strong>A few other points though for the sake of knowledge:</strong></p>

<ul>
<li>Truncate removes the data in the table on all nodes.</li>
<li>If you truncate the table and then restore on only one node, you will lose data with replication factor = 2 and three nodes.</li>
<li>In your case, Truncate is not necessary</li>
</ul>

<p><strong>Why Truncate?</strong></p>

<p>Truncate is recommended in certain scenarios because the data you restore will have older timestamps than the new data.</p>

<p>The example in the link you sent is rather apt to explain one of those scenarios.
If you accidently delete a lot of data and wish to restore your old data, you will need to remove the tombstones which mark those rows as deleted by truncating first.</p>
",['table']
51793195,51800521,2018-08-10 19:45:46,Data Partitioning and Replication on Cassandra cluster,"<p>I have a 3 node Cassandra cluster with RF=3. Now when I do <code>nodetool status</code> I get the <strong>owns</strong> for each node in the cluster as 100%.</p>

<p>But when I have 5 nodes in the cluster wit RF=3. The <strong>owns</strong> is 60%(approx as shown in image below).</p>

<p>Now as per my understanding the partitioner will calculate the hash corresponding to first replica node and the data will also be replicated as per the RF on the other nodes. 
Now we have a 5 node cluster and RF is 3. </p>

<p>Shouldn't 3 nodes be owning all the data evenly(100%) as partitioner will point to one node as per the partitoning strategy and then same data be replicated to remaining nodes which equals RF-1? It's like the data is getting evenly distributed among all the nodes(5) even though RF is 3.</p>

<p><a href=""https://i.stack.imgur.com/BU4K5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BU4K5.png"" alt=""enter image description here""></a></p>

<p><strong>Edit1</strong>:</p>

<p>As per my understanding the reason for 60%(approx) <strong>owns</strong> for each node is because the RF is 3. It means there will be 3 replicas for each row. It means there will be 300% data. Now there are 5 nodes in the cluster and partitioner will be using the default random hashing algorithm which will distribute the data evenly across all the nodes in the cluster.</p>

<p><strong>But now the issue is that we checked all the nodes of our cluster and all the nodes contain all the data even though the RF is 3.</strong> </p>

<p><strong>Edit2</strong>:</p>

<p>@Aaron I did as specified in the comment. I created a new cluster with 3 nodes.</p>

<p><a href=""https://i.stack.imgur.com/GwMbq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GwMbq.png"" alt=""enter image description here""></a> </p>

<p>I created a Keyspace ""test"" and set the class to simplestrategy and RF to 2.</p>

<p><a href=""https://i.stack.imgur.com/ytYBl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ytYBl.png"" alt=""enter image description here""></a></p>

<p>Then I created a table ""emp"" having partition key (id,name).</p>

<p><a href=""https://i.stack.imgur.com/yt6cv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yt6cv.png"" alt=""enter image description here""></a></p>

<p>Now I inserted a single row into the first node.</p>

<p>As per your explanation, It should only be in 2 nodes as RF=2.</p>

<p>But when I logged into all the 3 nodes, i could see the row replicated in all the nodes.</p>

<p>I think since the keyspace is getting replicated in all the nodes therefore, the data is also getting replicated.</p>
",<database><cassandra><nosql><cassandra-3.0>,"<p>Percent ownership is not affected (at all) by actual data being present.  You could add a new node to a single node cluster (RF=1) and it would <em>instantly</em> say 50% on each.</p>

<p>Percent ownership is purely about the percentage of token ranges which a node is responsible for.  When a node is added, the token ranges are recalculated, but data doesn't actually move until a streaming event happens.  Likewise, data isn't actually removed from its original node until <code>cleanup</code>.</p>

<p>For example, if you have a 3 node cluster with a RF of 3, each node will be at 100%.  Add one node (with RF=3), and percent ownership drops to about 75%.  Add a 5th node (again, keep RF=3) and ownership for each node correctly drops to about 3/5, or 60%.  Again, with a RF of 3 it's all about each node being responsible for a set of primary, secondary, and tertiary token ranges.</p>

<blockquote>
  <p>the default random hashing algorithm which will distribute the data evenly across all the nodes in the cluster.</p>
</blockquote>

<p>Actually, the distributed hash with Murmur3 partitioner will evenly distribute the token ranges, <strong><em>not</em></strong> the data.  That's an important distinction.  If you wrote all of your data to a single partition, I guarantee that you would <em>not</em> get even distribution of data.</p>
",['partitioner']
51824869,51828463,2018-08-13 14:37:39,Multi-Keyspace with Cassandra's Java Driver on fly,"<p>Currently, We use Cassandra's Java driver to configure session based on the attributes defined in properties file which looks like this:</p>

<pre><code>Cluster cluster = Cluster.builder()
            .addContactPoints(contactPoints)
            .withPort(environment.getRequiredProperty(""cassandra.port"", Integer.class))
            .withCredentials(username, password)
            .build();

Session session = cluster.connect(environment.getRequiredProperty(""cassandra.keyspace""));
</code></pre>

<p>What we want to achieve now is to use multi-keyspace on fly, if there is a way to do it, being able to detect the cassandra connection for a given request without changing much on the existing logic except the configuration. Any suggestion or help on this would be a great way to start</p>
",<java><cassandra><datastax-enterprise><datastax-java-driver>,"<p>The best way to achieve this is to specify table as <code>keyspace.table</code> in all queries, and don't rely on the keyspace set in the session.  You can of course have multiple session objects - one per keyspace, but it's not recommended, as every session is quite heavyweight, and open at least 2 connections to every node in the cluster.</p>

<p>Recent versions of the Java driver support setting the keyspace via <a href=""https://docs.datastax.com/en/drivers/java/3.5/"" rel=""nofollow noreferrer""><code>setKeyspace</code> function of the <code>SimpleStatement</code> class</a>, but it requires support on the Cassandra side as well (don't remember which version although, at least DSE 6.0 supports this).</p>

<p>(Update): The setting keyspace on query level isn't merged into OSS Java driver, and <a href=""https://docs.datastax.com/en/drivers/java-dse/1.6/com/datastax/driver/core/SimpleStatement.html#setKeyspace-java.lang.String-"" rel=""nofollow noreferrer"">available only in DSE Java driver</a>.</p>
",['table']
51832406,51835229,2018-08-14 01:05:15,update cassandra from spark,"<p>I'm a table in cassandra <code>tfm.foehis</code> that have data.</p>

<p>When i did the first charge of data from spark to cassandra, I used this set of commands:</p>

<pre><code>import org.apache.spark.sql.functions._
import com.datastax.spark.connector._
import org.apache.spark.sql.cassandra._

val wkdir=""/home/adminbigdata/tablas/""
val fileIn= ""originales/22_FOEHIS2.csv""
val fileOut= ""22_FOEHIS_PRE2""
val fileCQL= ""22_FOEHISCQL""

val data = sc.textFile(wkdir + fileIn).filter(!_.contains(""----"")).map(_.trim.replaceAll("" +"", """")).map(_.dropRight(1)).map(_.drop(1)).map(_.replaceAll("","", """")).filter(array =&gt; array(6) != ""MOBIDI"").filter(array =&gt; array(17) != """").saveAsTextFile(wkdir + fileOut)
val firstDF = spark.read.format(""csv"").option(""header"", ""true"").option(""inferSchema"", ""true"").option(""mode"", ""DROPMALFORMED"").option(""delimiter"", ""|"").load(wkdir + fileOut)
val columns: Array[String] = firstDF.columns
val reorderedColumnNames: Array[String] = Array(""hoclic"",""hodtac"",""hohrac"",""hotpac"",""honrac"",""hocdan"",""hocdrs"",""hocdsl"",""hocol"",""hocpny"",""hodesf"",""hodtcl"",""hodtcm"",""hodtea"",""hodtra"",""hodtrc"",""hodtto"",""hodtua"",""hohrcl"",""hohrcm"",""hohrea"",""hohrra"",""hohrrc"",""hohrua"",""holinh"",""holinr"",""honumr"",""hoobs"",""hooe"",""hotdsc"",""hotour"",""housca"",""houscl"",""houscm"",""housea"",""houser"",""housra"",""housrc"")
val secondDF= firstDF.select(reorderedColumnNames.head, reorderedColumnNames.tail: _*)
secondDF.write.cassandraFormat(""foehis"", ""tfm"").save()
</code></pre>

<p>But when I load new data using the same script, I get errors. I don't know what's wrong?
This is the message:</p>

<pre><code>java.lang.UnsupportedOperationException: 'SaveMode is set to ErrorIfExists and Table
tfm.foehis already exists and contains data.
Perhaps you meant to set the DataFrame write mode to Append?
Example: df.write.format.options.mode(SaveMode.Append).save()"" '
</code></pre>
",<apache-spark><cassandra><spark-cassandra-connector>,"<p>The error message clearly says you that you need to use Append mode &amp; shows what you can do with it.  In your case it happens because destination table already exists, and writing mode is set to ""error if exists"".  If you still want to write data, the code should be following:</p>

<pre><code>import org.apache.spark.sql.SaveMode
secondDF.write.cassandraFormat(""foehis"", ""tfm"").mode(SaveMode.Append).save()
</code></pre>
",['table']
51929246,51930846,2018-08-20 10:57:20,spark sql to transfer data between Cassandra tables,"<p>Please find the Cassandra table below.</p>

<p>I am trying to copy data from 1 Cassandra table to another Cassandra table with same structure.</p>

<p>Please help me.    </p>

<pre><code>CREATE TABLE data2 (
        d_no text,
        d_type text,
        sn_perc int,
        tse_dt timestamp,
        f_lvl text,
        ign_f boolean,
        lk_loc text,
        lk_ts timestamp,
        mi_rem text,
        nr_fst text,
        perm_stat text,
        rec_crt_dt timestamp,
        sr_stat text,
        sor_query text,
        tp_dat text,
        tp_ts timestamp,
        tr_rem text,
        tr_type text,
        PRIMARY KEY (device_serial_no, device_type)
    ) WITH CLUSTERING ORDER BY (device_type ASC)
</code></pre>

<p>Data inserted using:</p>

<pre><code>Insert into data2(all column names) values('64FCFCFC','HUM',4,'1970-01-02 05:30:00’ ,’NA’,true,'NA','1970-01-02 05:40:00',’NA’,'NA','NA','1970-02-01 05:30:00','NA','NA','NA','1970-02-03 05:30:00','NA','NA');
</code></pre>

<p>Note:
The 4th column timestamp when i try to insert like this  '1970-01-02 05:30:00’ ,and in dtaframe also timestamp inserted correctly ,but when insert from dataframe to cassandra and  use select * from table, i see its being inserted like 1970-01-02 00:00:00.000000+0000</p>

<h2>similarly for all time stamp columns its happening .</h2>

<p>pom.xml</p>

<pre><code>&lt;dependencies&gt;
       &lt;dependency&gt;
    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;
    &lt;version&gt;2.3.0&lt;/version&gt;
&lt;/dependency&gt;
&lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-sql --&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt;
    &lt;version&gt;2.3.1&lt;/version&gt;
&lt;/dependency&gt;
&lt;!-- https://mvnrepository.com/artifact/com.datastax.spark/spark-cassandra-connector --&gt;
&lt;dependency&gt;
    &lt;groupId&gt;com.datastax.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-cassandra-connector_2.11&lt;/artifactId&gt;
    &lt;version&gt;2.3.1&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<hr>

<p>I want to read these values and write it into another Cassandra table  using spark Scala. See code below:</p>

<pre><code>val df2 = spark.read
                       .format(""org.apache.spark.sql.cassandra"")
                       .option(""spark.cassandra.connection.host"",""hostname"")
                       .option(""spark.cassandra.connection.port"",""9042"")
                       .option( ""spark.cassandra.auth.username"",""usr"")
                       .option(""spark.cassandra.auth.password"",""pas"")
                       .option(""keyspace"",""hr"")
                       .option(""table"",""data2"")
                       .load()
Val df3 =doing some processing on df2.
df3.write
         .format(""org.apache.spark.sql.cassandra"")
         .mode(""append"")
         .option(""spark.cassandra.connection.host"",""hostname"")
         .option(""spark.cassandra.connection.port"",""9042"")
         .option( ""spark.cassandra.auth.username"",""usr"")
         .option(""spark.cassandra.auth.password"",""pas"")
         .option(""spark.cassandra.output.ignoreNulls"",""true"")
         .option(""confirm.truncate"",""true"")
         .option(""keyspace"",""hr"")
         .option(""table"",""data3"")
         .save()
</code></pre>

<p>But i am getting below error, when i try to insert data using above code,</p>

<pre><code>java.lang.IllegalArgumentException: requirement failed: Invalid row size: 18 instead of 17.
    at scala.Predef$.require(Predef.scala:224)
    at com.datastax.spark.connector.writer.SqlRowWriter.readColumnValues(SqlRowWriter.scala:23)
    at com.datastax.spark.connector.writer.SqlRowWriter.readColumnValues(SqlRowWriter.scala:12)
    at com.datastax.spark.connector.writer.BoundStatementBuilder.bind(BoundStatementBuilder.scala:99)
    at com.datastax.spark.connector.writer.GroupingBatchBuilder.next(GroupingBatchBuilder.scala:106)
    at com.datastax.spark.connector.writer.GroupingBatchBuilder.next(GroupingBatchBuilder.scala:31)
    at scala.collection.Iterator$class.foreach(Iterator.scala:891)
    at com.datastax.spark.connector.writer.GroupingBatchBuilder.foreach(GroupingBatchBuilder.scala:31)
    at com.datastax.spark.connector.writer.TableWriter$$anonfun$writeInternal$1.apply(TableWriter.scala:233)
    at com.datastax.spark.connector.writer.TableWriter$$anonfun$writeInternal$1.apply(TableWriter.scala:210)
    at com.datastax.spark.connector.cql.CassandraConnector$$anonfun$withSessionDo$1.apply(CassandraConnector.scala:112)
    at com.datastax.spark.connector.cql.CassandraConnector$$anonfun$withSessionDo$1.apply(CassandraConnector.scala:111)
    at com.datastax.spark.connector.cql.CassandraConnector.closeResourceAfterUse(CassandraConnector.scala:145)
    at com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:111)
    at com.datastax.spark.connector.writer.TableWriter.writeInternal(TableWriter.scala:210)
    at com.datastax.spark.connector.writer.TableWriter.insert(TableWriter.scala:197)
    at com.datastax.spark.connector.writer.TableWriter.write(TableWriter.scala:183)
    at com.datastax.spark.connector.RDDFunctions$$anonfun$saveToCassandra$1.apply(RDDFunctions.scala:36)
    at com.datastax.spark.connector.RDDFunctions$$anonfun$saveToCassandra$1.apply(RDDFunctions.scala:36)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
    at org.apache.spark.scheduler.Task.run(Task.scala:109)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
</code></pre>
",<apache-spark><cassandra><apache-spark-sql><spark-cassandra-connector>,"<p>That's a known problem (<a href=""https://datastax-oss.atlassian.net/browse/SPARKC-541"" rel=""nofollow noreferrer"">SPARKC-541</a>) - you're copying the data from table that has DSE Search enabled to the table without it. You simply need to drop this column as part of your transformations:</p>

<pre><code>val df3 = df2.drop(""solr_query"").... // your transformations
</code></pre>

<p>Or you can simply use the newer driver (2.3.1 if you're using OSS driver), or corresponding DSE release that contains this fix.</p>
",['table']
51941608,51941870,2018-08-21 04:33:36,Cassandra - How to make table for friend accept or not,"<p>I want to make table like this,</p>

<pre><code>CREATE TABLE friendInvite {
  user text,
  invitee text,
  accepted boolean
  PRIMARY(invitee, user)
}
</code></pre>

<p>and expected queries are</p>

<pre><code>1. SELECT * FROM friendInvite WHERE invitee=""me"" and accepted=false
2. UPDATE friendInvite SET accepted=true WHERE invitee=""me"" and user=""you""
</code></pre>

<p>I think the query #1 is not good at performance because of accepted condition. </p>

<p>How can i handle this on Cassandra?</p>

<p>I cannot imagine using secondary index for accepted. because it will be updated to true if invitee accepts the offer. Is it ok If i use secondary index for accepted column?</p>
",<cassandra>,"<p>I would create 2 tables.</p>

<pre><code>CREATE TABLE friendInvites {
  user text,
  invitee text,
  PRIMARY(invitee, user)
}
</code></pre>

<p>This table holds open friend requests and serves your query #1:</p>

<pre><code>1. SELECT * FROM friendInvite WHERE invitee=""me""
</code></pre>

<p>Then i would create a second table, where you store the accepted friend requests:</p>

<pre><code>CREATE TABLE acceptedRequests {
  user text,
  invitee text,
  PRIMARY(user, invitee)
}
</code></pre>

<p>When you accept a request, the entry has to be removed from <em>friendInvites</em> and inserted into <em>acceptedRequests</em> </p>
",['table']
52076770,52085945,2018-08-29 11:50:39,Cassandra Key Cache,"<p>Cassandra Key cache is a map structure where  key is {sstable_file_descriptor+partition_key} and value is partition offset, now why Cassandra during read, checks all sstables (using bloom filter), if the data may present in that stable. Why can't key cache be like partition_key=sstable_file_descriptor+offset</p>
",<cassandra>,"<p>Its actually <code>(tableid, indexName, descriptor, partition_key)</code> (KeyCacheKey extends CacheKey). The same partition key can exist on multiple tables, and on multiple sstables within them. In order to key by just the key you would need additional structure which would be quite a bit more coordination and contention.</p>

<p>The keycache does not store all data either, only things considered to be likely to get a hit based on Window TinyLfu algorithm. There are potentially billions of keys in a single table so it cannot store them all. The absence from the keycache does not ensure that it does not exist so the bloom filter must be checked anyway. Something to note too, the BF check is in memory and very fast. If the BF passes it checks the cache next. Before any of this it actually also filters based on the range of columns and tokens within an sstable, and skips ones whose data would be tombstoned by the min/max timestamps (see queryMemtableAndDiskInternal).</p>
",['table']
52102505,52102742,2018-08-30 17:26:51,Difference between PreparedStatements created from two different keyspaces,"<p>I have multiple PreparedStatements which I create during bean initialization just once with current setup of single keyspace, but I'm now trying to work with multiple keyspaces while table schema remains the same as following:</p>

<pre><code>//session1 is from connecting to keyspace1
//while session2 to keyspace2

PreparedStatements ps =  session1.prepare(sameStmt);
PreparesStatement ps1 = session2.prepare(sameStmt);
</code></pre>

<p>What I'm trying to figure out is, if there's difference between two besides being two different objects and from two different keyspaces considering the query string is same for both? The thing I want to achieve is to be able to create the PreparedStatement only once irrespective of keyspace if possible. Any suggestions?</p>
",<cassandra><datastax-enterprise><datastax-java-driver>,"<p>This is not something that Cassandra allows by design.  Even if the table schema is the same between the tables in the two keyspaces, there is no way to be sure of this.  For safety, Cassandra generates different prepared statements ids and therefore they are treated as different prepared statements.</p>
",['table']
52209466,52209467,2018-09-06 17:34:59,unconfigured table error with Capitalized table name,"<p>All my Cassandra column family names are capitalized (like <code>FunTable</code>, <code>SomeOtherTable</code>, etc.) -- I'm switching from the Thrift API to CQL, and whenever I make a query (like <code>SELECT * FROM FunTable</code>) it fails with <code>cassandra.InvalidRequest: Error from server: code=2200 [Invalid query] message=""unconfigured table funtable""</code></p>

<p>What's going on?</p>
",<cassandra><cql>,"<p>It turns out, as I should have figured out from the table name being lower-cased in the error message, that CQL is case-insensitive, except inside quoted strings (similar to other SQL dialects). So the solution is just to put double quotes around the table name, like so: <code>SELECT * FROM ""FunTable""</code></p>

<p>If you are specifying the keyspace, you need to put the quote just around the table name, not around the combination of keyspace and table name, e.g. <code>SELECT * FROM good_keyspace.""FunTable""</code></p>
",['table']
52210852,52211050,2018-09-06 19:20:41,Where is Memtable located in cassandra?,"<p>It is mentioned in DataStax that there is one memtable per column-family in Cassandra. </p>

<p>Lets' assume I have a 2 Node Cassandra cluster with RF=2. Now if one of the node fails then the other node is there from which we can retrieve the data. But it is possible that say in my cluster node1 fails then node2 handles the requests for data and after some time node1 is up and node2 fails then it is node1 which handles the request.</p>

<p>So where is memtable located in the cluster? If it is in the node then the statement that there is only one memtable per column-family is incorrect.</p>
",<cassandra><nosql><cassandra-3.0>,"<p>The memtable is an in memory data structure which can be kept on or offheap for each table on each node.</p>

<p>The memtables periodically flush to new sstables which are merged with the memtable for reads. The commitlog provides durability for the memtable until its flushed.</p>

<p>On a read its the coordinators job to merge the data from the different replicas depending on consistency level requested. If your CL covers both node1 and node2 the coordinator will resolve the missing data. When node1 or node2 go down or a mutation to them are dropped the coordinator will store the mutation in a hint to be delivered when it comes back up. If all that fails the anti entropy repairs will fix any inconsistencies when run.</p>
",['table']
52266171,52266457,2018-09-10 22:11:24,Golang gocql cannot connect to Cassandra (using Docker),"<p>I am trying to setup and connect to a Cassandra single node instance using docker and Golang and it is not working.</p>

<p>The closest information I could find to addressing connection issues between the golang <code>gocql</code> package and Cassandra is available here: <a href=""https://stackoverflow.com/questions/29121904/cassandra-cqlsh-connection-refused"">Cassandra cqlsh - connection refused</a>, however there are many different upvote answers with no clear indication of which is preferred. It is also a protected question (no ""me toos""), so a lot of community members seem to be having trouble with this.</p>

<p>This problem should be slightly different, as it is using Docker and I have tried most (if not all of the solutions linked to above). </p>

<pre><code>version: ""3""  
services:  
  cassandra00:
    restart: always
    image: cassandra:latest
    volumes: 
      - ./db/casdata:/var/lib/cassandra
    ports: 
      - 7000:7000
      - 7001:7001
      - 7199:7199
      - 9042:9042
      - 9160:9160
    environment:
      - CASSANDRA_RPC_ADDRESS=127.0.0.1
      - CASSANDRA_BROADCAST_ADDRESS=127.0.0.1
      - CASSANDRA_LISTEN_ADDRESS=127.0.0.1
      - CASSANDRA_START_RPC=true
  db:
    restart: always
    build: ./db
    environment:
      POSTGRES_USER: patientplatypus
      POSTGRES_PASSWORD: SUPERSECRETFAKEPASSD00T
      POSTGRES_DB: zennify
    expose:
      - ""5432""
    ports:
      - 5432:5432
    volumes:
      - ./db/pgdata:/var/lib/postgresql/data
  app:
    restart: always
    build: 
      context: .
      dockerfile: Dockerfile
    command: bash -c 'while !&lt;/dev/tcp/db/5432; do sleep 10; done; realize start --run'
    # command: bash -c 'while !&lt;/dev/tcp/db/5432; do sleep 10; done; go run main.go'
    ports:
      - 8000:8000
    depends_on:
      - db
      - cassandra00
    links:
      - db
      - cassandra00
    volumes:
      - ./:/go/src/github.com/patientplatypus/webserver/
</code></pre>

<p>Admittedly, I am a little shaky on what listening addresses I should pass to Cassandra in the environment section, so I just passed 'home':</p>

<pre><code>  - CASSANDRA_RPC_ADDRESS=127.0.0.1
  - CASSANDRA_BROADCAST_ADDRESS=127.0.0.1
  - CASSANDRA_LISTEN_ADDRESS=127.0.0.1
</code></pre>

<p>If you try and pass <code>0.0.0.0</code> you get the following error:</p>

<pre><code>cassandra00_1  | Exception (org.apache.cassandra.exceptions.ConfigurationException) encountered during startup: listen_address cannot be a wildcard address (0.0.0.0)!
cassandra00_1  | listen_address cannot be a wildcard address (0.0.0.0)!
cassandra00_1  | ERROR [main] 2018-09-10 21:50:44,530 CassandraDaemon.java:708 - Exception encountered during startup: listen_address cannot be a wildcard address (0.0.0.0)!
</code></pre>

<p>Overall, however I think that I am getting the correct start up procedure for Cassandra (afaict) because my terminal outputs that Cassandra started up as normal and is listening on the appropriate ports:</p>

<pre><code>cassandra00_1  | INFO  [main] 2018-09-10 22:06:28,920 StorageService.java:1446 - JOINING: Finish joining ring
cassandra00_1  | INFO  [main] 2018-09-10 22:06:29,179 StorageService.java:2289 - Node /127.0.0.1 state jump to NORMAL
cassandra00_1  | INFO  [main] 2018-09-10 22:06:29,607 NativeTransportService.java:70 - Netty using native Epoll event loop
cassandra00_1  | INFO  [main] 2018-09-10 22:06:29,750 Server.java:155 - Using Netty Version: [netty-buffer=netty-buffer-4.0.44.Final.452812a, netty-codec=netty-codec-4.0.44.Final.452812a, netty-codec-haproxy=netty-codec-haproxy-4.0.44.Final.452812a, netty-codec-http=netty-codec-http-4.0.44.Final.452812a, netty-codec-socks=netty-codec-socks-4.0.44.Final.452812a, netty-common=netty-common-4.0.44.Final.452812a, netty-handler=netty-handler-4.0.44.Final.452812a, netty-tcnative=netty-tcnative-1.1.33.Fork26.142ecbb, netty-transport=netty-transport-4.0.44.Final.452812a, netty-transport-native-epoll=netty-transport-native-epoll-4.0.44.Final.452812a, netty-transport-rxtx=netty-transport-rxtx-4.0.44.Final.452812a, netty-transport-sctp=netty-transport-sctp-4.0.44.Final.452812a, netty-transport-udt=netty-transport-udt-4.0.44.Final.452812a]
cassandra00_1  | INFO  [main] 2018-09-10 22:06:29,754 Server.java:156 - Starting listening for CQL clients on /127.0.0.1:9042 (unencrypted)...
cassandra00_1  | INFO  [main] 2018-09-10 22:06:29,990 ThriftServer.java:116 - Binding thrift service to /127.0.0.1:9160
</code></pre>

<p>In my golang code I have the following package that is being called (simplified to show relevant section):</p>

<pre><code>package data

import(
    ""fmt""
    ""github.com/gocql/gocql""
)

func create_userinfo_table() {
    &lt;...&gt;
    fmt.Println(""replicating table in cassandra"")
    cluster := gocql.NewCluster(""localhost"") //&lt;---error here!
    cluster.ProtoVersion = 4
    &lt;...&gt;
}
</code></pre>

<p>Which results in the following error in my terminal:</p>

<pre><code>app_1          | [21:52:38][WEBSERVER] : 2018/09/10 
21:52:38 gocql: unable to dial control conn 127.0.0.1: 
dial tcp 127.0.0.1:9042: connect: connection refused

app_1          | [21:52:38][WEBSERVER] : 2018/09/10 
21:52:38 gocql: unable to dial control conn ::1: 
dial tcp [::1]:9042: connect: cannot assign requested address

app_1          | [21:52:38][WEBSERVER] : 2018/09/10 
21:52:38 Could not connect to cassandra cluster: gocql: 
unable to create session: control: unable to connect to initial hosts: 
dial tcp [::1]:9042: connect: cannot assign requested address
</code></pre>

<p>I have tried several variations on the connection address </p>

<p><code>cluster := gocql.NewCluster(""localhost"")</code></p>

<p><code>cluster := gocql.NewCluster(""127.0.0.1"")</code></p>

<p><code>cluster := gocql.NewCluster(""127.0.0.1:9042"")</code></p>

<p><code>cluster := gocql.NewCluster(""127.0.0.1:9160"")</code></p>

<p>These seemed likely candidates for example, but no luck.</p>

<p>Does anyone have any idea what I am doing wrong?</p>
",<database><docker><go><cassandra><docker-compose>,"<p>Use the service name <code>cassandra00</code> for the hostname per the docker-compose documentation <a href=""https://docs.docker.com/compose/compose-file/#links"" rel=""nofollow noreferrer"">https://docs.docker.com/compose/compose-file/#links</a></p>

<blockquote>
  <p>Containers for the linked service are reachable at a hostname identical to the alias, or the service name if no alias was specified.</p>
</blockquote>

<p>Leave the <code>CASSANDRA_LISTEN_ADDRESS</code> envvar unset (or pass <code>auto</code>) per <a href=""https://docs.docker.com/samples/library/cassandra/"" rel=""nofollow noreferrer"">https://docs.docker.com/samples/library/cassandra/</a> </p>

<blockquote>
  <p>The default value is auto, which will set the listen_address option in cassandra.yaml to the IP address of the container as it starts. This default should work in most use cases.</p>
</blockquote>
",['listen_address']
52282517,52594150,2018-09-11 18:54:53,How default_time_to_live would delete rows without tombstones in Cassandra?,"<p>From <a href=""https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlAboutDeletes.html"" rel=""noreferrer"">How is data deleted?</a></p>

<blockquote>
  <p>Cassandra allows you to set a default_time_to_live property for an entire table. Columns and rows marked with regular TTLs are processed as described above; but when a record exceeds the table-level TTL, <strong>Cassandra deletes it immediately, without tombstoning or compaction</strong>.</p>
</blockquote>

<p>This is also answered <a href=""https://stackoverflow.com/a/50060436/3517383"">here</a></p>

<blockquote>
  <p>If a table has default_time_to_live on it then rows that exceed this time limit are <strong>deleted immediately without tombstones being written</strong>. </p>
</blockquote>

<p>And commented in LastPickle's post <a href=""http://thelastpickle.com/blog/2016/07/27/about-deletes-and-tombstones.html#comment-3949581514"" rel=""noreferrer"">About deletes and tombstones</a></p>

<blockquote>
  <p>Another clue to explore would be to use the TTL as a default value if that's a good fit. TTLs set at the table level with 'default_time_to_live' <strong>should not generate any tombstone at all in C*3.0+</strong>. Not tested on my hand, but I read about this.</p>
</blockquote>

<p>I've made the simplest test that I could imagine using <code>LeveledCompactionStrategy</code>:</p>

<pre><code>CREATE KEYSPACE IF NOT EXISTS temp WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'};

CREATE TABLE IF NOT EXISTS temp.test_ttl (
    key text,
    value text,
    PRIMARY KEY (key)
) WITH  compaction = { 'class': 'LeveledCompactionStrategy'}
  AND default_time_to_live = 180;
</code></pre>

<ol>
<li><code>INSERT INTO temp.test_ttl (key,value) VALUES ('k1','v1');</code></li>
<li><code>nodetool flush temp</code></li>
<li><code>sstabledump mc-1-big-Data.db</code>
<a href=""https://i.stack.imgur.com/Lqxqf.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Lqxqf.png"" alt=""enter image description here""></a></li>
<li>wait for 180 seconds (default_time_to_live)</li>
<li><code>sstabledump mc-1-big-Data.db</code>
<a href=""https://i.stack.imgur.com/sLUQU.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/sLUQU.png"" alt=""enter image description here""></a>
The tombstone isn't created yet</li>
<li><code>nodetool compact temp</code></li>
<li><code>sstabledump mc-2-big-Data.db</code>
<a href=""https://i.stack.imgur.com/IGUrN.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/IGUrN.png"" alt=""enter image description here""></a>
The <strong>tombstone is created</strong> (and not dropped on compaction due to gc_grace_seconds)</li>
</ol>

<p>The test was performed using apache cassandra 3.0.13</p>

<p>From the example I conclude that isn't true that <code>default_time_to_live</code> not require tombstones, at least for version 3.0.13.
However this is a very simple test and I'm forcing a major compaction with <code>nodetool compact</code> so I may not be recreating the scenario where default_time_to_live magic comes into play. </p>

<p>But how would C* delete without tombstones? Why this should be a different scenario to using TTL per insert?</p>
",<cassandra><cassandra-3.0>,"<p>I have been fooled by the piece of documentation you mentioned when answering this question on our blog (<a href=""http://thelastpickle.com/blog/2016/07/27/about-deletes-and-tombstones.html"" rel=""noreferrer"">The Last Pickle Blog</a>). I probably answered this one too quickly, even though I wrote this a thing 'to explore', even saying I did not try it explicitly.</p>

<blockquote>
  <p>Another clue to explore would be to use the TTL as a default value if
  that's a good fit. TTLs set at the table level with
  'default_time_to_live' <strong>should not generate any tombstone at all in
  C*3.0+</strong>. Not tested on my hand, but I read about this.</p>
</blockquote>

<p>So my sentence above is wrong. Basically, the default can be overwritten by the TTL at the query level and I do not see how Cassandra could handle this without tombstones. </p>

<blockquote>
  <p>From the example I conclude that isn't true that
  <code>default_time_to_live</code> not require tombstones, at least for version
  3.0.13.</p>
</blockquote>

<p>Also, I am glad to see you did not believe me or Datastax documentation but tried it by yourself. This is definitively the right approach.</p>

<blockquote>
  <p>But how would C* delete without tombstones? Why this should be a
  different scenario to using TTL per insert?</p>
</blockquote>

<p>Yes, exactly this, </p>

<p>C*heers.</p>

<hr>

<p>Alain Rodriguez - @arodream - alain@thelastpickle.com
France / Spain</p>

<p>The Last Pickle - Apache Cassandra Consulting
<a href=""http://www.thelastpickle.com"" rel=""noreferrer"">http://www.thelastpickle.com</a></p>
",['table']
52402907,52435581,2018-09-19 09:45:00,Cassandra 2.1 changing snitch from EC2Snitch to GossipingPropertyFileSnitch,"<p>Currently we have used EC2Snitch using two AZs in a single AWS region. The goal was to provide resiliency even when one AZ is not available. Most data are replicated with RF=2, so each AZ gets a copy based on Ec2Snitch.</p>

<p>Now we have come to a conclusion to move to GossipingPropertyFileSnitch. Reason primarily is that we have realized that one AZ going down is a remote occurrence and even if it happens, there are other systems in our stack that don't support this; so eventually whole app goes down if that happens.</p>

<p>Other reason is that with EC2Snitch and two AZs, we had to scale in factor of 2 (one in each AZ). With GossipingPropertyFileSnitch using just one rack, we can scale in factor of 1. </p>

<p>When we change this snitch setting, will the topology change? I want to avoid having a need to run nodetool repair. We always had failures with running nodetool repair and it runs forever.</p>
",<cassandra><nodetool>,"<p>Whether the topology changes depends on how you carry out the change. If you assign the same logical dc and rack to the node as what it's currently configured to, you shouldn't get a topology change.</p>

<p>You have to match the rack to the AZ after updating to <code>GossipingPropertyFileSnitch</code>. You need to do a rolling restart for the re-configuration to take place.</p>

<p>Example <code>cassandra-rackdc.properties</code> for 2 nodes in 1 dc across 2 AZs:</p>



<pre class=""lang-sh prettyprint-override""><code># node=10.0.0.1, dc=first, AZ=1
dc_suffix=first
# Becomes
dc=first
rack=1

# node=10.0.0.2, dc=first, AZ=2
dc_suffix=first
# Becomes
dc=first
rack=2
</code></pre>

<p>On a side note you need to explore why repairs are failing. Unfortunately they are very important for cluster health. </p>
","['rack', 'dc']"
52420224,52420749,2018-09-20 07:53:56,how to delete a row using non primary key in cassandra,"<p>This is my table with <code>id</code> as the primary key:</p>
<p><img src=""https://i.stack.imgur.com/WcxA4.jpg"" alt=""enter image description here"" /></p>
<p>I want to delete the row based on <code>msisdn</code> column.</p>
<p>How can I do that?</p>
<p>Please suggest</p>
",<cassandra>,"<p>The answer is: By writing a tool that first does a <code>select</code> using <code>allow filtering</code> to determine the primary key, then do the delete using the determined primary key.</p>

<p>The <em>actual</em> answer is: You don't, because what I just proposed would be terribly slow, put your cluster under a lot of stress, and generally reveal a misconception of how Cassandra data modeling works.</p>

<p>Cassandra data modeling is fundamentally different than SQL data modeling. In SQL, you have your primary keys, you normalize, and then you are pretty confident that you will be able to query and write most things with good performance. In Cassandra you model for your queries, and by queries, I mean anything that contains a <code>where</code>.</p>

<p>Assuming that you designed your table like this for a reason, what you could do is add a materialized view that adds <code>msisdn</code> to the primary key, turns it into the partitioning key, and turns <code>id</code> into a clustering column. That way, you could at least query efficiently without doing an <code>allow filtering</code>. You will still need two roundtrips, one for reading, one for deleting, but that cost may be acceptable. If it is not, you need to fundamentally redesign your table, keeping in mind that you will need to query by <code>msisdn</code>.</p>
",['table']
52430749,52431301,2018-09-20 17:58:30,Fetch the specific number of time series data from Cassandra,"<p>Obviously when dealing with time-series data which relates to some natural partition key like sensor id it can be used as a primary key. But what to do if we are interested in a global view and there is no natural candidate for the partition key? If we model the schema like this:</p>

<pre><code>CREATE TABLE my_data(
 year smallint,
 day smallint,
 date timestamp,
 value text
 PRIMARY KEY ((year, day), timestamp)
) WITH CLUSTERING ORDER BY (date DESC);
</code></pre>

<p>It is (probably) going to work just fine for most cases but given we know what year and days to fetch.</p>

<p>What if we don't care what day is it but we expect to see first 50 most recent items? What if we then want to see next 50 items? Is there a way to do it in Cassandra? What is the recommended way of doing this?</p>
",<cassandra><time-series>,"<p>Keep a 2nd table of the year/days. When reading can grab from it first. When adding to my_data update that as well but keep a cache of days inserted so each app would only try the insert once per day. ie for example adding extra key so can have multiple streams not just a single table per time series:</p>

<pre><code>CREATE TABLE my_data (
  key blob,
  year smallint,
  day smallint,
  date timestamp,
  value text
  PRIMARY KEY ((key, year, day), timestamp)
) WITH CLUSTERING ORDER BY (date DESC);

CREATE TABLE my_data_keys (
   key blob,
   year smallint,
   day smallint,
   PRIMARY KEY ((key), year, day)
)
</code></pre>

<p>For inserts:</p>

<pre><code>INSERT INTO my_data_keys (key, year, day) VALUES (0x01, 1, 2)
INSERT INTO my_data ...
</code></pre>

<p>Then keep a in memory Set somewhere that you stored that key/year/data so you dont need to insert it every time. To read most recent:</p>

<pre><code>SELECT year, day FROM my_data_keys WHERE key = 0x01; 
</code></pre>

<p>driver returns iterator, for each element in it make query to my_data until 50 records reached.</p>

<p>If inserts are frequent enough can just work backwards from ""today"", issuing queries until you get 50 events. If data sparse though that can be a lot of wasted reads and another table work better.</p>
",['table']
52528838,52533822,2018-09-27 03:08:43,How to get X% percentile in Cassandra,"<p>Consider a table with structure:</p>

<pre><code>CREATE TABLE statistics (name text, when timestamp, value int, 
PRIMARY KEY ((name, when)));
</code></pre>

<p>What is the best way to calculate, for example, 50% value percentile by name?
I thought about:</p>

<p>a) writing custom aggregate function + query like:</p>

<pre><code>SELECT PERCENTILE(value, 0.5) FROM statistics WHERE name = '...'
</code></pre>

<p>b) count elements by name first</p>

<pre><code>SELECT COUNT(value) FROM statistics WHERE name = '...'
</code></pre>

<p>then find (0.5/count)th row value with paging when it is sorted by value ascending. Say, if count is 100 it will be 50th row.</p>

<p>c) your ideas</p>

<p>I'm not sure if case A can handle the task. Case B might be tricky when there is odd number of rows.</p>
",<cassandra><cql><cassandra-3.0>,"<p>As long as you always provide <code>name</code> - this request can be very expensive without specifying partition and having everything within one. I am assuming you mean <code>((name), when)</code> not <code>((name, when))</code> in your table, otherwise what your asking is impossible without full table scans (using hadoop or spark).</p>

<p>The UDA would work - but it can be expensive unless your willing to accept an approximation. To have it perfectly accurate you need to do 2 pass (ie doing a count, than a 2nd pass to go X into set, but since no isolation this isnt gonna be perfect either). So if you need it perfectly accurate your best bet is probably to just pull entire <code>statistics[name]</code> partition locally or to have UDA build up entire set (or majority) in a map (not recommended if partitions get large at all) before calculating. ie:</p>

<pre><code>CREATE OR REPLACE FUNCTION all(state tuple&lt;double, map&lt;int, int&gt;&gt;, val int, percentile double)
  CALLED ON NULL INPUT RETURNS tuple&lt;double, map&lt;int, int&gt;&gt; LANGUAGE java AS '
java.util.Map&lt;Integer, Integer&gt; m = state.getMap(1, Integer.class, Integer.class);
m.put(m.size(), val);
state.setMap(1, m);
state.setDouble(0, percentile);
return state;';

CREATE OR REPLACE FUNCTION calcAllPercentile (state tuple&lt;double, map&lt;int, int&gt;&gt;)
  CALLED ON NULL INPUT RETURNS int LANGUAGE java AS 
  'java.util.Map&lt;Integer, Integer&gt; m = state.getMap(1, Integer.class, Integer.class);
  int offset = (int) (m.size() * state.getDouble(0));
  return m.get(offset);';

CREATE AGGREGATE IF NOT EXISTS percentile (int , double) 
  SFUNC all STYPE tuple&lt;double, map&lt;int, int&gt;&gt;
  FINALFUNC calcAllPercentile
  INITCOND (0.0, {});
</code></pre>

<p>If willing to accept an approximation you can use a sampling reservoir, say 1024 elements you store and as your UDA gets elements you replace elements in it at a decreasingly statistical chance. (<a href=""https://en.wikipedia.org/wiki/Reservoir_sampling#Algorithm_R"" rel=""nofollow noreferrer"">vitter's algorithm R</a>) This is pretty easy to implement, and IF your data set is expected to have a normal distribution will give you a decent approximation. If your data set is not a normal distribution this can be pretty far off. With a normal distribution theres actually a lot of other options as well but R is I think easiest to implement in a UDA. like:</p>

<pre><code>CREATE OR REPLACE FUNCTION reservoir (state tuple&lt;int, double, map&lt;int, int&gt;&gt;, val int, percentile double)
  CALLED ON NULL INPUT RETURNS tuple&lt;int, double, map&lt;int, int&gt;&gt; LANGUAGE java AS '
java.util.Map&lt;Integer, Integer&gt; m = state.getMap(2, Integer.class, Integer.class);
int current = state.getInt(0) + 1;
if (current &lt; 1024) {
    // fill the reservoir
    m.put(current, val);
} else {
    // replace elements with gradually decreasing probability
    int replace = (int) (java.lang.Math.random() * (current + 1));
    if (replace &lt;= 1024) {
        m.put(replace, val);
    }
}
state.setMap(2, m);
state.setDouble(1, percentile);
state.setInt(0, current);
return state;';

CREATE OR REPLACE FUNCTION calcApproxPercentile (state tuple&lt;int, double, map&lt;int, int&gt;&gt;)
  CALLED ON NULL INPUT RETURNS int LANGUAGE java AS 
  'java.util.Map&lt;Integer, Integer&gt; m = state.getMap(2, Integer.class, Integer.class);
  int offset = (int) (java.lang.Math.min(state.getInt(0), 1024) * state.getDouble(1));
  if(m.get(offset) != null)
      return m.get(offset);
  else
      return 0;';

CREATE AGGREGATE IF NOT EXISTS percentile_approx (int , double) 
  SFUNC reservoir STYPE tuple&lt;int, double, map&lt;int, int&gt;&gt;
  FINALFUNC calcApproxPercentile
  INITCOND (0, 0.0, {});
</code></pre>

<p>In above, the percentile function will get slower sooner, playing with size of sampler can give you more or less accuracy but too large and you start to impact performance. Generally a UDA over more than 10k values (even simple functions like <code>count</code>) starts to fail. Important to recognize in these scenarios too that while the single query returns a single value, theres a ton of work to get it. So a lot of these queries or much concurrency will put a lot of pressure on your coordinators. This does require >3.8 (I would recommend 3.11.latest+) for <a href=""https://issues.apache.org/jira/browse/CASSANDRA-10783"" rel=""nofollow noreferrer"">CASSANDRA-10783</a></p>

<p><em>note: I make no promises that I havent missed an off by 1 error in example UDAs - I did not test fully, but should be close enough you can make it work from there</em></p>
",['table']
52654033,52654925,2018-10-04 19:40:53,Is it possible to sort a Cassandra Column Family by a specific column of a list of a user-defined datatype?,"<p>I'm having a little hard time understanding Cassandra. I simply couldn't write this question without making it look like confusing, but as I detail it below it may become clearer.</p>

<p>Suppose I have this datatype that I've created:</p>

<pre><code>CREATE TYPE transaction (
    transaction_id UUID,
    value float,
    transaction_date timestamp,
    PRIMARY KEY (transaction_id, transaction_date)
);
</code></pre>

<p>PS: I'm using it as if it was a 'class', but that might be a logical mistake of mine, please correct me if it can't be used as such.</p>

<p>Anyway, also I have this Column Family, in which I've created a list of this 'transaction' datatype:</p>

<pre><code>CREATE TABLE transactions_history_by_date (
    wallet_address UUID,
    user_id UUID,
    transactions list &lt;transaction&gt;,
    PRIMARY KEY (wallet_address, transaction_date))
WITH CLUSTERING ORDER BY (transaction_date DESC);
</code></pre>

<p>So what I'd like to know if this Column Family above is correct. I'd like to get all the transactions of a wallet, sorted by the transaction date (but the date is a column of the 'transaction' datatype - and to complicate it even more, in this Column Family there's a list of transactions, and not just a single one).</p>
",<list><sorting><cassandra><nosql><cqlsh>,"<p>No, in Cassandra you can sort only on the value of the clustering column - in this case you need to move <code>transaction_date</code> into table itself...</p>
",['table']
52660195,52662060,2018-10-05 07:14:01,Cassandra query by map,"<p>I have a table with the following column,</p>

<pre><code>name text, //partition key
tags map&lt;text, text&gt;
</code></pre>

<p>I also have a secondary index on ""tags"" column. Now I want to query like,</p>

<pre><code>select * from &lt;table_name&gt; where tags contains {'a':'b','x':'y'}
</code></pre>

<ol>
<li>Is it possible? If not, can I query with only ""contains {'a':'b'}""?</li>
<li>Is this a bad design? If yes, how to rectify this? (note: name has 1->n relation with tags)</li>
</ol>
",<database-design><cassandra>,"<p><strong>Question 1</strong></p>

<p>For map collections Cassandra allows creation of an index on keys, values or entries (this is available only for maps).</p>

<p>So, first you would create your index on the map:</p>

<pre><code>CREATE INDEX &lt;index_name&gt; ON &lt;table_name&gt; (ENTRIES(&lt;map_column&gt;));
</code></pre>

<p>Then you could query:</p>

<pre><code>SELECT * FROM &lt;table_name&gt; WHERE &lt;map_column&gt;['&lt;map_key&gt;'] = '&lt;map_value&gt;';
</code></pre>

<p>Another solution would be to froze your collection and create an index on it:</p>

<pre><code>CREATE INDEX &lt;index_name&gt; ON table (FULL(&lt;map_column&gt;));
</code></pre>

<p>Then you could query for values with:</p>

<pre><code>SELECT * FROM &lt;table_name&gt; WHERE &lt;map_column&gt; = ['&lt;value&gt;'...];
</code></pre>

<p><strong>I think the above solutions are not very good since you could easily scan your whole cluster. Your access type will use and index and not a partition key.</strong></p>

<hr>

<p><strong>Question 2</strong></p>

<p>Another solution would be to create a table like this:</p>

<pre><code>CREATE TABLE &lt;table_name&gt; ( key TEXT, value TEXT, name TEXT, PRIMARY KEY ((key, value), name));
</code></pre>

<p>Key and value columns will hold the values for the tags. They will also be partition key so you could query your data like:</p>

<pre><code>SELECT * FROM &lt;table_name&gt; WHERE key = 'key' AND value = 'value';
</code></pre>

<p>You will need to run several queries in order to search for all tags, but you can aggregate the result at the application level.</p>
",['table']
52699125,52700709,2018-10-08 09:23:27,Cassandra - search by clustered key,"<p>This is my <code>diseases</code> table definition:</p>

<pre><code>id text,
drugid text,
name
PRIMARY KEY (drugid, id)
</code></pre>

<p>Now I want to perform search by <code>drugid</code> column only (all values in this column are unique). This primary key was created due to quick drug search.</p>

<p>Now - what will be best solution to filter this table using <code>id</code>? Creating  new table? Pass additional value (<code>drugid</code>) to <code>SELECT</code>? Is it option with only <code>id</code>?</p>

<p><strong>Thans for help</strong> :)</p>
",<database><cassandra><nosql>,"<p>Looking at your table definition, the partition key is drugid. This means that your queries will have to include the drugid. But since id is also part of the primary key, you could do something like:</p>

<p><code>select * from diseases where drugid = ? and id = ?</code></p>

<p>Unfortunately just having the id is not possible, unless you create a secondary index on it. Which wouldn't be very good since you could trigger a full cluster scan.</p>

<p>So, the solutions are:</p>

<ul>
<li>specify the partition key (if possible), in this case drugid</li>
<li>create a new table that will have the id as partition key; in this case you will need to maintain both tables;</li>
</ul>

<p>I guess the solution you'll choose depends on your data set. You should test to see how each solution behaves.</p>

<p><strong>Should you use a secondary index?</strong></p>

<p>When specifying the partition key, Cassandra will read the exact data from the partition and from only one node.</p>

<p>When you create a secondary index, Cassandra needs to read the data from partitions spread across the whole cluster. There are performance impact implications when an index is built over a column with lots of distinct values. Here is some more reading on this matter - Cassandra at Scale: <a href=""https://pantheon.io/blog/cassandra-scale-problem-secondary-indexes"" rel=""nofollow noreferrer"">The Problem with Secondary Indexes</a></p>

<p>In the above article, there is an interesting comment by @doanduyhai:</p>

<blockquote>
  <p>""There is only 1 case where secondary index can perform very well and
  NOT suffer from scalability issue: when used in conjunction with
  PARTITION KEY. If you ensure that all of your queries using secondary
  index will be of the form : </p>
</blockquote>

<pre><code>SELECT ... FROM ... WHERE partitionKey=xxx AND my_secondary_index=yyy
</code></pre>

<blockquote>
  <p>then you're safe to go. Better, in this
  case you can mix in many secondary indices. Performance-wise, since
  all the index reading will be local to a node, it should be fine""</p>
</blockquote>

<p>I would stay away from secondary indexes.</p>

<p>From what you described, id will have distinct values, more or less, so you might run into performance issues since ""a general rule of thumb is to index a column with low cardinality of few values"".</p>

<p>Also, if id is a clustering column, the data will be stored in an ordered manner. The clustering column(s) determine the data’s on-disk sort order only within a partition key. The default order is ASC.</p>

<p>I would suggest some more reading - <a href=""https://docs.datastax.com/en/cql/3.3/cql/cql_using/useWhenIndex.html#useWhenIndex__when-no-index"" rel=""nofollow noreferrer"">When not to use an index</a> and <a href=""https://docs.datastax.com/en/cql/3.3/cql/cql_using/useSecondaryIndex.html"" rel=""nofollow noreferrer"">Using a secondary index</a></p>
",['table']
52734539,52734976,2018-10-10 07:07:18,Error on cassandra SELECT Query with order by,"<p>Now I'm working with Cassandra 3.11 and I encounter an error.</p>

<p>Below SCHEMA is my cassandra model created by Cassandra python driver.</p>

<pre><code>CREATE TABLE motosense.collection (
    id uuid,
    created_at timestamp,
    accel_data blob,
    model_name text,
    rssi int,
    sensor_id int,
    sensor_version text,
    PRIMARY KEY (id, created_at) ) WITH CLUSTERING ORDER BY (created_at DESC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';
CREATE INDEX collection_sensor_id_idx ON motosense.collection (sensor_id);
</code></pre>

<p>After creating above table and execute below query,</p>

<pre><code>SELECT id, created_at FROM calculated_collection WHERE sensor_id = 1 ORDER BY created_at DESC;
</code></pre>

<p>I get this error:</p>

<blockquote>
  <p>code=2200 [Invalid query] message=""ORDER BY with 2ndary indexes is not
  supported.""</p>
</blockquote>

<p>When I execute query without <code>order by</code>, I get unordered data query set.
How can I solve this problem.</p>
",<cassandra>,"<blockquote>
  <p>Secondary indexes are not designed to allow multiple queries on the
  same table. You should build another table that would satisfy your
  query. As a secondary index query pulls back data from multiple
  partitions, it may or may not come back in sorted order (which is why
  ORDER BY is not allowed on secondary index queries).</p>
</blockquote>

<p>The above quote is from the link below.</p>

<p>You should have a look on this, is the same issue - <a href=""https://stackoverflow.com/questions/25920078/order-by-with-2ndary-indexes-is-not-supported"">ORDER BY with 2ndary indexes is not supported</a>.</p>

<p>This is also related - <a href=""https://stackoverflow.com/questions/52730925/why-secondary-index-and-clustering-columns-order-by-cannot-be-used-toge"">Why Secondary Index ( = ?) and Clustering Columns (order by) CANNOT be used together for CQL Query?</a></p>
",['table']
52739192,52758499,2018-10-10 11:27:15,Columnar storage: Cassandra vs Redshift,"<p>How is columnar storage in the context of a NoSQL database like Cassandra different from that in Redshift. If Cassandra is also a columnar storage then why isn't it used for OLAP applications like Redshift?</p>
",<cassandra><amazon-redshift><column-oriented>,"<p>The storage engines of Cassandra and Redshift are very different, and are created for different cases.
Cassandra's storage not really ""columnar"" in wide known meaning of this type of databases, like Redshift, Vertica etc, it is much more closer to key-value family in NoSQL world. The SQL syntax used in Cassandra is not any ANSI SQL, and it has very limited set of queries that can be ran there. Cassandra's engine built for fast writing and reading of records, <strong>based on key</strong>, while Redshift's engine is built for <strong>fast aggregations</strong> (MPP), and has wide support for analytical queries, and stores,encodes and compresses data on column level.</p>

<p>It can be easily understood with following example:</p>

<p>Suppose we have a table with user id and many metrics (for example weight, height, blood pressure etc...).
I we will run aggregate the query in Redshift, like average weight, it will do the following (in best scenario):</p>

<ol>
<li><p>Master will send query to nodes.</p></li>
<li><p>Only the data for this specific column will be fetched from storage.</p></li>
<li><p>The query will be executed in parallel on all nodes.</p></li>
<li><p>Final result will be fetched to master.</p></li>
</ol>

<p>Running same query in Cassandra, will result in scan of all ""rows"", and each ""row"" can have several versions, and only the latest should be used in aggregation. If you familiar with any key-value store (Redis, Riak, DynamoDB etc..) it is less effective than scanning all keys there.</p>

<p>Cassandra many times used for analytical workflows with Spark, acting as a <strong>storage</strong> layer, while Spark acting as actual <strong>query engine</strong>, and basically shouldn't be used for analytical queries by its own. With each version released more and more aggregation capabilities are added, but it is very far from being real analytical database. </p>
",['table']
52752022,52753664,2018-10-11 04:04:25,Cassandra 3.11 SSTableLoader mechanics,"<p>I've been using the SSTableLoader utility to bulk transfer data between two distinct Cassandra clusters and I was wondering if anyone else has run into the same issues.  The source cluster has data, the destination does not.</p>

<p>I've read the datastax page on the utility's details but still I have some unanswered questions about how it works.</p>

<p>I am using the utility on the source cluster's live nodes and the commands follow this format:</p>

<pre><code>sstableloader -d target.host.ip -v -f /etc/cassandra/cassandra.yaml /cassandra/data/keyspace1/table1-uuid
</code></pre>

<p>The clusters are both setup with 256 vnodes each with 6 nodes in each cluster.  The schema is RF = 3 in both environments and the tables are all structured the same.</p>

<p>So my questions are as follows:</p>

<p>1) The utility pulls source cluster information from the cassandra.yaml you specify, but you have to specify an absolute path to the SSTables still.  So does running SSTableLoader from a single node give me the entire table at the destination once complete?  It seems difficult to verify since the token ranges are different at the destination cluster.</p>

<p>2) The datastax information says:</p>

<blockquote>
  <p>To get the best throughput from SSTable loading, you can use multiple
  instances of sstableloader to stream across multiple machines. No hard
  limit exists on the number of SSTables that sstableloader can run at
  the same time, so you can add additional loaders until you see no
  further improvement.</p>
</blockquote>

<p>Does this mean that for a single table, I would start multiple instances of SSTableLoader across multiple source machines?  Or does it just mean that I can use SSTableLoader for multiple different tables on multiple machines at the same time.  I'm trying to understand if the throughput gain they are mentioning is for a single table or for just multiple tables in flight.</p>

<p>3) What syntax modification is needed to run from snapshots instead?  I took a snapshot and tested by running the same command but further down into the snapshot directory of the table and it didn't parse correctly it was saying ""snapshot"" is an invalid keyspace.</p>

<p>Anyway thanks hope I was clear enough with my questions.</p>
",<cassandra><datastax><cassandra-3.0><scylla>,"<p>1) If your RF=3 and your cluster had 3 nodes, than each node holds <strong>ALL</strong> the data. Still there could be some minor changes due to updates that did not propagate to all replicas yet. If the number of nodes in your cluster is bigger than the RF (you your case 6 nodes, RF=3), than every node holds a combination of 50% of the data (different token ranges).
Anyhow, you need to run the <code>sstableloader</code> on all keyspaces + tables from each of your source nodes to the new cluster's destination nodes (assuming 1:1 ratio).</p>

<p>2) Yes, you can run multiple sstableloaders on the same table / keyspace from each of the source nodes, to it's matching destination node in parallel. But it also means you can do it for different keyspace / tables, as long as eventually you performed it from all source nodes for all keyspace / tables to their matching destination nodes (assuming 1:1 ratio).</p>

<p>3) Restoring from Backup (Snapshot) is a different procedure which does not involve using <code>sstableloader</code>. You can read more about it <a href=""https://docs.scylladb.com/operating-scylla/procedures/restore/"" rel=""noreferrer"">here</a>.</p>

<p>There is also an option to use <code>nodetool refresh</code> to load sstables from all source nodes to the new destination nodes, but it should be used only when the num_nodes=RF. Read more about it <a href=""https://docs.scylladb.com/operating-scylla/nodetool-commands/refresh/"" rel=""noreferrer"">here</a></p>
",['table']
52774946,52782274,2018-10-12 08:01:40,How to understand page_state can only be used for SAME query in CQL Paging,"<p>From <a href=""https://github.com/apache/cassandra/blob/trunk/doc/native_protocol_v4.spec#L991"" rel=""nofollow noreferrer"">doc</a> of CQL3 v4,page_state can only be used for <em>same</em> query for getting next page. </p>

<p>However, the QUERY consists of <code>&lt;query&gt;</code> and <code>&lt;query_parameters&gt;</code>, also from the <a href=""https://github.com/apache/cassandra/blob/trunk/doc/native_protocol_v4.spec#L309"" rel=""nofollow noreferrer"">doc</a>. At least <code>&lt;paging_state&gt;</code> field in <code>&lt;query_parameters&gt;</code> will be replaced with new value. So how to tell two queries are same? </p>

<p>Maybe my assumption is wrong that <code>&lt;paging_state&gt;</code> is the same for all next paging queries.</p>
",<cassandra><cassandra-3.0><cql3>,"<p>No 2 queries will be exactly the same. Each has a unique to the connection stream id. The paging state passed down in the response of 1 query can be returned in the same query to adjust where the query starts and to track how many results to return (based on limit of the 1st). Note that the paging state is the last partition and row returned with amount to return remaining total and within current partition (two byte arrays, two integers).</p>

<p>The driver wont allow it - but if you change your query and pass a paging state from a different request it can have weird results. Especially if the paging state was from a table with different schema it could result in an error when it tries to compare keys.</p>
",['table']
52782941,52782984,2018-10-12 15:45:40,what is better view or another table in cassandra?,"<p>I got table that I need to search by not indexed field. What is better, to make separate table with data I need and indexed by that field or make view? what is drawbacks of each chose? May be I can use secondary Index in that case instead?</p>
",<cassandra>,"<p>A second table will be better hands down. Only disadvantage is it requires more of your effort.</p>

<p>Materialized views have issues where they get outta sync and theres no way to repair them, only drop and recreate (they are now considered experimental and not prod ready). Secondary indexes require huge scatter gather queries that make your 99th percentile your average (while also being difficult to size appropriately). Ultimately for any heavy load, MVs or 2i will break, but its easy to add.</p>
",['table']
52794589,52795088,2018-10-13 15:37:28,Cassandra - data modelling help needed,"<p>I try to design social network ( kind of ) application.
I have a User, he has Follower(s), and there is a Timeline.
My timeline table look like:</p>

<pre><code>user_id
second_party_user_id
created
other_fields
</code></pre>

<p>So, when 'second_party_user' posts any new content, I get all people who follow him, and insert into their Timeline second_party_user's post.
When user comes to see timeline, I do a simple request to his timeline by user_id.
The problem is that I need to get ordered items. And if I want to get ordered by created, I need to put it as a second clustering column, not a third one.
At the same time, if I put it as a second clustering column, ie:</p>

<pre><code>user_id
created
second_party_user_id
other_fields
</code></pre>

<p>then I would have a problem when one user unfollows second_party_user, ie how can I delete by (user_id, second_party_user_id).</p>

<p>Any help will be highly appreciated!
Thank you in advance.</p>
",<cassandra>,"<p>To handle those features you can use two tables, one for get the timeline ordered and another one to handle the elimination in both tables.</p>

<pre><code>//order timeline by created date
user_id(pk) 
created(ck)
second_party_user_id 
other_fields

//with this table you can get created to delete in the first one and 
//delete this table with (user_id,second_party_user_id)
user_id(ck) 
second_party_user_id (ck)
created
other_fields
</code></pre>
",['table']
52855849,52856173,2018-10-17 13:16:57,Spark Dataframe.cache() behavior for changing source,"<p>My use case:  </p>

<ol>
<li>Create a dataframe from a cassandra table.</li>
<li>Create a output dataframe by filtering on a column and modify that column's value.</li>
<li>Write the output dataframe to cassandra with a TTL set, so all the modified records are deleted after a short period (2s)</li>
<li>Return the output dataframe to a caller that writes it to filesystem after some time. I can only return a dataframe to the caller and I don't have further control. Also, i can't increase the TTL.</li>
</ol>

<p>By the time, step 4 is executed, the output dataframe is empty. This is because, spark re-evaluates the dataframe on the action, and due to lineage the cassandra query is done again, which now yields no records.<br>
To avoid this, I added a step after step 2: </p>

<p>2a) <code>outputDataframe.cache()</code></p>

<p>This ensures that during step 5, cassandra is not queried, and I get desired output records in my file as well. I have below queries on this approach:</p>

<ol>
<li>Is it possible that, in cases where spark doesn't find the cached data (cache lookup fails), it will go up the lineage and run the cassandra query? If yes, what is the way to avoid that in all cases?</li>
<li>I have seen another way of doing the caching:  <code>df.rdd.cache()</code>. Is this any different than calling <code>cache()</code> on the dataframe?</li>
</ol>

<p>For reference, my current code looks as follows:</p>

<pre class=""lang-scala prettyprint-override""><code>//1
val dfOrig = spark
      .read
      .format(""org.apache.spark.sql.cassandra"")
      .options(Map(""keyspace"" -&gt; ""myks"", ""table"" -&gt; ""mytable"", ""pushdown"" -&gt; ""true""))
      .load()
//2
val df = dfOrig.filter(""del_flag = 'N'"").withColumn(""del_flag"", lit(""Y""))
//3
df.write.format(""org.apache.spark.sql.cassandra"")
      .options(Map(""keyspace"" -&gt; ""myks"", ""table"" -&gt; ""mytable"", ""spark.cassandra.output.ttl"" -&gt; ""120""))
      .mode(""append"")
      .save()
//4
// &lt;After quite some processing, mostly after the TTL, and in the calling code&gt;
df.write.format(""csv"").save(""some.csv"") 
</code></pre>
",<dataframe><apache-spark><apache-spark-sql><cassandra><spark-cassandra-connector>,"<blockquote>
  <p>Is it possible that, in cases where Spark doesn't find the cached data (cache lookup fails), it will go up the lineage and run the Cassandra query? </p>
</blockquote>

<p>Yes it is possible. Cached data can be removed by the cache cleaner (primarily in <code>MEMORY_ONLY</code> mode), can be lost when the corresponding node is decommissioned (crashed, preempted, released by dynamic allocation). Additionally other options, like speculative execution, can affect cache behavior.</p>

<p>Finally data might not be fully cached in first place.</p>

<blockquote>
  <p>If yes, what is the way to avoid that in all cases?</p>
</blockquote>

<p>Don't use <code>cache</code> / <code>persist</code> if you require strong consistency guarantees - it wasn't designed with use cases like this one in mind. Instead export data to a persistent, reliable storage (like HDFS) and read it from there.</p>

<p>You could also use <code>checkpoint</code> with HDFS <code>checkpointDir</code>.</p>

<p>You might be tempted to use more reliable caching mode like <code>MEMORY_AND_DISK_2</code> - this might reduce the probability of recomputing the data, at the cost of </p>

<blockquote>
  <p>df.rdd.cache(). Is this any different than calling cache() on the dataframe?</p>
</blockquote>

<p>It is different (the primary difference is the serialization strategy), but not when it comes to the properties which are of interest in the scope of this question.</p>

<p><strong>Important</strong>:</p>

<p>Please note that caching behavior might not be the biggest issue in your code. Reading from and appending to a single table can result in all kinds of undesired or undefined behaviors in complex pipelines, unless additional steps are taken to ensure that reader doesn't pick newly written records.</p>
",['table']
52868299,52870666,2018-10-18 06:39:57,Cassandra Predicates on non-primary-key columns (eventtype) are not yet supported for non secondary index queries,"<p>i developed a table as shown as below with primary key as id which is a uuid type</p>

<pre><code> id                                   | date                     | eventtype    | log      | password | priority | sessionid | sourceip     | user       | useragent
--------------------------------------+--------------------------+--------------+----------+----------+----------+-----------+--------------+------------+------------
 6b47e9b0-d11a-11e8-883c-5153f134200b |                     null | LoginSuccess |  demolog |     1234 |       10 |    Demo_1 | 123.12.11.11 |       Aqib |  demoagent
 819a58d0-cd3f-11e8-883c-5153f134200b |                     null | LoginSuccess |  demolog |     1234 |       10 |    Demo_1 | 123.12.11.11 |       Aqib |  demoagent
 f4fae220-d133-11e8-883c-5153f134200b | 2018-10-01 04:01:00+0000 | LoginSuccess |  demolog |     1234 |       10 |    Demo_1 | 123.12.11.11 |       Aqib |  demoagent
</code></pre>

<p>But when i try to query some thing like below</p>

<pre><code>select * from loginevents where eventtype='LoginSuccess';
</code></pre>

<p>i get an error like below</p>

<pre><code>InvalidRequest: Error from server: code=2200 [Invalid query] message=""Predicates on non-primary-key columns (eventtype) are not yet supported for non secondary index queries""
</code></pre>

<p>This is my table</p>

<pre><code>cqlsh:events&gt; describe loginevents;

CREATE TABLE events.loginevents (
    id uuid PRIMARY KEY,
    date timestamp,
    eventtype text,
    log text,
    password text,
    priority int,
    sessionid text,
    sourceip text,
    user text,
    useragent text
) WITH bloom_filter_fp_chance = 0.01
    AND caching = '{""keys"":""ALL"", ""rows_per_partition"":""NONE""}'
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy'}
    AND compression = {'sstable_compression': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99.0PERCENTILE';
</code></pre>

<p>How can i solve this</p>
",<cassandra>,"<p>An immediate answer to your question would be to create a secondary index on the column <code>eventtype</code> like this:</p>

<pre><code>CREATE INDEX my_index ON events.loginevents (eventtype);
</code></pre>

<p>Then you can filter on this particular column :</p>

<pre><code>SELECT * FROM loginevents WHERE eventtype='LoginSuccess';
</code></pre>

<p>However this solution can badly impact the performances of your cluster.</p>

<p>If you come from the SQL world and are new to Cassandra, go read an introduction on cassandra modeling, like <a href=""https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling"" rel=""nofollow noreferrer"">this one</a>.</p>

<p>The first thing is to identify the query, then create the table according to.</p>

<p>In Cassandra, data are distributed in the cluster according to the partition key, so reading records that belong to the same partition is very fast.</p>

<p>In your case, maybe a good start would be to group your records based on the <code>eventtype</code> :</p>

<pre><code>CREATE TABLE events.loginevents (
  id uuid,
  date timestamp,
  eventtype text,
  log text,
  password text,
  priority int,
  sessionid text,
  sourceip text,
  user text,
  useragent text,
  PRIMARY KEY (eventtype, id)
</code></pre>

<p>) </p>

<p>Then you can do select like this :</p>

<pre><code>SELECT * FROM loginevents WHERE eventtype='LoginSuccess';
</code></pre>

<p>or even :</p>

<pre><code>SELECT * FROM loginevents WHERE eventtype in ('LoginSuccess', 'LoginFailure');
</code></pre>

<p>(It's not a perfect model, it definitely needs to be improved before production.)</p>
",['table']
53036031,53042082,2018-10-28 21:00:30,Provisioned write capacity in Cassandra,"<p>I need to capture time-series sensor data in Cassandra. The best practices for handling time-series data in DynamoDB is as follow:</p>

<ol>
<li>Create one table per time period, provisioned with write capacity less than 1,000 write capacity units (WCUs).</li>
<li>Before the end of each time period, prebuild the table for the next period.</li>
<li>As soon as a table is no longer being written to, reduce its provisioned write capacity. Also reduce the provisioned read capacity of earlier tables as they age, and archive or delete the ones whose contents will rarely or never be needed. </li>
</ol>

<p>Now I am wondering how I can implement the same concept in Cassandra! Is there any way to manually configure write/read capacity in Cassandra as well? </p>
",<cassandra><time-series><amazon-dynamodb>,"<p>This really depends on your own requirements that you need to discuss with development, etc.</p>

<p>There are several ways to handle time-series data in Cassandra:</p>

<ol>
<li>Have one table for everything. As Chris mentioned, just include the time component into partition key, like a day, and store data per sensor/day. If the data won't be updated, and you know in advance how long they will be kept, so you can set TTL to data, then you can use <a href=""http://thelastpickle.com/blog/2016/12/08/TWCS-part1.html"" rel=""nofollow noreferrer"">TimeWindowCompactionStrategy</a>. Advantage of this approach is that you have only one table and don't need to maintain multiple tables - that's make easier for development and maintenance.</li>
<li>The same approach as you described - create a separate table for period of time, like a month, and write data into them. In this case you can effectively drop the whole table when data ""expires"". Using this approach you can update data if necessary, and don't require to set TTL on data. But this requires more work for development and ops teams as you need to reach multiple tables. Also, take into account that there are some limits on the number of tables in the cluster - it's recommended not to have more than 200 tables as every table requires a memory to keep metadata, etc. Although, some things, like, a bloom filter, could be tuned to occupy less memory for tables that are rarely read.</li>
</ol>
",['table']
53080250,53082129,2018-10-31 09:33:55,How to recover deleted data in cassandra?,"<p>If I <code>deleted</code> a value in a cell from a table, but later, I want to recover it back, how can I do it?</p>

<p>I know that, cassandra doesn’t really <strong>delete data</strong>, it just mark it as <strong>deleted</strong>, so how can I recover the data?</p>

<p>Usecase for example: I want to delete all information from a user, so I first delete the information in cassandra database, then, I try to delete his information in somewhere else, but it comes to an error, so I have to stop the deletion process and recover the deleted data from cassandra database.</p>

<p>How can I do that?</p>
",<cassandra>,"<p>Unfortunately not. You could however use sstabledump (Cassandra >= 3.0) to inspect sstable contents, but there are some drawbacks:</p>

<ul>
<li>if the data was not flushed to disk (thus being in the memtable) it will be deleted before reaching to sstable</li>
<li>you need to find the sstable that the data belongs to</li>
</ul>

<p>Probably there are some other drawbacks that I miss right now.</p>

<p><strong>Some workarounds</strong></p>

<ul>
<li>first copy the data to another table and then perform the delete. After you delete the information from the other location, you can safely delete it from your backup table.</li>
<li>a new column (""pending_delete"") where you would record the state. You would only query for your ""live"" data.</li>
<li>a new table where you would store the pk of the data to be deleted and delete it from the both tables after the operation on the other location is successful.</li>
</ul>

<p>Choosing the right solution I guess depends on your use case and the size of your data.</p>
",['table']
53148928,53149877,2018-11-05 05:42:06,Manage many to many relationship in Cassandra,"<p>I have these two tables:</p>

<pre><code>create table users (
    id UUID,
    email ascii,
    created_at timeuuid,
    primary key(id, email)
);
create table groups (
    id UUID,
    name ascii,
    created_at timeuuid,
    primary key(id, name)
);
</code></pre>

<blockquote>
  <p>A user can be in multiple groups, a group can obviously have multiple users.</p>
</blockquote>

<p>So I've two ways to maintain a many-to-many relationship (taken from <a href=""https://stackoverflow.com/questions/44550900/many-to-many-in-cassandra-3"">here</a>), one is:</p>

<pre><code>CREATE TABLE user_group (
  user UUID,
  group UUID,
  PRIMARY KEY (user, group)
)
</code></pre>

<p>Another one is (using sets):</p>

<pre><code>CREATE TABLE user_jn_group (
  user UUID PRIMARY KEY,
  groups set&lt;UUID&gt;
)
CREATE TABLE group_jn_user (
  group UUID PRIMARY KEY,
  users set&lt;UUID&gt;
)
</code></pre>

<p>I'm using Cassandra 3.9.0. I know both approaches have their own advantages, disadvantages. I want the least duplicity, but also I have an equal weight to read/write speed. Also, is there any more hidden cost behind any of both approaches?</p>
",<cassandra><cql><cassandra-3.0>,"<p>Using collections for this is probably impractical because of the size limit on collections (although that shouldn't be a concern for a system with just a few users), chances are high that the set of users in a group will be too large.</p>

<p>It's also worth noting that your solution based on the <code>user_group</code> table won't work as it won't support querying by group. You would need to maintain <strong><em>another</em></strong> table to support this query (and always maintain the two records):</p>

<pre><code>CREATE TABLE group_user (
  user UUID,
  group UUID,
  PRIMARY KEY (group, user)
)
</code></pre>

<p>This will allow querying by group.</p>

<hr>

<p>Additional options:</p>

<p><strong>Add a secondary index to <code>user_group</code></strong>:
<br/>
Another approach is to expand the <code>user_group</code> solution: if you have a secondary index on the <code>group</code> field, you'll be able to perform lookups in both ways:</p>

<pre><code>CREATE INDEX ON user_group (group);
</code></pre>

<p><strong>Use a materialized view</strong>
<br/>You can also use a materialized view instead of a <code>group_user</code> table. The data between <code>user_group</code> and this view will be kept in sync by cassandra (eventually):</p>

<pre><code>CREATE MATERIALIZED VIEW group_user
AS SELECT group, user
FROM user_group
WHERE user IS NOT NULL AND group IS NOT NULL
PRIMARY KEY (group, user);
</code></pre>

<p>With this, you'll have to add a record to <code>user_group</code> only and the view will take care of searches by group.</p>

<p>As you noted, each has pros and cons that can't be detailed here. Please check the docs on limitations of each option.</p>
",['table']
53309291,53342666,2018-11-14 21:54:32,Cassandra Data modeling IoT best practices,"<p>I am fairly new to Cassandra and I am trying to understand how to design my tables for IoT sensors.</p>

<p>The idea is to have several devices, each with several sensors attached to it sending data periodically (up to around 200000 values per device per day per sensor)</p>

<p>I'd like to be able to query for the latest value of a sensor for a specific list of sensors and devices in more or less real-time. Also devices do not always send data and may be down for long periods of time.</p>

<p>After a lot of reading I came up with something like this</p>

<pre><code>CREATE TABLE ""sensor_data"" (
    deviceid TEXT,
    sensorid TEXT,
    ts timestamp,
    value TEXT,
    PRIMARY KEY ((deviceid, sensorid), ts)
) WITH CLUSTERING ORDER BY (ts DESC);
</code></pre>

<p>The idea behind this would be to perform one query per device and sensor such as </p>

<pre><code>Select deviceid, sensorid, ts, value where deviceid = ""device1"" and sensorid = ""temperature"" limit 1
</code></pre>

<p>And run this for each device and sensor. It's not one query to return it all (Which would be ideal) but seems to be fast enough to run for potentially up to 100 sensors or so (With possibilities for parallelizing the queries) for a few devices.</p>

<p>However from what I have read so far, I understand this would give me a lot of columns for my row and it might be complicated in terms of long term storage and Cassandra limitations.</p>

<p>I am thinking that maybe adding something like the date to the table like so (as seen on some blogs and guides) might be a good idea</p>

<pre><code>CREATE TABLE ""sensor_data"" (
    deviceid TEXT,
    sensorid TEXT,
    date TEXT
    ts timestamp,
    value TEXT,
    PRIMARY KEY ((deviceid, sensorid, date), ts)
) WITH CLUSTERING ORDER BY (ts DESC);
</code></pre>

<p>And then query like</p>

<pre><code>Select deviceid, sensorid, date, ts, value where deviceid = ""device1"" and sensorid = ""temperature"" and date = ""2018-11-14"" limit 1
</code></pre>

<p>Does that even make sense? It feels like it might mitigate storage issues and allow for easier archiving of old data in the future however how do I go about querying for the latest value of a specific sensor and device if that device was down for a day or more? Do I really have to query for 1 day, if nothing is found, query the previous day and so forth (Maybe limit it to only the last few days or so)?</p>

<p>Are there better ways to handle this in Cassandra or am I in the right direction?</p>
",<cassandra><data-modeling><iot><cassandra-3.0>,"<p>Part of the problem that you'll run into is that each sensor will be having 200k readings per day. In general, you want to keep each partition under <a href=""https://docs.datastax.com/en/dse-planning/doc/planning/planningPartitionSize.html"" rel=""nofollow noreferrer"">100k rows</a>. So, your second idea (having date as part of the PK) may have perf issues.</p>

<p>Really what you are looking to do is what we refer to as 'bucketing'; how to group things together so queries are usable and performant.</p>

<p>To really help with this, we will need to understand a little more information: </p>

<ul>
<li>How many devices do you have? Will that number grow or is it finite?</li>
<li>In plain English, what is an example of queries that you are trying to answer?</li>
</ul>

<p><strong>Incorporating this into the answer based on your answers (below):</strong></p>

<p>Alright, here is a potential idea... </p>

<p>We DO care about bucketing though to try to stay around the 100k/partition optimal rows in a partition.</p>

<p>You're going to want two tables:</p>

<ol>
<li>Lookup table</li>
<li>Sensor table</li>
</ol>

<p>Lookup table will look something like:</p>

<pre><code>CREATE TABLE lookup-table (
deviceid TEXT,
sensor-map MAP,
PRIMARY KEY (deviceid)
);
</code></pre>

<ul>
<li><code>deviceid</code> is the unique ID for each device</li>
<li><code>sensor-map</code> is a JSON <a href=""https://docs.datastax.com/en/cql/3.3/cql/cql_reference/cql_data_types_c.html"" rel=""nofollow noreferrer"">map</a> of sensors that a given device has and a corresponding unique ID for that specific sensor (e.g. {temperature: 183439, humidity : 84543292, other-sensor : blah})</li>
<li>That way each device has a mapping of sensors that is available to it</li>
<li>Example query would be: <code>SELECT * FROM lookup-table WHERE deviceid = 1234;</code></li>
<li>Another approach would be to have individual columns for each type of sensor and the unique ID for each sensor as a value</li>
</ul>

<p>Sensor table will look like:</p>

<pre><code>CREATE TABLE sensor_data (
sensorid TEXT,
sensor_value (whatever data type fits what you need),
ts TIMESTAMP,
reading_date date,
time_bucket int,
PRIMARY KEY ((reading_date, sensorid, time_bucket), ts)
) WITH CLUSTERING ORDER BY (ts DESC);
</code></pre>

<ol>
<li>As each sensor will get 200k readings/day AND we want to keep each partition under 100k rows, that means we want to do two partitions for each sensor each day</li>
<li>How could you bucket? You should do it in two parts:you need to bucket daily; each sensor gets a new partition each day (<code>reading_date</code>) and split each day into two (due to the amount of readings that you're expecting); AM or PM; AM equals bucket 1, PM equals bucket 2. Or use 24 hour time where 0-1200 equals 1, 1300-2399 equals 2</li>
<li>Within your application provide the specific <code>sensorid</code> and
<code>time_bucket</code> will come from the time that you're actually requesting
the query (e.g. if time is 1135 hours, then <code>time_bucket = 1</code>) and <code>reading_date</code> will come from the actual day that you are querying</li>
<li>Since you are clustering with <code>ts DESC</code> then it will retrieve the
latest reading for that given <code>sensorid</code>. So it would look like
<code>SELECT * from sensor_data WHERE reading_date = 12/31/2017 AND sensorid = 1234 AND time_bucket = 1 LIMIT 1;</code></li>
<li>By maintaining <code>ts</code> as a clustering column, you'll be able to keep all of the readings for a given sensor; none will be overwritten</li>
</ol>

<p><strong>Important to know</strong>: this works great if there is an even distribution of sensor readings throughout the 24-hour day. However, if you're reading heavily in the morning and not at all in the afternoon, then it isn't an even and we'll have to figure out another way to bucket. But, I think that you get what is going on. </p>

<p><strong>To query:</strong></p>

<ul>
<li>There will be one query to retrieve all of the <code>sensorid</code> that a device has; once you have those <code>sensorid</code>, you can then use it for the next step</li>
<li>There will be <em>n</em> queries for each <code>sensor_value</code> for each <code>sensorid</code></li>
<li>Since we are bucketing (via <code>time_bucket</code>), you should have an even distribution throughout all of the partitions</li>
</ul>

<p><strong>Lastly: give me the latest <code>sensorid</code> by a given value</strong>
To do that there are a couple of different ways...</p>

<ul>
<li>Run a Spark job: to do that, you'll have to lift and shift the data to run the Spark query</li>
<li>Use DataStax Enterprise: with DSE you have an integrated Analytics component based on Spark so you can run Spark jobs without having to manage a separate Spark cluster. Disclosure: I work there, btw</li>
<li>Create an additional Cassandra (C*) table and do some parallel writes</li>
</ul>

<p>For the additional C* table: </p>

<pre><code>CREATE TABLE sensor_by_value (
sensor-value INT,
ts TIMESTAMP,
sensorid TEXT,
reading_date DATE,
time_bucket INT,
PRIMARY KEY ((sensor-value, reading_date), ts)
) WITH CLUSTERING ORDER BY (ts DESC);
</code></pre>

<p>You will definitely have to do some time bucketing here: </p>

<ul>
<li>Remember, we don't want any more than 100k rows per partition</li>
<li>You'll have to understand the possible values (range)</li>
<li>The frequency of each reading</li>
<li>If you have 100 devices, 100 sensors, and each sensor being read up to 200k per day, then you have a potential for up to 2B sensor readings per day...</li>
<li>Typically, what I have my customers do is run some analysis on their data to understand these bits of info, that way you can be sure to account for it</li>
<li>How much you have to bucket will depend on the frequency</li>
<li>Good luck! :-)</li>
</ul>

<p><strong>Final tip</strong></p>

<p>Look into compaction strategies: specifically <a href=""https://docs.datastax.com/en/cassandra/3.0/cassandra/operations/opsConfigureCompaction.html"" rel=""nofollow noreferrer"">time window compaction strategy</a> (TWCS) and adding a <code>default_time_to_live</code></p>

<ul>
<li><p>Your data seems immutable after the initial insert</p></li>
<li><p>TWCS will make the operational overhead of compaction much lower as you fine-tune it for the time window that you need</p></li>
<li><p>A <code>default_ttl</code> will also help with the operational overhead of deleting data after you don't need it anymore.</p></li>
</ul>

<p>Does this answer and/or satisfy that queries that you're trying to answer? If not, let me know and we can iterate.</p>

<p>To learn all of this stuff, go to <a href=""https://academy.datastax.com"" rel=""nofollow noreferrer"">DataStax Academy</a> for a ton of free training. Data Modeling (DS 220) is a great course!</p>
",['table']
53333140,53333464,2018-11-16 07:18:13,Partition count in cassandra,"<p>What does the number of partitions(estimate) in the tablestats query indicate?
When we query tablestats at the different nodes of a multi-node cassandra,we see different value for stable count and number of partitions at each node.Does this indicate the number of rows/partition key at a particular node?</p>
",<cassandra><cassandra-3.0>,"<p>Yes, that's indicate a number of partitions of specific table on given node.  Different number of partitions could be because of the data distribution in your table, number of vnodes owned by this node, etc.</p>
",['table']
53514995,53515378,2018-11-28 08:21:55,Cassandra: Why do I not have to include all partition keys in query?,"<p>Currently, I am dealing with Cassandra.</p>

<p>While reading a blog post, it is said:</p>

<blockquote>
  <p>When issuing a CQL query, you must include all partition key columns,
  at a minimum.
  (<a href=""https://shermandigital.com/blog/designing-a-cassandra-data-model/"" rel=""nofollow noreferrer"">https://shermandigital.com/blog/designing-a-cassandra-data-model/</a>)</p>
</blockquote>

<p>However, in my database it seems like it possible without including all partition keys. Here the table:</p>

<pre><code>CREATE TABLE usertable (
    personid text,
    name text,
    ""timestamp"" timestamp,
    active boolean,
    PRIMARY KEY ((personid, name), timestamp)
) WITH
  CLUSTERING ORDER BY (""timestamp"" DESC)
  AND comment=''
  AND read_repair_chance=0
  AND dclocal_read_repair_chance=0.1
  AND gc_grace_seconds=864000
  AND bloom_filter_fp_chance=0.01
  AND compaction={ 'class':'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy',
  'max_threshold':'32',
  'min_threshold':'4' }
  AND compression={ 'chunk_length_in_kb':'64',
  'class':'org.apache.cassandra.io.compress.LZ4Compressor' }
  AND caching={ 'keys':'ALL',
  'rows_per_partition':'NONE' }
  AND default_time_to_live=0
  AND id='23ff16b0-c400-11e8-55c7-2b453518a213'
  AND min_index_interval=128
  AND max_index_interval=2048
  AND memtable_flush_period_in_ms=0
  AND speculative_retry='99PERCENTILE';
</code></pre>

<p>So I can do <code>select * from usertable where personid = 'ABC-02';</code>. However, according to the blog post, I have to include <code>timestamp</code> as well.</p>

<p>Can someone explain this? </p>
",<database><cassandra>,"<p>In cassandra, partition key spreads data around cluster. It computes the hash of partition key and determine the location of data in the cluster.</p>

<p>One exception is, if you use ALLOW FILTERING or secondary index it does not require you too include all partition keys in where query.</p>

<p>For further information take a look at blog post:</p>

<blockquote>
  <p>The purpose of a partition key is to split the data into partitions
  where an entire partition is stored on a single node in the cluster
  (with each node storing many partitions). When data is read or written
  from the cluster, a function called Partitioner is used to compute the
  hash value of the partition key. This hash value is used to determine
  the node/partition which contains that row. The clustering key is used
  further to search for a row within a given partition.</p>
  
  <p>Select queries in Apache Cassandra look a lot like select queries from
  a relational database. However, they are significantly more
  restricted. The attributes allowed in ‘where’ clause of Cassandra
  query must include the full partition key and additional clauses may
  only reference the clustering key columns or a secondary index of the
  table being queried.</p>
  
  <p>Requiring the partition key attributes in the ‘where’ helps Cassandra
  to maintain constant result-set retrieval time as the cluster is
  scaled-out by allowing Cassandra to determine the partition, and thus
  the node (and even data files on disk), that the query must be
  directed to.</p>
  
  <p>If a query does not specify the values for all the columns from the
  primary key  in the ‘where’ clause, Cassandra will not execute it and
  give the following warning :</p>
  
  <p>‘InvalidRequest: Error from server: code=2200 [Invalid query]
  message=”Cannot execute this query as it might involve data filtering
  and thus may have unpredictable performance. If you want to execute
  this query despite the performance unpredictability, use ALLOW
  FILTERING” ‘</p>
</blockquote>

<p><a href=""https://www.instaclustr.com/apache-cassandra-scalability-allow-filtering-partition-keys/"" rel=""nofollow noreferrer"">https://www.instaclustr.com/apache-cassandra-scalability-allow-filtering-partition-keys/</a></p>

<p><a href=""https://www.datastax.com/dev/blog/a-deep-look-to-the-cql-where-clause"" rel=""nofollow noreferrer"">https://www.datastax.com/dev/blog/a-deep-look-to-the-cql-where-clause</a></p>
",['table']
53526292,53601689,2018-11-28 18:56:55,Failing Cassandra INSERT and UPDATE statements - Unexpected Exception,"<p>I've recently been getting the following error on the system.log files on both my production and demo clusters. Each cluster has 2 nodes and replication factor is 2. No changes have been made to my knowledge. I cannot figure out what the reason behind the error is. It is causing INSERT and UPDATE statements to fail.</p>

<pre><code>[SharedPool-Worker-27] ERROR org.apache.cassandra.transport.Message - Unexpected exception during request; channel = [id: 0xeb429d31, /14.0.0.1:34495 =&gt; /14.0.0.2:9042]                
    java.lang.AssertionError: -2146739295
    at org.apache.cassandra.db.BufferExpiringCell.&lt;init&gt;(BufferExpiringCell.java:46) ~[apache-cassandra-2.1.10.jar:2.1.10]
    at org.apache.cassandra.db.BufferExpiringCell.&lt;init&gt;(BufferExpiringCell.java:39) ~[apache-cassandra-2.1.10.jar:2.1.10]
    at org.apache.cassandra.db.AbstractCell.create(AbstractCell.java:176) ~[apache-cassandra-2.1.10.jar:2.1.10]
    at org.apache.cassandra.cql3.UpdateParameters.makeColumn(UpdateParameters.java:65) ~[apache-cassandra-2.1.10.jar:2.1.10]
    at org.apache.cassandra.cql3.Constants$Setter.execute(Constants.java:314) ~[apache-cassandra-2.1.10.jar:2.1.10]
    at org.apache.cassandra.cql3.statements.UpdateStatement.addUpdateForKey(UpdateStatement.java:110) ~[apache-cassandra-2.1.10.jar:2.1.10]
    at org.apache.cassandra.cql3.statements.UpdateStatement.addUpdateForKey(UpdateStatement.java:57) ~[apache-cassandra-2.1.10.jar:2.1.10]
    at org.apache.cassandra.cql3.statements.ModificationStatement.getMutations(ModificationStatement.java:708) ~[apache-cassandra-2.1.10.jar:2.1.10]
    at org.apache.cassandra.cql3.statements.ModificationStatement.executeWithoutCondition(ModificationStatement.java:495) ~[apache-cassandra-2.1.10.jar:2.1.10]
    at org.apache.cassandra.cql3.statements.ModificationStatement.execute(ModificationStatement.java:481) ~[apache-cassandra-2.1.10.jar:2.1.10]
    at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:238) ~[apache-cassandra-2.1.10.jar:2.1.10]
    at org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:493) ~[apache-cassandra-2.1.10.jar:2.1.10]
    at org.apache.cassandra.transport.messages.ExecuteMessage.execute(ExecuteMessage.java:138) ~[apache-cassandra-2.1.10.jar:2.1.10]
    at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:439) [apache-cassandra-2.1.10.jar:2.1.10]
    at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:335) [apache-cassandra-2.1.10.jar:2.1.10]
    at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.23.Final.jar:4.0.23.Final]
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333) [netty-all-4.0.23.Final.jar:4.0.23.Final]
    at io.netty.channel.AbstractChannelHandlerContext.access$700(AbstractChannelHandlerContext.java:32) [netty-all-4.0.23.Final.jar:4.0.23.Final]
    at io.netty.channel.AbstractChannelHandlerContext$8.run(AbstractChannelHandlerContext.java:324) [netty-all-4.0.23.Final.jar:4.0.23.Final]
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_45]
    at org.apache.cassandra.concurrent.AbstractTracingAwareExecutorService$FutureTask.run(AbstractTracingAwareExecutorService.java:164) [apache-cassandra-2.1.10.jar:2.1.10]
    at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:105) [apache-cassandra-2.1.10.jar:2.1.10]
    at java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]
</code></pre>

<p>These are async requests. On the client side I'm seeing a fail on the future as well.I'm using cassandra-2.1.10.I haven't done a rolling restart of the nodes yet, but I don't think that will fix the problem.</p>

<p>Also noticed that it seems like the failed inserts/updates happen after a few successful inserts/updates. The request statements themselves (formatting) are fine. Any help would be appreciated.</p>

<p>UPDATE: I've looked into the cassandra source code. It contains the following:</p>

<pre><code>assert timeToLive &gt; 0 : timeToLive;
assert localExpirationTime &gt; 0 : localExpirationTime;
</code></pre>

<p>Looks like it's failing on the second assert statement. The table has a TTL value set in its properties for 1728000 seconds. No ttl is being set in the insert/update statements. So I don't understand why some of the statements are failing on this assert.</p>

<p><strong>EDIT:</strong> on the client applications I see the following error messages:</p>

<p>Client 1 connects to cluster 1:</p>

<pre><code>16:36:01.102 [New I/O worker #64] WARN  - /14.0.0.2:9042 replied with server error (java.lang.AssertionError: -2146571535), trying next host
</code></pre>

<p>Client 2 connects to cluster 2: </p>

<pre><code>16:30:01.302 [cluster1-nio-worker-7] WARN  - /14.0.0.4:9042 replied with server error (java.lang.AssertionError: -2146571895), defuncting connection.
</code></pre>

<p>I believe what is happening is when the above errors happen the client drops the connection and reconnects. During this time other async requests fail.</p>
",<cassandra><cql><cassandra-2.0><cassandra-3.0><cassandra-2.1>,"<p>One of the tables had its 'default_time_to_live' set to about 19 years. The reason behind the problem was the <a href=""https://en.wikipedia.org/wiki/Year_2038_problem"" rel=""nofollow noreferrer"">2038 timestamp problmem</a>. Even though the ttl value itself on each cell is the number of seconds left, seems like cassandra internally tries to convert the expiry time into a timestamp. So current timestamp + ttl (19+) years = a timestamp beyond 19 January, 2038. This was causing the overflow and the negative number seen in the exception shown above. Reducing the default ttl value on the table fixed the problem stopped the assertion error from happening. </p>

<p>Seemed like a few assertion errors, would cause the connections to reset and in the meanwhile other writes would fail.</p>
",['table']
53704297,53718723,2018-12-10 11:02:48,Issues migrating to Apache Cassandra 3.11.3 from DSE 5.0.9,"<p>We are looking at migrating from DSE 5.0.9 to Apache Cassandra 3.11.3. We've gotten quite far and managed to fix various issues (including the EverywhereStrategy one) but are running into an issue with the system.local table.</p>

<p>The migration/upgrade was done on just one server, so far. When we start Cassandra 3.11.3 on this one node we get an error when loading system.local:</p>

<pre><code>INFO [main] 2018-12-07 10:56:12,963 ColumnFamilyStore.java:411 - Initializing system.local
INFO [SSTableBatchOpen:1] 2018-12-07 10:56:12,993 BufferPool.java:230 - Global buffer pool is enabled, when pool is exhausted (max is 512.000MiB) it will allocate on heap
ERROR [SSTableBatchOpen:1] 2018-12-07 10:56:13,013 DebuggableThreadPoolExecutor.java:239 - Error in ThreadPoolExecutor
java.lang.RuntimeException: Unknown column server_id during deserialization
at org.apache.cassandra.db.SerializationHeader$Component.toHeader(SerializationHeader.java:321) ~[apache-cassandra-3.11.3.jar:3.11.3]
at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:522) ~[apache-cassandra-3.11.3.jar:3.11.3]
at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:385) ~[apache-cassandra-3.11.3.jar:3.11.3]
at org.apache.cassandra.io.sstable.format.SSTableReader$3.run(SSTableReader.java:570) ~[apache-cassandra-3.11.3.jar:3.11.3]
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_172]
at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_172]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_172]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_172]
at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81) [apache-cassandra-3.11.3.jar:3.11.3]
at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_172]
</code></pre>

<p>Looking at another Cassandra 3.11.3 cluster we have here, system_id doesn't exist in the table. It does, however, in the DSE 5.0.9 version of the table.
Without being able to load system.local, we end up then getting the following warning:</p>

<pre><code>WARN [main] 2018-12-06 10:43:57,241 SystemKeyspace.java:1087 - No host ID found, created a0bb8c11-2864-4d58-9c0c-59b97b16c48e (Note: This should happen exactly once per node).
</code></pre>

<p>(there is no host ID as system.local didn't load)
which then causes for the following error:</p>

<pre><code>ERROR [main] 2018-12-06 10:43:58,295 CassandraDaemon.java:708 - Exception encountered during startup
java.lang.RuntimeException: A node with address dubdc1-oatjeeramp2dmcassandra-04/10.109.158.254 already exists, cancelling join. Use cassandra.replace_address if you want to replace this node.
at org.apache.cassandra.service.StorageService.checkForEndpointCollision(StorageService.java:558) ~[apache-cassandra-3.11.3.jar:3.11.3]
at org.apache.cassandra.service.StorageService.prepareToJoin(StorageService.java:804) ~[apache-cassandra-3.11.3.jar:3.11.3]
at org.apache.cassandra.service.StorageService.initServer(StorageService.java:664) ~[apache-cassandra-3.11.3.jar:3.11.3]
at org.apache.cassandra.service.StorageService.initServer(StorageService.java:613) ~[apache-cassandra-3.11.3.jar:3.11.3]
at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:379) [apache-cassandra-3.11.3.jar:3.11.3]
at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:602) [apache-cassandra-3.11.3.jar:3.11.3]
at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:691) [apache-cassandra-3.11.3.jar:3.11.3]
</code></pre>

<p>At this point system.local has been overwritten and the new host ID value stored, and Cassandra has shutdown.</p>

<p>Adding <code>-Dcassandra.replace_node=&lt;ip address&gt;</code> to cassandra-env.sh results in an error saying that the node has already been bootstrapped so can't be used. I know I can get around this by deleting all of the data, but I really don't want to have to do that.</p>

<p>Recovering a backup of system.local will allow us to start up DSE again. Currently the node is back running DSE5.0.9</p>

<p>Has anyone seen this issue before, and do you have any advice on how to resolve it?</p>
",<cassandra>,"<p>Steps:</p>

<ol>
<li>Exact available configurations copied from DSE to OSS C*.</li>
<li><p>Altered few keyspace/tables:</p>

<p>alter keyspace dse_system with replication = {'class': 'NetworkTopologyStrategy', 'DC3': '3'}; //DC1,DC2=OSS C* </p>

<p>//if you are using spark
alter table cfs_archive.sblocks with compaction = {'class':'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'};</p>

<p>alter table cfs.sblocks with compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'};</p></li>
<li><p>auto_bootstrap: false
       JVM_OPTS=""$JVM_OPTS -Dcassandra.allow_unsafe_replace=true""
       JVM_OPTS=""$JVM_OPTS -Dcassandra.replace_address=...</p></li>
</ol>

<p>Be careful, test everything in lower env.
Please go through this link for additional information: 
<a href=""https://www.mail-archive.com/user@cassandra.apache.org/msg58077.html"" rel=""nofollow noreferrer"">https://www.mail-archive.com/user@cassandra.apache.org/msg58077.html</a></p>
",['table']
53709650,53710172,2018-12-10 16:22:49,what are the difference between the data back up using nodetool and cqlsh copy command?,"<p>Currently we have two options to take data back up of the tables in a Cassandra keyspace. We can either user <code>nodetool</code> commands or use the <code>copy</code> command from the <code>cqlsh</code> terminal.</p>

<p>1) What are the differences between these commands ?</p>

<p>2) Which one is most appropriate ?</p>

<p>3) Also if we are using <code>nodetool</code> to take backup we would generally flush the data from mem tables to sstables before we issue the <code>nodetool snapshot command</code>. So my question is should we employ the same techinque of flushing the data if we use the <code>cqlsh copy</code> command ?</p>

<p>Any help is appreciated.</p>

<p>Thanks very much.</p>
",<cassandra><cassandra-2.0><cassandra-3.0><cassandra-2.1>,"<p><em>GREAT</em> question!</p>

<blockquote>
  <p>1) What are the differences between these commands ?</p>
</blockquote>

<p>Running a <code>nodetool snapshot</code> creates a hard-link to the SSTable files on the requested keyspace.  It's the same as running this from the (Linux) command line:</p>

<pre><code>ln {source} {link}
</code></pre>

<p>A <code>cqlsh COPY</code> is essentially the same as doing a <code>SELECT * FROM</code> on a table.  It'll create a text file with the table's data in whichever format you have specified.</p>

<p>In terms of their difference from a <em>backup</em> context, a file created using <code>cqlsh COPY</code> will contain data from all nodes.  Whereas <code>nodetool snapshot</code> needs to be run on each node in the cluster.  In clusters where the number of nodes is greater than the replication factor, each snapshot will only be valid for the node which it was taken on.</p>

<blockquote>
  <p>2) Which one is most appropriate ?</p>
</blockquote>

<p>It depends on what you're trying to do.  If you simply need backups for a node/cluster, then <code>nodetool snapshot</code> is the way to go.  If you're trying to export/import data into a new table or cluster, then <code>COPY</code> is the better approach.</p>

<p>Also worth noting, <code>cqlsh COPY</code> takes a while to run (depending on the amount of data in a table), and can be subject to timeouts if not properly configured.  <code>nodetool snapshot</code> is nigh instantaneous; although the process of compressing and SCPing snapshot files to an off-cluster instance will take some time.</p>

<blockquote>
  <p>3) Should we employ the same technique of flushing the data if we use the <code>cqlsh</code> copy command ?</p>
</blockquote>

<p>No, that's not necessary.  As <code>cqlsh COPY</code> works just like a <code>SELECT</code>, it will follow the normal <a href=""https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlAboutReads.html"" rel=""nofollow noreferrer"">Cassandra read path</a>, which will check structures <em>both</em> in RAM and on-disk.</p>
",['table']
53744677,53970494,2018-12-12 13:58:23,Query Cassandra UDT via Spark SQL,"<p>We'd like to query data from Cassandra DB via <strong>SparkSQL</strong>. The problem is that data is stored in cassandra as <strong>UDT</strong>.
The structure of UDT is deeply nested and it contains arrays of variable length, so it would be very difficult to decompose data to flat structure.
I couldn't find any working example how to to query such UDTs via SparkSQL - especially to filter the results based on UDT values.</p>

<p>Alternatively, could you suggest different ETL pipeline (Query engine, Storage engine, ...), which would be more suitable for our use-case ?</p>

<p><strong>Our ETL pipeline:</strong></p>

<p>Kafka (duplicated events) -> Spark streaming -> <strong>Cassandra (deduplication to store only latest event)</strong> &lt;- Spark SQL &lt;- analytics platform (UI)</p>

<p><strong>Solutions we've tried so far:</strong></p>

<p>1) Kafka -> Spark -> <strong>Parquet</strong> &lt;- Apache Drill</p>

<p>Everything worked well, we could query and filter arrays and nested data structures.</p>

<p>Problem: couldn't deduplicate data (rewrite parquet files with latest events)</p>

<p>2) Kafka -> Spark -> <strong>Cassandra</strong> &lt;- Presto</p>

<p>Solved problem 1) with data deduplication.</p>

<p>Problem: Presto doesn't support UDT types (<a href=""https://prestodb.io/docs/current/connector/cassandra.html"" rel=""nofollow noreferrer"">presto doc</a>, <a href=""https://github.com/prestodb/presto/issues/6047"" rel=""nofollow noreferrer"">presto issue</a>)</p>

<p><strong>Our main requirements are:</strong></p>

<ul>
<li>support for data deduplication. We may receive many events with same ID and we need to store only the latest one.</li>
<li>storing deeply nesteed data structure with arrays</li>
<li>distributed storage, scalable for future expansion</li>
<li>distributed query engine with SQL-like query support (for connection with Zeppelin, Tableau, Qlik, ... ). The query doesn't have to run in real time, few minutes delay is acceptable.</li>
<li>support for schema evolution (AVRO style)</li>
</ul>

<p>Thank your for any suggestions</p>
",<apache-spark><cassandra><parquet><presto><spark-cassandra-connector>,"<p>You can just use the dot-syntax to perform queries on the nested elements. For example, if I have following CQL definitions:</p>

<pre><code>cqlsh&gt; use test;
cqlsh:test&gt; create type t1 (id int, t text);
cqlsh:test&gt; create type t2 (id int, t1 frozen&lt;t1&gt;);
cqlsh:test&gt; create table nudt (id int primary key, t2 frozen&lt;t2&gt;);
cqlsh:test&gt; insert into nudt (id, t2) values (1, {id: 1, t1: {id: 1, t: 't1'}});
cqlsh:test&gt; insert into nudt (id, t2) values (2, {id: 2, t1: {id: 2, t: 't2'}});
cqlsh:test&gt; SELECT * from nudt;

 id | t2
----+-------------------------------
  1 | {id: 1, t1: {id: 1, t: 't1'}}
  2 | {id: 2, t1: {id: 2, t: 't2'}}

(2 rows)
</code></pre>

<p>Then I can load that data as following:</p>

<pre><code>scala&gt; val data = spark.read.format(""org.apache.spark.sql.cassandra"").
     options(Map( ""table"" -&gt; ""nudt"", ""keyspace"" -&gt; ""test"")).load()
data: org.apache.spark.sql.DataFrame = [id: int, t2: struct&lt;id: int, t1: struct&lt;id: int, t: string&gt;&gt;]

scala&gt; data.cache
res0: data.type = [id: int, t2: struct&lt;id: int, t1: struct&lt;id: int, t: string&gt;&gt;]

scala&gt; data.show
+---+----------+
| id|        t2|
+---+----------+
|  1|[1,[1,t1]]|
|  2|[2,[2,t2]]|
+---+----------+
</code></pre>

<p>And then query the data to select only specific values of field in UDT:</p>

<pre><code>scala&gt; val res = spark.sql(""select * from test.nudt where t2.t1.t = 't1'"")
res: org.apache.spark.sql.DataFrame = [id: int, t2: struct&lt;id: int, t1: struct&lt;id: int, t: string&gt;&gt;]

scala&gt; res.show
+---+----------+
| id|        t2|
+---+----------+
|  1|[1,[1,t1]]|
+---+----------+
</code></pre>

<p>You can use either <code>spark.sql</code>, or corresponding <code>.filter</code> functions - depending on your programming style.  This technique works with any struct type data, coming from different sources, like, JSON, etc.</p>

<p>But take into account that you won't get optimizations from Cassandra connector like you get when querying by partition key(s)/clustering column(s)</p>
",['table']
53744868,54249343,2018-12-12 14:09:23,Cassandra question v3.11.3 ... select count(*) from table1,"<p>I have imported more than 1 core records in a table and when I do Select query count(*) it gives me error. I know it is a costly query but, can any one help me get a solution for the same.</p>

<p>SELECT COUNT(*) FROM TABLE1;</p>

<p>Error: OperationTimedOut: errors={'10.20.30.10': 'Client request timeout. See Session.execute_async'}, last_host=10.20.30.10</p>
",<cassandra>,"<p>After performing multiple R&amp;D I got a solution for count(*) issue.</p>

<p>Steps: </p>

<ol>
<li>Setup presto on Cassandra Cluster (I used presto-server-0.215 version (presto-server-0.215.tar.gz and used jdk: jdk-8u151-linux-x64.tar.gz))
1.1. presto-server-0.215.tar.gz: <a href=""https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.215/"" rel=""nofollow noreferrer"">https://repo1.maven.org/maven2/com/facebook/presto/presto-server/0.215/</a>
1.2. jdk-8u151-linux-x64.tar.gz: <a href=""https://www.oracle.com/technetwork/java/javase/downloads/java-archive-javase8-2177648.html"" rel=""nofollow noreferrer"">https://www.oracle.com/technetwork/java/javase/downloads/java-archive-javase8-2177648.html</a></li>
<li>Install presto on one cassandra server and which will make it as coordinator and rest of the nodes in the cluster will be worker, please refer below URL for setting up presto.
Refer URL: <a href=""https://github.com/prestodb/presto/issues/3382"" rel=""nofollow noreferrer"">https://github.com/prestodb/presto/issues/3382</a></li>
<li>You need add firewall rule for Presto Port which you have mentioned in config.properties file (I'm using RHEL 7.x OS)</li>
<li>Do changes in launcher.py ---> Line number '214' path of jdk installed ""command = ['/opt/jdk1.8.0_151/bin/java', '-cp', classpath]""</li>
<li>Start presto ---> ./launcher start</li>
<li><p>Open presto console <a href=""http://localhost:8081"" rel=""nofollow noreferrer"">http://localhost:8081</a> and you should see coordinator and worker nodes in the console.</p></li>
<li><p>Download ""presto-cli-0.215-executable.jar"" (URL: <a href=""https://prestodb.io/docs/current/installation/cli.html"" rel=""nofollow noreferrer"">https://prestodb.io/docs/current/installation/cli.html</a>) and rename it to prestocli (Give 755 permission) and then test count(*) for a big table using 'prestocli' a table which has 75 Lakhs records which was giving error when we run in cqlsh.
7.1. CQLSH Error: Error from server: code=1200 [Coordinator node timed out waiting for replica nodes' responses] message=""Operation timed out - received only 0 responses."" info={'received_responses': 0,        'required_responses': 1, 'consistency': 'ONE'}</p></li>
<li>Below is the solution for count(*)...</li>
</ol>

<p>[root@casdb01 bin]# ./prestocli --server localhost:8081
presto> SELECT count(*) FROM cassandra.datamart.big_table;</p>

<h2>  _col0</h2>

<p>7587418
(1 row)</p>

<p>Query 20190118_070908_00005_38tiw, FINISHED, 1 node
Splits: 1,298 total, 1,298 done (100.00%)
0:53 [7.59M rows, 7.24MB] [142K rows/s, 139KB/s]</p>

<ol start=""9"">
<li>For any application query you can presto as the interface to perform count(*).</li>
</ol>

<p>Special thanks to my team met who helped me to get this result (Venkatesh Bhat).</p>
",['table']
53860841,53861577,2018-12-20 00:08:44,How to Model Table to run query's based on status field which is changing over time,"<p>Currently we have a table which we query using shipment_id and in future we have a need to query based on status field
Current Table : </p>

<pre><code>CREATE TABLE shipment ( 
    shipment_id text,
    tenant_id text,
    actual_arrival_time text,
    actual_dep_time text,
    email_ids set,
    is_deleted boolean,
    modified_by text,
    modified_time timestamp,
    planned_arrival_time text,
    planned_dep_time text,
    route_id text,
    shipment_departure_date text,
    status_code text,
    PRIMARY KEY (shipment_id, tenant_id) 
); 

CREATE INDEX shipment_id_index ON shipment (tenant_id);
</code></pre>

<p>Current Query's</p>

<p>1) SELECT * FROM shipment where tenant_id=?0 ALLOW FILTERING ;</p>

<p>2) SELECT * FROM shipment WHERE shipment_id=?0 and tenant_id=?1 ;</p>

<p>Pending/Future Query's </p>

<p>list of shipment id's for given status code as of now 
3) SELECT * FROM shipment WHERE tenant_id = 'y' and status_code  = x ?  ;</p>

<p>4) list of shipment id's for given status code for last 1 week </p>

<p>5) list of shipment id's for which got delayed </p>

<p>Above table There could be 10-15 unique tenants
 and will have 1 shipment_id,1 tenant_id 1 row per table 
and status_code will be changing over the time as shipment progresses , from Shipment_started, shipment_progress, shipment_delayed, shipment_delayed_completed and shipment_completed etc 
each shipment in its life time will go through 3-5 statuses , the current table
will be updated only when there is a status change for a given shipment_id  .</p>

<p>I need to create a new table which can address query's something like below </p>

<p>3) list of shipment's for a given tenant which have status_code = 'x' as of now </p>

<p>4) list of shipment's for a given tenant which have status_code = 'x' for last 1 week </p>

<p>5) list of shipment's for which got delayed ?</p>
",<cassandra><cassandra-2.0><cassandra-3.0>,"<p>In Cassandra, you model your tables based on your queries, so you may actually create a table for every query you may execute. Also using <code>ALLOW FILTERING</code> in your queries is something that should be used for development and testing purposes only and not in your actual prodcution application (check the answer here: <a href=""https://stackoverflow.com/questions/51702419/cassandra-cqlengine-allow-filtering/51706543#51706543"">Cassandra CQLEngine Allow Filtering</a>).</p>

<p>So for each of the cases/queries you mentioned I suggest the following:</p>

<p><code>1) SELECT * FROM shipment where tenant_id=?0 ALLOW FILTERING;</code></p>

<p>this should be addressed by the following table:</p>

<pre><code>CREATE TABLE shipment ( 
    tenant_id text,
    shipment_id text,
    actual_arrival_time text,
    actual_dep_time text,
    email_ids set,
    is_deleted boolean,
    modified_by text,
    modified_time timestamp,
    planned_arrival_time text,
    planned_dep_time text,
    route_id text,
    shipment_departure_date text,
    status_code text,
    PRIMARY KEY (tenant_id, shipment_id) 
);
</code></pre>

<p>here the <code>tenant_id</code> is the <code>partition key</code> so if you execute your query: <code>SELECT * FROM shipment where tenant_id='x';</code>
then you don't need to use <code>ALLOW FILTERING</code> any more.</p>

<p><strong>Update:</strong> I've also added the <code>shipment_id</code> as part of the primary key to handle same <code>cardinality</code> in case the <code>tenant_id</code> is not unique so that the <code>primary key</code> be consisted of both <code>tenant_id</code> and <code>shipment_id</code> to avoid overwriting records with same <code>tenant_id</code> As per @Himanshu Ahire's comment.</p>

<p><code>2)SELECT * FROM shipment WHERE shipment_id='x' and tenant_id='y';</code></p>

<p>this should be addressed by the following table:</p>

<pre><code>CREATE TABLE shipment ( 
    shipment_id text,
    tenant_id text,
    actual_arrival_time text,
    actual_dep_time text,
    email_ids set,
    is_deleted boolean,
    modified_by text,
    modified_time timestamp,
    planned_arrival_time text,
    planned_dep_time text,
    route_id text,
    shipment_departure_date text,
    status_code text,
    PRIMARY KEY ((shipment_id, tenant_id)) 
);
</code></pre>

<p>here the <code>shipment_id</code> and <code>tenant_id</code> both are used as <a href=""https://docs.datastax.com/en/cql/3.3/cql/cql_using/useCompositePartitionKeyConcept.html"" rel=""nofollow noreferrer"">composite partition key</a></p>

<p><code>3) SELECT * FROM shipment WHERE tenant_id = 'y' and status_code = 'x';</code></p>

<p><code>4) list of shipment id's for given status code for last 1 week</code></p>

<p><code>5) list of shipment id's for which got delayed</code></p>

<p>these should be addressed by the following table:</p>

<pre><code>CREATE TABLE shipment (
    status_code text,
    tenant_id text,
    shipment_id text,
    actual_arrival_time text,
    actual_dep_time text,
    email_ids set,
    is_deleted boolean,
    modified_by text,
    modified_time timestamp,
    planned_arrival_time text,
    planned_dep_time text,
    route_id text,
    shipment_departure_date text,
    PRIMARY KEY ((tenant_id, status_code), actual_arrival_time) 
) WITH CLUSTERING ORDER BY (actual_arrival_time DESC);
</code></pre>

<p>here you should also use both <code>tenant_id</code> and <code>status_code</code> as <code>composite partition key</code> and the <code>actual_arrival_time</code> as <code>clustering column</code>
so you can easily create queries like:</p>

<p><code>3) SELECT * FROM shipment WHERE tenant_id = 'y' and status_code = 'x';</code></p>

<p><code>4) SELECT * FROM shipment WHERE tenant_id = 'y' and status_code = 'x' and actual_arrival_time &gt;= 'date of last week';</code></p>

<p><code>5) SELECT * FROM shipment WHERE tenant_id = 'y' and status_code = 'x' and actual_arrival_time &gt; planned_arrival_time;</code></p>

<p>just a note for query number 4 you can send the date of last week from your application code or using <a href=""https://docs.datastax.com/en/cql/3.3/cql/cql_reference/timeuuid_functions_r.html"" rel=""nofollow noreferrer"">cql functions</a></p>
",['table']
53873346,53876852,2018-12-20 17:20:21,How to obtain row count estimates in in Cassandra using the Java client driver,"<p>If the only thing I have available is a <code>com.datastax.driver.core.Session</code>, is there a way to get a rough estimate of row count in a Cassandra table from a remote server? Performing a count is too expensive. I understand I can get a partition count estimate through JMX but I'd rather not assume <a href=""https://wiki.apache.org/cassandra/JmxSecurity"" rel=""nofollow noreferrer"">JMX has been configured</a>. (I think that result must be multiplied by number of nodes and divided by replication factor.) Ideally the estimate would include cluster keys too, but everything is on the table.</p>

<p>I also see there's a <a href=""https://docs.datastax.com/en/cql/3.3/cql/cql_using/useQuerySystem.html"" rel=""nofollow noreferrer"">size_estimates table in the system keyspace</a> but I don't see much documentation on it. Is it periodically refreshed or do the admins need to run something like <code>nodetool flush</code>? </p>

<p>Aside from not including cluster keys, what's wrong with using this as a very rough estimate?</p>

<pre><code>select sum(partitions_count)
from system.size_estimates
where keyspace_name='keyspace' and table_name='table';
</code></pre>
",<cassandra>,"<p>The size estimates is updated on a timer every 5 minutes (overridable with <code>-Dcassandra.size_recorder_interval</code>).</p>

<p>This is a very rough estimate, but you could from the token of the partition key find the range it belongs in and on each of the replicas pull from this table (its local replication and unique to each node, not global) and divide out the size and the number of partitions for a very vague approximate estimate of the partition size. There are so many assumptions and averaging that occurs in this path even before writing to this table. Cassandra errs on efficiency side at cost of accuracy and is more for general uses like spark bulk reading so take it with a grain of salt.</p>

<p>Its not useful now but looking towards the future post 4.0 freeze there will be many new virtual tables, including possibly ones to get accurate statistics on specific and ranges of partitions on demand. </p>
",['table']
53977433,53979355,2018-12-30 12:05:33,Is it possible to select X records from every clustering key Y in a partition?,"<p>Given a table with <code>PRIMARY KEY (pkey, ckey_a, ckey_b, etc) WITH CLUSTERING ORDER BY (ckey_a, ckey_b, etc)</code>, is it possible to make a select statement to get the first 30 records of every <code>ckey_b</code> grouping from a specific primary key/wide row?</p>
",<cassandra><nosql>,"<p>You can do a <code>SELECT * FROM table GROUP BY pkey, ckey_a, ckey_b</code> to get the unique and ckey_c's and such from ckey_b. You can limit by partition (see alex's answer) but you cannot currently limit by the group. If your query isnt called often (this is expensive) you can create a UDA that combines with the GROUP BY to limit the number by the group, throwing away the rest. This is very expensive though since the coordinator still gets all the values, just filters them out before sending back to client.</p>
",['table']
54006795,54009933,2019-01-02 12:56:02,Cassandra sequential repair does not repair all nodes on one run?,"<p>Day before yesterday, I had issued a full sequential repair on one of my nodes in a 5 node Cassandra cluster for a single table using the below command.</p>

<pre><code>nodetool repair -full -seq -tr &lt;keyspace&gt; &lt;table&gt; &gt; &lt;logfile&gt;
</code></pre>

<p>Now the node on which the command was issued was repaired properly as can be infered from the below command</p>

<pre><code>nodetool cfstats -H &lt;keyspace.columnFamily&gt;
</code></pre>

<p>The same, however, cannot be said about other nodes as for them I get a random value of repair %, significantly lesser. </p>

<p>I am not sure what is happening over here, looks like the only node that was repaired for the keyspace and column family was the node on which the repair command was issued. Any guesses on what might be going on here, or how to properly investigate into the issue</p>

<p>Thanks !</p>
",<cassandra><datastax><scylla>,"<p>You said your cluster has 5 nodes, but not which <strong>replication factor</strong> (RF) you are using for your table - I'll assume you used the common RF=3. When RF=3, each piece of data is replicated 3 times across the five nodes.</p>

<p>The key point you have missed is that in such a setup each specific node does <strong>not</strong> contain all the data. How much of the total data does it contain? Let's do some simple math: if the amount of actual data inserted into the table is X, then the total amount of data stored by the cluster is 3*X (since RF=3, there are three copies of each piece of data). This total is spread across 5 nodes, so each node will hold (3*X)/5, i.e., 3/5*X.</p>

<p>When you start a repair on one specific node, it repairs only the data which this node has, i.e., as we just calculated, 3/5 of the total data. What this repair does is for each piece of data held by this node, it compares this data against the copies held by other replicas, repairs the inconsistencies and repairs <strong>all</strong> these copies. This means when the repair is over, in the node which we repaired all its data was repaired. But for other nodes, not all their data was repaired - just the parts which intersected with the node who initiated this repair. This intersection should be roughly 3/5*3/5 or 36% of the data (of course everything is distributed randomly, so you're likely to get a number close to 36% but not exactly 36%).</p>

<p>So as you probably realized by now, this means that ""nodetool repair"" is not a cluster-wide operation. If you start it on one node, it is only guaranteed to repair all the data on one node, and may repair less on other nodes. So you must run the repair on each one of the nodes, separately.</p>

<p>Now you may be asking: Since repairing node 1 also repaired 36% of node 2, wouldn't it be a waste to also repair node 2, since we already did 36% of the work? Indeed, it is a waste. So Cassandra has a repair option ""-pr"" (""primary range"") which ensures that only one of the 3 replicas for each piece data will repair it. With RF=3, ""nodetool repair -pr"" will be three times faster than without ""-pr""; You still need to run it separately on each of the nodes, and when all nodes finish your data will be 100% repaired on all nodes.</p>

<p>All of this is fairly inconvenient and it's also hard to recover from transient failures during a long repair. This is why the both commercial Cassandra offerings - from Datastax and ScyllaDB - offer a separate repair tool which is more convenient than ""nodetool repair"", making sure that the entire cluster is repaired in the most efficient way possible, and recovering from transient problems without redoing the lengthy repair process from the beginning.</p>
",['table']
54071367,54072783,2019-01-07 09:11:05,Cassandra truncate performance,"<p>I have been recently told that, cassandra truncate is not performant and it is anti pattern. But, I do not know why? </p>

<p>So, I have 2 questions:</p>

<ul>
<li><p>Is it more performant to have upsert of all records then doing truncate?</p></li>
<li><p>Does truncate operation creates tombstones? </p></li>
</ul>

<p>Cassandra Version: 3.x</p>
",<cassandra><truncate>,"<p>From the cassandra docs:</p>

<blockquote>
  <p>Note: TRUNCATE sends a JMX command to all nodes, telling them to
  delete SSTables that hold the data from the specified table. If any of
  these nodes is down or doesn't respond, the command fails and outputs
  a message like the following</p>
</blockquote>

<p>So, running truncate will issue a deletion of all sstables belonging to your cassandra table, which will be quite fast but must be acknowledged by all nodes. Depending on your cassandra.yml this will snapshot your data before:</p>

<blockquote>
  <p><em>auto_snapshot</em>  (Default: true) Enable or disable whether a snapshot is
  taken of the data before keyspace truncation or dropping of tables. To
  prevent data loss, using the default setting is strongly advised. If
  you set to false, you will lose data on truncation or drop.</p>
  
  <p>When creating or modifying tables, you enable or disable the key cache
  (partition key cache) or row cache for that table by setting the
  caching parameter. Other row and key cache tuning and configuration
  options are set at the global (node) level. Cassandra uses these
  settings to automatically distribute memory for each table on the node
  based on the overall workload and specific table usage. You can also
  configure the save periods for these caches globally.</p>
</blockquote>

<p>To your question:</p>

<ul>
<li>upserts will be much slower (when there is significant data in your table)</li>
<li>truncate does not write tombstones at all (instead it will delete <em>all</em> on <em>all nodes</em> for your truncated table sstables immediately)</li>
</ul>
",['table']
54086837,54087947,2019-01-08 07:11:28,Cassandra changing Primary Key vs Firing multiple select queries,"<p>I have a table that stores list products that a user has. The table looks like this.</p>

<pre><code>create table my_keyspace.userproducts{
  userid,
  username,
  productid,
  productname,
  producttype,
Primary Key(userid)
}
</code></pre>

<p>All users belong to a group, there could be min 1 to max 100 users in a group</p>

<pre><code>userid|groupid|groupname|
1     |g1     | grp1  
2     |g2     | grp2  
3     |g3     | grp3  
</code></pre>

<p>We have  new requirement to display all products for all users in a single group.</p>

<p>So do i change my userproducts so that my Partition Key is now groupid and make userid as my cluster key, so that i get all my results in one single query.     </p>

<p>Or do I keep my table design as it is and fire multiple select queries by selecting all users in a group from second table and then fire one select query for each user, consolidate data in my code and then return it to the users</p>

<p>Thanks.</p>
",<cassandra><nosql><cassandra-3.0><database-partitioning><scylla>,"<p>Even before getting to your question, your data modelling as you presented it has a problem: You say that you want to store ""a list products that a user has"". But this is not what the table you presented has - your table has a single product for each userid. The ""userid"" is the key of your table, and each entry in the table, i.e, each unique userid, has one combination of the other fields.</p>

<p>If you really want each user to have a list of products, you need the primary key to be <code>(userid, productid)</code>. This means that each record is indexed by <strong>both</strong> a userid and a productid, or in other words - a userid has a list of records each with its own productid. Cassandra allows you to efficiently fetch all the productid records for a single userid because it implements the first part of the key as a ""partition key"" but the second part is a ""clustering key"".</p>

<p>Regarding your actual question, you indeed have two options: Either do multiple queries on your original tables, or do so-called <em>denormalization</em>, i.e., create a second table with exactly what you want searchable immediately. For the second option you can either do it manually (update both tables every time you have new data), or let Cassandra update the second table for you automatically, using a feature called <strong>Materialized Views</strong>.</p>

<p>Which of the two options - multiple queries or multiple updates - to use really depends on your workload. If it has many updates and rare queries, it is better to leave updates quick and make queries slower. If, on the other hand, it has few updates but many queries, it is better to make updates slower (when each update needs to update both tables) but make queries faster. Another important issue is how much query latency is important for you - the multiple queries option not only increases the load on the cluster (which you can solve by throwing more hardware at the problem) but also increases the latency - a problem which does not go away with more hardware and for some use cases may become a problem.</p>

<p>You can also achieve a similar goal in Cassandra by using the <strong>Secondary Index</strong> feature, which has its own performance characteristics (in some respects it is similar to the ""multiple queries"" solution).</p>
",['table']
54174377,54183213,2019-01-14 00:07:28,Cassandra how to see active user connections,"<p>In cassandra (am using DSE), </p>

<ol>
<li>how do I check how many users are connected to the database? Any way to check node wise?</li>
<li>Is there any auditing info stored which will tell me which all users connected along with info such as IP address and driver used etc?</li>
<li>In Opscenter there is a metric called ""Native clients"", where is this info stored in the db to query for? Does this include internal communication between the nodes and backups etc?</li>
</ol>
",<cassandra>,"<blockquote>
  <ol>
  <li><p>How do I check how many users are connected to the database? Any way to check node wise?</p></li>
  <li><p>Is there any auditing info stored which will tell me which all users connected along with info such as IP address and driver used etc?</p></li>
  </ol>
</blockquote>

<p>DSE has a <a href=""https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/mgmtServices/performance/aboutPerformanceService.html"" rel=""noreferrer"">performance service</a> feature which you can enable to make this information available via cql.  To enable this particular capability, configure the following in dse.yaml as described in the <a href=""https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/mgmtServices/performance/collectingUserActivityDiagnostics.html"" rel=""noreferrer"">docs</a>:</p>

<pre><code>user_level_latency_tracking_options:                                            
   enabled: true 
</code></pre>

<p>With this enabled, you can now query a variety of tables, for example:</p>

<pre><code>cqlsh&gt; select * from dse_perf.user_io;

 node_ip   | conn_id         | last_activity                   | read_latency | total_reads | total_writes | user_ip   | username  | write_latency
-----------+-----------------+---------------------------------+--------------+-------------+--------------+-----------+-----------+---------------
 127.0.0.1 | 127.0.0.1:55116 | 2019-01-14 14:08:19.399000+0000 |         1000 |           1 |            0 | 127.0.0.1 | anonymous |             0
 127.0.0.1 | 127.0.0.1:55252 | 2019-01-14 14:07:39.399000+0000 |            0 |           0 |            1 | 127.0.0.1 | anonymous |          1000

(2 rows)
cqlsh&gt; select * from dse_perf.user_object_io;

 node_ip   | conn_id         | keyspace_name | table_name | last_activity                   | read_latency | read_quantiles | total_reads | total_writes | user_ip   | username  | write_latency | write_quantiles
-----------+-----------------+---------------+------------+---------------------------------+--------------+----------------+-------------+--------------+-----------+-----------+---------------+-----------------
 127.0.0.1 | 127.0.0.1:55252 |             s |          t | 2019-01-14 14:07:39.393000+0000 |            0 |           null |           0 |            1 | 127.0.0.1 | anonymous |          1000 |            null
 127.0.0.1 | 127.0.0.1:55116 |             s |          t | 2019-01-14 14:08:19.393000+0000 |         1000 |           null |           1 |            0 | 127.0.0.1 | anonymous |             0 |            null

</code></pre>

<p>Note that there is a cost to enabling the performance service, and it can be enabled and disabled selectively using <code>dsetool perf userlatencytracking [enable|disable]</code>.</p>

<p>In a future release of Apache Cassandra (4.0+) and DSE (likely 7.0+), there will be a <code>nodetool clientstats</code> command (<a href=""https://issues.apache.org/jira/browse/CASSANDRA-14275"" rel=""noreferrer"">CASSANDRA-14275</a>), and a corresponding <code>system_views.clients</code> table (<a href=""https://issues.apache.org/jira/browse/CASSANDRA-14458"" rel=""noreferrer"">CASSANDRA-14458</a>) that includes connection info.  This will include the driver name, if the driver client provides one (newer ones do).</p>

<blockquote>
  <ol start=""3"">
  <li>In Opscenter there is a metric called ""Native clients"", where is this info stored in the db to query for? Does this include internal communication between the nodes and backups etc?</li>
  </ol>
</blockquote>

<p>I'm not too up to speed on OpsCenter.  From what I know OpsCenter typically stores it's data in the <code>OpsCenter</code> keyspace, you can configure data collection parameters by following this <a href=""https://docs.datastax.com/en/opscenter/6.1/opsc/configure/opscConfigureDataCollectionExpiration_c.html"" rel=""noreferrer"">doc</a>.</p>
",['table']
54189456,54196807,2019-01-14 21:30:26,How can I version my data with Cassandra?,"<p>I want to use the key-value pair feature of Cassandra. Until now, I have been using Kyotocabinet but it does not support multiple writes and hence, I want to use Cassandra for versioning my tabular data.</p>

<pre><code>Roll No, Name, Age, Sex
14BCE1008, Aviral, 22, Male
14BCE1007, Shantanu, 22, Male
</code></pre>

<p>The above data is tabular(csv). It's version 1.
Next is version 2:</p>

<pre><code>Roll No, Name, Age, Sex
14BCE1008, Aviral, 22, Male
14BCE1007, Shantanu, 22, Male
14BCE1209, Piyush, 22, Male
</code></pre>

<p>Hence, I would call the above version as version 2 with the following diff:
<code>insert_patch</code>: <code>14BCE1209</code> as key(PK) and <code>14BCE1209, Piyush, 22, Male</code> as value.
I am familiar with the creation of the table but unable to figure out the versioning part.</p>
",<database><cassandra><bigdata><database-versioning>,"<p>To have multiple versions of data in your table if you use composite primary key instead of primary key consisting of one field. </p>

<p>So table definition could look as following (if you ""know"" the version number prior inserting the data):</p>

<pre><code>create table test( 
  id text,
  version int,
  payload text,
  primary key (id, version)
) with clustering order by (version desc);
</code></pre>

<p>and inserting data as:</p>

<pre><code>insert into test (id, version, payload) values ('14BCE1209', 1, '....');
insert into test (id, version, payload) values ('14BCE1209', 2, '....');
</code></pre>

<p>to select the latest value for given key you can use <code>LIMIT 1</code>:</p>

<pre><code>SELECT * from test where id = '14BCE1209' LIMIT 1;
</code></pre>

<p>and to select latest versions for all partitions (not recommended, just for example - need a special approach for effective processing):</p>

<pre><code>SELECT * from test PER PARTITION LIMIT 1;
</code></pre>

<p>But this will work only for cases when you know version in advance. If you don't know, then you can use <code>timeuuid</code> type for version instead of the <code>int</code>:</p>

<pre><code>create table test( 
  id text,
  version timeuuid,
  payload text,
  primary key (id, version)
) with clustering order by (version desc);
</code></pre>

<p>and inserting data as (instead of <code>now()</code> you can use current timestamp generated from your code):</p>

<pre><code>insert into test (id, version, payload) values ('14BCE1209', now(), '....');
insert into test (id, version, payload) values ('14BCE1209', now(), '....');
</code></pre>

<p>and select will work the same as above...</p>
",['table']
54222010,54222176,2019-01-16 17:07:16,On Cassandra how to enable LDAP authentication,"<p>I have a Cassandra cluster running on Ubuntu. I would like to enable authentication so that not everyone will have access to the Cassandra database and run queries.</p>

<p>Enabling simple authentication is available at <a href=""https://docs.datastax.com/en/cassandra/3.0/cassandra/configuration/secureConfigNativeAuth.html"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/cassandra/3.0/cassandra/configuration/secureConfigNativeAuth.html</a></p>

<p>But, I am looking for integrating Cassandra with LDAP, Active Directory</p>
",<cassandra><nosql><cassandra-3.0><wide-column-store>,"<p>You will have to change the default authenticator from AllowAllAuthenticator to PasswordAuthenticator or some custom authenticator.</p>

<p>You can also enable roles for a finer grained access.</p>

<p>Check the following:</p>

<ul>
<li><a href=""http://cassandra.apache.org/doc/latest/operating/security.html?highlight=authenticator#authentication"" rel=""nofollow noreferrer"">http://cassandra.apache.org/doc/latest/operating/security.html?highlight=authenticator#authentication</a></li>
<li><a href=""https://docs.datastax.com/en/cassandra/3.0/cassandra/configuration/secureConfigNativeAuth.html"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/cassandra/3.0/cassandra/configuration/secureConfigNativeAuth.html</a></li>
</ul>

<p>Later edit: since you need LDAP autentication you can use the one created by Instaclustr. Details - <a href=""https://www.instaclustr.com/apache-cassandra-ldap-authentication/"" rel=""nofollow noreferrer"">Apache Cassandra LDAP Authentication</a> and the <a href=""https://github.com/instaclustr/cassandra-ldap"" rel=""nofollow noreferrer"">source code</a>.</p>
",['authenticator']
54358688,54368761,2019-01-25 03:47:04,Cassandra collection tombstones,"<p>I have created a table with a collection. Inserted a record and took sstabledump of it and seeing there is range tombstone for it in the sstable. Does this tombstone ever get removed? Also when I run sstablemetadata on the only sstable, it shows ""Estimated droppable tombstones"" as 0.5"", Similarly it shows one record with epoch time as insert time for - ""Estimated tombstone drop times: 1548384720:         1"". Does it mean that when I do sstablemetadata on a table having collections, the estimated droppable tombstone ratio and drop times values are not true and dependable values due to collection/list range tombstones?</p>

<pre><code>CREATE TABLE ks.nmtest (
    reservation_id text,
    order_id text,
    c1 int,
    order_details map&lt;text, text&gt;,
    PRIMARY KEY (reservation_id, order_id)
) WITH CLUSTERING ORDER BY (order_id ASC)

user@cqlsh:ks&gt; insert into nmtest (reservation_id , order_id , c1, order_details ) values('3','3',3,{'key':'value'});
user@cqlsh:ks&gt; select * from nmtest ;
 reservation_id | order_id | c1 | order_details
----------------+----------+----+------------------
              3 |        3 |  3 | {'key': 'value'}
(1 rows)

[root@localhost nmtest-e1302500201d11e983bb693c02c04c62]# sstabledump mc-5-big-Data.db 
WARN  02:52:19,596 memtable_cleanup_threshold has been deprecated and should be removed from cassandra.yaml
[
  {
    ""partition"" : {
      ""key"" : [ ""3"" ],
      ""position"" : 0
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 41,
        ""clustering"" : [ ""3"" ],
        ""liveness_info"" : { ""tstamp"" : ""2019-01-25T02:51:13.574409Z"" },
        ""cells"" : [
          { ""name"" : ""c1"", ""value"" : 3 },
          { ""name"" : ""order_details"", ""deletion_info"" : { ""marked_deleted"" : ""2019-01-25T02:51:13.574408Z"", ""local_delete_time"" : ""2019-01-25T02:51:13Z"" } },
          { ""name"" : ""order_details"", ""path"" : [ ""key"" ], ""value"" : ""value"" }
        ]
      }
    ]
  }

SSTable: /data/data/ks/nmtest-e1302500201d11e983bb693c02c04c62/mc-5-big
Partitioner: org.apache.cassandra.dht.Murmur3Partitioner
Bloom Filter FP chance: 0.010000
Minimum timestamp: 1548384673574408
Maximum timestamp: 1548384673574409
SSTable min local deletion time: 1548384673
SSTable max local deletion time: 2147483647
Compressor: org.apache.cassandra.io.compress.LZ4Compressor
Compression ratio: 1.0714285714285714
TTL min: 0
TTL max: 0
First token: -155496620801056360 (key=3)
Last token: -155496620801056360 (key=3)
minClustringValues: [3]
maxClustringValues: [3]
Estimated droppable tombstones: 0.5
SSTable Level: 0
Repaired at: 0
Replay positions covered: {CommitLogPosition(segmentId=1548382769966, position=6243201)=CommitLogPosition(segmentId=1548382769966, position=6433666)}
totalColumnsSet: 2
totalRows: 1
Estimated tombstone drop times:
1548384720:         1
</code></pre>

<p>Another quuestion was on the nodetool tablestats output - what does slice refer to in cassandra?</p>

<pre><code>    Average live cells per slice (last five minutes): 1.0
    Maximum live cells per slice (last five minutes): 1
    Average tombstones per slice (last five minutes): 1.0
    Maximum tombstones per slice (last five minutes): 1
    Dropped Mutations: 0
</code></pre>
",<collections><cassandra>,"<p>sstablemetadata does not have the information about your table that is not held within the sstable as it is not guaranteed to be run on system that has Cassandra running, and even if it was its very complex to be able to know how to pull the schema information from it.</p>

<p>Since the <code>gc_grace_seconds</code> is a table parameter and not in the metadata it defaults to assuming a <code>0</code> gc grace so the droppable times listed in that histogram will be more a histogram of the tombstone creation times by default. If you know your gc grace you can add it as a <code>-g</code> parameter to your sstablemetadata call. like:</p>

<pre><code>sstablemetadata -g 864000 mc-5-big-Data.db
</code></pre>

<p>see <a href=""http://cassandra.apache.org/doc/latest/tools/sstable/sstablemetadata.html"" rel=""nofollow noreferrer"">http://cassandra.apache.org/doc/latest/tools/sstable/sstablemetadata.html</a> for information on the tools output.</p>

<p>With collections it's just normal range tombstone with all that it entails. They are used to prevent the requirement of a read-before-write when overwriting the value of a multicell collection.</p>
",['table']
54435081,54442696,2019-01-30 07:12:20,how many partition is suitable per each node in cassandra,"<p>I'm testing cassandra.
before setting my cassandra nodes at server. </p>

<p><strong>I wonder how many partitions are suitable per each node .</strong></p>

<ul>
<li>i'm planning to save 0~18000000 data per each partition.</li>
<li>Q: partitions?????? / 1node ==> stable</li>
</ul>

<pre>
    CREATE TABLE if not exists  access_log
 (  time_boundary int, --yyyymmddhh
    user_id TEXT,  --user01
    timestamp bigint, --yyyymmddhhmmssms
    url TEXT, -- /file/file/blabla~~
    menu_id TEXT, 
    ip TEXT,
    params map, -- 
    PRIMARY KEY((time_boundary),user_id,timestamp)
 ) ;
</pre>
",<cassandra><nosql><partition>,"<p>I consider a safe rule of thumb (depends heavily on schema and write rates) to be about 1-4 billion partitions within a table (per node) you start to get issues with repair over streaming with the <code>2^20</code> max merkle tree depth.</p>

<p>With incremental repair you can go higher (only recommended on latest 3.11.x, and even there test heavily), providing you are using LeveledCompactionStrategy. With SizeTieredCompactionStrategy <code>ALTER TABLE yourtable WITH min_index_interval = 4096 and max_index_interval = 4096</code> (or higher if you see issues) to prevent an OOM when the compactions preallocate the index summaries.</p>
",['table']
54637593,54673425,2019-02-11 19:18:14,What should I try when cassandra appears to be running and listening but cqlsh can't connect?,"<p>I'm new to Cassandra and was trying it out. Despite Cassandra apparently running and listening (according to <code>lsof</code>) I can't connect to it. <code>sudo systemctl status cassandr</code> also reports <code>active (running)</code>.</p>

<pre><code>$ sudo lsof -Pnl +M -i4 
COMMAND  PID     USER   FD   TYPE DEVICE SIZE/OFF NODE NAME
(omitting unrelated services - nothing here about cassandra)

$ sudo lsof -Pnl +M -i6
COMMAND   PID     USER   FD   TYPE DEVICE SIZE/OFF NODE NAME
sshd      752        0    4u  IPv6  16234      0t0  TCP *:22 (LISTEN)
java    23659      300   68u  IPv6 158110      0t0  TCP 127.0.0.1:7199 (LISTEN)
java    23659      300   69u  IPv6 158122      0t0  TCP 127.0.0.1:36921 (LISTEN)
java    23659      300   84u  IPv6 158212      0t0  TCP 127.0.0.1:9160 (LISTEN)
java    23659      300  151u  IPv6 158205      0t0  TCP 127.0.0.1:7000 (LISTEN)



$ sudo nodetool status
Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address    Load       Owns (effective)  Host ID                               Token                                    Rack
UN  127.0.0.1  125.98 KiB  100.0%            e0bbc831-8686-43a6-b99c-8ea5596c8581  3840369556391221198                      rack1
</code></pre>

<p>For Nix users who might be reading this (also easy to grasp for non-Nix users), my config is just:</p>

<pre><code>  services.cassandra = {
    enable = true;
    listenAddress = ""127.0.0.1"";
    rpcAddress = ""127.0.0.1"";
  };
</code></pre>

<p>The error I get is:</p>

<pre><code>$ cqlsh 127.0.0.1 9160
Connection error: ('Unable to connect to any servers', {'127.0.0.1': error(111, ""Tried connecting to [('127.0.0.1', 9160)]. Last error: Connection refused"")})
</code></pre>
",<cassandra><nixos>,"<p>I should have checked on the nixpks issue tracker, as I found the <a href=""https://github.com/NixOS/nixpkgs/issues/50954#issuecomment-441270430"" rel=""nofollow noreferrer"">answer there</a>; I needed:</p>

<pre><code>extraConfig = {
  start_native_transport = true;
};
</code></pre>
",['start_native_transport']
54638630,54646072,2019-02-11 20:29:35,How can I order my results by a secondary index or column in Cassandra?,"<p>I am looking to build a table that will allow me to filter rows by timestamp and region. Here is what I have below:  </p>

<pre><code>  CREATE TABLE event_start(  
        user_id text,  
        ts timestamp,  
        region text,  
        PRIMARY KEY(ts, region)  
    )  
    WITH CLUSTERING ORDER BY(region DESC);
</code></pre>

<p>When I try and select and order the results by region, I run into ""ORDER BY is only supported when the partition key is restricted by an EQ or an IN.""
I think I understand the error message, but I am having trouble conceiving of a solution. Would this be related to the primary key vs the clustering key?</p>

<p>Thanks!</p>
",<cassandra><nosql><sql-order-by>,"<p>You can't directly query on this table by clustering column.
You must have to provide Partition key(ts).</p>

<p>You can query without partition key – only by clustering columns:
BUT it causes Cassandra to scan ALL partitions!!!!
For this you need to append <strong>ALLOW FILTERING</strong> in your query.</p>

<p><strong>Below is the Querying Order:</strong></p>

<ol>
<li>First – must provide a partition key </li>
<li>Clustering column(s) can follow after </li>
<li>it Can perform either equality(=) or range queries on clustering columns </li>
<li>All equality comparison must come before inequality comparison </li>
<li>Range searches are a binary search followed by a linear read</li>
</ol>

<p>You can revert your key columns to satisfy your query:</p>

<pre><code>CREATE TABLE event_start(  
      user_id text,  
      ts timestamp,  
      region text,  
      PRIMARY KEY(region, ts)  
  );
</code></pre>
",['table']
54661388,54663562,2019-02-13 02:02:08,querying and deleting a chat inbox system using cassandara,"<p>I'm making a chat application using Cassandra. I have a table called ""chats"" where I store the chatid,userid and timestamp. I have another table where I store the messages . (chatid , messageid,message).  When ever a new message comes in for a chatid I want to update the timestamp value in the ""chats"" table for that chatid. I want to use    CLUSTERING ORDER BY on the timestamp col. When a user needs to get all of their chats I query this table using userid to get all the chats in order by timestamp. The problem i'm having is I'm not able to delete a row form the ""chats"" table using userid and chatid. If I make userid and chatid Primary key I am not able to use just the userid to get the all chats for a user. How can I go about solving my problem.</p>
",<database><cassandra>,"<p>In your table chats:</p>

<p>You should create userid as Partition key. chatid &amp; timestamp should be clustering column.
Please see below schema:</p>

<pre><code>CREATE TABLE chats (    
    userid text,
    chatid text,
    timestamp timestamp,
    PRIMARY KEY ((userid), chatid, timestamp)
) WITH CLUSTERING ORDER BY (userid ASC, timestamp DESC );
</code></pre>

<p>Now you can only delete by userid, ALso If you want to delete a particular chat you must be knowing user of that chat. so you can put both while deleting by chatid.</p>

<p>Now If you want to use CLUSTERING ORDER BY on the timestamp col then it must be a clustering column as created above.</p>

<p>In case you only want to update timestamp in a row is not possible directly, As it will insert a new row. So you need to manage your code by:  </p>

<p>First select by userid &amp; chatid.<br>
if exists then delete it &amp; insert a new entry with updated timestamp.</p>
",['table']
54734959,54735935,2019-02-17 15:56:14,Cassandra select distinct and order by cqlsh,"<p>I am new to Cassandra and to this forum. I' m executing Cassandra query using cqlsh but I don't know how to do execute a query like sql <code>select distinct a, b, c from table order by d asc</code> using Cassandra. How can I do? What would be the structure of the table?</p>
",<cassandra><sql-order-by><distinct>,"<p>Your <code>primary key</code> consists of <code>partition keys</code> and <code>clustering columns</code>.</p>

<ul>
<li>DISTINCT queries must only request partition keys.</li>
<li>ORDER BY is supported on clustered columns.</li>
</ul>

<p>Suppose we have a sample table like following,</p>

<pre><code>CREATE TABLE Sample ( 
 field1 text,
 field2 text,
 field3 text,
 field4 text,
 PRIMARY KEY ((field1, field2), field3));
</code></pre>

<p>DISTINCT requires all the parition keys to be passed comma separated.</p>

<p>So you can't run this query <code>select distinct field1 from Sample;</code>. A valid expression would be <code>select distinct field1, field2 from Sample;</code>.</p>

<p>It internally hits all the nodes in the cluster to find all the partition keys so if you have millions of partitions in you table, I would expect a performance drop with multiple nodes.</p>

<p>By default, records will be in ascending order for field3. Below query will provide records in descending order of field3.</p>

<pre><code>select * from Sample where field1 = 'a' and field2 = 'b' order by field3 desc;
</code></pre>

<p>If you already know your query patterns and the way you would require data to be ordered, you can design the table in that way. Suppose you always require records in descending order for field3 , you could have designed your table in this way.</p>

<pre><code>CREATE TABLE Sample ( 
 field1 text,
 field2 text,
 field3 text,
 field4 text,
 PRIMARY KEY ((field1, field2), field3))
WITH CLUSTERING ORDER BY (field3 DESC);
</code></pre>

<p>Now querying without order by will result in the same result.</p>

<p>You can use order by with multiple clustered columns. But you can't skip the order. To understand that let's have a sample table like below,</p>

<pre><code>CREATE TABLE Sample1 ( 
 field1 text,
 field2 text,
 field3 text,
 field4 int,
 field5 int,
 PRIMARY KEY ((field1, field2), field3, field4));
</code></pre>

<p>I added few dummy records.
<a href=""https://i.stack.imgur.com/t9d6y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/t9d6y.png"" alt=""enter image description here""></a></p>

<p>You may use order by multiple columns like this <code>select * from Sample1 where field1 = 'a' and field2 = 'b' order by field3 desc, field4 desc;</code></p>

<p>NOTE : All fields need to be either in positive order (<code>field3 asc, field4 asc</code>) or negative order (<code>field3 desc, field4 desc</code>). You can't do (<code>field3 asc, field4 desc</code>) or vice versa.</p>

<p>Above query will result in this.
<a href=""https://i.stack.imgur.com/psbUX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/psbUX.png"" alt=""enter image description here""></a>  </p>

<p>By writing we can't skip the order in order by, I meant we cant do something like <code>select * from Sample1 where field1 = 'a' and field2 = 'b' order by field4 desc;</code></p>

<p>I hope this helps !</p>
",['table']
54787579,54800860,2019-02-20 13:34:03,ReadTimeout: Error from server: code=1200 [Coordinator node timed out waiting for replica nodes,"<p>I am getting below error on,</p>

<pre><code>ReadTimeout: Error from server: code=1200 [Coordinator node timed out waiting for replica nodes' responses] message=""Operation timed out - received only 0 responses."" info={'received_responses': 0, 'required_responses': 1, 'consistency': 'ONE'}
</code></pre>

<p>version details, [cqlsh 5.0.1 | Cassandra 3.11.4 | CQL spec 3.4.4 | Native protocol v4]</p>

<pre><code>cqlsh:infinito&gt; select count(id) from list_subscriber;
</code></pre>

<p>This table contains only 10 lacks of records, and primary key is on 'Id' column only having int type.</p>

<p>I am trying to increase some timeout params <code>(request_timeout_in_ms:)</code> but no luck</p>

<p>Any help is appreciated.</p>
",<cassandra><cassandra-3.0><cqlsh>,"<p>I ended up doing below options, increased below options value by 10 times.</p>

<p>edited /etc/cassandra/cassandra.yaml file</p>

<pre><code>sudo nano /etc/cassandra/cassandra.yaml

# How long the coordinator should wait for read operations to complete
read_request_timeout_in_ms: 50000
# How long the coordinator should wait for seq or index scans to complete
range_request_timeout_in_ms: 100000
# How long the coordinator should wait for writes to complete
write_request_timeout_in_ms: 20000
# How long the coordinator should wait for counter writes to complete
counter_write_request_timeout_in_ms: 50000
# How long a coordinator should continue to retry a CAS operation
# that contends with other proposals for the same row
cas_contention_timeout_in_ms: 10000
# How long the coordinator should wait for truncates to complete
# (This can be much longer, because unless auto_snapshot is disabled
# we need to flush first so we can snapshot before removing the data.)
truncate_request_timeout_in_ms: 600000
# The default timeout for other, miscellaneous operations
request_timeout_in_ms: 100000

# How long before a node logs slow queries. Select queries that take longer than
# this timeout to execute, will generate an aggregated log message, so that slow queries
# can be identified. Set this value to zero to disable slow query logging.
slow_query_log_timeout_in_ms: 5000
</code></pre>

<p>And then opened terminal and executed below command</p>

<pre><code>cqlsh --request-timeout=6000
</code></pre>

<p>Everything looks ok.</p>
",['auto_snapshot']
54899204,54899665,2019-02-27 06:18:18,Cassandra If clause conditon from different table,"<p>i have to Update /Insert data in shopping_cart table only if some quantity is present in  inventory table in cassandra , and this needs to be atomic operation as the invetory table is being updated frequently , I was trying to use a light weight  transaction some thing like below </p>

<pre><code>update shopping_cart
set 
quantity=1
where
item='item1'
if
 (select quantity from inventory where item='item1') = 2;
</code></pre>

<p>but i am getting an error </p>

<pre><code>mismatch input '(' expecting K_NOT 
</code></pre>

<p>probably light weight transaction is not the best way to do this also i think that if clause would not be supporting query from different table in cassandra .</p>

<p>so what would be the best way to achive the above operation in cassandra without compromising on atomicty .</p>
",<cassandra><atomic>,"<p>Above type of a query wont be possible - two options I can think of,</p>

<ol>
<li>Keep the quantity in the shopping_cart table also when you want to update the quantity use a batch statement to update in both tables.</li>
<li>Handling this in the application itself but will have more pain.</li>
</ol>
",['table']
54938850,55132608,2019-03-01 06:06:52,Configuration for cassandra with rac and vnodes in single data center,"<p>I was wondering which configuration will be best suited for even distribution of data among nodes.</p>

<ol>
<li>5 nodes with 3 racs (2 nodes(node 1,node4) on rac1 , 2 nodes on rac2 (node2,node4) , 1 node on rac3 (node3))
Replication factor 3 and Read / Write on Quorum</li>
</ol>

<p>In this case I am wondering whether the node3 which is the only node in rac3 will have more data than other nodes since replication strategy suggests that replicas will be but in nodes on different rac.</p>

<ol start=""2"">
<li>6 nodes with 3 racs (2 nodes(node 1,node4) on rac1 , 2 nodes on rac2 (node2,node4) , 2 nodes on rac3 (node3, node6)) 
Replication factor 3 and Read / Write on Quorum</li>
</ol>

<p>In this case data will be distributed equally among all nodes.</p>

<p>Want to know whether my understanding is correct or not?</p>
",<cassandra>,"<p>In the case of 5 nodes across 3 racks, yes, one node will be under greater load/stress.</p>

<p>It's a good idea to scale the cluster in multiples of the rack count to keep the data balanced across nodes. For example, in a 3 rack cluster you should add 3 nodes each time you expand the cluster.</p>

<p>If you choose to use multiple racks the ideal rack count should be ≥ your chosen <em>replication factor</em>. This allows Cassandra to store each replica in a separate rack.</p>

<p>In the case of a rack outage the other replicas would be still available.</p>

<p>For example, with RF=3 and 3 racks and queries at <code>QUORUM</code>, you can sustain the failure of a single rack. Whereas, with RF=3 and 2 racks at <code>QUORUM</code>, there is no guarantee that 2 replicas will still be available in the case of a rack failure.</p>

<p>Racks are for informing Cassandra about fault domains. If your running in your own data center, as the name implies, racks should be assigned based on the rack the node is located in. If you're running in the cloud, the best option is to map racks to AWS <em>availability zones</em> (or whatever is equivalent for your provider).</p>
",['rack']
54942614,55124564,2019-03-01 10:24:45,Invalid Column name Error in DSE Analytics Spark,"<p>I have one table whose structure roughly is as follows -></p>

<pre><code>CREATE TABLE keyspace_name.table_name (
  id text PRIMARY KEY,
  type text,
  bool_yn boolean,
  created_ts timestamp,
  modified_ts timestamp
)
</code></pre>

<p>Recently I added new column in the table -></p>

<pre><code>alter table keyspace_name.table_name first_name text;
</code></pre>

<p>And when I query on the given column from table in cqlsh, it gives me the result. For eg.</p>

<pre><code>select first_name from keyspace_name.table_name limit 10;
</code></pre>

<p>But if I try to perform the same query in dse spark-sql</p>

<p>It is giving me the following error.</p>

<blockquote>
  <p>Error in query: cannot resolve '<code>first_name</code>' given input columns: [id, type, bool_yn, created_ts, modified_ts]; </p>
</blockquote>

<p>I don't know what's wrong in spark-sql. I've tried nodetool repair but problem still persists</p>

<p>Any help would be appreciated. Thanks</p>
",<apache-spark><cassandra><apache-spark-sql><datastax><spark-cassandra-connector>,"<p>If table schema changes, the Spark metastore doesn't automatically refresh the schema changes, so manually remove the old tables from spark sql with a <code>DROP TABLE</code> command, then run <code>SHOW TABLES</code>. The new table with latest schema will be automatically created. This will not change the data in Cassandra.</p>
",['table']
54983225,54985822,2019-03-04 12:24:37,Group by on Primary Partition,"<p>I am not able to perform Group by on a primary partition. I am using Cassandra 3.10. When I group by I get the following error.
<code>InvalidReqeust: Error from server: code=2200 [Invalid query] message=""Group by currently only support groups of columns following their declared order in the Primary Key</code>. My column is a primary key even still I am facing the problem.</p>

<p>My schema is</p>

<pre><code>Table trends{
name text,
price int,
quantity int,
code text,
code_name text,
cluster_id text
uitime timeuuid,
primary key((name,price),code,uitime))
with clustering order by (code DESC, uitime DESC)
</code></pre>

<p>And the command that I run is: <code>select sum(quantity) from trends group by code;</code></p>
",<cassandra>,"<p>For starters your schema is invalid. You cannot set clustering order on code because it is the partition key. The order is going to be determined by the hash of it (unless using byte order partitioner - but don't do that).</p>

<p>The query and thing your talking about does work though. For example you can run</p>

<pre><code>&gt; SELECT keyspace_name, sum(partitions_count) AS approx_partitions FROM system.size_estimates GROUP BY keyspace_name;

 keyspace_name      | approx_partitions
--------------------+-------------------
        system_auth |               128
              basic |           4936508
          keyspace1 |               870
 system_distributed |                 0
      system_traces |                 0
</code></pre>

<p>where they schema is:</p>

<pre><code>CREATE TABLE system.size_estimates (
    keyspace_name text,
    table_name text,
    range_start text,
    range_end text,
    mean_partition_size bigint,
    partitions_count bigint,
    PRIMARY KEY ((keyspace_name), table_name, range_start, range_end)
) WITH CLUSTERING ORDER BY (table_name ASC, range_start ASC, range_end ASC)
</code></pre>

<p>Perhaps the pseudo-schema you provided differs from the actual one. Can you provide output of <code>describe table xxxxx</code> in your question?</p>
","['partitioner', 'table']"
55070975,55073990,2019-03-08 21:03:54,Slow Query Logger Cassandra on server side in Cassandra 3.11.4,"<p>As per </p>

<p><a href=""https://issues.apache.org/jira/browse/CASSANDRA-12403"" rel=""nofollow noreferrer"">https://issues.apache.org/jira/browse/CASSANDRA-12403</a></p>

<p>I tried to enable slow query logging with below steps on Cassandra 3.11.4 ,
but not able to query them any idea what is the issue, or am i missing any step .</p>

<pre><code>`/nodetool getlogginglevels
 Logger Name                                        Log Level
 ROOT                                                    INFO
 com.thinkaurelius.thrift                               ERROR
 org.apache.cassandra                                   DEBUG
 org.apache.cassandra.db                                DEBUG
 org.apache.cassandra.db.monitoring                     DEBUG


In cassandra.yaml 
</code></pre>

<p><code># can be identified. Set this value to zero to disable slow query logging.
   slow_query_log_timeout_in_ms: 500</code></p>

<p><code>cat logback.xml</code>
   [deafult logback xml][1]</p>

<pre><code>[1]: https://github.com/apache/cassandra/blob/trunk/conf/logback.xml
</code></pre>
",<cassandra><cassandra-2.0><cassandra-3.0><spark-cassandra-connector>,"<p>Step is correct, you should check system.log for any logged slow queries. Also, you need to check system_traces keyspace where session and event table created. </p>
",['table']
55095992,55523339,2019-03-11 05:59:51,Cassandra: How to use 'ORDER BY' without PRIMARY KEY restricted by EQ or IN?,"<p>I have a table in Scylla (a Cassandra compatible database) defined as the following:</p>

<pre><code>create table s.items (time timeuuid, name text, primary key (time));
</code></pre>

<p>I want to run a query that gets all items after a certain time, similar to the following:</p>

<pre><code>select * from s.items where time&gt;7e204790-43bf-11e9-9759-000000000004 order by time asc;
</code></pre>

<p>But I am told that <code>ORDER BY is only supported when the partition key is restricted by an EQ or an IN.</code> To get around this I can make a table and query similar to the following:</p>

<pre><code>create table s.items (yes boolean, time timeuuid, name text, primary key (yes, time));

select * from s.items where yes=true and time&gt;7e204790-43bf-11e9-9759-000000000004 order by time asc;
</code></pre>

<p>While this works, it doesn't seem like the best solution. As I'm fairly new to Scylla and CQL, is there a better/proper way to do this?</p>
",<database><cassandra><cql><scylla>,"<p>Your solution of adding that one boolean key and always setting it to yes basically creates one huge partition with all your data. This is rarely what you really want. If this one partition is your entire data, it means that even if you have a 10-node cluster with 8 CPUs on each node, just 3 CPUs out of all 80 in your cluster will be doing any work (because each partition belongs to a certain CPU, and with RF=3 there are three replicas).</p>

<p>If you're wondering why your original solution didn't work and Scylla refused the ""ORDER BY"", well, the problem is that although Scylla can scan the entire table to look for entries after time X (you'll need to add 'ALLOW FILTERING' to the query), it has no efficient way to <strong>sort</strong> what it finds by time. Internally, the different partitions are not sorted by the partition key, but rather by a ""token"", a hash function of the the partition key. This hashing with its randomizing effect is important to balance the load between all CPUs on the cluster, but prevents Scylla (or Cassandra) from reading the partitions in the original key order.</p>

<p>One thing you can do is to do what Alex suggested above, which is a middle-ground between your original setup and your proposed solution: Don't have one item per partition, or all the items in a single partition, but something in the middle: For example, imagine that in your workload, every day you collect 100MB of data. So you use the day number as the partition key (instead of your bool). All the data of one particular day will sit in one partition. <strong>Inside</strong> each day's partition, the different entries (rows) will be sorted by the clustering-key order, which will be time. With this setup, to retrieve all the items after some specific day, just start querying each individual day, one by one. E.g., query day 134, then day 135, they 136, then, etc... Inside each day, the results will be already sorted. So problem solved.</p>

<p>This technique is a fairly well-known ""time series"" data modeling. Scylla (and Cassandra) even have a special compaction strategy tuned for this modeling, TWCS (time-window compaction strategy).</p>
",['table']
55158639,55160046,2019-03-14 09:10:28,How do I create a table using the Spark Cassandra Connector?,"<p>I've recently begun using the Spark Cassandra Connector and I've manually created my table and been able to save data. Here's a simplified snippet from the docs:</p>

<pre><code>CREATE TABLE test.words (word text PRIMARY KEY, count int);
</code></pre>

<pre><code>val collection = sc.parallelize(Seq((""cat"", 30), (""fox"", 40)))
collection.saveToCassandra(""test"", ""words"", SomeColumns(""word"", ""count""))
</code></pre>

<p>Is there a way to create tables programmatically by inferring the schema from case classes without actually writing the raw queries? </p>
",<scala><apache-spark><cassandra><spark-cassandra-connector>,"<p>Yes, you can do this with <code>saveAsCassandraTable</code> and <code>saveAsCassandraTableEx</code> as <a href=""https://github.com/datastax/spark-cassandra-connector/blob/master/doc/5_saving.md#saving-rdds-as-new-tables"" rel=""nofollow noreferrer"">described in documentation</a>. The first function will create table automatically, based on your data (be careful that it will take one column as a partition key).  The second function will allow your to customize schema by specifying partition key, clustering columns, etc., like this (code is from documentation):</p>

<pre><code>val p1Col = new ColumnDef(""col1"",PartitionKeyColumn,UUIDType)
val c1Col = new ColumnDef(""col2"",ClusteringColumn(0),UUIDType)
val c2Col = new ColumnDef(""col3"",ClusteringColumn(1),DoubleType)
val rCol = new ColumnDef(""col4"",RegularColumn,IntType)

// Create table definition
val table = TableDef(""test"",""words"",Seq(p1Col),Seq(c1Col, c2Col),Seq(rCol))

// Map rdd into custom data structure and create table
val rddOut = rdd.map(s =&gt; outData(s._1, s._2(0), s._2(1), s._3))
rddOut.saveAsCassandraTableEx(table, SomeColumns(""col1"", ""col2"", ""col3"", ""col4""))
</code></pre>
",['table']
55162505,55176109,2019-03-14 12:26:06,Can I write a program to see if a table exists in YugaByte's YCQL (Cassandra) api?,"<p>Is there a programmatic way to check if a table exists in YugaByte's YCQL (Cassandra) api?</p>

<p>For example, in Postgres one can do something like:</p>

<p><a href=""https://stackoverflow.com/questions/20582500/how-to-check-if-a-table-exists-in-a-given-schema"">How to check if a table exists in a given schema</a></p>

<pre><code>SELECT EXISTS (
   SELECT 1
   FROM   information_schema.tables 
   WHERE  table_schema = 'schema_name'
   AND    table_name = 'table_name'
   );
</code></pre>

<p>Is there an equivalent in YCQL?""</p>
",<database><cassandra><distributed-database><yugabytedb>,"<p>Yes, you can do the same for YugaByte DB's YCQL. Here's an example that shows how to check for the existence of a keyspace and that of a table via cqlsh.</p>

<p>Setup:</p>

<pre><code>cqlsh&gt; CREATE KEYSPACE IF NOT EXISTS ksp;

cqlsh&gt; CREATE TABLE IF NOT EXISTS ksp.t(k int PRIMARY KEY, v int);
</code></pre>

<p>To check if a keyspace exists</p>

<pre><code>cqlsh&gt; select count(*) from system_schema.keyspaces 
       where keyspace_name = 'ksp';


 count
-------
     1

(1 rows)
cqlsh&gt; select count(*) from system_schema.keyspaces 
       where keyspace_name = 'non-existent-ksp';

 count
-------
     0

(1 rows)
</code></pre>

<p>To check if a table exists</p>

<pre><code>cqlsh&gt; select count(*) from system_schema.tables 
       where keyspace_name = 'ksp' and table_name = 't';

 count
-------
     1

(1 rows)
cqlsh&gt; select count(*) from system_schema.tables 
        where keyspace_name = 'ksp' and table_name = 'non-existent-t';

 count
-------
     0

(1 rows)
</code></pre>
",['table']
55243714,55445088,2019-03-19 14:46:27,Cassandra-stress : how to install and set it up outside cassandra cluster,"<p>I am about to use simple cassnadra cluster (3 nodes, x.x.x.104-106). I'm using CentOS7, so i used datastax repository, Cassandra 3.0. 
I read on forum, it is better to install the cassandra-stress outside the cluster, otherwise it consumes CPU of the node.</p>

<p>Could you please help me, how to install it? </p>

<p>I tried to copied cassandra-stress.sh separately, but it is dependent on some cassandra files (probably created during installation). </p>

<p>So I decided to install whole Cassandra on separate server, in the same network space. Now, I'm struggling with the correct setup, how to run cassandra-stress tool against the cassandra cluster.</p>

<p>In cassandra.yaml I setup Cassandra name, listen_adress to public_ip, rpc_address to loopback address, I set seeds to cassandra cluster nodes (x.x.x.104-106)... but in general it does not make sense to set it up, since I dont wan't create another node in the Cassandra cluster.</p>

<p>Could you please help me?</p>

<p>Edit: Maybe using something like this might be the correct way?</p>

<p>cassandra-stress user profile=/usr/cassandra/stress-file.yaml ops(insert=1,books=1) n=10000 -node x.x.x.104,x.x.x.105,x.x.x.106 -port native= ?
Telnet [cassandra_node_ip_ddress] 7000 works fine</p>
",<cassandra><cassandra-3.0><cassandra-stress>,"<p>on every node:
in cassandra.yaml set rpc_address to IP address
in cassanda-env.sh set LOCAL_JMX=no and jmx options autenticate=false 
open firewall port 7199
restart firewall and cassandra</p>

<p>on cassandra-stress server:</p>

<pre><code>cassandra-stress user profile=/usr/cassandra/stress-books.yaml ops\ 
(insert=1,books=1\) 
n=10000 -node 172.16.20.104,172.16.20.105,172.16.20.106 -port native=9042 
thrift=9160 jmx=7199
</code></pre>

<p>Note! JMX communication is not secured</p>
",['rpc_address']
55259169,55259654,2019-03-20 11:00:15,Spark get ttl column from cassandra,"<p>I am trying to get the ttl column from cassandra, but so far I couldn't make it work.</p>

<p>Here is what I tried so far:</p>

<pre><code>SparkSession sparkSession = SparkSession.builder()
        .appName(""Spark Sql Job"").master(""local[*]"")
        .config(""spark.sql.warehouse.dir"", ""file:///c:/tmp/spark-warehouse"")
        .config(""spark.cassandra.connection.host"", ""localhost"")
        .config(""spark.cassandra.connection.port"", ""9042"")
        .getOrCreate();

SQLContext sqlCtx = sparkSession.sqlContext(); 

Dataset&lt;Row&gt; rowsDataset = sqlCtx.read()
        .format(""org.apache.spark.sql.cassandra"")
        .option(""keyspace"", ""myschema"")
        .option(""table"", ""mytable"").load();

rowsDataset.createOrReplaceTempView(""xyz"");   
rowsDataset = sparkSession.sql(""select ttl(emp_phone) from vouchers"");   
rowsDataset.show();
</code></pre>
",<java><apache-spark><cassandra>,"<p>From spark-cassandra-connector doc:</p>
<blockquote>
<p>The select method allows querying for TTL and timestamp of the table cell.</p>
<p>Example Using Select to Retreive TTL and Timestamp</p>
</blockquote>
<pre><code>val row = rdd.select(&quot;column&quot;, &quot;column&quot;.ttl, &quot;column&quot;.writeTime).first
val ttl = row.getLong(&quot;ttl(column)&quot;)
val timestamp = row.getLong(&quot;writetime(column)&quot;)       
</code></pre>
<blockquote>
<p>The selected columns can be given aliases by calling as on the column
selector, which is particularly handy when fetching TTLs and
timestamps.</p>
</blockquote>
<p><a href=""https://github.com/datastax/spark-cassandra-connector/blob/master/doc/3_selection.md"" rel=""nofollow noreferrer"">https://github.com/datastax/spark-cassandra-connector/blob/master/doc/3_selection.md</a></p>
",['table']
55280002,55290377,2019-03-21 12:02:09,Cassandra update previous rows after insert,"<p>I have this schema in cassandra:</p>

<pre><code>create table if not exists 
converstation_events(
    timestamp timestamp, 
    sender_id bigint, 
    conversation_id bigint, 
    message_type varchar, 
    message text, 
    primary key ((conversation_id), sender_id, message_type, timestamp));
</code></pre>

<p>And there is a message_type with value conversation_ended, is there a way to denormalise the data so I can do queries on those conversations that have already ended? </p>

<p>I've thought about having an extra field that can be updated by a trigger when a conversation_ended message hits the system, does this make sense?</p>
",<cassandra><data-modeling><cqlsh>,"<p>In Cassandra you need to model your data in a way the answers your questions.  It's not like a RDBMS where you create you model first then create your queries.  So think backwards...</p>

<p>When you do a query in cassandra (for the most part...) you need to query by the primary key and you can use your clustering key(s) to filter or a select ranges. a <a href=""https://www.datastax.com/dev/blog/a-deep-look-to-the-cql-where-clause"" rel=""nofollow noreferrer"">great post</a> on it.</p>

<p>Your <code>converstation_events</code> table will give you answers about a conversation, filtering by sender, type and time.  ** if you want to filter by time you must include <code>sender_id</code> and <code>message_type</code> in the query.</p>

<p>But you want all conversations of a given type so you'll need another table to answer this query.  If you want all the conversation that are <code>conversation_ended</code> you could create a second table to map message type to conversation, like- </p>

<pre><code>conversation_by_message_type (
    message_type varchar, 
    conversation_id bigint, 
    timestamp timestamp, 
    primary key ((message_type), timestamp, conversation_id));
</code></pre>

<p>On the client side you'll have to add a record to <code>conversation_by_message_type</code> anytime you insert a converstation_events event with a given <code>message_type</code> that you might want to look up.  I have <code>timestamp</code> in this table so you can sort or filter by time or <code>time</code> and <code>conversation_id</code>.</p>

<p>To find all the ended conversations you could do queries like</p>

<pre><code>&lt;ids&gt; = select conversation_id from conversation_by_message_type where message_type = 'conversation_ended'

select * from conversation_events where conversation_id IN (&lt;ids&gt;)
</code></pre>
",['table']
55290883,55779027,2019-03-21 23:50:49,Invalid STRING constant error in Apache Cassandra (using Python),"<p>I'm new to Apache Cassandra (using Python 3) and I'm trying to create a table based on a csv file. Here's how the file looks like this one:
<a href=""https://i.stack.imgur.com/aYRS1.jpg"" rel=""noreferrer"">https://i.stack.imgur.com/aYRS1.jpg</a> (sorry but I don't have enough reputation points to post the image here)</p>

<p>First I create the table</p>

<pre><code>query1 = ""CREATE TABLE IF NOT EXISTS table1(artist text, title text, \
            length text, sessionId text, itemInSession text, PRIMARY KEY (sessionId, title, artist))""     

session.execute(query1)
</code></pre>

<p>And then I try to read the file and insert the desired data into the table:</p>

<pre><code>file = 'event_datafile_new.csv'

with open(file, encoding = 'utf8') as f:
    csvreader = csv.reader(f)
    next(csvreader) # skip header
    for line in csvreader:
        query = ""INSERT INTO table1(artist, title, length, sessionId, itemInSession)""
        query = query + ""VALUES(%s, %s, %s, %s, %s)""
        session.execute(query, (line[0], line[9], line[5], line[8], line[3]))
</code></pre>

<p>However, I get the follow error:</p>

<pre><code>---&gt; 13         session.execute(query, (line[0], line[9], line[5], line[8], line[3]))

/opt/conda/lib/python3.6/site-packages/cassandra/cluster.cpython-36m-x86_64-linux-gnu.so in cassandra.cluster.Session.execute (cassandra/cluster.c:38536)()

/opt/conda/lib/python3.6/site-packages/cassandra/cluster.cpython-36m-x86_64-linux-gnu.so in cassandra.cluster.ResponseFuture.result (cassandra/cluster.c:80834)()

InvalidRequest: Error from server: code=2200 [Invalid query] message=""Invalid STRING constant (288.9922) for ""length"" of type float""
</code></pre>

<p>Even when I tried changing the format of ""length"" to float - and %s to %f on the INSERT statement - it didn't workout. Does anyone know what might be the issue? Many thanks! :)</p>
",<python><python-3.x><cassandra>,"<p>Whenever you read from a file with csvreader: ""Each row read from the csv file is returned as a list of strings No automatic data type conversion is performed unless the QUOTE_NONNUMERIC format option is specified"" from: <a href=""https://docs.python.org/3/library/csv.html"" rel=""noreferrer"">https://docs.python.org/3/library/csv.html</a></p>

<p>with a table defined with types such as:</p>

<pre><code>""CREATE TABLE IF NOT EXISTS table1(artist text, title text, \
            length double, sessionId int, itemInSession int, PRIMARY KEY (sessionId, title, artist))""
</code></pre>

<p>If you cast your values to the correct type it should work work. I tried this and it worked.</p>

<pre><code>session.execute(query, (line[0], line[9], float(line[5]), int(line[8]), int(line[3])))
</code></pre>
",['table']
55412814,55415771,2019-03-29 07:55:04,Cassandra altering table to add new columns adds null as text,"<p>I have a cassandra table with data in it.</p>

<p>I add three new columns <code>country</code> as <code>text</code>, <code>lat</code> and <code>long</code> as <code>double</code>.</p>

<p>When these columns are added null values are inserted against the already present rows in the table. However, null is inserted as <code>text</code> in country column and <code>null</code> as value in lat and long columns.</p>

<p><a href=""https://i.stack.imgur.com/qnunv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qnunv.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/2jq0A.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2jq0A.png"" alt=""enter image description here""></a></p>

<p>Is this something that is the default behavior and can I add null as value under the newly created text columns?</p>
",<cassandra>,"<p>Cassandra uses <code>null</code> to show that value is missing, not that this is explicitly inserted.  In your case, when you add new columns - they are just added to table's specification stored in Cassandra itself - existing data (stored in SSTables) is not modified, so when Cassandra reads old data it doesn't find values for that columns in SSTable, and output null instead.</p>

<p>But you can have the same behavior without adding new columns - just don't insert value for specific <strong>regular</strong> column (you must have non-null values for columns of primary key!). For example:</p>

<pre><code>cqlsh&gt; create table test.abc (id int primary key, t1 text, t2 text);
cqlsh&gt; insert into test.abc (id, t1, t2) values (1, 't1-1', 't2-1');
cqlsh&gt; insert into test.abc (id, t1) values (2, 't1-2');
cqlsh&gt; insert into test.abc (id, t2) values (3, 't3-3');
cqlsh&gt; SELECT * from test.abc;

 id | t1   | t2
----+------+------
  1 | t1-1 | t2-1
  2 | t1-2 | null
  3 | null | t3-3

(3 rows)
</code></pre>
",['table']
55414679,55417963,2019-03-29 09:51:19,Insert nested json into cassandra,"<p>Im trying to insert this nested object into cassandra database, but cannot figure out how to design the table for this. I want all the data from this object to be stored in cassandra.</p>

<p>Below I pasted the json im trying to insert.</p>

<p>Any suggestions?</p>

<pre><code>{
  ""status"": ""success"",
  ""data"": {
    ""resultType"": ""vector"",
    ""result"": [
      {
        ""metric"": {
          ""__name__"": ""up"",
          ""env"": ""demosite"",
          ""instance"": ""localhost:9100"",
          ""job"": ""node""
        },
        ""value"": [
          1553849977.349,
          ""1""
        ]
      },
      {
        ""metric"": {
          ""__name__"": ""up"",
          ""instance"": ""ub-lab-server:9090"",
          ""job"": ""prometheus""
        },
        ""value"": [
          1553849977.349,
          ""1""
        ]
      }
    ]
  }
}
</code></pre>
",<python><cassandra>,"<p>There are multiple ways to do this.</p>

<p>If your use case is just to store the JSON as a string , just serialize the whole JSON as a blob into a single column
Create a table as : </p>

<pre><code>CREATE TABLE IF NOT EXISTS my_table(
  ID &lt;text/bigint&gt;
  DATA text,
  CREATEDATE timestamp
)
</code></pre>

<p>If you want to represent all the nested attributes as separate columns , you will have start from bottom up.
i.e first create <a href=""https://docs.datastax.com/en/cql/3.3/cql/cql_using/useCreateUDT.html"" rel=""nofollow noreferrer"">User Defined Types</a> for each nested field.
In reference to your json , an example would be : </p>

<pre><code>CREATE TYPE metric (
      name text,
      env text,
      instance text,
      job text
);

CREATE TYPE value(
      field1 text,
      field2 text
);
</code></pre>

<p>Once you have the bas UDTs created , start with creating Composite UDTs , referencing base UDTs : </p>

<pre><code>CREATE TYPE result(
  metric metric,
  value value,
);

CREATE TYPE data(
  resultType text,
  result set&lt;result&gt;,
);    
</code></pre>

<p>And then finally bootstrap your table as following : </p>

<pre><code>CREATE TABLE IF NOT EXISTS my_table (
  ID &lt;text/bigint&gt;
  status data
)
</code></pre>

<p>Be careful in deciding on your primary/partition keys based on your querying and storage
usecases.</p>
",['table']
55517211,55520062,2019-04-04 13:28:31,lost cassandra authentication user after adding a new node,"<p>When ı add a new node to my cassandra cluster, ı lost my authentication user. </p>

<p>update seeds and restart nodes it turns default user -u cassandra -p cassandra</p>

<p>ı lost my user which ı have already created before</p>
",<cassandra><cassandra-3.0>,"<p>I have seen this happen before.  What happens, is that the new node forces a token range re-calculation.  If your new node is also a <em>seed</em> node, this complicates matters as seed nodes <em>do not</em> bootstrap data, and must be populated via repair/rebuild.  Essentially, your pre-existing user is probably still there, but the node primarily responsible for its token in the <code>system_auth.roles</code> table has changed, but data movement has not occured.</p>

<p>First, double-check the replication strategy used on the <code>system_auth</code> keyspace.  By default, it is set to <code>{'class':'SimpleStrategy','replication_factor':'1'}</code> which is not sufficient (IMO) for anything other than local development.  I always recommend changing this to the <code>NetworkTopologyStrategy</code>, and then specify replication by data center.</p>

<p>With that complete, run a repair on each node:</p>

<p><code>nodetool repair system_auth -full</code></p>

<p>That should bring back your previous user.</p>

<p>Note: Instead of a full repair, you might be able to get away with querying each table in <code>system_auth</code> at consistency <code>ALL</code> (which forces a read repair):</p>

<pre><code>dba@cqlsh&gt; use system_auth;
dba@cqlsh:system_auth&gt; consistency ALL;
Consistency level set to ALL.
dba@cqlsh:system_auth&gt; SELECT COUNT(*) FROM roles;
dba@cqlsh:system_auth&gt; SELECT COUNT(*) FROM role_permissions;
dba@cqlsh:system_auth&gt; SELECT COUNT(*) FROM role_members;
dba@cqlsh:system_auth&gt; SELECT COUNT(*) FROM resource_role_permissons_index;
</code></pre>

<p>With either the full repair or read repair complete, your previous user should work again.</p>
",['table']
55521650,55522466,2019-04-04 17:16:44,Cassandra create and load data atomicity,"<p>I have got a web service which is looking for the last create table
[name_YYYYMMddHHmmss]
I have a persister job that creates and loads a table (insert or bulk)  </p>

<p>Is there something that hides a table until it is fully loaded ?</p>

<p>First, I have created a technical table, it works but I will need one by keyspace (using <code>cassandraAuth</code>). I don’t like this.</p>

<p>I was thinking about tags, but it doesn’t seem to exist.
 - create a table with tag and modify or remove it when the table is loaded. </p>

<p>There is also the table comment option.
Any ideas?</p>
",<cassandra>,"<p>Table comment is a good option. We use it for some service information about the table, e.g. table versions tracking.</p>
",['table']
55573249,55574763,2019-04-08 12:14:20,How to flush Cassandra CDC changes periodically to disk?,"<p><strong>Desired behaviour</strong></p>

<p>I'm trying to configure cassandra cdc in a way that the commitlogsegments are flushed periodically to the cdc_raw directory (let's say every 10 seconds). </p>

<p>Based upon documentation from <a href=""http://abiasforaction.net/apache-cassandra-memtable-flush/"" rel=""nofollow noreferrer"">http://abiasforaction.net/apache-cassandra-memtable-flush/</a> and from <a href=""https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/config/configCDCLogging.html"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/config/configCDCLogging.html</a> I found:</p>

<blockquote>
  <p><strong>memtable_flush_period_in_ms</strong> – This is a CQL table property that
  specifies the number of milliseconds after which a memtable should be
  flushed. This property is specified on table creation.</p>
</blockquote>

<p>and</p>

<blockquote>
  <p>Upon flushing the memtable to disk, CommitLogSegments containing data
  for CDC-enabled tables are moved to the configured cdc_raw directory.</p>
</blockquote>

<p>Putting those together I would think that by setting <code>memtable_flush_period_in_ms: 10000</code> cassandra flushes it's CDC changes to disk every 10 seconds, which is what I want to accomplish. </p>

<p><strong>My configuration</strong></p>

<p>Based upon aforementioned and my configuration I would expect that the memtable gets flushed to the cdc_raw directory every 10 seconds. I'm using the following configuration:</p>

<p>cassandra.yaml:</p>

<pre><code>cdc_enabled: true
commitlog_segment_size_in_mb: 1 
commitlog_total_space_in_mb: 2
commitlog_sync: periodic
commitlog_sync_period_in_ms: 10000
</code></pre>

<p>table configuration:</p>

<pre><code>memtable_flush_period_in_ms = 10000
cdc = true
</code></pre>

<p><strong>Problem</strong></p>

<p>The memtable is <em>not</em> flushed periodically to the cdc_raw directory, but instead gets flushed to the commitlogs directory when a certain size threshold is reached. </p>

<p>In detail, the following happens:</p>

<p>When a commitlogsegment reaches 1MB, it's flushed to the commitlog directory. There is a maximum of 2 commitlogs in the commitlog directory (see configuration commitlog_total_space_in_mb: 2). When this threshold is reached, the oldest commitlog file in the commitlog directory is moved to the cdc_raw directory. </p>

<p><strong>Question</strong></p>

<p>How to flush Cassandra CDC changes periodically to disk?</p>
",<cassandra>,"<p>Apache Cassandra's CDC in current version is tricky.</p>

<p>Commit log is 'global', meaning changes to any table go to the same commit log.</p>

<ul>
<li>Your commit log segment can (and will) contain logs from tables other than the ones with CDC enabled. These include system tables.</li>
<li>Commit log segment is deleted and moved to <code>cdc_raw</code> directory after every logs in the commit log segment are flushed.</li>
</ul>

<p>So, even you configure your CDC-enabled table to flush every 10 sec, there are logs from other tables still in the same commit log segment, which prevent from moving commit log to CDC directory.</p>

<p>There is no way to change the behavior other than trying to speed up the process by reducing <code>commitlog_segment_size_in_mb</code> (but you need to be careful not to reduce it to the size smaller than your single write requset).</p>

<p>This behavior is improved and will be released in next major version v4.0. You can read your CDC as fast as commit log is synced to disk (so when you are using <code>periodic</code> commit log sync, then you can read your change every <code>commit_log_sync_period_in_ms</code> milliseconds.</p>

<p>See <a href=""https://issues.apache.org/jira/browse/CASSANDRA-12148"" rel=""nofollow noreferrer"">CASSANDRA-12148</a> for detail.</p>

<p>By the way, you set <code>commitlog_total_space_in_mb</code> to 2, which I definitely do not recommend. What you are seeing right now is that Cassandra flushes every table when your commit log size exceeded this value to make more space. If you cannot reclaim your commit log space, then Cassandra would start throwing error and rejects writes.</p>
",['table']
55586697,55591133,2019-04-09 06:50:24,How to build a Dynamic Query in Cassandra?,"<p>As per the Forums and Few Folks experience i understood Java Driver can help as per the following post ..</p>

<p><a href=""https://stackoverflow.com/questions/36427449/cassandra-3-java-driver-build-dynamic-queries]"">Cassandra 3 Java Driver build dynamic queries</a><a href=""https://stackoverflow.com/questions/51171052/dealing-with-dynamic-query-string-in-cassandra"">1</a></p>

<p>But is there any way to build a query in Cassandra with out java driver.
Unfortunately No One Answered the query here 
<a href=""https://stackoverflow.com/questions/51171052/dealing-with-dynamic-query-string-in-cassandra"">Dealing with dynamic Query String in Cassandra</a> . Took It as Chance to Raise it again . </p>

<p>Thanks,
Prasad</p>
",<cassandra><cassandra-3.0><dynamicquery>,"<p>There are 2 aspects here:</p>

<ol>
<li>Easiest one - the building of the query itself - this could be done by just concatenating strings, or by using <a href=""https://docs.datastax.com/en/developer/java-driver/3.7/manual/statements/built/"" rel=""nofollow noreferrer"">QueryBuilder</a> as it was discussed in the first question.</li>
<li>Most complex one - how the query is executed. In Cassandra it's required that you provided a partition key (at least) when you performing a query. Otherwise you will perform the full table scan that most probably will end with with read timeout. </li>
</ol>

<p>To mitigate 2nd problem, people are doing denormalization and creating auxiliary tables where particular field is partition key. But this couldn't be done automatically, as you may end with very skewed data distribution for specific tables. Secondary indexes are also have limitations, and best work with partition key as well. </p>

<p>P.S. In DataStax Enterprise, this could be slightly relaxed by adding <a href=""https://docs.datastax.com/en/dse/6.7/dse-dev/datastax_enterprise/search/searchTOC.html"" rel=""nofollow noreferrer"">DSE Search</a> index on the table, but performance would be slightly worse than pure Cassandra.</p>
",['table']
55645126,55647272,2019-04-12 05:39:30,Cassandra data modeling blob,"<p>I am thinking of using cassandra for storing my data. I have a server_id, start_time, end_time, messages_blob.</p>

<pre><code>CREATE TABLE messages (
    server_id uuid,
    start bigint,
    end bigint,
    messages_blob blob,

    PRIMARY KEY ((server_id), start,end)
) WITH CLUSTERING ORDER BY (start,end);
</code></pre>

<p>I have two types of queries:</p>

<ol>
<li>get all server_ids and messages_blob at start time > 100 and start time &lt; 300.</li>
<li>get all messages_blob's for a bunch of server_ids at a time.</li>
</ol>

<p>Can the above schema help me do it? I need to put billions of records in this table very quickly and do reads after all inserts have happened. The reads queries are not too many, compared to writes, but i need the data back as quickly as possible.</p>
",<sql><cassandra><nosql><data-modeling>,"<p>With this table structure you can only execute the 2nd query - you'll just need to execute queries for every single <code>server_id</code> separately, best via async API.</p>

<p>For 1st query, this table structure won't work, as Cassandra needs to know partition key (<code>server_id</code>) to perform query - otherwise it will require a full scan that will timeout when you have enough data in table. </p>

<p>To execute this query you have several choices.</p>

<p>Add another table that will have <code>start</code> as partition key, and there you can store primary keys of records in first table. Something like this:</p>

<pre><code>create table lookup (start bigint, server_id uuid, end bigint, 
   primary key(start, server_id, end));
</code></pre>

<p>this will require that you write data into 2 tables, or you maybe can use materialized view for this task (although it could be problematic if you use OSS Cassandra, as it has plenty of bugs there).  But you'll need to be careful with size of partition for that lookup table.</p>

<p>Use Spark for scanning the table - because you have <code>start</code> as first clustering column, then Spark will able to perform predicates pushdown, and filtering will happen inside Casasndra. But it will be much slowly than using lookup table.</p>

<p>Also, be very careful with blobs - Cassandra doesn't work well with big blobs, so if you have blobs with size more than 1Mb, you'll need to split them into multiple pieces, or (better) to store them on file system, or some other storage, like, S3, and keep in Cassandra only metadata.</p>
",['table']
55669145,55678299,2019-04-13 19:32:50,Is a select with MAX() and GROUP BY() efficient or will it read all rows,"<p>I have a created cassandra table  like so:</p>

<pre><code>create table messages
    (user_id int, peer_id int, send_on timestamp, message text, 
    PRIMARY KEY (user_id, peer_id, send_on))
    WITH CLUSTERING ORDER BY (peer_id ASC, send_on DESC);
</code></pre>

<p>and populated with data.</p>

<p>I want to query the latest message for each peer_id for a given user and what I came up with was:</p>

<pre><code>select peer_id, max(send_on), message 
  from messages 
  where user_id = 1 group by peer_id;
</code></pre>

<p>I was wondering if this is going to read ALL the messages and just extract the latest or it is smart enough to only pick up the latest message.</p>

<p>The reason I asking is because populate the table with the following values:</p>

<pre><code>1, 1, now(), hello 1
1, 1, now(), hello 2
1, 1, now(), hello 3
1, 2, now(), hello 4
1, 2, now(), hello 5
1, 2, now(), hello 6
...
1, 3, now(), hello 9
</code></pre>

<p>And when I run the query I see the expected result:</p>

<pre><code>select peer_id, max(send_on), message from messages where user_id = 1 group by peer_id;

 peer_id | system.max(send_on)             | message
---------+---------------------------------+---------
       1 | 2019-04-13 19:20:48.567000+0000 | hello 3
       2 | 2019-04-13 19:21:07.929000+0000 | hello 6
       3 | 2019-04-13 19:21:22.081000+0000 | hello 9

(3 rows)
</code></pre>

<p>However with tracing on, I see:</p>

<pre><code> activity                                                                                                                      | timestamp                  | source    | source_elapsed | client
-------------------------------------------------------------------------------------------------------------------------------+----------------------------+-----------+----------------+-----------
                                                                                                            Execute CQL3 query | 2019-04-13 19:24:54.948000 | 127.0.0.1 |              0 | 127.0.0.1
 Parsing select peer_id, max(send_on), message from messages where user_id = 1 group by peer_id; [Native-Transport-Requests-1] | 2019-04-13 19:24:54.956000 | 127.0.0.1 |           8812 | 127.0.0.1
                                                                             Preparing statement [Native-Transport-Requests-1] | 2019-04-13 19:24:54.957000 | 127.0.0.1 |          10234 | 127.0.0.1
                                                                    Executing single-partition query on messages [ReadStage-2] | 2019-04-13 19:24:54.962000 | 127.0.0.1 |          14757 | 127.0.0.1
                                                                                    Acquiring sstable references [ReadStage-2] | 2019-04-13 19:24:54.962000 | 127.0.0.1 |          14961 | 127.0.0.1
                                       Skipped 0/0 non-slice-intersecting sstables, included 0 due to tombstones [ReadStage-2] | 2019-04-13 19:24:54.962000 | 127.0.0.1 |          15211 | 127.0.0.1
                                                                       Merged data from memtables and 0 sstables [ReadStage-2] | 2019-04-13 19:24:54.963000 | 127.0.0.1 |          15665 | 127.0.0.1
                                                                          Read 9 live rows and 0 tombstone cells [ReadStage-2] | 2019-04-13 19:24:54.963000 | 127.0.0.1 |          15817 | 127.0.0.1
                                                                                                              Request complete | 2019-04-13 19:24:54.964448 | 127.0.0.1 |          16448 | 127.0.0.1
</code></pre>

<p>So it seems like it reads ALL 9 rows. Is there a way to optimize this? Maybe change my schema?</p>
",<cassandra><cql><cassandra-3.0>,"<p>The two options I can think of would be for you to make another table that acts as an index to the max record for each userID and peerID. Those two fields would make up your Partition key and then would contain the rest of the data you need to find the max record in your messages table for that userID and peerID. The data would get upserted anytime you put data onto it so you'd always just write the most recent message to that table and it would always be the max. The other thing you could do is just store the last message there altogether and then you don't have to reference your messages table there at all for the actual data. Same partition key that I mentioned before, just write the actual message there too. </p>
",['table']
55721827,56176795,2019-04-17 07:09:21,Django Cassandra engine - how to define table name,"<p>I am using <a href=""https://pypi.org/project/django-cassandra-engine/"" rel=""noreferrer"">Django Cassandra</a> and I have defined my model, which I have this to name a table:</p>

<pre><code>    class Meta:
    db_table = ""table_name""
</code></pre>

<p>However, Cassandra doesn't create the table with my custom name. What am I missing?</p>
",<django><cassandra>,"<p><a href=""https://datastax.github.io/python-driver/api/cassandra/cqlengine/models.html"" rel=""noreferrer"">https://datastax.github.io/python-driver/api/cassandra/cqlengine/models.html</a></p>

<blockquote>
  <p><strong>table_name</strong> = None Optional. Sets the name of the CQL table for this model. If left blank, the table name will be the name of the
  model, with it’s module name as it’s prefix. Manually defined table
  names are not inherited.</p>
</blockquote>

<pre><code>class Meta:
    __table_name__ = ""table_name""
</code></pre>
",['table']
55813370,55814059,2019-04-23 14:20:37,Cassandra insert value disappear,"<p>I want to use the Cassandra database system to create tables. The original data is in the picture.</p>

<p>So I create these tables and insert the value</p>

<pre><code>Create table course(
    Course_ID text PRIMARY KEY,
    Course_Name text,
    student_id text

);
</code></pre>

<p>However when I want to select all the student id from course American History :<code>select * from course where Course_Name = 'Biology';</code></p>

<pre><code>Error from server: code=2200 [Invalid query] message=""Cannot execute this query as it might involve data filtering and thus may have unpredictable performance. If you want to execute this query despite the performance unpredictability, use ALLOW FILTERING""
</code></pre>

<p>Then when I try to print out all the table, I found all the value with some part of duplicate value is missing... Is it because of the way I design table is wrong? How can I change it and select all the student id from one course?
Thanks!!</p>
",<cassandra><cql>,"<p>The issue is that your query for the table <code>course</code> is not using the primary key; unlike relational databases, the tables in Cassandra are designed based on the query that you are going to execute, in this case, you can include the course name as part of the composite key:</p>

<pre><code>Create table course(
  Course_ID text,
  Course_Name text,
  student_id text,
  PRIMARY KEY (Course_Name, Course_ID)
);
</code></pre>

<p>There are already answers explaining the difference between the keys like <a href=""https://stackoverflow.com/questions/24949676/difference-between-partition-key-composite-key-and-clustering-key-in-cassandra"">this one</a>, you may also want to read <a href=""https://www.datastax.com/dev/blog/the-most-important-thing-to-know-in-cassandra-data-modeling-the-primary-key"" rel=""nofollow noreferrer"">this article from Datastax</a></p>
",['table']
55832953,55834521,2019-04-24 14:51:35,How can I without the 'allow filtering' to have the conditional query in CQL?,"<p>I am the new leaner in CQL.
I have the (restaurants) table.
By implemented 'desc restaurants', it shows the information in
<a href=""https://i.stack.imgur.com/dnYdy.png"" rel=""nofollow noreferrer"">restaurants table information</a></p>
<p>How can I run the following query without using 'allow filtering',
<code>select name from restaurants where borough = 'brooklyn';</code></p>
<p>What are the necessary steps in order to be able to run the query without 'allow filtering'?</p>
<p>Thanks so much.</p>
",<cassandra><cql>,"<p>With Cassandra (CQL) you need to take a query-based modeling approach.  This means that you'll need a table specifically-designed to serve queries by <code>borough</code>.</p>

<p>A good question to start with, is why (in your current table design) is <code>id</code> your sole primary key?  Do you have to support a query by <code>id</code>?  If not, then you shouldn't need that table.  However, as <code>borough</code> is not unique, it makes sense to cluster an additional column(s) into your primary key definition.</p>

<p>I'd go with something like this:</p>

<pre><code>CREATE TABLE stackoverflow.restaurants_by_borough (
    borough text,
    name text,
    id int,
    buildingnum text,
    cuisinetype text,
    phone text,
    street text,
    zipcode int,
    PRIMARY KEY (borough, name, id)
) WITH CLUSTERING ORDER BY (name ASC, id ASC);
</code></pre>

<p>Now this query will work:</p>

<pre><code>select name from restaurants_by_borough
where borough = 'brooklyn';
</code></pre>

<p>It will return data for each borough, sorted by <code>name</code>.  Also, if any two restaurants have the same <code>name</code> (there's probably more than one McDonalds in Brooklyn) <code>id</code> is added to the end of the key to ensure uniqueness.</p>
",['table']
55851201,55859076,2019-04-25 14:05:08,How can I create the Table and insert the json data by using JAVA in CQL?,"<p>I am the new learner in CQL.
I am using the docker env to run the Cassandra.</p>

<p>In previous, I have the two tables(restaurants and Inspection) with inserted the data by <code>csv</code>  and the following setting:</p>

<p><a href=""https://i.stack.imgur.com/UJ1pQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UJ1pQ.png"" alt=""enter image description here""></a></p>

<p>Since <code>join</code> method are not supported in CQL, I need to re-insert the joined data set(JSON) to a new table(call InspectionrestaurantNY).</p>

<p>Therefore, I tried to create the InspectionrestaurantNY table:
<a href=""https://i.stack.imgur.com/Gsg26.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Gsg26.png"" alt=""enter image description here""></a></p>

<p>Then, I have the <code>jav</code> which help me to install the <code>json</code> file.
But I got the error, and I don't know what table(InspectionrestaurantNY)  setting should I create to insert the <code>json</code> data. </p>

<p>I ran the <code>java -jar JSonFile2Cassandra.jar -host 192.168.99.101 -port 3000 -keyspace restaurantsNY -columnFamily InspectionsRestaurants -file InspectionsRestaurantsNY.json</code>, it shown the following error:</p>

<p><a href=""https://i.stack.imgur.com/7uCYA.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7uCYA.jpg"" alt=""enter image description here""></a></p>

<p>And, my <code>json</code> file is stored as like this:
<a href=""https://i.stack.imgur.com/4DVly.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4DVly.png"" alt=""enter image description here""></a><a href=""https://i.stack.imgur.com/pvLNj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pvLNj.png"" alt=""enter image description here""></a><a href=""https://i.stack.imgur.com/ZlxQN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZlxQN.png"" alt=""enter image description here""></a><a href=""https://i.stack.imgur.com/bza8T.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bza8T.png"" alt=""enter image description here""></a></p>

<p>What table setting should I build up first to insert the JSON data?</p>

<p>How to solve the JAVA error?</p>

<p>Thank you so much.</p>
",<java><json><cassandra><cql>,"<p>Seems like you are using wrong table name when running jar to insert JSON. The command you shared is </p>

<blockquote>
  <p>java -jar JSonFile2Cassandra.jar -host 192.168.99.101 -port 3000
  -keyspace restaurantsNY -columnFamily InspectionsRestaurants -file InspectionsRestaurantsNY.json</p>
</blockquote>

<p>Shouldnt it be </p>

<blockquote>
  <p>java -jar JSonFile2Cassandra.jar -host 192.168.99.101 -port 3000
  -keyspace restaurantsNY -columnFamily InspectionsRestaurantsNY -file InspectionsRestaurantsNY.json</p>
</blockquote>

<p>i.e use correct table name <strong>InspectionsRestaurantsNY</strong> for -columnFamily argument in above command.</p>

<p>Also its always better not to use camel case convention as CQL identifier names are case-insensitive. If you really really want case sensitive names then you should enclose names in double quotes. If double quotes are not used then Cassandra will convert names with mixed case to lower case. But in above query I dont think that is cause of error. I think its due to wrong column family name.   </p>

<p>Check here for mixed cases names
<a href=""https://docs.datastax.com/en/cql/3.3/cql/cql_reference/ucase-lcase_r.html"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/cql/3.3/cql/cql_reference/ucase-lcase_r.html</a></p>
",['table']
55857204,55869433,2019-04-25 20:33:01,Optimization of a query which uses arithmetic operations in WHERE clause,"<p>I need to retrieve records where the expiration date is today. The expiration date is calculated dynamically using two other fields (<code>startDate</code> and <code>durationDays</code>):</p>

<pre><code>SELECT * FROM subscription WHERE startDate + durationDays &lt; currentDate() 
</code></pre>

<p>Does it make sense to add two indexes for these two columns? Or should I consider adding a new column <code>expirationDate</code> and create an index for it only?</p>
",<cassandra><cql>,"<pre><code>SELECT * FROM subscription WHERE startDate + durationDays &lt; currentDate() 
</code></pre>

<blockquote>
  <p>I'm wondering how does Cassandra handle such a filter as in my example? Does it make a full scan?</p>
</blockquote>

<p>First of all, your question is predicated on CQL's ability to perform (date) arithmetic.  It cannot.</p>

<pre><code>&gt; SELECT * FROM subscription WHERE startDate + durationDays &lt; currentDate();
SyntaxException: line 1:43 no viable alternative at input '+' (SELECT * FROM subscription WHERE [startDate] +...)
</code></pre>

<p>Secondly the <code>currentDate()</code> function does not exist in Cassandra 3.11.4.</p>

<pre><code>&gt; SELECT currentDate() FROM system.local;
InvalidRequest: Error from server: code=2200 [Invalid query] message=""Unknown function 'currentdate'""
</code></pre>

<p>That <em>does</em> work in Cassandra 4.0, which as it has not been released yet, you really shouldn't be using.</p>

<p>So let's assume that you've created your secondary indexes on <code>startDate</code> and <code>durationDays</code> and you're just querying on those, without any arithmetic.</p>

<p>Does it execute a full table scan?</p>

<p><strong>ABSOLUTELY.</strong></p>

<p>The reason, is that querying solely on secondary index columns does not have a partition key.  Therefore, it has to search for these values on all partitions on all nodes.  In a large cluster, your query would likely time out.</p>

<p>Also, when it finds matching data, it has to keep querying.  As those values are not unique; it's entirely possible that there are several results to be returned.  Carlos in 100% correct is advising you to rebuild your table based on what you want to query.</p>

<p>Recommendations:</p>

<ul>
<li>Try not to build a table with secondary indexes.  Like ever.</li>
<li>If you have to build a table with secondary indexes, try to have a partition key in your <code>WHERE</code> clause to keep the query isolated to a single node.</li>
<li>Any filtering on dynamic (computed) values needs to be done on the application side.</li>
<li>In your case, it might make more sense to create a column called <code>expirationDate</code>, do your date arithmetic in your app, and then <code>INSERT</code> that value into your table.</li>
<li>You'll also want follow the ""time bucket"" pattern for handling time series data (which is what this appears to be).  Say that <code>month</code> works as a ""bucket"" (it may or may not for your use case).  <code>PRIMARY KEY ((month),expirationDate,id)</code> would be a good key.  This way, all the subscriptions for a particular month are stored together, clustered by <code>expirationDate</code>, with <code>id</code> on the end to act as a tie-breaker for uniqueness.</li>
</ul>
",['table']
55906125,55922614,2019-04-29 15:06:29,How to design NoSQL Database to select data order by timestamp,"<p>To summarize I want to create a SMACK architecture (Spark, Mesos, Akka, Cassandra and Kafka). I want to make an entry point where I can return the 50 last element add in my DB. Here's my Database :</p>

<pre><code>create table fireman
(
    uuid uuid primary key,
    date text,
    heartrate int,
    id text,
    location text,
    ratecommunication int,
    temperature int,
    time timestamp
);
</code></pre>

<p>I tried to query this :</p>

<pre><code>SELECT * FROM scala_fireman.fireman WHERE temperature &gt; 0 ORDER BY date LIMIT 5 ALLOW FILTERING ;
</code></pre>

<p>But I got this error :</p>

<pre><code>ORDER BY is only supported when the partition key is restricted by an EQ or an IN.
</code></pre>

<p>So my question is <strong>how can SELECT to get the n last element I add ?</strong></p>

<p>I saw that I could order the table by doing this :</p>

<pre><code>) WITH CLUSTERING ORDER BY (time DESC);
</code></pre>

<p>But to do this I need to change time as primary key but some data are add at the same time so I can't set it as primary key.</p>
",<cassandra><nosql><cassandra-3.0>,"<p>As Cassandra requires a query-based modeling approach, we'll need to build a table specifically to handle this query:</p>

<pre><code>SELECT * FROM scala_fireman.fireman 
WHERE temperature &gt; 0 ORDER BY date LIMIT 5 ALLOW FILTERING;
</code></pre>

<p>Do you ever query by <code>uuid</code>?  If so, then we can build a new table.  If not, you <em>will</em> need to change your primary key for this to work.  Building a single PRIMARY KEY on an ID column severely limits your query flexibility (as you are finding out).</p>

<blockquote>
  <p>This is a POC so as many as I can, for the moment I sent 1 million fireman</p>
</blockquote>

<p>This is going to be your first obstacle.  Cassandra can only support 2 billion cells per partition, and it'll get slow long before that.  So we'll want to limit the number of firemen events per partition by ""time bucketing.""  As an example, I'll use a <code>month_bucket</code>, but you should determine if that really works for your business requirements.</p>

<p>Next, you want to <code>ORDER BY</code> date, so we'll use that as a clustering key.  Actually, as <code>date</code> is a text field, we'll use <code>time</code> as I'm sure you don't want results returned in ASCII-betical order.  Quick education on the <code>ORDER BY</code> clause, is that it is completely superfluous.  You can <em>only</em> enforce it on the predetermined order of your clustering keys.  It shouldn't ever need to be in a query.</p>

<p><strong>Note:</strong> The reason you're getting the error you're seeing, is that sort order an only be enforced <em>within</em> a partition of data.  It cannot be enforced on a result set.</p>

<p>Also, I see you're doing an open-ended range query on <code>temperature</code>.  Usually, that's a bad idea (and the reason you need <code>ALLOW FILTERING</code> in your original query).  But <em>within</em> a partition, it shouldn't be too bad.  As long as that partition isn't too big.  We'll cluster on that, as well.</p>

<p>And of course, it's possible that multiple firemen could be involved in an event on the same date at the same temperatures, so we'll add <code>uuid</code> on the end to enforce uniqueness.  Your new primary key should look something like this:</p>

<pre><code>    PRIMARY KEY ((month_bucket),time,temperature,uuid))
</code></pre>

<p>So if try this table definition:</p>

<pre><code>create table fireman_events_by_date_and_temp (
    uuid uuid,
    month_bucket int,
    date text,
    heartrate int,
    id text,
    location text,
    ratecommunication int,
    temperature int,
    time timestamp,
    PRIMARY KEY ((month_bucket),time,temperature,uuid))
    WITH CLUSTERING ORDER BY (time DESC, temperature ASC, uuid ASC);
</code></pre>

<p>Now if I load some data and run your query:</p>

<pre><code>&gt; SELECT time,temperature,heartrate,location
  FROM fireman_events_by_date_and_temp
  WHERE month_bucket=201904
  AND temperature &gt; 0
  LIMIT 5
  ALLOW FILTERING;

 time                            | temperature | heartrate | location
---------------------------------+-------------+-----------+----------
 2019-04-30 13:40:03.253000+0000 |         644 |       144 |       SF
 2019-04-30 13:39:51.944000+0000 |         644 |       144 |       SF
 2019-04-30 13:39:39.859000+0000 |         644 |       144 |       SF
 2019-04-30 13:39:30.331000+0000 |         644 |       144 |       SF
 2019-04-30 13:39:15.945000+0000 |         644 |       144 |       NY

(5 rows)
</code></pre>

<p>Normally, I wouldn't recommend use of <code>ALLOW FILTERING</code>.  But as long as you're querying on a partition key (<code>month_bucket</code>) all data should still be served by the same node.</p>

<p>Also, I wrote this article on result set ordering in Cassandra in 2015, and in it I demonstrate the use of these modeling techniques.  It's still quite relevant four years later (especially to problems like this):</p>

<p><a href=""https://www.datastax.com/dev/blog/we-shall-have-order"" rel=""nofollow noreferrer"">We Shall Have Order!</a></p>

<p>Give that a read, and see if it helps.</p>
",['table']
56001443,56001994,2019-05-06 08:28:03,A correct way to alter Cassandra table via C#,"<p>My issue is in the next.</p>

<p>I have the next simple model in my code:</p>

<pre class=""lang-cs prettyprint-override""><code>public class Client
{
    public Guid Id { get; set; }

    public string Name { get; set; }
}
</code></pre>

<p>I defined a mapping for it:</p>

<pre class=""lang-cs prettyprint-override""><code>public class CustomMappings : Mappings
{
    public CustomMappings()
    {
        For&lt;Client&gt;().TableName(""clients"")
                     .PartitionKey(x =&gt; x.Id);
    }
}
</code></pre>

<p>I created the table via <code>Table&lt;TEntity&gt;.CreateIfNotExist()</code> method:</p>

<pre class=""lang-cs prettyprint-override""><code>var table = new Table&lt;Client&gt;(session);
table.CreateIfNotExists();
</code></pre>

<p>And I can insert my data by the next way:</p>

<pre class=""lang-cs prettyprint-override""><code>IMapper mapper = new Mapper(session);

var client = new Client
{
    Id = Guid.NewGuid(),
    Name = ""John Smith""
};

await mapper.UpdateAsync(client);
</code></pre>

<p>After this, I've changed my model by adding a new property:</p>

<pre class=""lang-cs prettyprint-override""><code>public class Client
{
    public Guid Id { get; set; }

    public string Name { get; set; }

    public string Surname { get; set; }
}
</code></pre>

<p>I need to alter this table, because I want to add surname column to it.
Of course, I have the exception without it when I try to insert a value:</p>

<pre class=""lang-cs prettyprint-override""><code>Cassandra.InvalidQueryException: Undefined column name surname
   at Cassandra.Requests.PrepareHandler.Prepare(PrepareRequest request, IInternalSession session, Dictionary`2 triedHosts)
   at Cassandra.Requests.PrepareHandler.Prepare(IInternalSession session, Serializer serializer, PrepareRequest request)
   at Cassandra.Session.PrepareAsync(String query, IDictionary`2 customPayload)
   at Cassandra.Mapping.Statements.StatementFactory.GetStatementAsync(ISession session, Cql cql, Nullable`1 forceNoPrepare)
   at Cassandra.Mapping.Mapper.ExecuteAsync(Cql cql)

</code></pre>

<p>But class <code>Cassandra.Data.Linq.Table&lt;TEntity&gt;</code> does not contain neither nor <code>.AlterOrCreate()</code> nor <code>.Alter()</code> methods. Also, we don't have <code>.GetAlter()</code> method in <code>Cassandra.Mapping.Statements.CqlGenerator</code>.</p>

<p>Which way is more appropriate to solve this problem? I have two assumptions (besides creating a pull request with needed methods to datastax csharp driver repository on github :)).</p>

<ol>
<li>To alter tables via cql script in .cql file which will be executed in c# code.</li>
<li>To create a new table after each changes of a model and migrate old data to it.</li>
</ol>

<p>I'm a newbee in Cassandra and I have suspicions that needed method does not exist in the library for good reason. Maybe, are there any problems with consistency after altering because Cassandra is distributed database?</p>
",<c#><cassandra><datastax>,"<p>Changes in the Cassandra's schema should be done very accurately - you're correct about distributed nature of it, and when making changes you need to take into account. Usually it's recommended to make changes via only one node, and after execution of any DDL statement (create/drop/alter) you need to check for schema agreement (for example, via <a href=""https://docs.datastax.com/en/drivers/csharp/3.9/html/M_Cassandra_Metadata_CheckSchemaAgreementAsync.htm"" rel=""nofollow noreferrer"">method <code>CheckSchemaAgreementAsync</code></a> of <code>Metadata</code> class), and don't execute next statement until schema is in agreement.</p>

<p>Talking about changes themselves - I'm not sure that C# driver is able to automatically generate the changes for schema, but you can execute the changes as CQL commands, as described in <a href=""https://docs.datastax.com/en/dse/5.1/cql/cql/cql_reference/cql_commands/cqlAlterTable.html"" rel=""nofollow noreferrer"">documentation</a> (please read carefully about limitations!).  The changes in schema could be separated into 2 groups:</p>

<ol>
<li>That could be applied to table without need to migrate the data</li>
<li>That will require creation of new table with desired structure, and migration of data.</li>
</ol>

<p>In the first group we can do following (maybe not a full list):</p>

<ul>
<li>Add a new regular column to table</li>
<li>Drop a regular column from table</li>
<li>Rename the clustering column</li>
</ul>

<p>Second group includes everything else:</p>

<ul>
<li>Changing the primary key - adding or removing columns to/from it</li>
<li>Renaming of non-clustering columns</li>
<li>Changing the type of the column (it's really recommended to create completely new column with required type, copy data, and then drop original column - it's not recommended to use the same name with different type, as it could make your data inaccessible)</li>
</ul>

<p>Data migration could be done by different tools, and it may depend on the specific requirements, like, type change, etc. But it's a different story.</p>
",['table']
56031809,56041958,2019-05-07 23:50:06,Cassandra in cluster - 1000 query in 1s [1000ms],"<p>I would like to query the same database (with script) 1000 times in 1s. </p>

<p>So I did some simulation test with nodejs for loop. I sent out 50 query request but the response came back very slow in around 400 ms </p>

<p>I am using simply loop - amount of query </p>

<p>setInterval - amount of 1k-1s requests</p>

<p>My question: is it possible to do 1000 queries in 1s, even force it somehow?
I would also like to test how the database works on more queries (10000, 100000). The logic thing would be that database returns busy state.</p>

<pre><code>funcname(){
.
 for(var i=0;i&lt;50; i++) {
.
.
   client[ randomHostId ].execute(query, 
     [ ids[ Math.floor(Math.random() *   1000) ] ], 
     { prepare: true },function (err, result) {...})
.
.
}
const time = setInterval(funcname, 1000);
</code></pre>
",<javascript><node.js><cassandra>,"<p>In the example you provided, the code is doing what is commonly known as ""fire and forget"": starting a bunch of operations and don't await for the result to launch new ones.</p>

<p>Usually you need to control the amount of operation that occur in parallel. You can achieve it by using popular control flow libraries such as <a href=""http://bluebirdjs.com/docs/getting-started.html"" rel=""nofollow noreferrer"">bluebird</a> or <a href=""https://github.com/caolan/async"" rel=""nofollow noreferrer"">async utilities</a>.</p>

<p>There's also an example in the repository on how to insert large number of rows in a table limiting the amount of parallel requests: <a href=""https://github.com/datastax/nodejs-driver/blob/master/examples/concurrent-executions/execute-in-loop.js"" rel=""nofollow noreferrer"">https://github.com/datastax/nodejs-driver/blob/master/examples/concurrent-executions/execute-in-loop.js</a></p>

<p>There's also a concurrent utilities API built-in the driver that can help you: <a href=""https://docs.datastax.com/en/developer/nodejs-driver/4.1/features/concurrent-api/"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/developer/nodejs-driver/4.1/features/concurrent-api/</a></p>

<pre><code>const query = 'INSERT INTO table1 (id, value) VALUES (?, ?)';
const parameters = [[1, 'a'], [2, 'b'], [3, 'c'], ]; // ...
const result = await executeConcurrent(client, query, parameters);
</code></pre>

<p>See also: <a href=""https://stackoverflow.com/a/54810019/208683"">https://stackoverflow.com/a/54810019/208683</a></p>
",['table']
56074743,56106876,2019-05-10 09:33:52,Cassandra Timeout with COUNT query,"<blockquote>
  <p>NOTE: I am having more than 15967908 records. I am a newbie to
  Cassandra. Reference: <a href=""https://stackoverflow.com/questions/51744943/is-there-a-way-to-effectively-count-rows-of-a-very-huge-partition-in-cassandra"">Is there a way to effectively count rows of a very huge partition in Cassandra?</a>    </p>
  
  <p><a href=""https://stackoverflow.com/questions/30575125/coordinator-node-timed-out-waiting-for-replica-nodes-in-cassandra-datastax-while"">Coordinator node timed out waiting for replica nodes in Cassandra Datastax while insert data</a></p>
</blockquote>

<p>Hi pals, I search for other answers but it didn't work out for me. They had mentioned that we need to increase the time out in <strong>cassandra.yaml</strong> file but the problem is I don't have the file.</p>

<p>I have installed the Cassandra with HomeBrew. Following is the version that I am currently running on my MacBook</p>

<pre><code>cqlsh:cassandra_training&gt; show version;
[cqlsh 5.0.1 | Cassandra 3.11.4 | CQL spec 3.4.4 | Native protocol v4]
</code></pre>

<p>When I do </p>

<pre><code>cqlsh:cassandra_training&gt; select count(*) from access_point_logs;
</code></pre>

<p>Then getting the following error</p>

<pre><code>ReadTimeout: Error from server: code=1200 [Coordinator node timed out waiting for replica nodes' responses] message=""Operation timed out - received only 1 responses."" info={'received_responses': 1, 'required_responses': 1, 'consistency': 'ONE'}
</code></pre>

<p>Which file I need to increase the timeout. As I am not getting the <strong>cassandra.yaml</strong> file.</p>

<p>My Cassandra installation path is as follows</p>

<pre><code>/usr/local/Cellar/cassandra/3.11.4
</code></pre>

<p>Is there any way to count the number of records.</p>
",<macos><cassandra><homebrew>,"<p>I sorted out the issue. While creating the table I was not properly adding the <strong>Primary Key</strong> with <strong>Cluster Key</strong> because of which it was showing this kind of error. </p>

<p>Earlier I had created a table structure in the following way</p>

<pre><code>create table access_point_logs (
    id bigint primary key,
    wd_number varchar,
    ip_address varchar,
    mac_address varchar,
    created_at timestamp,
    updated_at timestamp,
    access_point_id bigint
);
</code></pre>

<p>Now changed it to the following</p>

<pre><code>create table access_point_logs(
    id bigint,
    wd_number varchar,
    ip_address varchar,
    mac_address varchar,
    created_at timestamp,
    updated_at timestamp,
    access_point_id bigint,
    primary key ((wd_number), created_at, mac_address)
) with clustering order by (created_at desc);
</code></pre>

<p>Just in case if any newbie like me I would like to add the following definition and examples to understand what Partition Key and what Cluster Key is</p>

<p>Carefully observe the change with the following</p>

<pre><code>primary key ((wd_number), created_at, mac_address);


Partition Key - wd_number

Cluster Key - created_at, mac_address
</code></pre>

<blockquote>
  <p>Partition Key - Which particular node to store the data in the
  Cluster.</p>
  
  <p>Clustering Key - Mainly used for sorting the data and how to display
  default order while fetching the data.</p>
  
  <p>Primary Key - Maybe a combination of Partition Key + Cluster Key or
  just a Partition Key</p>
</blockquote>

<p>Hope it may help someone. In case if anyone has queries please feel free to fire. I will do my best to answer those.</p>
",['table']
56105766,56109208,2019-05-13 04:29:13,Need help writing a Cassandra UDF function,"<p>Need help writing a cassandra udf to add/divide/multiply two variable. Tried the below code however it doesn't seem to work. Also I have no experience in java, so may be I am not able to debug. Help appreciated on this.</p>

<pre class=""lang-sql prettyprint-override""><code>CREATE FUNCTION my_adder(val1 double,val2 double )
    RETURNS double LANGUAGE java
    BODY
        return (val1 == null || val2 == null)?null:Double.valueOf( val1.doubleValue() + val2.doubleValue());
    END BODY;
</code></pre>
",<cassandra><user-defined-functions><cassandra-3.0>,"<p>Null inputs should be handled either using <code>RETURNS NULL ON NULL INPUT</code> or <code>CALLED ON NULL INPUT</code> as documented <a href=""https://docs.datastax.com/en/cql/3.3/cql/cql_reference/cqlCreateFunction.html"" rel=""nofollow noreferrer"">here</a>.</p>

<p>So in this case</p>

<pre><code>CREATE OR REPLACE FUNCTION my_adder(val1 double,val2 double )
RETURNS NULL ON NULL INPUT
RETURNS double LANGUAGE java AS 'return val1 + val2;';
</code></pre>

<p>Testing it:</p>

<pre><code>create table mytable(
p int,
foo double,
bar double,
primary key (p));

insert into mytable (p, foo, bar)
values (0, 1.0, 2.0);
insert into mytable (p, foo, bar)
values (1, NULL, 2.0);
insert into mytable (p, foo, bar)
values (2, 1.0, NULL);

select p,my_adder(foo, bar) from mytable;

 p | test.my_adder(foo, bar)
---+---------------------------------
 1 |                            null
 0 |                               3
 2 |                            null
</code></pre>
",['table']
56247133,56276587,2019-05-21 22:33:01,Design schema to query select beetwen two separate dates,"<p>I have a table for messages, like this:</p>

<pre><code>messages (
    user_id uuid,
    id uuid,
    message blob,
    created_at timestamp,
    updated_at timestamp,
    PRIMARY KEY (user_id, id)
)
</code></pre>

<p>I can create a MV for sort and select messages by <code>updated_at</code>.  But when I need to select last updated messages from last time when client sync (e.g. <code>select where updated_at &gt; 1555602962006 and created_at &lt; 1555602962006</code>) - only way is to select all messages by <code>updated_at</code> and filter rows in code? Is this a normal practice in production?</p>

<p>Maybe in this case, it's possible to create some token to sort by concatenating <code>created_at</code> and <code>updated_at</code> or something?</p>
",<cassandra><nosql><cql>,"<p>So the tricky part about this, is that Cassandra will not allow a range query on multiple, different columns.  Secondly, range queries only work <em>within</em> a partition, so you'll need to come up with a time ""bucketing"" strategy.</p>

<p>One way that I have modeled around this in the past, was to have one column for a timestamp, but different <em>rows</em> for the type of time.  I'll put together an example here, using your table above.  For a time bucket, I'll use month; essentially, I know I'll only ever query for messages created/updated in the last month (that may or may not work for you).</p>

<pre><code>CREATE TABLE messages_by_month (
  month_bucket int,
  event_time timestamp,
  user_id uuid,
  id uuid,
  message text,
  event text,
  PRIMARY KEY (month_bucket, event_time, id));
</code></pre>

<p>After INSERTing some data, I can now query for a range of created and updated event times, like this:</p>

<pre><code>SELECT id,event_time,event,message
FROM messages_by_month 
WHERE month_bucket=201905
  AND event_time &gt; 1558619000000
  AND event_time &lt; 1558624900000;

 id                                   | event_time                      | event   | message
--------------------------------------+---------------------------------+---------+---------
 a4d60c29-ad4e-4023-b869-edf1ea6207e2 | 2019-05-23 14:00:00.000000+0000 | CREATED |     hi!
 66e78a1e-dbcb-4f64-a0aa-6d5b0e64d0ed | 2019-05-23 14:20:00.000000+0000 | CREATED |     hi!
 f1c59bf4-1351-4527-a24b-80bb6e3a2a5c | 2019-05-23 15:00:00.000000+0000 | UPDATED |    hi2!
 a4d60c29-ad4e-4023-b869-edf1ea6207e2 | 2019-05-23 15:20:00.000000+0000 | UPDATED |    hi3!

(4 rows)
</code></pre>

<p>While this exact example may not work for you, the idea is to think about organizing your data differently to support your query.</p>

<p>Note:  With this example, you'll need to watch your partition sizes.  You may end up having to add another partition key if this table gets more than 50k-100k messages in a month.</p>
",['table']
56346838,56352130,2019-05-28 16:49:21,Tombstone scanning in system.log,"<p>I have a cassandra cluster with less delete use case. I found in my system.log ""<strong>Read 10 live and 5645464 tombstones cells in keyspace.table</strong>"" What does it mean? please help to understand.</p>

<p>Thanks.</p>
",<cassandra><datastax><scylla><tombstone>,"<p>One more important thing to keep in mind when working with Cassandra is that tombstones cells do not directly correlate to deletes.  </p>

<p>When you insert <code>null</code> value to an attribute when performing your insert, Cassandra internally marks that attribute/cell as a tombstone. So, even if you don't have a lot of deletes happening, you could end up with an enormous number of tombstones. Easy and simple solution is to not insert <code>null</code> values for an attribute while inserting.</p>

<p>As per this statement <code>Read 10 live and 5645464 tombstones cells in keyspace.table</code> goes, there might be a table scan for a query happening that is scanning <code>10</code> cells and <code>5645464</code> number of tombstones (cells with <code>null</code> value) while doing so is what I am guessing. Need to understand what type of queries are being executed to gain more insight into that.</p>
",['table']
56392631,56409404,2019-05-31 09:46:43,ExpressJS app which receives around 70 requests per second - slow Cassandra performance,"<p>This is not a question related to a code, but more to a server performance and things that I should check. So I have a ExpressJS server which is connected to a cassandra db (1 seed node and 2 nodes on 1 cluster, so in total 3 nodes). The API is running on the same server as the cassandra db seed node. I have in total 3 servers in local network.</p>

<p>So the structure looks like this -</p>

<p>server 1 running API and seed cassandra node.
server 2 running cassandra node.
server 3 running cassandra node.</p>

<p>Each server has 8GM of ram and 2.5Ghz CPU.</p>

<p>By default there are around 70 requests coming in each second which does the following -</p>

<p>1) Calls a function which reads the data from a table from the cassandra (using materialized view).
2) Reads another table from cassandra db (using materialized view).
3) Posts data to the third table in cassandra.</p>

<p>The 2nd function that is called is quite similar, it does 1 read using materialized view and 1 post. </p>

<p>The proportional difference between the function called each second is around 30 times function 1 is called (which does 2 reads and 1 post), and around 40 times function 2 is called (which does 1 read and 1 post).</p>

<p>Everything would be great, but the latency of the requests are jumping from time to time, sometimes it takes around 10ms, but every 5 - 10 seconds it goes up to 3-30 seconds. Also cassandra seems to be unstable - during the period when there are 3-30 second request times, the cassandra seem to time out on some of the requests.</p>

<p>What would be the first thing that I should check? Do I need additional nodes and how can I figure out if I have enough nodes for the amount of data being sent in to cassandra db? Should I separate API from cassandra nodes - thus hold API server on a separate server, e.g., server 4?</p>
",<express><cassandra><express-cassandra>,"<p>Materialized views are great for reads operations, but they come with the expense of writes; you'll need to account for some overhead required to execute its magic:</p>

<ul>
<li>materialized view will require additional resources to track updates on its sources; this will get worse when you interact with multiple materialized views, as in the first scenario that you propose.</li>
<li>If the data of the post is written in the same source of the materialized view, this will depend on the complexity of the primary key used in the table, as explained <a href=""https://www.datastax.com/dev/blog/materialized-view-performance-in-cassandra-3-x"" rel=""nofollow noreferrer"">here</a>.</li>
</ul>

<p>The first option that I would explore is to denormalize and create a separate table for the first function, so you will do one read, instead of two.</p>

<p>There is a lot of speculation in my answer, as there are a lot of unknowns on the structure and your table schema; you may get a better insight if you enable tracing, on our case we have gotten good results with openzipkin as explained by <a href=""https://thelastpickle.com/files/2015-09-24-using-zipkin-for-full-stack-tracing-including-cassandra/presentation"" rel=""nofollow noreferrer"">TLP</a></p>
",['table']
56523458,56531353,2019-06-10 09:10:03,How to connect Node.js app properly to Cassandra node?,"<p>I have two Cassandra nodes hosted on two IPs. Now, when I am connecting to one of those nodes via cassandra-driver in Node.js from Windows, I am getting 'connection timed out' and 'host unreachable' kind of errors. However, I am able to connect to those nodes via CQLSH from outside of their network.</p>

<p>Am I doing something wrong? Here is the sample code.</p>

<pre><code>var cassandra = require('cassandra-driver');

var uuid = require('uuid')
var client = new cassandra.Client({ contactPoints: ['(PUBLIC IP Here) X.X.X.X:9042'], localDataCenter: 'datacenter1', keyspace: 'ksks' });

function getAllPersons() {
    var query = 'SELECT * FROM person';
    return new Promise((resolve, reject) =&gt; {
        client.connect(function (err) {
            if (err) return console.error(err);
            else {
                client.execute(query, function (err, result) {
                    if(err)  {
                        reject([])
                    }
                    else {
                        if(result.rows.length) {
                            resolve(result.rows);
                        }
                        else {}
                            reject([])
                    }
                });
            }
        });
     });
}
</code></pre>

<p>In the cassandra.yaml, I have these values:</p>

<pre><code>listen_address: 10.0.0.4
native_transport_port: 9042
rpc_address: 10.0.0.4
rpc_port: 9160
api_port: 10000
api_address: 10.0.0.4
</code></pre>

<p>What do I need to change? The nodes are working fine and can CQLSH to each other (from remote networks as well).</p>
",<node.js><cassandra><cassandra-3.0><scylla><cassandra-driver>,"<p>If you are using a public IP to access the cluster remotely your configurations are not correct. </p>

<p>This table might help you figure out the correct IPs in the scylla.yaml:</p>

<p><code>https://docs.scylladb.com/operating-scylla/procedures/cluster-management/ec2_dc/#ec2-configuration-table</code></p>

<p>Do NOT use Ec2Snitch unless you are on AWS. The table is just a reference. </p>
",['table']
56533028,56535352,2019-06-10 20:19:42,check if table is empty in cassandra DB,"<p>I am trying to find a way to determine if the table is empty in Cassandra DB.</p>

<pre><code>cqlsh&gt; SELECT * from examples.basic ;

 key | value
-----+-------

(0 rows)
</code></pre>

<p>I am running <code>count(*)</code> to get the value of the number of rows , but I am getting warning message, So I wanted to know if there is any better way to check if the table is empty(zero rows). </p>

<pre><code>cqlsh&gt; SELECT count(*) from examples.basic ;

 count
-------
     0

(1 rows)

Warnings :
Aggregation query used without partition key

cqlsh&gt;
</code></pre>
",<cassandra>,"<p>Aggregations, like count, can be an overkill for what you are trying to accomplish, specially with the star wildcard, as if there is any data on your table, the query will need to do a full table scan. This can be quite expensive if you have several records.</p>

<p>One way to get the result you are looking for is the query</p>

<p><code>cqlsh&gt; SELECT key FROM keyspace1.table1 LIMIT 1;</code></p>

<h2>Empty table:</h2>

<p>The resultset will be empty</p>

<pre><code> cqlsh&gt; SELECT key FROM keyspace1.table1 LIMIT 1;

 key
 -----

 (0 rows)
</code></pre>

<h2>Table with data:</h2>

<p>The resultset will have a record</p>

<pre><code>cqlsh&gt; SELECT key FROM keyspace1.table1 LIMIT 1;

key
----------------------------------
uL24bhnsHYRX8wZItWM6xKdS0WLvDsgi

(1 rows)
</code></pre>
",['table']
56575224,56612963,2019-06-13 07:18:01,How to update table in Phantom for cassandra Scala,"<p>I created following table for Cassandra</p>

<pre><code>abstract class MessageTable extends Table[ConcreteMessageModel, Message] {

  override def tableName: String = ""messages""

  // String because TimedUUIDs are bad bad bad
  object id extends Col[String] with PartitionKey {
    override lazy val name = ""message_id""
  }

  object phone extends Col[String]

  object message extends Col[String]

  object provider_message_id extends Col[Option[String]]
  object status extends Col[Option[String]]

  object datetime extends DateColumn {
    override lazy val name = ""message_datetime""
  }

  override def fromRow(r: Row): Message = Message(phone(r), message(r), Some(UUID.fromString(id(r))), None, status(r), Some( ZonedDateTime.ofInstant(datetime(r).toInstant, ZoneOffset.UTC) ))
}
</code></pre>

<p>In above table, I want to be able to update the table based on <code>id</code> or <code>provider_message_id</code>.</p>

<p>I can easily update the row using <code>id</code></p>

<pre><code>update().where(_.id eqs message.id)...
</code></pre>

<p>But I can't update the table using <code>provider_message_id</code></p>

<pre><code>update().where(_.provider_message_id eqs callback_id)...
</code></pre>

<p>How can I use multiple fields to update the table in cassandra</p>
",<scala><cassandra><phantom-dsl>,"<p>There is a restriction with Cassandra updates is that they will work only with the primary key. The primary key can be one column (named partition key), or multiple columns (a partition key, and one or many clustering keys).</p>

<p>In the case that you are providing, you need to ensure that both <code>id</code> and <code>provider_message_id</code> are part of the primary key, the description of the table with cql should be something similar to:</p>

<pre><code>cqlsh:&gt; DESCRIBE keyspace1.messages;
...
CREATE TABLE keyspace1.messages (
    id text,
    phone text,
    message text,
    provider_message_id text,
    status text,
    datetime date,
    PRIMARY KEY (id, provider_message_id)
) WITH CLUSTERING ORDER BY (provider_message_id ASC)
    ...
</code></pre>

<p>Also, please note that you will need to use <code>id</code> and <code>provider_message_id</code> in all the update queries (there is no update by <code>id</code> or <code>provider_message_id</code>). Your code will look as:</p>

<pre><code>update().where(_.id eqs message.id).and(_.provider_message_id eqs callback_id)...
</code></pre>
",['table']
56589300,56589792,2019-06-13 22:29:25,Multiple Endpoints in Cassandra cluster Connection,"<p>I want to give multiple Cassandra endpoints from the config file to my Java application. </p>

<p>Ex: 
cassandra host: ""host1, host2""</p>

<p>I tried <code>addContactPoints(host)</code>, but it did not work. If one of the Cassandra node goes down, I don't want my application to go down.</p>

<pre class=""lang-java prettyprint-override""><code>cluster = Cluster.builder()
  .withClusterName(cassandraConfig.getClusterName())
  .addContactPoints(cassandraConfig.getHostName())
  .withSocketOptions(new SocketOptions().setConnectTimeoutMillis(30000).setReadTimeoutMillis(30000))
  .withPoolingOptions(poolingOptions).build();
</code></pre>
",<java><cassandra><datastax-java-driver>,"<p>The java driver is resilient to one of the contact points provided not being available. Contact points are used for establishing an initial connection [*].  As long as the driver is able to communicate with one contact point, it should be able to query the <code>system.peers</code> and <code>system.local</code> table to discover the rest of the nodes in the cluster.</p>

<p>* They are also added to a list of initial hosts in the cluster, but typically the contact points provided map to a node in the system.peers table.</p>
",['table']
56644888,56652944,2019-06-18 08:32:32,"Column counts are larger than 1996099046, unable to calculate percentiles","<p>while I am running TableHistograms getting below message:</p>

<p>NodeTool TableHistograms keyspace TableName</p>

<pre><code>Column counts are larger than 1996099046, unable to calculate percentiles

Percentile  SSTables     Write Latency      Read Latency    Partition Size        Cell Count
                              (micros)          (micros)           (bytes)
50%             0.00              0.00              0.00         268650950               NaN
75%             0.00              0.00              0.00        3449259151               NaN
95%             0.00              0.00              0.00       25628284214               NaN
98%             0.00              0.00              0.00       44285675122               NaN
99%             0.00              0.00              0.00       44285675122               NaN
Min             0.00              0.00              0.00            105779                 0
Max             0.00              0.00              0.00       442856751229223372036854776000
</code></pre>

<p>Cassandra version:</p>

<pre><code>[cqlsh 5.0.1 | Cassandra 3.11.2 | CQL spec 3.4.4 | Native protocol v4]
Use HELP for help.

Replication factor 3
4 node cluster
Getting the above message in one node only 
Tried repairing the table but failed with streaming error :

40328:ERROR [StreamReceiveTask:53] 2019-06-10 13:54:33,684 StreamSession.java:593 - [Stream #c9214180-8b82-11e9-90ce-399bac480141] Streaming error occurred on session with peer &lt;IP ADDRESS&gt;
40329-java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.IllegalStateException: Unable to compute ceiling for max when histogram overflowed
40330-  at org.apache.cassandra.utils.Throwables.maybeFail(Throwables.java:51) ~[apache-cassandra-3.11.2.jar:3.11.2]
40331-  at org.apache.cassandra.utils.FBUtilities.waitOnFutures(FBUtilities.java:373) ~[apache-cassandra-3.11.2.jar:3.11.2]
40332-  at org.apache.cassandra.index.SecondaryIndexManager.buildIndexesBlocking(SecondaryIndexManager.java:383) ~[apache-cassandra-3.11.2.jar:3.11.2]
40333-  at org.apache.cassandra.index.SecondaryIndexManager.buildAllIndexesBlocking(SecondaryIndexManager.java:270) ~[apache-cassandra-3.11.2.jar:3.11.2]
40334-  at org.apache.cassandra.streaming.StreamReceiveTask$OnCompletionRunnable.run(StreamReceiveTask.java:216) ~[apache-cassandra-3.11.2.jar:3.11.2]
40335-  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_144]
40336-  at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_144]
40337-  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_144]
40338-  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_144]
--
0354:ERROR [Reference-Reaper:1] 2019-06-10 13:54:33,907 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@7bd8303d) to class org.apache.cassandra.io.util.ChannelProxy$Cleanup@1084465868:PATH/talename-5b621cd0c53311e7a612ffada4e45177/mc-26405-big-Index.db was not released before the reference was garbage collected
</code></pre>

<p>Table description includes :</p>

<pre><code>AND bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';
</code></pre>

<p>Any idea why it is happening? Any help or suggestion is welcome. </p>
",<cassandra><cassandra-3.0>,"<p>You cannot have 2 billion cells in a partition. Also having a secondary index on a table with a 44gb partition is going to have issues for multiple reasons. There really isn't much you can do to fix this short of dropping your index and building a new data model to migrate into. You could build a custom version of Cassandra to ignore that exception but something else will come up very soon as you are at the extreme limits of whats even theoretically possible. You are already past a point that I am surprised is running.</p>

<p>If the streaming error is from repairs you can ignore it while you fix your data model. If it's from bootstrapping I think you will need a custom version of Cassandra to stay running in meantime (or can just ignore the down node you are replacing). Keep in mind node failures are a serious threat to you now as bootstrapping likely will not work. When you put so much in a single partition it cannot be scaled out so there are limited options.</p>
",['table']
56660980,56662987,2019-06-19 06:04:22,Get column type of a table using cql command,"<p>I am trying to get column type of a table using cql command.
My table:</p>

<pre><code>CREATE TABLE users (
    id uuid,
    name text);
</code></pre>

<p>Now I am trying to get type of <code>name</code> column. With the help of some select query I want to get <code>text</code> as output.</p>

<p>My use case is: I am trying to <code>drop name column only if type of name is text</code></p>

<p>What script should I try?</p>
",<cassandra><cql>,"<p>From CQL you can read this data from system tables. In Cassandra 3.x, this information is located in the <code>system_schema.columns</code> table that has following schema:</p>

<pre><code>CREATE TABLE system_schema.columns (
    keyspace_name text,
    table_name text,
    column_name text,
    clustering_order text,
    column_name_bytes blob,
    kind text,
    position int,
    type text,
    PRIMARY KEY (keyspace_name, table_name, column_name)
) WITH CLUSTERING ORDER BY (table_name ASC, column_name ASC);
</code></pre>

<p>so you can use query like this to retrieve the data:</p>

<pre><code>select type from system_schema.columns where keyspace_name = 'your_ks' 
and table_name = 'users' and column_name = 'name';
</code></pre>

<p>In Cassandra 2.x, the structure of the system tables is different, so you may need to adapt your query.</p>

<p>If you're accessing cluster programmatically, then the driver hides differences between Cassandra versions, and you can use something like <a href=""https://docs.datastax.com/en/developer/java-driver/3.7/manual/metadata/"" rel=""nofollow noreferrer"">Metadata class from Java driver</a> to get information about table's structure and types of columns. But if you're doing schema changes programmatically, you <strong>must</strong> be careful and explicitly wait for <a href=""https://docs.datastax.com/en/developer/java-driver/3.7/manual/metadata/#schema-agreement"" rel=""nofollow noreferrer"">schema agreement</a>, like in <a href=""https://github.com/alexott/dse-java-playground/blob/master/src/main/java/com/datastax/alexott/demos/jdtest1/WhiteListPolicyExample.java#L37"" rel=""nofollow noreferrer"">following example</a>.</p>
",['table']
56732496,56732593,2019-06-24 08:19:12,CQL : ConfigurationException: Keyspace refined_zone_uat doesn't exist,"<pre><code>create table refined_zone_uat.unprocessed_contact_records_fraudring (
trade_acc_number text,
contact_sf_id text ,
email text ,
ip_address text ,
mobile_phone text ,
first_name text ,
last_name text ,
acc_sf_id text ,
device_id text,
street text ,
post_code text ,
added_date timestamp ,
org_code text,
PRIMARY KEY (contact_sf_id, added_date)
);
</code></pre>

<p>SHowing error : ""<code>ConfigurationException: Keyspace refined_zone_uat doesn't exist</code>""</p>

<p>But Keyspace : ""<code>unprocessed_contact_records_fraudring</code>"" is available.</p>
",<cassandra>,"<p>Here is the create table syntax Faisal</p>

<p><a href=""https://docs.datastax.com/en/archived/cql/3.3/cql/cql_using/useCreateTable.html"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/archived/cql/3.3/cql/cql_using/useCreateTable.html</a></p>

<p>Looks like keyspace and table in your statement is reversed. It should be </p>

<pre><code>CREATE TABLE {keyspace}.{table_name} (....);
</code></pre>
",['table']
56736096,56736357,2019-06-24 11:59:54,Disk space not decreasing after gc_grace_seconds (10 days) elapsed,"<p>I deleted a lot of data(10 billions rows) from my table (made a small app that query from LONG.MIN_VALUE up to LONG.MAX_VALUE in token range and DELETE some data). </p>

<p><strong>Disk space did not decrease after 20 days from then (also I run nodetool repair on 1 node from total of 6), but number of keys(estimate) have decrease accordingly.</strong></p>

<p>Will the space decrease in the future in a natural way, or there is some utility from cassandra I need to run to reclaim the space?</p>
",<cassandra>,"<p>In general, yes, the space will decrease accordingly (once compaction runs). Depending on the compaction strategy chosen for that table, it could take some time. Size Tiered Compaction Strategy for example requires, by default, that 4 sstables be the same size before being compacted. If you have very large SSTABLES then they may not get compacted for quite some time, or indefinitely if there are not 4 of the same size. A manual compaction would fix that situation, but it would put everything in a single sstable, which is not recommended either. If the resulting sstable of a manual compaction is very small, then it won't hurt you. If it ends up compacting to a ""large"" SSTABLE, then you have sacrificed ""now"" for ""later"" (again, because you now have only a single large sstable, it may take a very long time for it to participate in compaction). You can split the sstable after a manual compaction to remidy the situation you've created, but you'll have to take your node off-line to do it. Anyway, short answer is that over time the table should shrink accordingly - when depends on the compaction strategy chosen.</p>
",['table']
56773837,56774700,2019-06-26 13:21:05,How can i update the column to a particular value in a cassandra table?,"<p>Hi I am having a cassandra table. my table has around 200 records in it . later i have altered the table to add a new column named budget which is of type boolean . I want to set the default value to be true for that column . what should be the cql looks like.</p>

<p>I am trying the following command but it didnt work</p>

<pre><code>cqlsh:Openmind&gt; update mep_primecastaccount set budget = true ;
SyntaxException: line 1:46 mismatched input ';' expecting K_WHERE
</code></pre>

<p>appreciate any help
thank you </p>
",<cassandra><cql><cassandra-2.0><cassandra-3.0><cql3>,"<p>Any operation that would require a cluster wide read before write is not supported (as it wont work in the scale that Cassandra is designed for). You must provide a partition and clustering key for an update statement. If theres only 200 records a quick python script or can do this for you. Do a <code>SELECT * FROM mep_primecastaccount</code> and iterate through ResultSet. For each row issue an update. If you have a lot more records you might wanna use spark or hadoop but for a small table like that a quick script can do it.</p>
",['table']
56774336,56775967,2019-06-26 13:44:52,create cassandra table for scala nested case class,"<p>I am storing a scala case class data in Cassandra table, for that, I need to define User-defined type. I can write cql query but do not know how to parse it.com.datastax.driver.mapping.annotations.UDT
    I have tried this annotation but it does not work me. I think I'm completely out of the track. 
    I have also tried Session class belong to <code>com.datastax.driver.core.Session.</code>
    and my conclusion is I have no idea how to do it I am just using hit and trail.</p>

<pre><code>case class Properties(name: String,
label: String,
                           description: String,
                           groupName: String,
                           fieldDataType: String,
                           options: Seq[OptionalData]
                         )
object Properties{
  implicit val format: Format[Properties] = Json.format[Properties]
}


case class OptionalData(label: String, name: String)
object OptionalData{
  implicit val format: Format[OptionalData] = Json.format[OptionalData]
}
</code></pre>

<p>and my query is:</p>

<pre><code>val optionalData: String=
    """"""
      |CREATE TYPE IF NOT EXISTS optionaldata(
      |label text,
      |name text
      );
    """""".stripMargin

   val createPropertiesTable: String =       """"""
                          |CREATE TABLE IF NOT EXISTS prop(
                          |name text Primary Key,
                          |label text,
                          |description text,
                          |groupname text,
                          |fielddatatype text,
                          |options LIST&lt;frozen&lt;optionaldata&gt;&gt;
                          );
                        """""".stripMargin
</code></pre>

<blockquote>
  <p>com.datastax.driver.core.exceptions.InvalidQueryException: Unknown typ
      e leadpropdb3.optionaldata
      java.util.concurrent.ExecutionException: com.datastax.driver.core.exceptions.InvalidQueryException: Unknown type leadpropdb3.optionaldata
              at com.google.common.util.concurrent.AbstractFuture.getDoneValue(AbstractFuture.java:552)
              at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:513)
              at akka.persistence.cassandra.package$ListenableFutureConverter$$anon$2.$anonfun$run$2(package.scala:25)
              at scala.util.Try$.apply(Try.scala:213)
              at akka.persistence.cassandra.package$ListenableFutureConverter$$anon$2.run(package.scala:25)
              at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)
              at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
              at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
              at java.lang.Thread.run(Thread.java:748)
      Caused by: com.datastax.driver.core.exceptions.InvalidQueryException: Unknown type leadpropdb3.optionaldata </p>
</blockquote>
",<scala><cassandra><lagom>,"<p>From error message it's clear that the type wasn't created - you need to create it before creating the table - be very careful when executing CQL statements from your code - you need to wait until schema is an agreement, before you execute next statement. Here is an <a href=""https://github.com/alexott/dse-java-playground/blob/master/src/main/java/com/datastax/alexott/demos/jdtest1/WhiteListPolicyExample.java#L37"" rel=""nofollow noreferrer"">example of Java code</a> that does this - it's easy to convert it into Scala.</p>

<p>When you're using Object Mapper with Scala, you need to obey some rules (I hope that my blog post on that topic will be published soon):</p>

<ol>
<li>You need to use Java types - <code>List</code> instead of <code>Seq</code>, etc., or use <a href=""https://github.com/datastax/java-driver-scala-extras"" rel=""nofollow noreferrer"">extra codecs for Scala</a>;</li>
<li>Case classes should have empty constructor.</li>
</ol>

<p>but otherwise it's possible to use object mapper with Scala, like this:</p>

<pre class=""lang-scala prettyprint-override""><code>@UDT(name = ""scala_udt"")
case class UdtCaseClass(id: Integer, @(Field @field)(name = ""t"") text: String) {
  def this() {
    this(0, """")
  }
}

@Table(name = ""scala_test_udt"")
case class TableObjectCaseClassWithUDT(@(PartitionKey @field) id: Integer,
                                       udt: UdtCaseClass) {
  def this() {
    this(0, UdtCaseClass(0, """"))
  }
}

// ...
val mapperForUdtCaseClass = manager.mapper(classOf[TableObjectCaseClassWithUDT])
val objectCaseClassWithUDT = mapperForUdtCaseClass.get(new Integer(1))
println(""ObjWithUdt(1)='"" + objectCaseClassWithUDT + ""'"")
</code></pre>

<p>More examples are available in <a href=""https://github.com/alexott/dse-java-playground/blob/master/src/main/scala/com/datastax/alexott/ObjMapperTest.scala"" rel=""nofollow noreferrer"">my repo</a>.</p>
",['table']
56802034,56829845,2019-06-28 06:38:34,restore with AWS EBS' snapshots on separate cluster,"<p>I have a cluster with 3 nodes - say cluster1 on AWS EC2 instances. The cluster is up and running, took snapshot of the keyspace's volume. </p>

<p>Now I want to restore few tables/keyspaces from the snapshot volumes, so I created another cluster say cluster2 and attached the snapshot volumes on to the new cluster's ec2 nodes (same number of nodes). Cluster2 is not starting bcz the system keyspace in the snapshot taken was having cluster name as cluster1 and the cluster on which it is being restored is cluster2. How do I do a restore in this case? I do not want to do any modifications to the existing cluster.</p>

<p>Also when I do restore do I need to think about the token ranges of the old and new cluster's mapping?</p>
",<amazon-ec2><cassandra>,"<p>Before starting the cluster2, it's important to ensure that none of the IP addresses of the cluster1 are included in the seed list of the cluster2 to ensure that they are kept unaware between them. Also, to remove from the path <code>data_file_directories</code> (as defined in the cassandra.yaml), the following directories:</p>

<ul>
<li>system</li>
<li>system_auth</li>
<li>system_distributed</li>
<li>system_traces</li>
</ul>

<p><strong><code>system_schema</code> should not be touched, as it contains the schema definition of the keyspaces and tables.</strong></p>

<p>Start the cluster, one node at a time; the first node should include its own IP address at the beginning of the seed list; This will be a one time change, and the change should be removed once that the cluster is up and running.</p>

<p>At this moment you should have a separate cluster, with the information and structure of the original cluster at the time that the snapshot was taken. To test this, execute <code>nodetool gossipinfo</code> and only the nodes of the cluster2 should be listed, login into cqlsh <code>describe keyspaces</code> should list all your keyspaces, and executing queries of your application should retrieve your data. You will note that Cassandra already generated the system* keyspaces, as well as dealt with the token distribution.</p>

<p>The next step is to update the name of the restored cluster, in each one of the nodes:</p>

<ol>
<li>Log into cqlsh</li>
<li>Execute <code>UPDATE system.local SET cluster_name = 'cluster2' where key='local';</code></li>
<li>exit cqlsh</li>
<li>run <code>nodetool flush</code></li>
<li>run <code>nodetool drain</code></li>
<li>edit the cassandra.yaml file, update <code>cluster_name</code> with the name 'cluster2'</li>
<li>restart the cassandra service</li>
<li>wait until the node is reported as NORMAL with <code>nodetool status</code> or <code>nodetool netstats</code></li>
<li>repeat with a different node</li>
</ol>

<p>At this point you will have 2 independent clusters, with different name.</p>
",['cluster_name']
56861187,56863348,2019-07-02 23:32:18,Internal network application data model with Cassandra,"<p>I'm working on designing an application which will enable users to send requests to connect with each other, see their sent or received requests, make notes during their interactions for later reference if connected, and remove users from their contact lists.</p>

<p>In a RDBMS, the schema would be:</p>

<p>table User with column </p>

<ul>
<li>uid (a unique string for each user)</li>
</ul>

<p>table Request with columns:</p>

<ul>
<li><p>from - user id</p></li>
<li><p>to - user id Primary Key (from, to)</p></li>
<li><p>created - timestamp</p></li>
<li><p>message - string</p></li>
<li><p>expiry - timestamp</p></li>
</ul>

<p>table Connection with columns:</p>

<ul>
<li><p>from - user id</p></li>
<li><p>to - user id</p></li>
</ul>

<p>Primary Key (from, to) </p>

<ul>
<li><p>notes - String</p></li>
<li><p>created - timestamp</p></li>
<li><p>modified - timestamp</p></li>
<li><p>isFavourite - to is a favourite of from user, value 0 or 1</p></li>
<li><p>isActive - soft delete, value 0 or 1</p></li>
<li><p>pairedConnection - shows whether the connection between to and from was deactivated (the to user removed the from user from its contact list), value 0 or 1</p></li>
</ul>

<p>The queries I anticipate to be needed are:</p>

<ul>
<li><p>find the sent requests for a user</p></li>
<li><p>find the received requests for a user</p></li>
<li><p>find all the active contacts of a given user</p></li>
<li><p>find all the favourites of a user</p></li>
<li><p>find all the users who deleted the given from user from their lists </p></li>
<li><p>update the notes taken by a user when meeting another user he is connected with</p></li>
<li><p>update user as favourite</p></li>
<li><p>mark connection for soft deletion</p></li>
</ul>

<p>I'm trying to model this in Cassandra, but feel confused about the keys to choose for max efficiency.</p>

<p>So far, I have the following ideas, and would welcome feedback from more experienced Cassandra users:</p>

<pre><code>create table users(
uid text PRIMARY KEY
); 

create table requestsByFrom(
from text,
to text,
message text,
created timestamp,
expiry timestamp,
PRIMARY KEY (from,to)

create table requestsByTo(
from text,
to text,
message text,
created timestamp,
expiry timestamp,
PRIMARY KEY (to,from)
);

create table connections(
from text,
to text,
notes text,
created timestamp,
modified timestamp,
isFavourite boolean,
isActive boolean,
pairedConnection boolean,
PRIMARY KEY (from,to)
);

create table activeConnections(
from text,
to text,
isActive boolean,
PRIMARY KEY (from,isActive)
);

create table favouriteConnections(
from text,
to text,
isFavourite boolean,
PRIMARY KEY (from, isFavourite)
);

create table pairedConnection(
from text,
to text,
pairedConnection boolean,
PRIMARY KEY ((from,to), pairedConnection)
);
</code></pre>
",<cassandra><datamodel>,"<p>Cassandra has a different paradigm to RDBMS, and this is more evident with the way that the data modeling has to be done. You need to keep in mind that denormalization is preferred, and that you'll have repeated data.</p>

<p>The tables definition should be based on the queries to retrieve the data, this is partially stated in the definition of the problem, for instance:</p>

<blockquote>
  <p>find the sent requests for a user</p>
</blockquote>

<p>Taking the initial design of the table <code>requestsByFrom</code>, an alternative will be</p>

<pre><code>CREATE TABLE IF NOT EXISTS requests_sent_by_user(
    requester_email TEXT,
    recipient_email TEXT,
    recipient_name TEXT,
    message TEXT,
    created TIMESTAMP
PRIMARY KEY (requester_email, recipient_email)
) WITH default_time_to_live = 864000;
</code></pre>

<p>Note that <code>from</code> is a restricted keyword, the <code>expiry</code> information can be set with the definition of the default_time_to_live clause (TTL) which will remove the record after the time defined; this value is the amount of seconds after the record is inserted, and the example is 10 days (864,000 seconds).</p>

<p>The primary key is suggested to be the email address, but it can also be an UUID, name is not recommended as there can be multiple persons sharing the same name (like <code>James Smith</code>) or the same person can have multiple ways to write the name (following the example <code>Jim Smith</code>, <code>J. Smith</code> and <code>j smith</code> may refer to the same person).</p>

<p>The name <code>recipient_name</code> is also added as it is most likely that you'll want to display it; any other information that will be displayed/used with the query should be added.</p>

<blockquote>
  <p>find the received requests for a user</p>
</blockquote>

<pre><code>CREATE TABLE IF NOT EXISTS requests_received_by_user(
    recipient_email TEXT,
    requester_email TEXT,
    requester_name TEXT,
    message TEXT,
    created TIMESTAMP
PRIMARY KEY (recipient_email, requester_email)
) WITH default_time_to_live = 864000;
</code></pre>

<p>It will be preferred to add records to <code>requests_sent_by_user</code> and <code>requests_received_by_user</code> at the same time using a <a href=""https://docs.datastax.com/en/dse/6.7/cql/cql/cql_using/useBatch.html"" rel=""nofollow noreferrer"">batch</a>, which will ensure consistency in the information between both tables, also the TTL (expiration of the data) will be the same.</p>

<blockquote>
  <p>storing contacts</p>
</blockquote>

<p>In the question there are 4 tables of connections: <code>connections</code>, <code>active_connections</code>, <code>favourite_connections</code>, <code>paired_connections</code>, what will be the difference between them? are they going to have different rules/use cases? if that is the case, it makes sense to have them as different tables:</p>

<pre><code>CREATE TABLE IF NOT EXISTS connections(
    requester_email TEXT,
    recipient_email TEXT,
    recipient_name TEXT,
    notes TEXT,
    created TIMESTAMP,
    last_update TIMESTAMP,
    is_favourite BOOLEAN,
    is_active BOOLEAN,
    is_paired BOOLEAN,
    PRIMARY KEY (requester_email, recipient_email)
 );

CREATE TABLE IF NOT EXISTS active_connections(
    requester_email TEXT,
    recipient_email TEXT,
    recipient_name TEXT,
    last_update TIMESTAMP,
    PRIMARY KEY (requester_email, recipient_email)
);

CREATE TABLE IF NOT EXISTS favourite_connections(
    requester_email TEXT,
    recipient_email TEXT,
    recipient_name TEXT,
    last_update TIMESTAMP,
    PRIMARY KEY (requester_email, recipient_email)
);

CREATE TABLE IF NOT EXISTS paired_connections(
    requester_email TEXT,
    recipient_email TEXT,
    recipient_name TEXT,
    last_update TIMESTAMP,
    PRIMARY KEY (requester_email, recipient_email)
);
</code></pre>

<p>Note that the boolean flag is removed, the logic is that if the record exists in <code>active_connections</code>, it will be assumed that it is an active connection.</p>

<p>When a new connection is created, it may have several records in different tables; to bundle all those inserts or updates, it is preferred to use <a href=""https://docs.datastax.com/en/dse/6.7/cql/cql/cql_using/useBatchTOC.html"" rel=""nofollow noreferrer"">batch</a></p>

<blockquote>
  <p>find all the active contacts of a given user</p>
</blockquote>

<p>Based on the proposed tables, if the requester's email is test@email.com:</p>

<pre><code>SELECT * FROM active_connections WHERE requester_email = 'test@email.com'
</code></pre>

<blockquote>
  <p>update user as favourite</p>
</blockquote>

<p>It will be a batch updating the record in <code>connections</code> and adding the new record to <code>favourite_connections</code>:</p>

<pre><code>BEGIN BATCH

UPDATE connections 
SET is_favourite = true, last_update = dateof(now())
WHERE requester_email ='test@email.com' 
  AND recipient_email = 'john.smith@test.com';

INSERT INTO favourite_connections (
    requester_email, recipient_email, recipient_name, last_update
) VALUES (
    'test@email.com', 'john.smith@test.com', 'John Smith', dateof(now())
);
APPLY BATCH;
</code></pre>

<blockquote>
  <p>mark connection for soft deletion</p>
</blockquote>

<p>The information of the connection can be kept in <code>connections</code> with all the flags disabled, as well as the records removed from <code>active_connections</code>, <code>favourite_connections</code> and <code>paired_connections</code></p>

<pre><code>BEGIN BATCH

UPDATE connections 
SET is_active = false, is_favourite = false,
    is_paired = false, last_update = dateof(now())
WHERE requester_email ='test@email.com' 
  AND recipient_email = 'john.smith@test.com';

DELETE FROM active_connections 
WHERE requester_email = 'test@email.com' 
  AND recipient_email = 'john.smith@test.com';

DELETE FROM favourite_connections 
WHERE requester_email = 'test@email.com' 
  AND recipient_email = 'john.smith@test.com';

DELETE FROM paired_connections 
WHERE requester_email = 'test@email.com' 
  AND recipient_email = 'john.smith@test.com';

APPLY BATCH;
</code></pre>
",['table']
56963696,56964583,2019-07-10 05:04:05,Store undetermined data in Cassandra table,"<p>I am doing research in different NoSQL dbs, and now stuck with understanding Cassandra.</p>

<p>Lets say I have two models that have n fields same for both instances and m fields that differ.</p>

<pre><code>{
    name:""Bob"",
    surname:""Smith"",
    age:31,
    carName: ""toyota""
}

{
    name:""Ann"",
    surname:""Cox"",
    position:""Architect""
}
</code></pre>

<p>So here I have <code>name</code> and <code>surname</code> for both models, but other fields can be any fields.</p>

<p>Is it possible to design data table in Cassandra for such purpose or wide-column oriented databases not fit and better to use document oriented?</p>
",<database-design><cassandra>,"<p>If fields are relative to the same “entity” then create a table with all of them and specify only relevant fields during inserts. Simple as that. Instead, if they are relative to a different entity then create two different tables.</p>
",['table']
57026622,57036604,2019-07-14 10:39:55,Non-frozen UDTs are not allowed inside collections CassandraCSharpDriver,"<p>I am cassandra for custom logging my <code>.netcore</code> project, i am using CassandraCSharpDriver.</p>

<p><strong>Problem:</strong>
I have created UDT for params in log, and added list of paramUDT in Log table as frozen.
But i am getting error: <code>Non-frozen UDTs are not allowed inside collections</code>. I don't know why ia m getting this error because i am using Frozen attribute on list i am using in Log Model.</p>

<pre><code>logSession.Execute($""CREATE TYPE IF NOT EXISTS {options.Keyspaces.Log}.{nameof(LogParamsCUDT)} (Key text, ValueString text);"");
</code></pre>

<p>Here is model:</p>

<pre><code>   public class Log
    {
        public int LoggingLevel { get; set; }
        public Guid UserId { get; set; }
        public string TimeZone { get; set; }
        public string Text { get; set; }
        [Frozen]
        public IEnumerable&lt;LogParamsCUDT&gt; LogParams { get; set; }
    }
</code></pre>

<p>Question where i am doing wrong, is my UDT script not correct or need to change in model.</p>

<p>Thanks in advance</p>
",<c#><asp.net-core><cassandra><cassandra-3.0>,"<p>I've tried using that model and <code>Table.CreateIfNotExists</code> ran successfully.</p>

<p>Here is the the code:</p>

<pre><code>public class Program
    {
        public static void Main()
        {
            var cluster = Cluster.Builder().AddContactPoint(""127.0.0.1"").Build();
            var session = cluster.Connect();
            session.CreateKeyspaceIfNotExists(""testks"");
            session.ChangeKeyspace(""testks"");
            session.Execute($""CREATE TYPE IF NOT EXISTS testks.{nameof(LogParamsCUDT)} (Key text, ValueString text);"");
            session.UserDefinedTypes.Define(UdtMap.For&lt;LogParamsCUDT&gt;($""{nameof(LogParamsCUDT)}"", ""testks""));
            var table = new Table&lt;Log&gt;(session);
            table.CreateIfNotExists();
            table.Insert(new Log
            {
                LoggingLevel = 1,
                UserId = Guid.NewGuid(),
                TimeZone = ""123"",
                Text = ""123"",
                LogParams = new List&lt;LogParamsCUDT&gt;
                {
                    new LogParamsCUDT
                    {
                        Key = ""123"",
                        ValueString = ""321""
                    }
                }
            }).Execute();
            var result = table.First(l =&gt; l.Text == ""123"").Execute();
            Console.WriteLine(JsonConvert.SerializeObject(result));
            Console.ReadLine();
            table.Where(l =&gt; l.Text == ""123"").Delete().Execute();
        }
    }

    public class Log
    {
        public int LoggingLevel { get; set; }
        public Guid UserId { get; set; }
        public string TimeZone { get; set; }
        [Cassandra.Mapping.Attributes.PartitionKey]
        public string Text { get; set; }
        [Frozen]
        public IEnumerable&lt;LogParamsCUDT&gt; LogParams { get; set; }
    }

    public class LogParamsCUDT
    {
        public string Key { get; set; }

        public string ValueString { get; set; }
    }
</code></pre>

<p>Note that I had to add the <code>PartitionKey</code> attribute or else it wouldn't run.</p>

<p>Here is the CQL statement that it generated:</p>

<pre><code>CREATE TABLE Log (
    LoggingLevel int, 
    UserId uuid, 
    TimeZone text, 
    Text text, 
    LogParams frozen&lt;list&lt;""testks"".""logparamscudt""&gt;&gt;, 
    PRIMARY KEY (Text)
)
</code></pre>

<p>If I remove the <code>Frozen</code> attribute, then this error occurs: <code>Cassandra.InvalidQueryException: 'Non-frozen collections are not allowed inside collections: list&lt;testks.logparamscudt&gt;'</code>.</p>

<p>If your intention is to have a column like this <code>LogParams frozen&lt;list&lt;""testks"".""logparamscudt""&gt;&gt;</code> then the <code>Frozen</code> attribute will work. If instead you want only the UDT to be frozen, i.e., <code>LogParams list&lt;frozen&lt;""testks"".""logparamscudt""&gt;&gt;</code>, then AFAIK the <code>Frozen</code> attribute won't work and you can't rely on the driver to generate the <code>CREATE</code> statement for you.</p>

<p>All my testing was done against cassandra <code>3.0.18</code> using the latest C# driver (<code>3.10.1</code>).</p>
",['table']
57028043,57036880,2019-07-14 13:57:01,Ignore Attribute does't work CassandraCSharpDriver,"<p>I am using some properties in Entity model for maintaining relationships, I am using <code>[Ignore]</code> for ignoring that property from table.</p>

<pre><code>public class User : IdentityUser&lt;Guid&gt;
    {
        [Ignore]
        public string Password { get; set; }
        public string FirstName { get; set; }
        public string LastName { get; set; }
        public string CommonName { get; set; }
        public string ProfilePhoto { get; set; }
        public bool IsDeleted { get; set; }
        [Ignore]
        public virtual ICollection&lt;UserRole&gt; UserRoles { get; set; }
    }

var User = new Table&lt;User&gt;(dataSession);
                User.CreateIfNotExists();
</code></pre>

<p>When i try to create using above code i get error.</p>

<p><a href=""https://i.stack.imgur.com/3iqFZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3iqFZ.png"" alt=""enter image description here""></a></p>

<p><strong>Question:</strong> Am i using wrong script for creating table or wrong way to ignore?</p>

<p>Thanks in advance</p>
",<c#><cassandra><cassandra-3.0>,"<p>Check if you are using the correct namespace for the <code>Ignore</code> attribute. <code>Cassandra.Mapping.Attributes.Ignore</code> is the correct one and the other one is deprecated.</p>

<pre><code>public class Program
    {
        public static void Main()
        {
            var cluster = Cluster.Builder().AddContactPoint(""127.0.0.1"").Build();
            var session = cluster.Connect();

            var User = new Table&lt;User&gt;(session, MappingConfiguration.Global, ""users"", ""testks"");
            User.CreateIfNotExists();
            var u = new User
            {
                Id = Guid.NewGuid(),
                Password = ""123"",
                FirstName = ""123"",
                LastName = ""123"",
                CommonName = ""123"",
                ProfilePhoto = ""321"",
                IsDeleted = false,
                UserRoles = new List&lt;UserRole&gt;
                {
                    new UserRole
                    {
                        Text = ""text""
                    }
                }
            };
            User.Insert(u).Execute();
            var result = User.First(l =&gt; l.Id == u.Id).Execute();
            Console.WriteLine(JsonConvert.SerializeObject(result));
            Console.ReadLine();
            User.Where(l =&gt; l.Id == u.Id).Delete().Execute();
        }
    }

    public class User : IdentityUser&lt;Guid&gt;
    {
        [Cassandra.Mapping.Attributes.Ignore]
        public string Password { get; set; }

        public string FirstName { get; set; }
        public string LastName { get; set; }
        public string CommonName { get; set; }
        public string ProfilePhoto { get; set; }
        public bool IsDeleted { get; set; }

        [Cassandra.Mapping.Attributes.Ignore]
        public virtual ICollection&lt;UserRole&gt; UserRoles { get; set; }
    }

    public class IdentityUser&lt;T&gt;
    {
        [Cassandra.Mapping.Attributes.PartitionKey]
        public T Id { get; set; }
    }

    public class UserRole
    {
        public string Text { get; set; }
    }
</code></pre>

<p>Running the code above against Cassandra <code>3.0.18</code> with C# driver <code>3.10.1</code> seems to work correctly. The <code>Password</code> and <code>UserRoles</code> will not exist in the table schema and they will both be <code>null</code> when executing <code>SELECT</code> statements with <code>Linq2Cql</code>.</p>
",['table']
57037621,57077574,2019-07-15 10:13:23,Need to set authorization for a single table inside keyspace,"<p>I am using cassandra 3.11.4 and CQL spec 3.4.4 
All of my node microservices using cassandra do not use any kind of authentication. The authenticator field inside the cassandra yaml is currently set to authenticator: AllowAllAuthenticator</p>

<p>The issue is that I want to create authorization with roles attached to it for a single table inside my keyspace, while not requiring anything as such for other tables in the keyspace.</p>

<p>Is something like this possible? As I can check, my services using express cassandra start throwing error as soon as I change the authenticator: PasswordAuthenticator. </p>

<p>In my production, I would not want that since that will break my working services and they will start throwing error asking for auth info in the connections.</p>
",<authentication><cassandra><authorization>,"<pre><code>&lt;opinion&gt;
</code></pre>

<p>Running an unsecured Cassandra cluster is a terrible, <strong>terrible</strong> idea.  Especially one used by multiple services or applications.</p>

<pre><code>&lt;/opinion&gt;
</code></pre>

<p>Here's what I would do:</p>

<ol>
<li><p>Recreate the ""secure"" table to its own keyspace.  Keeping it in the original keyspace makes it easier to isolate and apply security.</p></li>
<li><p>Come up with a username and password for all services to use.  Ex: <code>shareduser</code>/<code>blahblahblah</code></p></li>
<li><p>Set your services to use the shared user and its password.  The Cassandra drivers can send credentials along with auth disabled on the cluster...it simply ignores them.</p></li>
<li><p>Turn on Authentication and Authorization on one node, and restart.  Your services will continue to connect through the remaining, insecure nodes.</p></li>
<li><p>Authenticate to the secure node with cqlsh.  Alter the replication of the <code>system_auth</code> keyspace to replicate to at least 3 nodes in each DC.  Then create both the shared user and the secured user.</p></li>
<li><p>Grant the secure user access to the required table in its new keyspace.  Grant the shared user access to the original keyspace.</p></li>
</ol>

<p>Ex:</p>

<pre><code>GRANT ALL PERMISSIONS ON keyspace2.securetable TO secureuser;

GRANT ALL ON KEYSPACE keyspace1 TO shareduser;
</code></pre>

<ol start=""7"">
<li><p>Verify that the users can access the required tables.</p></li>
<li><p>Turn on Authentication and Authorization on the remaining nodes, and restart Cassandra on them.</p></li>
</ol>

<p>These steps should allow your existing services should continue to work without issue.</p>
",['table']
57066145,57190953,2019-07-16 22:16:00,How to connect Python's cosmos_client to Cosmos DB instance using Cassandra API?,"<p>I have a Cosmos DB (Cassandra API) instance set up and I'd like to manage it's throughput from a Python application. I'm able to create a azure.cosmos.cosmos_client using the cassandra endpoint and primary password listed in Azure without errors, but all attempted interactions with the client result in ""azure.cosmos.errors.HTTPFailure: Status code: 404"". </p>

<p>I am already successfully interacting with this database through cassandra-driver in Python, but I'd like access to the cosmos-client to manage throughput provisioning via code. I want to autoscale throughput as database use fluctuates between high levels of utilization and almost no activity.</p>

<p>Creating a cosmos_client requires a valid URI, with schema (https/http/ftp etc...) included. The endpoint listed on azure which was successfully used to connect via cqlsh as well as the Python cassandra-driver did not specify schema. 
I added ""https://"" to the beginning of the provided endpoint and was able to create the client in Python (""http://"" results in errors, also verified incorrect addresses also result in errors even with ""https://""). 
Now that I have a client object created, any interaction I attempt with it gives me 404 errors.</p>

<pre><code>client = cosmos_client.CosmosClient(f'https://{COSMOS_CASSANDRA_ENDPOINT}', {'masterKey': COSMOS_CASSANDRA_PASSWORD} )

client.ReadEndpoint
        #'https://COSMOS_CASSANDRA_ENDPOINT'

client.GetDatabaseAccount(COSMOS_CASSANDRA_ENDPOINT)
        #azure.cosmos.errors.HTTPFailure: Status code: 404

client.ReadDatabase(EXISTING_KEYSPACE_NAME)
        #azure.cosmos.errors.HTTPFailure: Status code: 404
</code></pre>

<p>I'm wondering if using the cosmos_client is the correct way to interact with the Cosmos Cassandra instance to modify throughput from my Python application. If so, how should I set up the cosmos_client properly?
Perhaps there is a way to do this directly through database modifications using cassandra-driver.</p>
",<python><cassandra><azure-cosmosdb><autoscaling><azure-cosmosdb-cassandra-api>,"<p>I could never get this to work after toiling for a while with trying and failing to access the database via CosmosClient or DocumentClient in Python and .NET. Ultimately I found 2 methods that are each unfortunately a bit hacky and present some challenges that seem unnecessary.</p>

<p>What I ended up doing was accomplishing this via a subprocess calling to the Azure CLI to change throughput. This is the command that is executed: </p>

<pre><code>f'az cosmosdb cassandra table throughput update --account-name {__cosmos_instance_name} --keyspace-name {__cassandra_keyspace} --name {table_name} --resource-group {__cosmos_resource_group} --throughput {new_throughput}'
</code></pre>

<p>What is very unfortunate about both methods that I found to work is that this doesn't work when the target database is being throttled due to rate limiting. This meant we also had to implement some logic to throttle our own service's interactions with the database before calling the code to perform scaling.</p>

<p>Some other notes about our solution: 
The service is hosted in kubernetes, so we had the metric evaluation and scaling execution added to the lifecycle hooks on the pod. The auto-scaler is also also executed when we encounter suspected rate limiting during cassandra interactions when handling cassandra.cluster.NoHostAvailable exceptions. </p>

<p>...</p>

<p>The other way I could set the provisioned throughput from code was via executing cql directly through cassandra-driver by doing the following (in Python):</p>

<pre><code>from cassandra.cqlengine import connection

connection.setup(&lt;CONNECTION_SETUP_ARGS&gt;)
session = connection.get_session()
session.execute(""use &lt;CASSANDRA_NAMESPACE&gt;"")
session.execute(""alter table &lt;CASSANDRA_TABLE_NAME&gt; with cosmosdb_provisioned_throughput=&lt;DESIRED_THROUGHPUT&gt;"")
</code></pre>

<p>When I get a chance I'll switch to this approach since it doesn't require Azure CLI installation and subprocess calls.</p>

<p>I think I got this idea originally from <a href=""https://learn.microsoft.com/en-us/azure/cosmos-db/cassandra-spark-ddl-ops#alter-table"" rel=""nofollow noreferrer"">here</a>.</p>
",['table']
57117010,57117176,2019-07-19 17:15:34,When does table id changes,"<p>When does table id as recorded in system_schema.tables and data directory change and how does Cassandra read directories to get the latest schema</p>

<p>Looking at the data directory cd $CASSANDRA_DATA/keyspace/ I see two directories </p>

<p>drwxr-xr-x 4 cassandra users 4096 June 27 20:47 deviceData-c31406e0eda011e88cce75b7a7f02232</p>

<p>drwxr-xr-x 3 cassandra users 4096 June 30 15:22 deviceData-a0ba3490f28511e88cce75b7a7f02232</p>

<p>When querying the table schema I see  </p>

<pre><code>select keyspace_name, table_name, id from system_schema.tables where keyspace_name='devices' ;

keyspace_name | table_name | id
------------------------------------------------------------------------------
devices | deviceData | a0ba3490f28511e88cce75b7a7f02232
</code></pre>

<p>So my question is: what causes id to change (is it alter statement ) as well as how does C* decide which structure to use.</p>
",<cassandra><cassandra-3.0>,"<p>A table has a single UUID (and a single tablename-UUID directory) throughout its lifetime. The UUID is assigned on coordinator node when table creating statement is executed. The directories are created on replicas as they process schema change notifications.</p>

<p>You can get different tablename-UUID directories after you dropped a table and recreated with the same name. Only one table with a given name is active at a time.</p>

<p>You can also end up with two directories if you try to create a table with given name concurrently on two different nodes. You shouldn't try to do that though (<a href=""https://github.com/scylladb/scylla/issues/420"" rel=""nofollow noreferrer"">https://github.com/scylladb/scylla/issues/420</a>)</p>

<p><a href=""https://groups.google.com/forum/#!topic/scylladb-users/mbb2iquAAEA"" rel=""nofollow noreferrer"">Reference</a> </p>
",['table']
57150948,57151404,2019-07-22 17:25:02,"Spark: daily read from Cassandra and save to parquets, how to read only new rows?","<p>I am trying to build an ETL process with Spark. My goal is to read from 
Cassandra table and save into parquet files.</p>

<p>What I managed to do so far is reading an entire table from Cassandra, using a Cassandra connector (in pyspark):</p>

<pre><code>df = app.sqlSparkContext.read.format(""org.apache.spark.sql.cassandra"")\
        .option(""table"", my_table)\
        .option(""keyspace"",my_keyspace)\
        .load()
</code></pre>

<p>The issue is that my data is growing rapidly, and I would like to repeat the ETL process everyday where I read the newly added rows from Cassandra and save them into a new parquet file.</p>

<p>Having there is no ordering in my Cassandra table, I will not be able to read based on time, is there any way to do it from Spark side instead?</p>
",<apache-spark><cassandra><spark-cassandra-connector>,"<p>The effective filtering based on time is really possible only if you have time-based first clustering column, something like this:</p>

<pre><code>create table test.test (
  pk1 &lt;type&gt;,
  pk2 &lt;type&gt;,
  cl1 timestamp,
  cl2 ...,
  primary key ((pk1, pk2), cl1, cl2));
</code></pre>

<p>In this case, condition on <code>cl1</code>, like this:</p>

<pre class=""lang-scala prettyprint-override""><code>import org.apache.spark.sql.cassandra._
val data = { spark.read.cassandraFormat(""test"", ""test"").load()}
val filtered = data.filter(""cl1 &gt;= cast('2019-03-10T14:41:34.373+0000' as timestamp)"")
</code></pre>

<p>will be effectively pushed into Cassandra, and filtering will happen server side, retrieving only necessary data - this is easy to check with explain - it should generate something like this (pushed filter denoted as <code>*</code>):</p>

<pre><code>// *Filter ((cl1#23 &gt;= 1552228894373000))
// +- *Scan org.apache.spark.sql.cassandra.CassandraSourceRelation [pk1#21,pk2#22L,cl1#23,...] 
PushedFilters: [*GreaterThanOrEqual(cl1,2019-03-10 14:41:34.373)], 
ReadSchema: struct&lt;pk1:int,pk2:int,cl1:timestamp,...
</code></pre>

<p>In all other cases, filtering will happen on Spark side, retrieving all data from Cassandra.</p>
",['table']
57241604,57242883,2019-07-28 13:58:33,Cassandra Says Listening on Port 9042 But Couldn't Connect It,"<p>I've running cassandra on my local machine. </p>

<p>I've starting it <code>sudo service cassandra start</code>. And then check logs under <code>var/log/cassandra/system-log</code> and it says:</p>

<pre><code>INFO  [main] 2019-07-28 13:13:17,226 Server.java:162 - Starting listening for CQL clients on localhost/127.0.0.1:9042 (unencrypted)...
INFO  [main] 2019-07-28 13:13:17,270 CassandraDaemon.java:501 - Not starting RPC server as requested. Use JMX (StorageService-&gt;startRPCServer()) or nodetool (enablethrift) to start it
INFO  [SharedPool-Worker-1] 2019-07-28 13:13:27,133 ApproximateTime.java:44 - Scheduling approximate time-check task with a precision of 10 milliseconds
INFO  [OptionalTasks:1] 2019-07-28 13:13:27,298 CassandraRoleManager.java:339 - Created default superuser role 'cassandra'
</code></pre>

<p>Then I try to connect with <code>cqlsh</code> in terminal and it says: 
<code>Connection error: ('Unable to connect to any servers', {'127.0.0.1:9042': error(111, ""Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused"")})</code></p>

<p>What is wrong? Also I couldn't see 9042 port with <code>netstat -tulpn</code> command.</p>
",<cassandra><cqlsh>,"<ol>
<li>Go to /etc/cassandra/cassandra-env.sh</li>
</ol>

<p>Uncomment  </p>

<p><code># JVM_OPTS=""$JVM_OPTS -Djava.rmi.server.hostname=&lt;public name&gt;""</code>   </p>

<p>and change it to </p>

<pre><code>JVM_OPTS=""$JVM_OPTS -Djava.rmi.server.hostname==localhost""
</code></pre>

<ol start=""2"">
<li>Set listen_address and broadcast_rpc_address to local ip (get ip address from ifconfig).</li>
<li>Restart Cassandra.</li>
</ol>
","['broadcast_rpc_address', 'listen_address']"
57324790,57325558,2019-08-02 10:36:52,How many Max number of tables I can create in a given Cassandra cluster ? Is there a limit?,"<p>Lets say I'm having 6 node cluster having m4.2xl (~ 8CPU 32GB RAM)
 - How many max tables I can create across keyspaces? and  Is there a limit on max tables for a given Keyspace?      </p>

<p>Highly Appreciate your response!</p>
",<cassandra><datastax><cassandra-3.0>,"<p>There could be performance degradation when you have too many tables in the cluster. For every table you need to allocate an additional memory, etc. independent if anybody writes into it or not.  From DataStax documentation:</p>

<blockquote>
  <p>The table thresholds have additional dependencies on JVM Heap and the byte count. Each table uses approximately 1 MB of memory. For each table being acted on, there is a memtable representation in JVM Heap. Tables with large data models increase pressure on memory. Each keyspace also causes additional overhead in JVM memory; therefore having lots of keyspaces may also reduce the table threshold. </p>
</blockquote>

<p><a href=""https://docs.datastax.com/en/dse-planning/doc/planning/planningAntiPatterns.html#planningAntiPatterns__AntiPatTooManyTables"" rel=""nofollow noreferrer"">DataStax recommends</a> not to have more than 500, although I have seen more, but it required an additional tuning of table parameters.</p>
",['table']
57344823,57345048,2019-08-04 07:48:58,What is the data type of timestamp in cassandra,"<p>I notice that if my model has a field <code>expirationTime</code> of type <code>DateTime</code> then I cannot store it iin <code>timestamp</code> field in <code>Cassandra</code>.</p>

<pre><code>QueryBuilder.set(""expiration_time"",model.expirationTime) //data gets corrupted
</code></pre>

<p>But if I store time as <code>milli seconds</code> then it works.</p>

<pre><code>QueryBuilder.set(""expiration_time"",model.expirationTime.getMillis()) //WORKS
</code></pre>

<p>Question 1 - Does that mean that the <code>timestamp</code> field in <code>cassandra</code> is of type <code>long</code>? 
Question2 - Is it <code>cqlsh</code> which converts the time into readable format like <code>2018-05-18 03:21+0530</code>??</p>
",<cassandra>,"<p>From DataStax <a href=""https://docs.datastax.com/en/archived/cql/3.3/cql/cql_reference/cql_data_types_c.html"" rel=""nofollow noreferrer"">documentation on CQL types</a>:</p>

<blockquote>
  <p>Date and time with millisecond precision, encoded as 8 bytes since epoch. Can be represented as a string, such as 2015-05-03 13:30:54.234.</p>
</blockquote>

<p>In Java as input you can use either long with milliseconds, or string literal, supported in CQL, or <code>java.util.Date</code> (see the <a href=""https://github.com/datastax/java-driver/blob/3.x/driver-core/src/main/java/com/datastax/driver/core/TypeCodec.java#L1479"" rel=""nofollow noreferrer"">code</a>).  When reading, results mapped into <code>java.util.Date</code> in driver 3.x/1.x (see <a href=""https://docs.datastax.com/en/developer/java-driver/3.6/manual/#cql-to-java-type-mapping"" rel=""nofollow noreferrer"">full table for CQL&lt;->Java types mapping</a>), or to the <code>java.time.Instant</code> in the driver 4.x/2.x (see <a href=""https://docs.datastax.com/en/developer/java-driver/4.1/manual/core/#cql-to-java-type-mapping"" rel=""nofollow noreferrer"">CQL&lt;->Java types mapping</a>).</p>

<p>In Python/cqlsh, yes - the data is read as 8-byte long, then is then converted into string representation.</p>
",['table']
57407164,57412641,2019-08-08 07:17:51,Cassandra Time Window Compaction Strategy,"<p>There is Cassandra table:</p>

<pre><code>CREATE TABLE data.data (
dataid bigint,
sequencenumber bigint,
createdat timestamp,
datetime timestamp,
PRIMARY KEY (dataid, sequencenumber)) WITH CLUSTERING ORDER BY (sequencenumber ASC)
AND bloom_filter_fp_chance = 0.01
AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
AND comment = ''
AND compaction = {'class': 'org.apache.cassandra.db.compaction.TimeWindowCompactionStrategy', 'compaction_window_size': '7', 'compaction_window_unit': 'DAYS', 'max_threshold': '32', 'min_threshold': '4'}
AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
AND crc_check_chance = 1.0
AND dclocal_read_repair_chance = 0.1
AND default_time_to_live = 0
AND gc_grace_seconds = 3600
AND max_index_interval = 2048
AND memtable_flush_period_in_ms = 0
AND min_index_interval = 128
AND read_repair_chance = 0.0
AND speculative_retry = '99PERCENTILE';
CREATE INDEX data_datetime_idx ON data.data(datetime);
</code></pre>

<p>Writing data with write options ttl for 7 days.
What I noticed, every same day of week we hit big Cassandra node load, especially big wa (I/O). I think this is related with compaction strategy. Should I use this strategy with lesser compaction_window_strategy e.g. 3 days? How to tune compaction strategy with ttl? How these params correlates? O maybe I have wrong primary key?</p>

<p>Cassandra ring 3x nodes, 8CPU, 16GB ram. Every node load ~90GiB.    </p>
",<cassandra>,"<p>Your TWCS configuration seems sub-optimal. What you've told Cassandra to do is have a window/bucket (consolidation) occur every 7 days, which is also your TTL. Typically what you want, from what I have read, is 15-30 ""buckets"" for your TTL period. That being said, what you'd want to do in your case is take 7 days and divide that into, say, 30 buckets. If you changed it to 12 HOUR buckets, you'd have 14 buckets, which seems OK. </p>

<p>For 12 hours, STCS would occur for the current bucket/window. At the 12 hour mark, all of the sstables that existed in that window would be consolidated into a single sstable. After 7 days, you'd have 14 sstables in which the oldest could simply be deleted (v.s. a compaction comparison). </p>

<p>As long as you're not updating or deleting rows that cross windows, TWCS can save a lot of resources and is very efficient. We use it whenever we can. If you're updating rows that exist in a prior bucket, TWCS is not a good choice.</p>

<p>Also remember to turn off repair on the table that has TWCS. I have seen that mess things up quite badly. </p>

<p>As for your big wait I/O issues, it could be compaction, could be flushing, could be many things. With your current TWCS configuration, it could be compaction (depending on how many and how large the sstables are). I think you could try to use other tools to see where the busy threads are (e.g. ttop). Either way, I'd fix your TWCS configuration to be in line with best practices.</p>

<p>-Jim</p>
",['table']
57423067,57435052,2019-08-09 03:18:20,Which compaction strategy to use for both read/write intensive program using scylla db,"<p>I have a program intensively read and write (same amount of read and write, for write, 4/5 update and 1/5 insert).
Is SizedTired compaction better than Leveled one?</p>

<p>Also most of data have TTL 7 days and others are 1 day. In this case, 
is Time Window strategy preferred?</p>
",<cassandra><cql><cqlsh><scylla>,"<p>Timewindow isn't a good fit since you have updates which make it less ideal.
Sizetier performs the best with the cost of more volume usage.
Check the table for compaction algorithm selection here:
<a href=""https://www.scylladb.com/webinar/on-demand-webinar-best-practices-for-data-modeling/"" rel=""noreferrer"">https://www.scylladb.com/webinar/on-demand-webinar-best-practices-for-data-modeling/</a></p>

<p>Usually STCS is the best default</p>
",['table']
57479877,57480662,2019-08-13 14:26:58,Data is replicated/copied on my 2nd node even with a replication factor of 1 for the key-space,"<p>I have a Cassandra cluster of 3 nodes and I create a keyspace 'abcd' using SimpleStrategy and ReplicationFactor 1. Since I have chosen RF as 1, I assume that any writes to my node-1 should not be replicated across the other 2 nodes. </p>

<p>But when I inserted a record into keyspace/table, I saw this new row is getting inserted in to all nodes in my cluster.</p>

<p>My question is since I have chosen RF as 1 for this keyspace, I would have expected only one node (i.e. node-1) in this cluster should have owned this data, not the rest of the nodes.</p>

<p>Pease correct me if my understanding is wrong.</p>
",<cassandra><cassandra-2.0>,"<p>Since your RF is 1, your data is getting written to only one node. But you can access that data from running the select query from other nodes also as any node in a Cassandra cluster is able to access all the data present in Cluster.</p>

<p>If the node from which you are running the query does not have the data, it will fetch the data from other nodes and display the result.</p>

<p>You can check which exact node has the data by running <a href=""https://docs.datastax.com/en/archived/cassandra/2.1/cassandra/tools/toolsGetEndPoints.html"" rel=""nofollow noreferrer"">nodetool getendpoints</a>.</p>

<p>You will need to mention your keyspace, table name and partition key.  </p>
",['table']
57491220,57689051,2019-08-14 08:47:05,Cassandra is not deleting rows after TTL expires due to Boolean columns,"<p>Having a table where I am setting the TTL to 7 days, I expect that the Cassandra will delete the rows after 7 days. </p>

<p>My table contains Boolean columns which set the column to True ONLY when creating new rows on the table. The columns are never updated to new value and so these ones are not changing the TTL value with a new TTL value (because Cassandra is column-oriented database).</p>

<p>However, I noticed that after 7 days all the columns are set to NULL values (as expected to get deleted) except from the Boolean columns which remain True and as a result, the rows are never deleted. 
Checked the TTL value of all columns and they have NULL values which means that TTL has expired on ALL columns including the Boolean columns.</p>

<p>When setting the Boolean columns MANUALLY to NULL (after the 7 days) then the rows will be removed immediately as expected.</p>

<p>I can not understand why Cassandra is not setting the Boolean columns to NULL after TTL expires so the rows will be deleting automatically. 
Is the Cassandra working in a different way with Boolean columns and TTL values? </p>

<p>Working with:</p>

<ul>
<li>Python 3.6 and </li>
<li>Cassandra 3.11</li>
</ul>
",<cassandra><cassandra-3.0>,"<p><strong>Solution</strong>: After running <code>nodetool flush</code>, the issue fixed and the rows are getting deleted when TTL expires. </p>

<p>That's really weird but it is working and I can see the rows getting deleted immediately. I can not find a reason of not deleting expired rows on the table even though the rows have been marked as expired (<em>expires= true</em>) on sstables.</p>
",['table']
57673085,57683936,2019-08-27 10:50:04,Alternative to Cassandra's TimeUUID in PostgreSQL that supports relational operations,"<p>I need to migrate a table from Cassandra to PostgreSQL. </p>

<p><strong>What I need to migrate:</strong> The table has a TimeUUID column for storing time as UUID. This column also served as clustering key. Time was stored as UUID to avoid collisions when rows are inserted in the same millisecond. Also, this column was involved in where clause, typically <code>timeUUID between 'foo' and 'bar'</code> and it produced correct results.</p>

<p><strong>Where I need to migrate it to:</strong> I'm moving to Postgres so need to find a suitable alternative to this. PostgreSQL has UUID data type but from what I've read and tried so far it stores it as 4-byte int but it treats UUID similar to String when used in where clause with relational operator. </p>

<p><code>select * from table where timeUUID &gt; 'foo'</code> will have <code>xyz</code> in the result. </p>

<p>According to my understanding, it is not necessary for UUID or even TimeUUID to be always increasing. Due to this Postgres produces the wrong result when compared to Cassandra with the same dataset.</p>

<p><strong>What I've considered so far:</strong> I considered storing it as BIGINT but it will be susecptible to collisions for time resolution in milliseconds. I can go for resolution of mirco/nano seconds but I'm afraid BIGINT will exhaust it.</p>

<p>Storing UUID as CHAR will prevent collisions but then I'll lose the capability to apply relational operators on the column.</p>

<p>TIMESTAMP fits the best but I'm worried about timezone and collisions. </p>

<p><strong>What I exactly need <em>(tl;dr)</em>:</strong></p>

<ol>
<li><p>Some way to have higher time resolution or way to avoid collision (unique value generation).</p></li>
<li><p>The column should support relational operators, i.e 
<code>uuid_col &lt; 'uuid_for_some_timestamp'</code>.</p></li>
</ol>

<p>PS: This is a Java application.</p>
",<database><postgresql><cassandra><uuid>,"<h1>tl;dr</h1>

<p>Stop thinking in Cassandra terms. The designers made some flawed decisions in their design.</p>

<ul>
<li>Use <a href=""https://en.wikipedia.org/wiki/Universally_unique_identifier"" rel=""noreferrer"">UUID</a> as an <a href=""https://en.wikipedia.org/wiki/Unique_identifier"" rel=""noreferrer"">identifier</a>.</li>
<li>Use date-time types to track time. </li>
</ul>

<p>➥ Do not mix the two. </p>

<p>Mixing the two is the flaw in Cassandra. </p>

<h1>Cassandra abuses UUID</h1>

<p>Unfortunately, Cassandra abuses UUIDs. Your predicament shows the unfortunate foolishness of their approach. </p>

<p>The purpose of a UUID is strictly to generate an identifier without needing to coordinate with a central authority as would be needed for other approaches such as a sequence number. </p>

<p>Cassandra uses <a href=""https://en.wikipedia.org/wiki/Universally_unique_identifier#Version_1_(date-time_and_MAC_address)"" rel=""noreferrer"">Version 1 UUIDs</a>, which take the current moment, plus an arbitrary small number, and combine with the <a href=""https://en.wikipedia.org/wiki/MAC_address"" rel=""noreferrer"">MAC address</a> of the issuing computer. All this data goes to make up most of the <a href=""https://en.wikipedia.org/wiki/128-bit"" rel=""noreferrer"">128 bits</a> in a UUID. </p>

<p>Cassandra makes the terrible design decision to extract that moment in time for use in time-tracking, violating the intent of the UUID design. <strong>UUIDs were never intended to be used for time tracking.</strong> </p>

<p>There are several alternative Versions in the UUID standard. These alternatives do not necessarily contain a moment in time. For example, <a href=""https://en.wikipedia.org/wiki/Universally_unique_identifier#Version_4_(random)"" rel=""noreferrer"">Version 4 UUIDs</a> instead use random numbers generated from a cryptographically-strong generator. </p>

<p>If you want to generate Version 1 UUIDs, install the <a href=""https://www.postgresql.org/docs/current/uuid-ossp.html"" rel=""noreferrer""><em>uuid-ossp</em></a> plugin (“extension”) (wrapping the <a href=""http://www.ossp.org/pkg/lib/uuid/"" rel=""noreferrer"">OSSP uuid</a> library) usually bundled with Postgres. That plugin offers several functions you can call to generate UUID values. </p>

<blockquote>
  <p>[Postgres] stores it as 4-byte int</p>
</blockquote>

<p>Postgres defines UUID as a native data type. So how such values get stored is really none of our business, and could change in future versions of Postgres (or in its new pluggable storage methods). You pass in a UUID, and you’ll get back a UUID, that’s is all we know as users of Postgres. As a bonus, it is good to learn that Postgres (in its current “heap” storage method) stores UUID values efficiently as 128 bits, and not inefficiently as, for example, storing the text of the hex string canonically used to display a UUID to humans. </p>

<p>Note that Postgres has built-in support for <em>storing</em> UUID values, not <em>generating</em> UUID values. To generate values:</p>

<ul>
<li>Some folks use the <a href=""https://www.postgresql.org/docs/current/pgcrypto.html"" rel=""noreferrer""><em>pgcrypto</em></a> extension, if already installed in their database. That plugin can only generate Version 4 nearly-all-random UUIDs. </li>
<li>I suggest you instead use the <a href=""https://www.postgresql.org/docs/current/uuid-ossp.html"" rel=""noreferrer""><em>uuid-ossp</em></a> extension. This gives you a variety of Versions of UUID to choose. </li>
</ul>

<p>To learn more, see: <a href=""https://stackoverflow.com/q/12505158/642706""><em>Generating a UUID in Postgres for Insert statement?</em></a></p>

<p>As for your migration, I suggest “telling the truth” as a generally good approach. A date-time value should be stored in a date-type column with an appropriately labeled name. An identifier should be stored in a primary key column of an appropriate type (often integer types, or UUID) with an appropriately labeled name. </p>

<p>So stop playing the silly clever games that Cassandra plays. </p>

<p>Extract the date-time value, store it in a date-time column. Postgres has excellent date-time support. Specifically, you’ll want to store the value in a column of the SQL-standard type <code>TIMESTAMP WITH TIME ZONE</code>. This data type represents a moment, a specific point on the timeline.</p>

<p>The equivalent type in Java for representing a moment would be <code>Instant</code> or <code>OffsetDateTime</code> or <code>ZonedDateTime</code>. The JDBC 4.2 spec requires support only for the second, inexplicably, not the first or third. Search Stack Overflow for more of this Java and JDBC info as it has been covered many many times already.</p>

<p>Continue to use UUID but <em>only</em> as the designated primary key column of your new table in Postgres. You can tell Postgres to auto-generate these values.</p>

<blockquote>
  <p>Storing UUID as CHAR </p>
</blockquote>

<p>No, do not store UUID as text.</p>

<blockquote>
  <p>TIMESTAMP fits the best but I'm worried about timezone and collisions.</p>
</blockquote>

<p>There is a world of difference between <code>TIMESTAMP WITH TIME ZONE</code> and <code>TIMESTAMP WITHOUT TIME ZONE</code>. So never say just TIMESTAMP.</p>

<p>Postgres always stores a <code>TIMESTAMP WITH TIME ZONE</code> in UTC. Any time zone or offset information included with a submitted value is used to adjust to UTC, and then discarded. Java retrieves values of this type as UTC. So no problem. </p>

<p>The problem comes when using other tools that have the well-intentioned but tragically-flawed feature of dynamically applying a default time zone while generating text to display the value of the field. The value retrieved from Postgres is <em>always</em> in UCT, but its <em>presentation</em> may have been adjusted to another offset or zone. Either avoid such tools or be sure to set the default zone to UTC itself. All programmers, DBAs, and sysadmins should learn to work and think in UTC while on the job. </p>

<p><code>TIMESTAMP WITHOUT TIME ZONE</code> is entirely different. This type lacks the context of a time zone or offset-from-UTC. So <strong>this type cannot represent a moment</strong>. It holds a date and a time-of-day but that's all. And that is ambiguous of course. If the value is noon on the 23rd of January this year, we do not know if you mean noon in Tokyo, noon in Tehran, or noon in Toledo — all very different moments, several hours apart. The equivalent
type in Java is <code>LocalDateTime</code>. Search Stack Overflow to learn much more.</p>

<p><a href=""https://i.stack.imgur.com/Xtzeh.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Xtzeh.png"" alt=""Table of date-time types in Java (both legacy and modern) and in standard SQL.""></a></p>

<blockquote>
  <p>Time was stored as UUID to avoid collisions when rows are inserted in the same millisecond. </p>
</blockquote>

<p>Version 1 UUID track and time with a resolution as fine as 100 nanoseconds (1/10th of a microsecond), if the host computer hardware clock can do so. The <em>java.time</em> classes capture time with a resolution of microseconds (as of Java 9 and later). Postgres stores moments with a resolution of microseconds. So with Java &amp; Postgres, you’ll be close in this regard to Cassandra. </p>

<p>Storing the current moment. </p>

<pre><code>OffsetDateTime odt = OffsetDateTime.now( ZoneOffset.UTC ) ;
myPreparedStatement.setObject( … , odt ) ;
</code></pre>

<p>Retrieval.</p>

<pre><code>OffsetDateTime odt = myResultSet.getObject( … , OffsetDateTime.class ) ;
</code></pre>

<blockquote>
  <p>I can go for resolution of mirco/nano seconds</p>
</blockquote>

<p>No you cannot. Conventional computer clocks today cannot precisely track time in nanoseconds. </p>

<p>And using time-tracking solely as an identifier value is a flawed idea. </p>

<blockquote>
  <p>it is not necessary for UUID or even TimeUUID to be always increasing</p>
</blockquote>

<p>You can <em>never</em> count on a clock always increasing. Clocks get adjusted and reset. Computer hardware clocks are not that accurate. Not understanding the limitations of computer clocks is one of the naïve and unreasonable aspects of Cassandra’s design. </p>

<p>And this is why a Version 1 UUID uses an arbitrary small number (called the <code>clock sequence</code>) along with the current moment, because the current moment could repeat when a clock gets reset/adjusted. A responsible UUID implementation is expected to notice the clock falling back, and then increment that small number to compensate and avoid duplicates. Per RFC 4122 section 4.1.5:</p>

<blockquote>
  <p>For UUID version 1, the clock sequence is used to help avoid duplicates that could arise when the clock is set backwards in time or if the node ID changes.</p>
  
  <p>If the clock is set backwards, or might have been set backwards
     (e.g., while the system was powered off), and the UUID generator can
     not be sure that no UUIDs were generated with timestamps larger than
     the value to which the clock was set, then the clock sequence has to
     be changed.  If the previous value of the clock sequence is known, it
     can just be incremented; otherwise it should be set to a random or
     high-quality pseudo-random value.</p>
</blockquote>

<p>There is <strong>nothing in the <a href=""https://en.wikipedia.org/wiki/Universally_unique_identifier#Standards"" rel=""noreferrer"">UUID specifications</a> that promises to be “always increasing”</strong>. Circling back to my opening statement, Cassandra abuses UUIDs.</p>
",['table']
57691436,57698599,2019-08-28 11:32:45,Datastax Java driver 4.x : How to get cluster name?,"<p>After upgrading java driver for cassandra from 3.7 to 4.0 (or above) - I am unable to resolve the cluster name.</p>

<p>I need the name of the cassandra cluster to which the my application is connected using java driver. Earlier it was available as ""Cluster.getMetadata().getClusterName()"". But after upgrading to datastax-driver-core-4.0 or above- I am unable to resolve the Cluster name from CqlSession.getMetadata()..</p>

<p>This is very important because I have segregated operations based on different cluster.</p>
",<cassandra><datastax><datastax-java-driver><cassandra-cluster>,"<p>I believe cluster name is no longer provided by the java api.
Instead just query it from the system.local :</p>

<pre><code>SimpleStatement statement =SimpleStatement.newInstance(""SELECT cluster_name FROM system.local"");
ResultSet resultSet = session.execute(statement);
Row row = resultSet.one();
System.out.println(row.getString(""cluster_name""));
</code></pre>
",['cluster_name']
57747584,57758967,2019-09-01 16:13:10,Querying a High Cardinality Field,"<p>I am designing a data model for our orders for our upcoming Cassandra migration. An order has an orderId (arcane UUID field) and an orderNumber (user-friendly number). A getOrder query can be done by using any of the two.</p>

<p>My partition key is the orderId, so getByOrderId is not a problem. By getByOrderNumber is - there's a one-to-one mapping b/w the orderId and the orderNumber (high-cardinality field), so creating a local secondary index on each node would slow down my queries.</p>

<p>What I was wondering was that I could create a new table with the orderNumber as the partition key and the orderId as the only column (kind of a secondary index but maintained by me). So now, a getByOrderNumber query can be resolved in two calls.</p>

<p>Bear with me if the above solution is egregiously wrong, I am extremely new to Cassandra. As I understand, for such a column, if I used local secondary indices, Cassandra would have to query each node for a single order. So I thought why not create another table that stores the mapping.</p>

<p>What would I be missing on by managing this index myself? One thing I can see if for every write, I'll now have to update two tables. Anything else?</p>
",<database><cassandra><nosql><query-optimization>,"<blockquote>
  <p>I thought why not create another table that stores the mapping.</p>
</blockquote>

<p>That's okay. From Cassandra documentation:</p>

<blockquote>
  <p><strong>Do not use an index in these situations</strong>:</p>
  
  <p>On high-cardinality columns because you then query a huge volume of
  records for a small number of results. See Problems using a
  high-cardinality column index below.</p>
  
  <p><strong>Problems using a high-cardinality column index</strong></p>
  
  <p>If you create an index on a high-cardinality column, which has many
  distinct values, a query between the fields incurs many seeks for very
  few results. In the table with a billion songs, looking up songs by
  writer (a value that is typically unique for each song) instead of by
  their recording artist is likely to be very inefficient..</p>
  
  <p>It would probably be more efficient to manually maintain the table as
  a form of an index instead of using the built-in index. For columns
  containing unique data, it is sometimes fine performance-wise to use
  an index for convenience, as long as the query volume to the table
  having an indexed column is moderate and not under constant load.</p>
  
  <p>Conversely, creating an index on an extremely low-cardinality column,
  such as a boolean column, does not make sense. Each value in the index
  becomes a single row in the index, resulting in a huge row for all the
  false values, for example. Indexing a multitude of indexed columns
  having foo = true and foo = false is not useful.</p>
</blockquote>

<p>It's normal for Cassandra data modelling to have a denormalized data.</p>
",['table']
58004388,58017013,2019-09-19 05:43:16,Unable to execute a timeseries query using a timeuuid as the primary key,"<p>My goal is to do a sum of the messages_sent and emails_sent per each DISTINCT provider_id value for a given time range (fromDate &lt; stats_date_id &lt; toDate), but without specifying a provider_id. In other words, I need to know about any and all Providers within the specified time range and to sum their messages_sent and emails_sent.</p>

<p>I have a Cassandra table using an express-cassandra schema (in Node.js) as follows:</p>

<pre><code>module.exports = {
  fields: {
    stats_provider_id: {
      type: 'uuid',
      default: {
        '$db_function': 'uuid()'
      }
    },
    stats_date_id: {
      type: 'timeuuid',
      default: {
        '$db_function': 'now()'
      }
    },
    provider_id: 'uuid',
    provider_name: 'text',
    messages_sent: 'int',
    emails_sent: 'int'
  },
  key: [
    [
      'stats_date_id'
    ],
    'created_at'
  ],
  table_name: 'stats_provider',
  options: {
    timestamps: {
      createdAt: 'created_at', // defaults to createdAt
      updatedAt: 'updated_at' // defaults to updatedAt
    }
  }
}
</code></pre>

<p>To get it working, I was hoping it'd be as simple as doing the following:</p>

<pre><code>let query = {
    stats_date_id: {
      '$gt': db.models.minTimeuuid(fromDate),
      '$lt': db.models.maxTimeuuid(toDate)
    }
  };
let selectQueries = [
    'provider_name',
    'provider_id',
    'count(direct_sent) as direct_sent',
    'count(messages_sent) as messages_sent',
    'count(emails_sent) as emails_sent',
  ];
  // Query stats_provider table
  let providerData = await db.models.instance.StatsProvider.findAsync(query, {select: selectQueries});
</code></pre>

<p>This, however, complains about needing to filter the results:
<code>Error during find query on DB -&gt; ResponseError: Cannot execute this query as it might involve data filtering and thus may have unpredictable performance</code>.</p>

<p>I'm guessing you can't have a primary key and do date range searches on it?  If so, what is the correct approach to this sort of query?</p>
",<node.js><cassandra><time-series><cassandra-3.0><express-cassandra>,"<p>So while not having used Express-Cassandra, I can tell you that running a range query on your partition key is a hard ""no.""  The reason for this, is that Cassandra can't determine a single node for that query, so it has to poll every node.  As that's essentially a full scan of your table across multiple nodes, it throws that error to prevent you from running a bad query.</p>

<p>However, you <em>can</em> run a range query on a clustering key, provided that you are filtering on all of the keys prior to it.  In your case, if I'm reading this right, your PRIMARY KEY looks like:</p>

<p><code>PRIMARY KEY (stats_date_id, created_at)</code></p>

<p>That primary key definition is going to be problematic for two reasons:</p>

<ol>
<li><p><code>stats_date_id</code> is a TimeUUID.  This is <em>great</em> for data distribution.  But it sucks for query flexibility.  In fact, you will need to provide that exact TimeUUID value to return data for a specific partition.  As a TimeUUID has millisecond precision, you'll need to know the exact time to query <strong><em>down to the millisecond</em></strong>.  Maybe you have the ability to do that, but usually that doesn't make for a good partition key.</p></li>
<li><p>Any rows underneath that partition (<code>created_at</code>) will have to share that exact time, which usually leads to a lot of 1:1 cardinality ratios for partition:clustering keys.</p></li>
</ol>

<p>My advice on fixing this, is to partition on a date column that has a slightly lower level of cardinality.  Think about how many provider messages are usually saved within a certain timeframe.  Also pick something that won't store too many provider messages together, as you don't want unbound partition growth (Cassandra has a hard limit of 2 billion cells per partition).</p>

<p>Maybe something like: <code>PRIMARY KEY (week,created_at)</code></p>

<p>So then your CQL queries could look something like:</p>

<pre><code>SELECT * FROM stats_provider
WHERE week='201909w1'
  AND created_at &gt; '20190901'
  AND created_at &lt; '20190905';
</code></pre>

<p><strong>TL;DR;</strong></p>

<ol>
<li>Partition on a time bucket not quite as precise as something down to the ms, yet large enough to satisfy your usual query.</li>
<li>Apply the range filter on the first clustering key, <em>within</em> a partition.</li>
</ol>
",['table']
58098268,58099875,2019-09-25 12:21:50,"Joining streaming data on table data and update the table as the stream receives , is it possible?","<p>I am using spark-sql 2.4.1 , spark-cassandra-connector_2.11-2.4.1.jar and java8.
I have scenario , where I need join streaming data with C*/Cassandra table data.</p>
<p>If record/join found I need to copy the existing C* table record to another table_bkp and update the actual C* table record with latest data.</p>
<p>As the streaming data come in I need to perform this.
Is this can be done using spark-sql steaming ?
If so , how to do it ? any caveats to take care ?</p>
<p>For each batch how to get C* table data freshly ?</p>
<blockquote>
<p><strong>What is wrong I am doing here</strong></p>
</blockquote>
<p>I have two tables as below &quot;master_table&quot; &amp; &quot;backup_table&quot;</p>
<pre><code>table kspace.master_table(
    statement_id int,
    statement_flag text,
    statement_date date,
    x_val double,
    y_val double,
    z_val double,
    PRIMARY KEY (( statement_id ), statement_date)
) WITH CLUSTERING ORDER BY ( statement_date DESC );

table kspace.backup_table(
    statement_id int,
    statement_flag text,
    statement_date date,
    x_val double,
    y_val double,
    z_val double,
    backup_timestamp timestamp,
    PRIMARY KEY ((statement_id ), statement_date, backup_timestamp )
) WITH CLUSTERING ORDER BY ( statement_date DESC,   backup_timestamp DESC);


Each streaming record would have &quot;statement_flag&quot; which might be &quot;I&quot; or &quot;U&quot;.
If record with &quot;I&quot; comes we directly insert into &quot;master_table&quot;.
If record with &quot;U&quot; comes , need to check if there is any record for given ( statement_id ), statement_date in &quot;master_table&quot;.
     If there is record in &quot;master_table&quot; copy that one to &quot;backup_table&quot; with current timestamp i.e. backup_timestamp
     Update the record in &quot;master_table&quot; with latest record.
 
</code></pre>
<p>To achieve the above I am doing PoC/Code  like below</p>
<pre><code>Dataset&lt;Row&gt; baseDs = //streaming data from topic
Dataset&lt;Row&gt; i_records = baseDs.filter(col(&quot;statement_flag&quot;).equalTo(&quot;I&quot;));
Dataset&lt;Row&gt; u_records = baseDs.filter(col(&quot;statement_flag&quot;).equalTo(&quot;U&quot;));

String keyspace=&quot;kspace&quot;;
String master_table = &quot;master_table&quot;;
String backup_table = &quot;backup_table&quot;;


Dataset&lt;Row&gt; cassandraMasterTableDs = getCassandraTableData(sparkSession, keyspace , master_table);

writeDfToCassandra( baseDs.toDF(), keyspace, master_table);


u_records.createOrReplaceTempView(&quot;u_records&quot;);
cassandraMasterTableDs.createOrReplaceTempView(&quot;persisted_records&quot;);

Dataset&lt;Row&gt; joinUpdatedRecordsDs =  sparkSession.sql(
            &quot; select p.statement_id, p.statement_flag, p.statement_date,&quot;
            + &quot;p.x_val,p.y_val,p.z_val &quot;
            + &quot; from persisted_records as p &quot;
            + &quot;join u_records as u &quot;
            + &quot;on p.statement_id = u.statement_id  and p.statement_date = u.statement_date&quot;);

    

Dataset&lt;Row&gt; updated_records =   joinUpdatedRecordsDs
                            .withColumn(&quot;backup_timestamp&quot;,current_timestamp());

updated_records.show(); //Showing correct results 


writeDfToCassandra( updated_records.toDF(), keyspace, backup_table);  // But here/backup_table copying the latest &quot;master_table&quot; records 
</code></pre>
<blockquote>
<p><strong>Sample data</strong></p>
</blockquote>
<p><strong>For first record with &quot;I&quot; flag</strong></p>
<p>master_table</p>
<p><a href=""https://i.stack.imgur.com/QjzWf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QjzWf.png"" alt=""enter image description here"" /></a></p>
<p>backup_table</p>
<p><a href=""https://i.stack.imgur.com/HRMH0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HRMH0.png"" alt=""enter image description here"" /></a></p>
<p><strong>For second record with &quot;U&quot; flag , i.e. same as earlier except &quot;y_val&quot; column data</strong></p>
<p>master_table</p>
<p><a href=""https://i.stack.imgur.com/d2EBv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/d2EBv.png"" alt=""enter image description here"" /></a></p>
<p>backup_table</p>
<p>Expected</p>
<p><a href=""https://i.stack.imgur.com/P0eOC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/P0eOC.png"" alt=""enter image description here"" /></a></p>
<p>But actual table data is</p>
<p><a href=""https://i.stack.imgur.com/kj8Sl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kj8Sl.png"" alt=""enter image description here"" /></a></p>
<p><strong>Question:</strong></p>
<p>Till show the dataframe(updated_records) showing correct data.
But when I insert same dataframe(updated_records) into table , C* backup_table data shows exactly same as latest record of master_table , but which suppose to have earlier record of master_table.</p>
<pre><code>  updated_records.show(); //Showing correct results 
    
    
    writeDfToCassandra( updated_records.toDF(), keyspace, backup_table);  // But here/backup_table copying the latest &quot;master_table&quot; records 
</code></pre>
<p>So what am I doing wrong in above program code ?</p>
",<apache-spark-sql><cassandra><spark-streaming><cassandra-3.0><spark-cassandra-connector>,"<p>There are several ways to to do this with various levels of performance depending on how much data you need to check.</p>

<p>For example, if you are only looking up data by partition key the most efficient thing to do is to use joinWithCassandraTable on the Dstream. For every batch this will extract records matching the incoming partition keys. In structured streaming this would happen automatically with the correctly written SQL join and DSE. If DSE was not in use it would fully scan the table with each batch.</p>

<p>If instead you require the whole table for each batch, joining the DStream batch with a CassandraRDD will cause the RDD to be re-read completely on every batch. This is much more expensive if the entire table is not being re-written.</p>

<p>If you are only updating records without checking their previous values, it is sufficient to just write the incoming data directly to the C* table. C* uses upserts and last write win behaviors, and will just overwrite the previous values if they existed.</p>
",['table']
58345755,58352788,2019-10-11 17:09:23,How to load/read data from cassandra by partition by partition using java8?,"<p>I am using spring-boot , datastax-java-cassandra-connector_2.11-2.4.1.jar and java8. </p>

<blockquote>
  <p>I have scenario where I need to read/load the data from C* table, but
  this table might have million of records.</p>
  
  <p>I need to load this data from C* table, is there anyway in
  java/spring-boot using datastax-java-cassandra-connector API I can
  pull the data partition by partition?</p>
</blockquote>
",<spring-boot><cassandra><datastax-enterprise><datastax-java-driver><spark-cassandra-connector>,"<p>while <code>select * from table</code> may work, more effective way could be to read data by token ranges with query like <code>select * from table where token(part_key) &gt; beginRange and token(part_key) &lt;= endRange</code>. The Spark Cassandra connector works the same way - it gets the list of all available token ranges, and then fetch data from every token range, but send it directly to the node that holds this token range (as opposite to <code>select * from table</code> that retrieves all data via coordinator node).</p>

<p>You need to be careful in calculation of the token boundaries, especially for begin &amp; end of the full range. You can find an <a href=""https://github.com/alexott/dse-java-playground/blob/master/src/main/java/com/datastax/alexott/demos/jdtest1/TokenRangesScan.java"" rel=""nofollow noreferrer"">example of the Java code in my repository</a> (it's too long to paste it here).</p>
",['table']
58406145,58409061,2019-10-16 05:14:11,Migrating data from a single node cassandra cluster to another single node cassandra cluster,"<p>I have a Single Node Cassandra Cluster which has around 44gb of data on it(/var/lib/cassandra/data/my_keyspace). The current storage is 1 tb and I need to migrate all the data to another VM which will have the same setup(single node cluster). My data-node has data being pushed to it every second so I can't afford any downtime(Some sensors are pushing time-series data).</p>

<pre><code>Keyspace :- CREATE KEYSPACE my_keysopace WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'}  AND durable_writes = true;

Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address    Load       Tokens       Owns (effective)  Host ID                               Rack

UN  127.0.0.1  43.4 GiB   256          100.0%            e0ae36db-f639-430c-91ad-6af3ffb6f906  rack1
</code></pre>

<p>After a bit of research I decided it's best to add the new node to existing cluster and then let the old node stream all the data and after streaming is done, decommission the old node.</p>

<p>Source :- <a href=""https://docs.datastax.com/en/archived/cassandra/2.0/cassandra/operations/ops_add_node_to_cluster_t.html"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/archived/cassandra/2.0/cassandra/operations/ops_add_node_to_cluster_t.html</a></p>

<ol>
<li>Configure old node as seed node for the new node    </li>
<li><strong>Add</strong> new node node to the ring(auto_bootstrap = true)    </li>
<li>Once the status is UN for both nodes, run nodetool <strong>cleanup</strong> on old node    </li>
<li><strong>Decommission</strong> the old node</li>
</ol>

<p>My only concern is will I be facing any <strong>data loss</strong>/ is this approach appropriate ?
Please let me know if I am missing anything here </p>

<p>Thanks</p>
",<cassandra><database-migration>,"<p><em>Firstly</em>, disclaimer, using a single node of C* voids the purpose of the distributed database. Minimal cluster size tends to be 3 so some nodes can go offline without downtime (I'm sure you've seen this warning before). Now with that out the way, let's discuss the process.</p>

<blockquote>
  <ol>
  <li>Configure old node as seed node for the new node</li>
  </ol>
</blockquote>

<p>Yep.</p>

<p>1.5. (Potentially missing step) The step you're missing is the consistency level of your queries needs to be verified. I see you're using <code>replication_factor</code> 1 for all keyspaces in use so make sure you're using a CONSISTENCY_LEVEL of ONE for your queries. </p>

<blockquote>
  <ol start=""2"">
  <li>Add new node node to the ring(auto_bootstrap = true)</li>
  </ol>
</blockquote>

<p>Sounds good. Make sure you've configured various ports / listen_address etc.</p>

<blockquote>
  <ol start=""3"">
  <li>Once the status is UN for both nodes, </li>
  </ol>
</blockquote>

<p>Once you reach UN double-check that the client isn't seeing any consistency errors.</p>

<blockquote>
  <p>3.5. run nodetool cleanup on old node</p>
</blockquote>

<p>3.5. (Redundant step) You don't need to run <code>nodetool cleanup</code>. You won't care about left over data from the decommissioned node, as all the data will be moved to the new node replacing it. </p>

<blockquote>
  <ol start=""4"">
  <li>Decommission the old node</li>
  </ol>
</blockquote>

<p>Yep. </p>

<ol start=""5"">
<li>(Missing step) You'll have to modify the new node to see itself as a seed once you've decomissioned the old node or it wont be able to re-start.</li>
</ol>
",['listen_address']
58409127,58409830,2019-10-16 08:39:20,Cassandra: Shipping Disk to New DC in order to sync 50TB of data,"<p>We're adding a new datacenter to our Cassandra cluster. Currently, we have a 15-node DC with RF=3 resulting in about 50TB~ of data.</p>

<p>We are adding another datacenter in a different country and we want both data centers to contain all the data. Obviously, synchronizing 50TB of data across the internet will take a gargantuan amount of time.</p>

<p>Is it possible to copy a full back to a few disks, ship that to the new DC and then recover? I'm just wondering what would be the procedure to do so.</p>

<p>Could someone give me a few pointers on this operation, if possible at all?
Or any other tips?</p>

<p>Our new DC is going to be smaller (6 nodes) for the time being, although enough space will be available. The new DC is mostly meant as a live-backup/failover and will not be the primary cluster for writing, generally speaking.</p>
",<database><cassandra><replication><database-replication><cassandra-3.0>,"<p>TL;DR; Due to the topology (node count) change between the two DCs, avoiding streaming the data in isn't possible AFAIK. </p>

<blockquote>
  <p>Our new DC is going to be smaller (6 nodes) for the time being</p>
</blockquote>

<p>The typical process isn't going to work due to token alignment on the nodes being different (new cluster's ring will change). So just copying the existing SSTables wont work, as the nodes that hold those tables, might not have the tokens corresponding to the data in the files and so C* wont be able to find said data.</p>

<p>Bulk loading the data to the new DC is out too, as you'll be overwriting the old data if you re-insert it. </p>

<p>To give you an overview of the process if you were to retain the topology:</p>

<ol>
<li>snapshot the data from the original DC</li>
<li>Configure the new DC. It's extremely important that you set initial_token for each machine. You can get a list of what tokens you need by running <code>nodetool ring</code> on the original cluster. This is why you need the same number of nodes. As importantly, when copying the SSTable files over, you need the files and the tokens to be from the same node.</li>
<li>ship the data to the new DC (Remember if the new node 10.0.0.1 got it's tokens from 192.168.0.100 in the old dc, then it also has to get it's snapshot data from 192.168.0.100).  </li>
<li>Start the new DC and ensure both DCs see eachother ok.</li>
<li>Rebuild and repair <code>system_distributed</code> and <code>system_auth</code> (assuming you have authentication enabled)</li>
<li>Update client consistency to whatever you need. (Do you want to write to both DCs? From your description sounds like a no so you might be all good). </li>
<li>Update the schema, ensure that you're using <code>NetworkTopologyStrategy</code> for any keyspce that you want to be shared, then add some replication for the new DC.</li>
</ol>

<pre><code>    ALTER KEYSPACE ks WITH REPLICATION = { 'class' : 'NetworkTopologyStrategy', 'oldDC' : 3, 'newDC':3 };
</code></pre>

<ol start=""8"">
<li>Run a full repair on each node in the new dc. </li>
</ol>
",['initial_token']
58438951,58444214,2019-10-17 18:44:21,Is it possible to count no. of rows belonging to a partition if I use clustering columns,"<p>updated after comment from Jim.</p>

<p>I have a database with schema</p>

<pre><code>field1 //partition key
field2 //clustering column
field3
</code></pre>

<p>I suppose <code>Cassandra</code> will calculate hash on field1, decide in which node this data entry will go into and will store it there. As I am using clustering column, for 2 data entries with same value of <code>field1</code> but different values of <code>field2</code>, the data will be stored as two rows.</p>

<pre><code>field1, field2.1, field3
field1, field2.2, field3
</code></pre>

<p>Is it possible to create a query which would return value <code>2</code> (count of rows) as there are two rows belonging to partition key <code>field1</code>? </p>
",<cassandra><cassandra-3.0>,"<p>Do a </p>

<blockquote>
  <p>select count(*) from table where field1 = “x” ;</p>
</blockquote>

<p>You should get two in case of the example shown in your question</p>
",['table']
58466825,58492704,2019-10-19 18:32:07,How do I connect to a Cassandra VM,"<p>The following code works if Cassandra and the code are on the same machine:</p>

<pre><code>using System;
using Cassandra;

namespace CassandraInsertTest
{
    class Program
    {
        static void Main(string[] args)
        {
            var cluster = Cluster.Builder()
            .AddContactPoint(""127.0.0.1"")
            .Build();

            var session = cluster.Connect(""test_keyspace"");

            session.Execute(""INSERT INTO test_table (id, col1, col2) VALUES (1, 'data1', 'data2')"");

            Console.WriteLine($""Finished"");
            Console.ReadKey();
        }
    }
}
</code></pre>

<p>Assuming a username and password is needed if the code is on one machine and cassandra is on a different machine (different ip address)?  So I have tried:</p>

<pre><code>        var cluster = Cluster.Builder()
        .AddContactPoint(""192.168.0.18"") &lt;- the ip address for the cassandra node
        .WithPort(9042)
        .WithCredentials(""username to log into the cassandra node"",""password to log into the cassandra node"")
        .Build();
</code></pre>

<p>I get the following error message:</p>

<pre><code>userone@desktop:~/Desktop/vsc$ dotnet run
Unhandled exception. Cassandra.NoHostAvailableException: All hosts tried for query failed (tried 192.168.0.18:9042: SocketException 'Connection refused')
   at Cassandra.Connections.ControlConnection.Connect(Boolean isInitializing)
   at Cassandra.Connections.ControlConnection.InitAsync()
   at Cassandra.Tasks.TaskHelper.WaitToCompleteAsync(Task task, Int32 timeout)
   at Cassandra.Cluster.Cassandra.SessionManagement.IInternalCluster.OnInitializeAsync()
   at Cassandra.ClusterLifecycleManager.InitializeAsync()
   at Cassandra.Cluster.Cassandra.SessionManagement.IInternalCluster.ConnectAsync[TSession](ISessionFactory`1 sessionFactory, String keyspace)
   at Cassandra.Cluster.ConnectAsync(String keyspace)
   at Cassandra.Tasks.TaskHelper.WaitToComplete(Task task, Int32 timeout)
   at Cassandra.Tasks.TaskHelper.WaitToComplete[T](Task`1 task, Int32 timeout)
   at Cassandra.Cluster.Connect(String keyspace)
   at HelloWorld.Program.Main(String[] args) in /home/userone/Desktop/vsc/Program.cs:line 17
userone@desktop:~/Desktop/vsc$ 
</code></pre>

<p>The iptables on the node (the cassandra server) is currently set as follows:</p>

<pre><code>node1@node1:~$ sudo iptables -S
-P INPUT ACCEPT
-P FORWARD ACCEPT
-P OUTPUT ACCEPT
-A INPUT -i lo -j ACCEPT
-A IMPUT -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
-A INPUT -p tcp -m tcp --dport 22 -j ACCEPT
-A INPUT -p tcp -m tcp --dport 80 -j ACCEPT
-A INPUT -s 192.168.0.73/32 -p tcp -m multiport --dports 7000,7001,7199,9042,9160,9142 -m state --state NEW,ESTABLISHED -j ACCEPT
node1@node1:~$
</code></pre>

<p>Note 1: Both the machine with the app and the machine with cassandra installed can be pinged and tracerouted in both directions.</p>

<p>Note 2: I have tested the username and password and can log into the cassandra server without any issues when tried directly on the server.</p>

<p>Note 3: Cassandra is running in a VM which I just created today.  The VM is a guest machine on the host machine which runs the code.</p>

<p>Note 4: Both the Host OS and Guest OS are Linux.</p>
",<c#><.net><.net-core><cassandra>,"<pre><code>.AddContactPoint(""127.0.0.1"")
</code></pre>

<p>If that works from the same machine, then you probably have Cassandra <em>bound</em> to that IP.  If you need to connect to your node(s) remotely, then you need to bind a routeable IP to that node.</p>

<p>Run a <code>nodetool status</code>.  If you see your cluster status showing your node with an IP of 127.0.0.1, then connecting <em>to</em> the local machine <em>from</em> the local machine is the only scenario that will <em>ever</em> work.</p>

<p>Try running the following command on your node:</p>

<pre><code>grep _address cassandra.yaml
</code></pre>

<p>The IP address returned in the output is the only one that an application is allowed to connect to.  If you want to be able to connect to 192.168.0.18, then the <code>listen</code> and <code>rpc</code> addresses should look something like this:</p>

<pre><code>listen_address: 192.168.0.18
rpc_address: 192.168.0.18
</code></pre>

<p>Note that you'll need to change your <code>seeds</code> list, too.</p>

<p>Also, if you're on a VM/provider that has both internal and external IP addresses, then you'll also need to set your <code>broadcast_</code> addresses to the external IP:</p>

<pre><code>broadcast_address: 10.6.5.5
broadcast_rpc_address: 10.6.5.5
listen_address: 192.168.0.18
rpc_address: 192.168.0.18
</code></pre>

<p>But try setting just <code>listen</code> and <code>rpc</code> to 192.168.0.18 first.</p>

<p><strong>Edit 20191022</strong></p>

<blockquote>
  <p>Just wanted to double check, do I add 192.168.0.18 as the listen_address and rpc_address to the cassandra node where the cassandra node has the ip address 192.168.0.18?</p>
</blockquote>

<p>Yes.  Also make sure that your node's seed list is set like this:</p>

<pre><code>- seeds: ""192.168.0.18""
</code></pre>

<blockquote>
  <p>Before I did that, the value of the listen_address and rpc_address were set to localhost</p>
</blockquote>

<p>I thought so.</p>

<blockquote>
  <p>However, after making the changes you suggested, nodetool status now gives me</p>
</blockquote>

<pre><code>Failed to connect to 127.0.0.1:7199 - connection refused
</code></pre>

<p>Ironically, that's the same message nodetool returns when Cassandra is not running.  At this point I would check the system log and see if it is returning errors that may be preventing it from starting.  I suspect that the seed list still reads ""127.0.0.1"".</p>

<p><strong>tl;dr;</strong></p>

<p>If you intend to connect to your cluster/node remotely, then you <em>cannot</em> use the default configurations which bind Cassandra to the home IP (127.0.0.1/localhost).  And that includes all <code>_address</code> settings, as well as your <code>seeds</code> list.</p>
","['rpc_address', 'listen_address']"
58482964,58580482,2019-10-21 09:00:33,Can I user counter type field as primary key of my C* table?,"<p>When I am trying to create table like below</p>
<pre><code> create table if not exists counter_temp(id counter PRIMARY KEY , comment text);
</code></pre>
<p>It is giving error as below</p>
<blockquote>
<p>Multiple markers at this line</p>
<ul>
<li><p>For a table with counter columns, all columns except the primary    key must be type counter</p>
</li>
<li><p>counter type is not supported for PRIMARY KEY part</p>
</li>
</ul>
</blockquote>
<p><strong>Question 1 :</strong></p>
<p>What is the reason , counter column not allowed as part of primary key?</p>
<p><strong>Question 2 :</strong>
While I am trying to create as below</p>
<p>create table if not exists counter_temp(id uuid PRIMARY KEY, counter_t counter, comment text)</p>
<blockquote>
<p>Error : Cant mix counter and non-counter columns in the same table</p>
<p>What is wrong here ? how to handle it correct way ?</p>
</blockquote>
<p><strong>Question 3 :</strong></p>
<p>I have a table emp( emp_id counter, emp_name text)  in Dev env where has data , now I need to copy that data into another SIT env  emp( emp_id counter, emp_name text) table ?</p>
<p>Can it be done  will it copy counter fields properly ?</p>
",<cassandra><datastax><cassandra-3.0>,"<p>Short answer for question 1 is <strong>No</strong>, as it was communicated in the error message. But Even if it was allowed, then it didn't make any sense - when you change value of the primary key, you're basically create a new row with different primary key.</p>

<p>for Q2 - if there is at least one <code>counter</code> column in the table, then all other regular columns should have type <code>counter</code>. If you need to add a comment field, just create a 2nd table, with <code>UUID</code> primary key &amp; insert or read data to/from 2 tables at the same time.</p>

<p>for Q3 - cqlsh's <code>COPY</code> command supports tables with counters for newer Cassandra versions (where fix for <a href=""https://issues.apache.org/jira/browse/CASSANDRA-9043"" rel=""nofollow noreferrer"">CASSANDRA-9043</a> is implemented).  Also, Spark Cassandra Connector is able to read from tables with counters &amp; write to them. But in both cases make sure that the target table is empty, otherwise new values will be appended to existing ones.</p>
",['table']
58503893,58590017,2019-10-22 12:03:42,ScyllaDB schema causes issues when imported with cassandra-stress,"<p>I'm currently using ScyllaDB in my environment and, due to technical reasons, researching moving to Cassandra. I'm trying to make cassandra-stress load up Cassandra cluster with data using the schema possibly identical to the one currently being used in ScyllaDB. Sadly, there are some issues.</p>

<p>The environment:</p>

<ul>
<li>ScyllaDB 3.0.7 (= Cassandra 3.0.8) running on Ubuntu 18.04</li>
<li>Cassandra 3.11.4 running on Ubuntu 18.04</li>
<li>cassandra-stress 3.0.18 (part of <code>cassandra-tools</code> pkg) running on Ubuntu 18.04</li>
</ul>

<p>The process is as follows:</p>

<ul>
<li>dump the schema from ScyllaDB (<code>desc keyspace_name</code>)</li>
<li>prepare the cassandra-stress yaml file - one keyspace, five tables total</li>
<li>run cassandra-stress (<code>cassandra-stress user profile=schema.yml cl=QUORUM duration=30s 'ops(insert=1)' -node 172.19.11.9 -rate threads=1</code>)</li>
</ul>

<p>Just to be sure there are no keyspace related issues, every run of cassandra-stress is done on a new keyspace (I'm incrementing the name).</p>

<p>Now when the schema is 1:1 as the one dumped from Scylla, definition of two tables (and only those two) causes the stress-tool to fail: <code>com.datastax.driver.core.exceptions.SyntaxError: line 1:35 no viable alternative at input 'WHERE' (UPDATE ""activities_bp_action"" SET  [WHERE]...)</code>.</p>

<p>The table definitions are as follows:</p>

<pre><code>table: activities_bp
table_definition: |
  CREATE TABLE activities_bp  (
    business_profile_id int,
    create_date timestamp,
    event_uuid uuid,
    PRIMARY KEY (business_profile_id, create_date, event_uuid)
  ) WITH CLUSTERING ORDER BY (create_date DESC, event_uuid ASC)
    AND compression = {'sstable_compression': 'org.apache.cassandra.io.compress.DeflateCompressor'}
</code></pre>

<pre><code>table: activities_bp_action
table_definition: |
  CREATE TABLE activities_bp_action  (
    business_profile_id int,
    action text,
    create_date timestamp,
    event_uuid uuid,
    PRIMARY KEY ((business_profile_id, action), create_date, event_uuid)
  ) WITH CLUSTERING ORDER BY (create_date DESC, event_uuid ASC)
    AND compression = {'sstable_compression': 'org.apache.cassandra.io.compress.DeflateCompressor'}
</code></pre>

<p>If the two lines containing <code>PRIMARY KEY</code> and <code>CLUSTERING ORDER</code> are replaced with what follows, the cassandra-stress runs fine with no errors and starts to fill up the cluster with data. However, the definitions now have drifted from the ones from ScyllaDB:</p>

<pre><code>    PRIMARY KEY (event_uuid, create_date)
  ) WITH CLUSTERING ORDER BY (create_date DESC)
</code></pre>

<p>Now after cassandra-stress is run with the modified definition, I can roll back to the unmodified one (the one that used to fail). If run on an already existing keyspace, the yaml works fine now and fills up the cluster with data. That would suggest that the problem occurs while creating tables?</p>

<p>I was not able to find the full query that cassandra-stress displays in its stack-trace, both when running cassandra-stress and Cassandra in debug modes, and the query puzzles me a little bit.</p>

<p>Any ideas why the problem occurs? Thanks!</p>

<p><strong>edit:</strong></p>

<p>Attaching <code>schema.yml</code>: <a href=""https://gist.github.com/schybbkoh/76cdbf19a2bb933419063526ff5ac44f"" rel=""nofollow noreferrer"">https://gist.github.com/schybbkoh/76cdbf19a2bb933419063526ff5ac44f</a></p>

<p><strong>edit:</strong></p>

<p>As it turns out, the ""runs fine with no errors and starts to fill up the cluster with data"" schema creates and fills with data only the last table defined in the schema. Something's wrong here.</p>
",<cassandra><scylla><cassandra-stress>,"<p>All right, problem solved. There were two issues:</p>

<ul>
<li><code>cassandra-stress 3.0.18</code> vs <code>Cassandra 3.11.4</code> utilise different CQL spec (a conflict occured)</li>
<li><code>cassandra-stress 3.x</code> does not support multiple table definitions in one YML (see <a href=""https://issues.apache.org/jira/browse/CASSANDRA-8780"" rel=""nofollow noreferrer"">https://issues.apache.org/jira/browse/CASSANDRA-8780</a>)</li>
</ul>
",['table']
58575526,58652515,2019-10-26 23:35:09,cassandra skip columns on copy data from csv file,"<p>I want to copy some columns from csv file to cassandra table. There's 300 columns in csv file and I only need the first ten columns. There's no header in the csv file.
I tried</p>

<pre><code>copy table from 'file.csv' with header=false and skipcols=[range(11,300)]
</code></pre>

<p>but it didn't work.</p>
",<cassandra>,"<p>Can you specify column names after table name i.e.</p>

<pre><code>COPY TABLE xx(c1,c2..C30) FROM ..
</code></pre>
",['table']
58685581,58958532,2019-11-03 23:35:57,Cassandra secondary index vs another table,"<p>I have a racesByID table. I also need to find the races by year. What are the pros and cons of using a secondary index on year column over creating a racesByYear table?</p>
",<cassandra>,"<p>It depends on how many race you have per year, 
Secondary indexes doesn't perform well if you have low or very high carnality, you will have performance issues in those case. for details check this link : <a href=""https://docs.datastax.com/en/cql/3.3/cql/cql_using/useWhenIndex.html#useWhenIndex__when-no-index"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/cql/3.3/cql/cql_using/useWhenIndex.html#useWhenIndex__when-no-index</a> </p>

<p>In most cases separates tables will perform better, the cons is that you have to manage the consistency and keep table in sync </p>

<p>I hope this help. </p>
",['table']
58826562,58834961,2019-11-12 20:58:22,Can cassandra nodes be highly portable?,"<p><strong>The Environment:</strong></p>

<pre class=""lang-none prettyprint-override""><code>Nodes: 5
Replication: 5
Consistency: 1
</code></pre>

<hr>

<p><strong>The code:</strong></p>

<pre><code>using System;
using Cassandra;

    namespace CassandraSelectTest
    {
        class Program
        {
            static void Main(string[] args)
            {
                var cluster = Cluster.Builder()
                .AddContactPoints(""192.168.0.18"",""192.168.0.21"",""192.168.0.22"",""192.168.0.23"",""192.168.0.24"")
                .WithPort(9042)
                .Build();

                var session = cluster.Connect(""test_keyspace"");

                var results = session.Execute(""SELECT * FROM test_table"");

                foreach(var result in results)
                {
                    Console.WriteLine(result.GetValue&lt;string&gt;(""col1""));
                }

                Console.WriteLine($""Finished"");
                Console.ReadKey();
            }
        }
    }
</code></pre>

<hr>

<p><strong>The problem:</strong></p>

<p>Some of the nodes ideally need to be highly portable, which results in the IP address of the node changing when it is in a different location, and then changing back to its normal IP address when back to its original location.  This happens a few times a week.</p>

<hr>

<p><strong>The question:</strong></p>

<p>Is it possible to configure a single node to have multiple IP addresses, or dynamic addresses which change automatically?</p>
",<c#><cassandra><datastax><distributed-database>,"<p>I think that in that scenario the driver will receive a protocol event which will make the driver refresh the cluster's topology. As long as the node's IP is up to date in <code>system.peers</code> table, i.e., you update <code>broadcast_rpc_address</code> / <code>rpc_address</code> / <code>listen_address</code> on the node's cassandra.yml the driver should be able to notice that the old IP is no longer in that table (which will remove the old host) and that there's a new IP (which will add a new host).</p>

<p>If the control connection is not able to reconnect to any of the IPs on the local metadata cache (e.g. if all the nodes go down and change their IP addresses at the same time) then the driver will not be able to reconnect. <a href=""https://datastax-oss.atlassian.net/browse/CSHARP-819"" rel=""nofollow noreferrer"">There's this open ticket</a> which will resolve this issue as long as the user provides hostnames as the contact points (and DNS resolution returns the updated IPs).</p>
",['table']
58876545,58877068,2019-11-15 11:55:49,spring boot webflux login with username or email address,"<p>I am new to spring boot webflux, and am using Cassandra as my database. I can't figure it out how to login with username or email. I have three tables</p>

<p>user table</p>

<pre><code>Create Table user (
   userid bigint,
   username text,
   password text,
   name text,
   email text,
   phone text,
   birthday text,
   biography text,
   PRIMARY KEY (userid)
)
</code></pre>

<p>user_username table</p>

<pre><code>Create Table user (
   username text,
   userid bigint,
   password text,
   name text,
   email text,
   phone text,
   birthday text,
   biography text,
   PRIMARY KEY (username)
)
</code></pre>

<p>user_email table</p>

<pre><code>Create Table user (
   email text,
   username text,
   userid bigint,
   password text,
   name text,
   phone text,
   birthday text,
   biography text,
   PRIMARY KEY (email)
)

@PostMapping(""/signin"")
public Mono&lt;ResponseEntity&lt;?&gt;&gt; login(@RequestBody AuthLoginRequest authLoginRequest) {
   return userService.findByUsernameOrEmail(authLoginRequest.getUsernameOrEmail()).map((user) -&gt; {
      if(passwordEncoder.encode(authLoginRequest.getPassword()).equals(user.getPassword())) {
          return ResponseEntity.ok(new ApiResponse(tokenProvider.generateToken(user)));
      }else {
          return ResponseEntity.status(HttpStatus.UNAUTHORIZED).build();
      }
   }.defaultIfEmpty(ResponseEntity.status(HttpStatus.UNAUTHORIZED).build());
}
</code></pre>

<p>This is where the issues comes in how will i return a user mono Mono . if they login with their username i have to query the user_username table or if they login with their email address i have to query the user_email table.</p>

<pre><code>@Service
public class UserService() {
   public Mono&lt;User&gt; findByUsernameOrEmail(String usernameOrEmail) {

   }
}
</code></pre>

<p>Do i need to zip the user_username and user_email class? Please i need a solution i haven't seen any related issue concerning this. please i need a working solution.</p>

<p>Based on your answer @NikolaB i have edited this question to show what i have done so far</p>

<pre><code>public Mono&lt;User&gt; findByUsernameOrEmail(String usernameOrEmail) {
  return userUsernameRepository.findById(usernameOrEmail)
     .map(userUsername -&gt; {
                System.out.println(""Checking name ""+userUsername.getName());// i printed out the name of the user
                return new User(userUsername);
            }).switchIfEmpty(userEmailRepository.findById(usernameOrEmail)
                .map(userEmail -&gt; {
                        System.out.println(""Checking name "" +userEmail.getName());// i printed out the name of the user
                        return new User(userEmail);
                    }));
}
</code></pre>

<p>Everything works well...</p>

<p>My user table</p>

<pre><code>public class User {
    //My Constructors
    public User(UserUsername userUsername) {
      System.out.println(""Userid "" + userUsername.getUserId());/I am getting the userId
      User user = new User();
      BeanUtils.copyProperties(userUsername, user);
      System.out.println(""New Userid "" + user.getUserId()); //I am getting the userId
    }

    public User(UserEmail userEmail) {
      System.out.println(""Userid "" + userEmail.getUserId()); /I am getting the userId
      User user = new User();
      BeanUtils.copyProperties(userEmail, user);
      System.out.println(""New Userid "" + user.getUserId()); /I am getting the userId
    }
}
</code></pre>

<p>My post mapping </p>

<p>Here the User is empty</p>

<pre><code>@PostMapping(""/signin"")
public Mono&lt;ResponseEntity&lt;?&gt;&gt; login(@RequestBody AuthLoginRequest authLoginRequest) {
   return userService.findByUsernameOrEmail(authLoginRequest.getUsernameOrEmail()).map((user) -&gt; {
   if(passwordEncoder.encode(authLoginRequest.getPassword()).equals(user.getPassword())) {
          return ResponseEntity.ok(new ApiResponse(tokenProvider.generateToken(user)));
      }else {
          return ResponseEntity.status(HttpStatus.UNAUTHORIZED).build();
      }
   }.defaultIfEmpty(ResponseEntity.status(HttpStatus.UNAUTHORIZED).build());
}
</code></pre>
",<spring><cassandra><spring-webflux><project-reactor>,"<p>You need to query the <code>user_username</code> table and if there is an entry, map it to <code>User</code> class and return it. If there isn't you need to query the <code>user_email</code> table and map it to <code>UserUsername</code> class if an entry exists.</p>

<p>Easiest way to do it would be with repositories:</p>

<pre><code>public interface UserUsernameRepository extends ReactiveCassandraRepository&lt;UserUsername, String&gt; {
}


public interface UserEmailRepository extends ReactiveCassandraRepository&lt;UserEmail, String&gt; {
}
</code></pre>

<p>Here is the usage:</p>

<pre><code>@Service
public class UserService() {

   @Autowired
   private UserUsernameRepository userUsernameRepository;

   @Autowired
   private UserEmailRepository userEmailRepository;

   public Mono&lt;User&gt; findByUsernameOrEmail(String usernameOrEmail) {
       return userUsernameRepository.findById(usernameOrEmail)
               .switchIfEmpty(userEmailRepository.findById(usernameOrEmail)
                   .map(userEmail -&gt; new UserUsername(userEmail))) // or some other way to map properties to UserUsername class
                   .map(userUsername -&gt; new User(userEmail)) // or some other way to map properties to wanted User class
       }
    }
</code></pre>

<p>If both queries returned no results (empty) then service method would return <code>Mono.empty()</code> which is exactly what you need.</p>

<p>Note that you have to implement property mapping in <code>new UserUsername</code> and <code>new UserUsername</code> constructors.</p>

<p><strong>Edit</strong></p>

<p>Ok I think I know where the problem is, in the User constructors you are creating a new <code>User</code> instance and mapping <code>UserUsername/UserEmail</code> instance properties to that instantiated <code>User</code> instance when in fact that instance is not tied to instance which is returned by the constructor. Either set the fields manually in those constructors like this:</p>

<pre><code>public User(UserUsername userUsername) {
      System.out.println(""Userid "" + userUsername.getUserId()); //I am getting the userId
      this.email = userUsername.getEmail();
      this.username = userUsername.getUsername();
      ...
}
</code></pre>

<p>or in the service method map with <code>BeanUtils</code>:</p>

<pre><code>public Mono&lt;User&gt; findByUsernameOrEmail(String usernameOrEmail) {
  return userUsernameRepository.findById(usernameOrEmail)
     .map(userUsername -&gt; {
                System.out.println(""Checking name ""+userUsername.getName());// i printed out the name of the user
                User user = new User();
                BeanUtils.copyProperties(userUsername, user);
                return user;
            }).switchIfEmpty(userEmailRepository.findById(usernameOrEmail)
                .map(userEmail -&gt; {
                        System.out.println(""Checking name "" +userEmail.getName());// i printed out the name of the user
                        User user = new User();
                        BeanUtils.copyProperties(userEmail, user);
                        return user;
                    }));
}
</code></pre>
",['table']
58957732,58959797,2019-11-20 15:17:49,Cassandra - defining a global TTL and changing an already existing one,"<p>I know that one can define a TTL when making an INSERT / UPDATE to a Cassandra column / table etc. (<code>INSERT INTO ... USING TTL 1234</code>).
One can also define a default TTL when creating a table (<code>CREATE TABLE ... WITH default_time_to_live=""1234"";</code>).</p>

<p>However is it possible to define defaults that are more global? Possible scenarios would include:</p>

<ul>
<li>Defining a global TTL when creating a keyspace (so that any tables inherit the TTL)? Judging by the docs it seems that it's not supported (<a href=""https://docs.datastax.com/en/dse/5.1/cql/cql/cql_reference/cql_commands/cqlCreateKeyspace.html"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/dse/5.1/cql/cql/cql_reference/cql_commands/cqlCreateKeyspace.html</a> &amp; <a href=""https://docs.datastax.com/en/dse/5.1/cql/cql/cql_using/useExpire.html"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/dse/5.1/cql/cql/cql_using/useExpire.html</a>), but thought it's worth asking just in case</li>
<li>Defining (in the <code>cassandra.yaml</code> config?) a global TTL for any new keyspaces</li>
<li>Defining a global TTL for any new data in the whole cluster</li>
</ul>

<p>The other question would be:</p>

<ul>
<li>Is there is a quick and cheap way to change (increase, decrease) TTL for an already-existing data (as globally as possible - hopefully table level)? According to the docs a re-insert is required (<a href=""https://docs.datastax.com/en/dse/5.1/cql/cql/cql_using/useExpire.html#useExpire__setting-a-ttl-for-a-specific-column-RYjSo4Iu"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/dse/5.1/cql/cql/cql_using/useExpire.html#useExpire__setting-a-ttl-for-a-specific-column-RYjSo4Iu</a>), so a simple <code>ALTER TABLE</code> will not work. Is it right? Can anyone suggest some better ideas? </li>
</ul>
",<cassandra><ttl>,"<p>Your kind of answered your own question. 1. TTL is only possible at column or at Table level. 2. To alter the TTL of an existing column you need to reinsert it with the new value. </p>

<p>You can set the TTL at table level with the create table ddl and with the table property default_time_to_live. </p>

<p>When you rewrite the column it will Upsert it with the new TTL. there wont be two rows lying. Remember every DML goes into cassandra with a timestamp and the latest update wins. </p>
",['table']
59080866,59083111,2019-11-28 02:17:09,How to understand the concept of wide row and related concepts in Cassandra?,"<p>I feel very difficult to understand the concept of wide row and related concepts from <em>Cassandra The Definite Guide</em>:</p>

<blockquote>
  <p>Cassandra uses a special primary key called <strong>a composite key (or compound key)</strong> to
  represent <strong>wide rows, also called partitions</strong>. <strong>The composite key</strong> consists of a partition
  key, plus an  optional set of clustering columns.  <strong>The partition key</strong> is used to determine
  the nodes on which rows are stored and can itself consist of multiple columns. The
  <strong>clustering columns</strong> are used to control how data is sorted for storage within a partition.  Cassandra also supports an additional construct called a <strong>static column</strong>, which is
  for storing data that is not part of the primary key but is shared by every row in a
  partition.</p>
  
  <p>Figure  4-5  shows  how  each  partition  is  uniquely  identified  by  <strong>a  partition  key</strong>,  and
  how the <strong>clustering keys</strong> are used to uniquely identify the rows within a partition.</p>
  
  <p><a href=""https://i.stack.imgur.com/TV148.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TV148.png"" alt=""enter image description here""></a></p>
</blockquote>

<p>Are a wide row and a partition synonyms?</p>

<p>In ""the partition key is used to determine the nodes on which <strong>rows</strong> are stored and can itself consist of multiple columns"" and ""each partition is uniquely identified by a partition key"", </p>

<ul>
<li><p>since a partition key is for a wide row, why are there multiple ""rows"" (does ""rows"" here mean ""wide rows"")? </p></li>
<li><p>how does the partition key  ""determine the nodes on which <strong>rows</strong> are stored""?</p></li>
<li><p>How can a partition key be used for ""each partition is uniquely identified by a partition key""?</p></li>
</ul>

<p>In ""the clustering columns are used to control how data is sorted for storage within a partition"", </p>

<ul>
<li>what is a clustering column, for example, what are the clustering columns in the figure?</li>
<li>How do the clustering columns ""control how data is sorted for storage within a partition""?</li>
</ul>

<p>In ""the clustering keys are used to uniquely identify the rows within a partition"",</p>

<ul>
<li>a partition is a synonym of a wide row, what does it mean by ""the rows within a partition""?</li>
<li>How ""the clustering keys are used to uniquely identify the rows within a partition""?</li>
</ul>

<p>Thanks.</p>
",<cassandra><column-family>,"<blockquote>
  <p>Are a wide row and a partition synonyms?</p>
</blockquote>

<p>partition and row can be considered synonym. wide row is a scenario where the chosen partition key will result in very large number of <code>cells</code> for that key. Consider a scenario which has all persons in a country and partition key used is city, then there will be one row for one city and all person will be <code>cells</code> in that row. For metro city this will lead to wide rows. Another example can be storing sensor data received every few seconds with sensorId as partition key, which will lead to huge number of <code>cells</code> some years down the line.</p>

<blockquote>
  <p>since a partition key is for a wide row, why are there multiple ""rows""
  (does ""rows"" here mean ""wide rows"")?</p>
</blockquote>

<p>Same as above.</p>

<blockquote>
  <p>how does the partition key ""determine the nodes on which rows are
  stored""?</p>
</blockquote>

<p>From partiton key hash (MurMur3Hash is default) is generated and each node in cassandra is responsible for range of values. Consider Hash of partition key value turns out to be 20 and Node1 is responsible for range 1 to 100 then that partiton will reside on Node1. </p>

<blockquote>
  <p>How can a partition key be used for ""each partition is uniquely
  identified by a partition key""?</p>
</blockquote>

<p>As explained above partition key decides on which node the data resides.. Data representation can be considered as huge map which can have only unique keys.</p>

<blockquote>
  <p>what is a clustering column, for example, what are the clustering
  columns in the figure?</p>
</blockquote>

<p>Consider a table created like <code>Create TABLE test (a text,b int, c text, PRIMARY KEY(a,b))</code> here <code>a</code> is partition key and <code>b</code> is clustering column. In the figure attached <code>clustering key</code> is the clustering column and whole enclosing box is cell.</p>

<blockquote>
  <p>How do the clustering columns ""control how data is sorted for storage
  within a partition""?</p>
</blockquote>

<p>Cassandra will sort the data using column <code>b</code> in the above example table in ascending table. It can be changed to descending as well.</p>

<pre><code>INSERT INTO test(a,b,c) VALUES('test',2,'test2')
INSERT INTO test(a,b,c) VALUES('test',1,'test1')
INSERT INTO test(a,b,c) VALUES('test-new',1,'test1')
</code></pre>

<p>If you run the above query in this order cassandra will store data in following order (Data representation has much more than below.. just check the order of column b):</p>

<pre><code>test -&gt; [b:1,c=test1] [b:2,c=test2]
test-new -&gt; [b:1,c=test1]
</code></pre>

<blockquote>
  <p>a partition is a synonym of a wide row, what does it mean by ""the rows
  within a partition""?</p>
</blockquote>

<p>Clustering column is used to identify <code>cells</code> (cells is a better term than row) within a partition. example <code>SELECT * from test where a='test' and b=1</code> will pick up the cell with <code>b:1</code> for partiton key test.</p>

<blockquote>
  <p>How ""the clustering keys are used to uniquely identify the rows within
  a partition""?</p>
</blockquote>

<p>Above answer should explain this as well.</p>
",['table']
59143085,59143979,2019-12-02 16:34:50,What to do with empty directories from Cassandra on disk?,"<p>I have Cassandra 3.11.4 and been running a test environment for a while. I have done nodetool cleanup, clearsnapshot, repair, compact etc and what remains in the data storage directory for my keyspace contains numerous ""empty"" directories.</p>

<p>When running du from the directory:</p>

<pre><code>0       ./a/backups
47804   ./a
0       ./b/backups
0       ./b
0       ./c/backups
0       ./c
0       ./d/backups
0       ./d
7748832 .
</code></pre>

<p>Just a portion of the data with names renamed to generic letters, but essentially there are many of these empty directories remaining. The tables referenced however have either already been dropped a long time ago i.e. longer than gc_grace_seconds but the directory links remain? These are not snapshots, as making a snapshot and clearing it with nodetool clearsnapshot works fine. </p>

<p>Before I manually delete each of the empty folders, which is going to be a pain as there are a lot of them; am I missing a step in maintaining my cluster which causes this or is it something that happens and would have to be handled regularly assuming many changes in my test schemas?</p>

<p>Snapshots get cleared and the /backups trailing kind of mean that these are incremental backups?</p>

<blockquote>
  <p><a href=""https://docs.datastax.com/en/cassandra/3.0/cassandra/operations/opsBackupIncremental.html"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/cassandra/3.0/cassandra/operations/opsBackupIncremental.html</a></p>
</blockquote>

<p>Even if it is though, there are no methods to remove these incremental backups that I can find at least with nodetool, and at the least, the setting for incremental_backups in cassandra.yaml is False.</p>

<p>I believe there are answers stating it is safe to delete these ""ghost"" directories but it would be extremely annoying if the keyspace has many of these. Also, maybe it is just my idea of wanting clean directories, would these ""ghost"" directories have an impact on performance? </p>
",<cassandra>,"<p>So the ""ghost"" table directories are either from:<br>
1) empty table - still a valid table, but no data ever inserted<br>
2) truncated tables<br>
3) dropped tables</p>

<p>In the first and second case, if you remove the directory, you could end up causing issues. If you want to validate whether the directory is in use for that table you can query:</p>

<pre><code>select id from system_schema.tables 
where keyspac_name = 'xxxx' and
      table_name = 'yyyy';
</code></pre>

<p>That ID is the id used for the directory extension for that table. Any other occurrences of that directory for that table for that keyspace are not in use.</p>

<p>-Jim</p>
",['table']
59551892,59552369,2020-01-01 10:58:28,Sorting with Cassandra,"<p>I have a table with schema:</p>

<pre><code>CREATE TABLE messages {
   chatroom_id,
   id,
   createdAt,
   senderType,
   ...,
 PRIMARY KEY ((chatroom_id), createdAt)
} WITH CLUSTERING ORDER BY (createdAt DESC);
</code></pre>

<p>also I have on this table secondary index on column senderType.</p>

<p>all the queries (until now) needed to be ordered by createdAt DESC</p>

<p>but now I need to do a new query like:</p>

<pre><code>select * from messages where chatroom_id = xx
and senderType = yy
order by createdAt ASC;
</code></pre>

<p>Is there any option to create this query besides materialized view?</p>

<p>Thanks.</p>
",<cassandra><cql>,"<p>Unfortunately the <a href=""https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/cql3/statements/SelectStatement.java#L1107"" rel=""nofollow noreferrer"">Cassandra code has an explicitly check</a> that no secondary indexes are used in query with <code>ORDER BY</code> inside.</p>

<p>IMHO you have following options:</p>

<ol>
<li>create a new table &amp; fill it from your code. It could be much faster from performance point of view than using materialized views. But this requires more coding in your app.</li>
<li>use materialized view - it's slower than explicit table, but doesn't require any additional code. But remember that MVs are still experimental in Cassandra, and you can get inconsistencies;</li>
<li>perform sorting in your application - if you don't have so much data, so you can fetch into your app &amp; sort - in this case, the query like <code>select * from messages where chatroom_id = xx and senderType = xx;</code> will work, just return data in <code>DESC</code>...</li>
</ol>

<p>for options 1 &amp; 2 I would recommend to change table or MV structure to include <code>senderType</code> as part of primary key, as it will be much faster to perform, something like:</p>

<pre><code>PRIMARY KEY ((chatroom_id), senderType, createdAt)
) WITH CLUSTERING ORDER BY (createdAt ASC);
</code></pre>
",['table']
59617657,59618346,2020-01-06 19:11:28,EACH_QUORUM VS QUORUM,"<p>This is a screenshot from the consistency level table according <a href=""https://docs.datastax.com/en/dse/5.1/cql/cql/cql_reference/cqlsh_commands/cqlshConsistency.html"" rel=""nofollow noreferrer"">to Datastax documentation</a>:</p>

<p><a href=""https://i.stack.imgur.com/6WxoS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6WxoS.png"" alt=""enter image description here""></a></p>

<p>What is the difference between EACH_QUORUM and QUORUM? <code>Each</code> and <code>all</code> DC's are the same AFAIK. In the QUORUM row the following is stated:</p>

<blockquote>
  <p>Some level of failure is possible</p>
</blockquote>

<p>Why? If one node is down in each DC? The same applies for EACH_QUORUM right? Why does EACH_QUORUM does not have some level of failure, since it is ALL_QUORUM and not ALL?</p>

<p>Both levels have the same in common (AFAIK):</p>

<ul>
<li>All/each (same right?) DC's needs to be online</li>
<li>51% or more of the nodes need to confirm the read/write.</li>
</ul>
",<cassandra>,"<p>The difference between QUORUM and EACH_QUORUM is as follows.</p>

<p>Assume you have 6 nodes in your cluster - 2 DCs with 3 nodes each and RF=3 for both DCs (all nodes have all data).</p>

<p>The QUORUM and EACH_QUORUM value is the same = 4 (6/2 + 1). However, which nodes can respond varies slightly. EACH_QUORUM has less combinations of what will satisfy the condition.</p>

<p>QUORUM requires 4 nodes to respond but with any combination of nodes. So for example, maybe 3 nodes from the local DC and 1 node from the remote DC respond. That's perfectly fine. </p>

<p>Now, with QUORUM_EACH, each DC must have a quorum respond. What the means is that 2 nodes from each DC must respond in this case, that's it (which 2 nodes in each DC is irrelevant) . 3 nodes from the local DC and 1 node from a remote DC does not qualify as 1 node in the remote dc is not a quorum of that dc. </p>

<p>Let's change the cluster node count to 7 instead of 6. DC1 has 4 nodes, DC2 has 3 nodes. DC1 RF = 4 and DC2 RF = 3 (all nodes have the data again). Here's where the fun begins with the odd number total in the RF.</p>

<p>While I'm not sure about the word ""failure"", but I can see certain scenarios where this could be problematic.</p>

<p>For QUORUM, 4 nodes need to respond (7/2 + 1 = 4) - any 4 nodes - including the scenario when all nodes from the local/larger DC responds (DC1 in this case). What if the most current data is on DC2? In this scenario, you could end up with undesirable results.</p>

<p>With QUORUM_EACH, 5 nodes would need to respond (Quorum of DC1 = 4/2+1 = 3, Quorum of DC2 = 3/2+1 = 2 ==> total = 5). With this scenario, you're forcing cassandra to return data from both DCs - and a QUORUM level from each DC which should give you good results.</p>

<p>Again, I'm trying in my head to determine where the additional ""failures"" could come with QUORUM v.s. QUORUM_EACH and I can't at the top of my head see it. It would seem if anything, QUORUM_EACH with an odd node count, is less flexible in unavailable nodes as a quorum in each DC must respond v.s. any quorum number of nodes from any DC. I can see where QUORUM may give you undesirable results though (explained above).</p>
",['dc']
59640207,59659747,2020-01-08 06:04:28,Getting error for value exceeding max_value_size_in_mb in cassandra.yaml,"<p>Now I observed some errors on my Cassandra system.log/debug.log and node was sudden DOWN after below errors.</p>

<p>CorruptSSTableException as value length exceeds the maximum of 287435456, which is set via max_value_size_in_mb in cassandra.yaml</p>

<p>While going through this value in Cassandra documentations found :-</p>

<p><strong>max_value_size_in_mb
This option is commented out by default.
Maximum size of any value in SSTables. Safety measure to detect SSTable corruption early. Any value size larger than this threshold will result into marking an SSTable as corrupted. This should be positive and less than 2048.
Default Value: 256</strong></p>

<p>In my case also this value is default which is 256. But my questions is :-</p>

<p><strong>1)What does it mean by this value? Is it max size of any SStables ?
2)Why node was down after exceeding this value or after this error ?</strong></p>

<p>Thanks in advance!</p>
",<cassandra><nosql><datastax><scylla>,"<blockquote>
  <p>What does it mean by this value?</p>
</blockquote>

<p>That either there is a Row in your database file (one of SSTables) larger than allowed maximum size or the SSTable file is corrupted because the metadata contains values (row size) that exceed reasonable limits.</p>

<blockquote>
  <p>Is it max size of any SStables?</p>
</blockquote>

<p>It is maximum allowed size of any Value (Row) (Single piece of data) (Key/Value pair) (Tuple) in the SSTable (part of a table).</p>

<p>Note 1: SSTable is a storage format of data for the database. It just sorted strings (rows). Usually, a table consists of a set of sstable files that contain different parts of data.</p>

<p>Note 2: How a table row can look like on disk? :</p>

<pre><code>Row [ Len: uint64_t, Data: char[Len] ]
Len must be &lt; max_value_size_in_mb (256MB by default)
</code></pre>

<blockquote>
  <p>Why node was down after exceeding this value or after this error ?</p>
</blockquote>

<p>I don't know exactly whether a node shuts down if it finds out corrupted sstable. It could just mark it as corrupted and ignore it since then. Thus, it may be not related.</p>

<p>Basically, if there 1 of your sstables is corrupted, then you either lose data or may see previous versions or deleted rows. So, from the perspective of consistency, it is not a very good idea to allow client to interact with a broken table. But behavior depends on the database. </p>
",['table']
59654997,59659256,2020-01-08 23:03:35,DSE (Cassandra) - Range search on int data type,"<p>I am a beginner using Cassandra. I created a table with below details and when I try to perform range search using token, I am not getting any results. Am I doing something wrong or is it my understanding of data model?</p>

<p><a href=""https://i.stack.imgur.com/D7Scr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/D7Scr.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/RbAGg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RbAGg.png"" alt=""enter image description here""></a></p>

<p>Query <code>select * from test where token(header)&gt;=2 and token(header)&lt;=4;</code></p>
",<cassandra><datastax><datastax-enterprise>,"<p>the <code>token</code> function calculates the token from the value based on the configured partitioner. The calculated value is the hash that is used to identify the node where the data is located, this is not a data itself.</p>

<p>Cassandra can perform range search on values only on clustering columns (only for some designs) only inside the single partition.  If you need to perform range on arbitrary column (also for partition keys), there is a DSE Search that allows you to index the table and perform different types of search, including range... But take into account that it will be much slower than traditional Cassandra queries.</p>

<p>In your situation, you can run 3 queries in parallel (to cover values 2,3,4), like this:</p>

<pre><code>select * from test where header = value;
</code></pre>

<p>and then combine results in your code.</p>

<p>I recommend to take DS201 &amp; DS220 courses on <a href=""https://academy.datastax.com/"" rel=""noreferrer"">DataStax Academy</a> to understand how Cassandra performs queries, and how to model data to make this possible.</p>
",['table']
59671310,59965580,2020-01-09 20:01:03,Databricks Spark Cassandra connectivity throwing exception: com.datastax.driver.core.exceptions.NoHostAvailableException,"<p>I have installed the Cassandra DB in Azure Virtual Machine and want to perform read/write operation through the Azure Databricks. I am going through the Databricks offcial <a href=""https://docs.databricks.com/data/data-sources/cassandra.html"" rel=""nofollow noreferrer"">documentation</a> which does not help me in configuration.<br>
I am sharing below my code cum configurations details:</p>

<pre><code>%sh
ping -c 2 vmname.westeurope.cloudapp.azure.com
</code></pre>

<p><strong>Response received:</strong></p>

<pre><code>PING vmname.westeurope.cloudapp.azure.com (13.69.10.10): 56 data bytes
--- vmname.westeurope.cloudapp.azure.com ping statistics ---
2 packets transmitted, 0 packets received, 100% packet loss
</code></pre>

<pre><code>// define the cluster name and cassandra host name
val sparkClusterName = ""adbazewdobucluster""
val cassandraHostIP = ""vmname.westeurope.cloudapp.azure.com""

dbutils.fs.put(s""/databricks/init/$sparkClusterName/cassandra.sh"",
  s""""""
     #!/usr/bin/bash
     echo '[driver].""spark.cassandra.connection.host"" = ""$cassandraHostIP""' &gt;&gt; /home/ubuntu/databricks/common/conf/cassandra.conf
   """""".trim, true)

// setting IP of the Cassandra server
spark.conf.set(""spark.cassandra.connection.host"", ""127.0.0.1"")

//verify sparkconf is set properly
spark.conf.get(""spark.cassandra.connection.host"")

</code></pre>

<p>and after applying all the configuration in spark I am trying to retrieve the records from the table resides in Cassandra DB, which is throwing the exception.</p>

<pre><code>val df = sqlContext
  .read
  .format(""org.apache.spark.sql.cassandra"")
  .options(Map( ""table"" -&gt; ""words_new"", ""keyspace"" -&gt; ""test""))
  .load
df.explain
</code></pre>

<p><strong>Exception:</strong></p>

<pre><code>com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: /127.0.0.1:9042 (com.datastax.driver.core.exceptions.TransportException: [/127.0.0.1:9042] Cannot connect))
</code></pre>

<p>I have checked the my Cassandra DB is running and read/write operation working fine directly.<br>
So my <strong>question is</strong>: Am I applying the configuration in a right way? If not so then How do I access the Cassandra from the Databricks notebook.<br>
I am using Scala for the Spark framework and my cluster and driver versions are as following:</p>

<pre><code>Databricks Runtime Version
6.2 (includes Apache Spark 2.4.4, Scala 2.11)

spark-cassandra-connector
com.datastax.spark:spark-cassandra-connector_2.11:2.4.1

cassandra version: 3.11.4
</code></pre>
",<apache-spark><cassandra><apache-spark-sql><azure-databricks><spark-cassandra-connector>,"<p>If you're running on Azure.. make sure to set broadcast_rpc_address to public IP address or dns hostname these settings must work for you -</p>

<p>Set rpc address to ip address of your network interface attached to your VM..on Windows - Hyper V Interface.</p>

<pre><code>rpc_address: &lt;**private ip** of your vm &gt; 
</code></pre>

<p>broadcast rpc address to public ip, on this ip external clients should get response from cassandra on port 9042</p>

<pre><code>broadcast_rpc_address: &lt;**public ip** or hostname.westeurope.cloudapp.azure.com&gt;
</code></pre>

<p>listen address as default to localhost / 127.0.0.1</p>

<pre><code>listen_address: **localhost**
</code></pre>
",['broadcast_rpc_address']
59680107,60064549,2020-01-10 10:44:48,What datatypes translate best between Java and Cassandra?,"<p>I am using Spring Data Cassandra. I am interested in the most painless way to get my project up and working quickly. I am trying to choose appropriate data types so I don't have to think about them going forward. </p>

<p>I have the following data I am creating in Cassandra. </p>

<ol>
<li>lat/long for location - Is BigDecimal the best Java 8 Type for this?</li>
<li>thumbnails - 32-64kb of binary data. I wish to store it directly in the record because of it's small size. What's the best Java 8 type for this?</li>
<li>timestamp - What's the best Java 8 Class for timestamps on Cassandra?</li>
</ol>
",<java-8><cassandra><cassandra-3.0><datastax-java-driver><spring-data-cassandra>,"<p>Right now, Spring uses Java driver 3.x that is compatible with Java 6, so there are some restrictions in out of box experience. The <a href=""https://docs.datastax.com/en/developer/java-driver/3.8/manual/#cql-to-java-type-mapping"" rel=""nofollow noreferrer"">driver manual has a table with mapping between Java &amp; CQL data types</a>.  Answering to your questions:</p>

<ol>
<li>It depends on your requirement - I think that doubles will be more than enough for that;</li>
<li>blobs are by default mapped into <code>java.nio.ByteBuffer</code> - if you need, you can add the codec for <code>byte[]</code>, but it will require coding on your side (see <a href=""https://docs.datastax.com/en/developer/java-driver/3.8/manual/custom_codecs/"" rel=""nofollow noreferrer"">doc on custom codecs</a>);</li>
<li>For mapping between timestamps &amp; Java 8 temporal types, there is a separate jar with so-called <a href=""https://docs.datastax.com/en/developer/java-driver/3.8/manual/custom_codecs/extras/"" rel=""nofollow noreferrer"">extra codec</a>s that you can register &amp; get direct mapping into <code>Instant</code>, etc.</li>
</ol>

<p>P.S. Newer driver 4.x supports Java 8 and higher, so you'll get timestamp mapping our of box. But there is no Spring Data version yet with support for this version.</p>
",['table']
59681359,59732653,2020-01-10 12:06:44,How do I fetch sorted logs in Cassandra?,"<p>In my application we store logs in Cassandra. User can see the logs after giving a start and an end date for the logs. We fetch the data on the basis of these dates and have implemented pagination as well such that the end date of page one becomes the start date for page 2. </p>

<p>Table:</p>

<pre><code>CREATE TABLE audit_trail (
    account_id bigint,
    user_id bigint,
    time timestamp,
    category int,
    ip_address text,
    action_description text,
    additional_data map&lt;text,text&gt;,
    source int,
    source_detail varchar,
    PRIMARY KEY ( (account_id), time )
     ) WITH CLUSTERING ORDER BY (time DESC);
</code></pre>

<p>Problem: </p>

<p>The logs we get are not sorted but scattered. For example upon hitting the query for logs of day 1 to 10 we might be getting logs for day 10,8,9,2,1, or in any other order. </p>

<p>Aim:</p>

<ol>
<li>to get the logs in sorted order such that logs from day 1 are shown at the top then day 2 and so on.</li>
<li>no data shuffling. As, upon collision the table is restructured in Cassandra which might give in data we already have seen in page 1 on page 2 again.</li>
</ol>

<p>Data throughput is large, usually around 1000 logs per  hour.</p>
",<sorting><logging><cassandra><database-performance><database-partitioning>,"<p>WITH CLUSTERING ORDER BY (time DESC);</p>

<p>Adding this at the end of the table solved the problem for me.</p>
",['table']
59693901,59694255,2020-01-11 11:18:40,Test Cassandras R + W > N,"<p>I want to explicitly show the <code>R + W &gt; N</code> rule at a presentation.</p>

<p>So my initial idea how to do this is as following:</p>

<pre><code>// setup
1: Create a Docker network with 3 Cassandra Nodes.  
2: Create a simple Keyspace with Replication-Factor of 3.  
3: Explicitly shutdown two of the docker nodes.

4: Create a write query inserting some data with W=1
   As two of the nodes are down but one is still online this should succeed

5: Bring both of the nodes back online
6: Reading the Data I just pushed to one node with R=1
</code></pre>

<p>If I understand <code>R + W &gt; N</code> correctly I should now have the chance of 2/3 to get inconsistent data. Which is exactly what I want to show.</p>

<p><strong>My Question is:<br>
Is there an option I need to disable in order to stop nodes from syncing when they get back online?</strong></p>

<p>So I would require to disable these options ?</p>
",<cassandra><cassandra-2.0><cassandra-3.0>,"<p>You need to disable hints on all nodes (set <code>hinted_handoff_enabled</code> in <code>cassandra.yaml</code> to <code>false</code>) - in this case, replication will happen only when you explicitly do the <code>nodetool repair</code>. </p>

<p>You also need to make sure that <code>read_repair_chance</code> and <code>dclocal_read_repair_chance</code> are set to 0 for a table where you'll do the test. The easiest way is just to specify these options when creating the table:</p>

<pre><code>create table (

....)
WITH read_repair_chance = 0.0 and dclocal_read_repair_chance = 0.0;
</code></pre>
",['table']
59830846,59834889,2020-01-20 21:06:33,Cassandra Virtual Nodes,"<p>Although it is asked many times and answered many times, I did not find a good answer anyway.
Neither in forums nor in cassandra docs.</p>

<p>How do virtual nodes work?</p>

<p>Suppose a node having 256 virtual nodes.
And docs say they are distributed randomly.
(put away how that ""randomly"" done...I have another,more urgent question):</p>

<ol>
<li><p>Is that right that every cassandra node (""physical"") actually responsible for several distinct locations in the ring? (for 256 locations)? Does that mean the ""physical"" node sort of ""spread"" on the whole circle? </p></li>
<li><p>How in that case re-balancing works? If I add a new node?
The ring will get an additional 256 nodes.
How those additional nodes will divide the data with the old nodes?
Will they, basically, appear as additional ""bicycle spokes"" randomly spread through the whole ring?</p></li>
</ol>

<p>A lot of info on the internet, but nobody makes a clear explanation...</p>
",<cassandra>,"<p>Vnodes break up the available range of tokens into smaller ranges, defined by the num_tokens setting in the cassandra.yaml file. The vnode ranges are randomly distributed across the cluster and are generally non-contiguous. If we use a large number for num_tokens to break up the token ranges, the random distribution means it is less likely that we will have hot spots.Using statistical computation, the point where all clusters of any size always had a good token range balance was when 256 vnodes were used. Hence, the num_tokens default value of 256 was the recommended by the community to prevent hot spots in a cluster.</p>

<p><strong>Ans 1:-</strong> It is a range of tokens based on num_tokens. if you have set 256 the you will get 256 token ranges which is default.</p>

<p><strong>Ans 2:-</strong> Yes, when you are adding or removing the nodes the tokens will distribute again in the cluster based on vnodes configurations.</p>

<p>you may refer for more details are here <a href=""https://docs.datastax.com/en/ddac/doc/datastax_enterprise/dbArch/archDataDistributeVnodesUsing.html"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/ddac/doc/datastax_enterprise/dbArch/archDataDistributeVnodesUsing.html</a></p>
",['num_tokens']
59845353,59849333,2020-01-21 16:26:00,Add missing monthly rows,"<p>I would like to list the missing date between two dates in a request for example</p>

<p>My data :</p>

<pre><code>YEAR_MONTH  | AMOUNT    
202001  |  500    
202001  |  600    
201912  |  100    
201910  |  200
201910  |  100     
201909  |  400
201601  | 5000
</code></pre>

<p>I want the request to return</p>

<pre><code>201912  |  100    
201911  |    0    
201910  |  300
201909  |  400     
201908  |    0
201907  |    0
201906  |    0
....    |    0
201712  |    0
</code></pre>

<p>i want the last 24 months from the date of execution</p>

<p>I did something similar with the dates but not YEAR MONTH <code>yyyyMM</code></p>

<pre><code>select date_sub(s.date_order ,nvl(d.i,0)) as date_order, case when d.i &gt; 0 then 0 else s.amount end as amount
from
(--find previous date
select date_order, amount, 
        lag(date_order) over(order by date_order) prev_date,
        datediff(date_order,lag(date_order) over(order by date_order)) datdiff
from
( --aggregate
 select date_order, sum(amount) amount from your_data group by date_order )s
)s
--generate rows
lateral view outer posexplode(split(space(s.datdiff-1),' ')) d as i,x
order by date_order;
</code></pre>

<p>I use Cassandra database with Apache Hive connector</p>

<p>Can someone help me ?</p>
",<sql><hive><cassandra><hiveql><date-range>,"<p><code>date_range</code> subquery generates 24 months (adjust if you want some other than 24 months range) back from current date. Left join it with your dataset, see comments in this demo code:</p>

<pre><code>with date_range as 
(--this query generates months range, check it's output
select date_format(add_months(concat(date_format(current_date,'yyyy-MM'),'-01'),-s.i),'yyyyMM') as year_month 
  from ( select posexplode(split(space(24),' ')) as (i,x) ) s --24 months
),

your_data as (--use your table instead of this example
select stack(7,
202001, 500,    
202001, 600,    
201912, 100,    
201910, 200,
201910, 100,     
201909, 400,
201601,5000 -----this date is beyond 24 months, hence it is not in the output
) as (YEAR_MONTH, AMOUNT )
)

select d.year_month, sum(nvl(s.amount,0)) as amount --aggregate
  from date_range d 
       left join your_data s on d.year_month=s.year_month
  group by d.year_month;
</code></pre>

<p>Result:</p>

<pre><code>d.year_month    amount
201801  0
201802  0
201803  0
201804  0
201805  0
201806  0
201807  0
201808  0
201809  0
201810  0
201811  0
201812  0
201901  0
201902  0
201903  0
201904  0
201905  0
201906  0
201907  0
201908  0
201909  400
201910  300
201911  0
201912  100
202001  1100
</code></pre>

<p>Use your table instead your_data subquery. Add <code>order by</code> if necessary.</p>
",['table']
59859771,59861178,2020-01-22 12:27:00,Delete records in Cassandra table based on time range,"<p>I have a Cassandra table with schema:</p>

<pre><code>CREATE TABLE IF NOT EXISTS TestTable(
    documentId text,
    sequenceNo bigint,
    messageData blob,
    clientId text
    PRIMARY KEY(documentId, sequenceNo))
WITH CLUSTERING ORDER BY(sequenceNo DESC);
</code></pre>

<p>Is there a way to delete the records which were inserted between a given time range? I know internally Cassandra must be using some timestamp to track the insertion time of each record, which would be used by features like TTL.</p>

<p>Since there is no explicit column for insertion timestamp in the given schema, is there a way to use the implicit timestamp or is there any better approach?</p>

<p>There is never any update to the records after insertion.</p>
",<cassandra><cqlsh>,"<p>It's an interesting question...</p>

<p>All columns that aren't part of the primary key have so-called WriteTime that could be retrieved using the <code>writetime(column_name)</code> function of CQL (<strong>warning</strong>: it doesn't work with collection columns, and return null for UDTs!).  But because we don't have nested queries in the CQL, you will need to write a program to fetch data, filter out entries by WriteTime, and delete entries where WriteTime is older than your threshold.  (<strong>note</strong> that value of <code>writetime</code> is in microseconds, not milliseconds as in CQL's <code>timestamp</code> type).</p>

<p>The easiest way is to use <a href=""https://github.com/datastax/spark-cassandra-connector/blob/master/doc/3_selection.md#example-using-select-to-retreive-ttl-and-timestamp"" rel=""nofollow noreferrer"">Spark Cassandra Connector's RDD API</a>, something like this:</p>

<pre class=""lang-scala prettyprint-override""><code>val timestamp = someDate.toInstant.getEpochSecond * 1000L
val oldData = sc.cassandraTable(srcKeyspace, srcTable)
      .select(""prk1"", ""prk2"", ""reg_col"".writeTime as ""writetime"")
      .filter(row =&gt; row.getLong(""writetime"") &lt; timestamp)
oldData.deleteFromCassandra(srcKeyspace, srcTable, 
      keyColumns = SomeColumns(""prk1"", ""prk2""))
</code></pre>

<p>where: <code>prk1</code>, <code>prk2</code>, ... are all components of the primary key (<code>documentId</code> and <code>sequenceNo</code> in your case), and <code>reg_col</code> - any of the ""regular"" columns of the table that isn't collection or UDT (for example, <code>clientId</code>). It's important that list of the primary key columns in <code>select</code> and <code>deleteFromCassandra</code> was the same.</p>
",['table']
59929541,59930141,2020-01-27 10:43:41,Does inserting into a Cassandra unset cell create a tombstone?,"<p>I am trying to enable invalidation of old measurements while keeping them in my Cassandra setup. Given the following table structure:</p>

<pre><code>ID|Test|result|valid|valid2
1 | 1  |  10  | False| unset
2 | 1  |  11  | True| False
3 | 1  |  12  | True| True
</code></pre>

<p>with primary key (ID,test)</p>

<p>Now if I insert the following SparkDataframe using the connector as normal with mode(""append"")</p>

<pre><code>ID|Test|valid2
1 | 1  | False
</code></pre>

<p>Will this create a tombstone? The purpose is to be able to ""invalidate"" certain rows in my tables when necessary. I understand tombstones are created when cells are outdated. But since there is no value in the cell, will a tombstone be created? </p>
",<cassandra>,"<p>Tombstones are created when you performing explicit DELETE, insert <code>null</code> value, or data is TTLed.  </p>

<p>If you don't specify the value for specific column, then the data for this cell is simply not set, and if you had some previous data before, then they won't be overwritten until you explicitly set them to <code>null</code>.  But in Spark, usually situation is different - by default it will insert nulls until you won't specify <code>spark.cassandra.output.ignoreNulls</code> as <code>true</code> - in this case it will treat nulls as unset, and won't owerwrite the previous data. </p>

<p>But when you specify incomplete row, then only provided pieces will be updated, keeping the previous data intact.</p>

<p>If we have following table and data:</p>

<pre><code>create table test.v2(id int primary key, valid boolean, v int);
insert into test.v2(id, valid, v) values(2,True, 2);
insert into test.v2(id, valid, v) values(1,True, 1);
</code></pre>

<p>we can check that data is visible in Spark:</p>

<pre><code>scala&gt; val data = spark.read.cassandraFormat(""v2"", ""test"").load()
data: org.apache.spark.sql.DataFrame = [id: int, v: int ... 1 more field]

scala&gt; data.show
+---+---+-----+
| id|  v|valid|
+---+---+-----+
|  1|  1| true|
|  2|  2| true|
+---+---+-----+
</code></pre>

<p>Now update the data:</p>

<pre><code>scala&gt; import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.SaveMode

scala&gt; val newData = Seq((2, false)).toDF(""id"", ""valid"")
newData: org.apache.spark.sql.DataFrame = [id: int, valid: boolean]

scala&gt; newData.write.cassandraFormat(""v2"", ""test"").mode(SaveMode.Append).save()

scala&gt; data.show
+---+---+-----+
| id|  v|valid|
+---+---+-----+
|  1|  1| true|
|  2|  2|false|
+---+---+-----+
</code></pre>
",['table']
59961978,59965495,2020-01-29 07:03:40,unable to access cassandra from azure data bricks,"<p>whenever i am trying to access my cassandra cluster from azure databricks getting below error,</p>

<p>java.io.IOException: Failed to open native connection to Cassandra at {xx.xx.xx.xx}:9142</p>

<p>Caused by: com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: /xx.xx.xx.xx:9142 (com.datastax.driver.core.exceptions.TransportException: [/xx.xx.xx.xx:9142] Cannot connect))</p>

<p>Cassandra was installed on azure vm ,both my azure vm &amp; data bricks are in same VNET.</p>

<p>can you please someone help me on this?</p>
",<azure><apache-spark><cassandra><databricks>,"<p>If you're running on Azure or AWS.. make sure to set broadcast_rpc_address to public IP address or dns hostname
these settings must work for you -</p>

<p>rpc_address: &lt;<strong>private ip of your vm</strong> > </p>

<p>broadcast_rpc_address: &lt;<strong>public ip</strong> or <strong>hostname.westeurope.cloudapp.azure.com</strong>></p>

<p>listen_address: <strong>localhost</strong> </p>
",['broadcast_rpc_address']
60152075,60153044,2020-02-10 14:03:17,Suggestion required to design partitioning in cassandra,"<p>I have to design the database for customers having prices for millions of materials they acquire through multiple suppliers for the next 24 months. So the database will store prices on a daily basis for every material supplied by a specific supplier for the next 24 months. So we keep past data. Now lookups will happen on:</p>

<ol>
<li>Find the price for a material by the supplier as of the specific date by a customer.</li>
<li>Find the price for a material by the supplier for a time period by a customer.</li>
</ol>

<p>I can think for the primary key as:</p>

<ol>
<li>Partition key: (customer Id, material Id, supplier Id, date) 
-- will this end up in perf issues as it will make so many partitions in long run?</li>
<li>Partition Key: (customer Id, material Id, supplier Id, monthbucket), clustering key: date
-- monthbucket will store data for material on monthly basis on same partition and will 
have value like '202002' for Feb 2020 dates.</li>
</ol>

<p>Another question is how can I make sure my data distributes evenly across nodes.</p>

<p>Note: </p>

<ol>
<li>combination of customer, material, supplier and date is unique.</li>
<li>two customer can have similar material ids.</li>
</ol>

<p>Key Points:
1. Some customers can have a very small data set while others can have huge data. How well we can distribute data evenly across partitions as Date is a constant field for all customers. Also, material Ids can be the same among different customers as that is there internal representation (maybe a numeric or alphanumeric)</p>

<ol start=""2"">
<li>Number of Suppliers per customer and material id varies from 1 - 20 in number.
do you have any suggestions or questions?</li>
</ol>

<p>Thanks.</p>
",<design-patterns><cassandra><database-partitioning>,"<p>It depends on how many suppliers do you have per customer. Because you always have queries on the customer ID &amp; material ID, then I suggest that at least that columns are making into partition key. If you have too many suppliers, you can move it into partition key as well. And I would avoid having monthly bucket - it will make querying hard.</p>

<p>So you can go with following primary keys:</p>

<ol>
<li><code>((customer, material, supplier), date)</code></li>
<li><code>((customer, material) supplier, date)</code></li>
</ol>

<p>both will allow to have both queries:</p>

<ol>
<li><code>select * from table where customer = ... and material = ... and supplier = ... and date = ...</code></li>
<li><code>select * from table where customer = ... and material = ... and supplier = ... and date &gt;= start and date &lt;= end</code></li>
</ol>

<p>but I would recommend to go with 1st one the partitions won't be too big, and not too small.</p>
",['table']
60167612,60183083,2020-02-11 11:15:36,How to select last timestamp by distinct columns?,"<p>Suppose there is table like this:</p>

<pre><code>| user_id | location_id | datetime            | other_field |
| ------- | ----------- | ------------------- | ----------- |
| 12      | 1           | 2020-02-01 10:00:00 | asdqwe      |
| 12      | 1           | 2020-02-01 10:30:00 | asdqwe      |
| 12      | 2           | 2020-02-01 10:40:00 | asdqwe      |
| 12      | 2           | 2020-02-01 10:50:00 | asdqwe      |
| 13      | 1           | 2020-02-01 10:10:00 | asdqwe      |
| 13      | 1           | 2020-02-01 10:20:00 | asdqwe      |
| 14      | 3           | 2020-02-01 09:00:00 | asdqwe      |
</code></pre>

<p>I want to select last <code>datetime</code> of each distinct <code>user_id</code> and <code>location_id</code>. This is what result I am looking for:</p>

<pre><code>| user_id | location_id | datetime            | other_field |
| ------- | ----------- | ------------------- | ----------- |
| 12      | 1           | 2020-02-01 10:30:00 | asdqwe      |
| 12      | 2           | 2020-02-01 10:50:00 | asdqwe      |
| 13      | 1           | 2020-02-01 10:20:00 | asdqwe      |
| 14      | 3           | 2020-02-01 09:00:00 | asdqwe      |
</code></pre>

<p>Here is the table description:</p>

<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE mykeyspace.mytable (
    user_id int,
    location_id int,
    datetime timestamp,
    other_field text,
    PRIMARY KEY ((user_id, location_id, other_field), datetime)
) WITH CLUSTERING ORDER BY (datetime ASC)
    AND read_repair_chance = 0.0
    AND dclocal_read_repair_chance = 0.1
    AND gc_grace_seconds = 864000
    AND bloom_filter_fp_chance = 0.01
    AND caching = { 'keys' : 'ALL', 'rows_per_partition' : 'NONE' }
    AND comment = ''
    AND compaction = { 'class' : 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold' : 32, 'min_threshold' : 4 }
    AND compression = { 'chunk_length_in_kb' : 64, 'class' : 'org.apache.cassandra.io.compress.LZ4Compressor' }
    AND default_time_to_live = 0
    AND speculative_retry = '99PERCENTILE'
    AND min_index_interval = 128
    AND max_index_interval = 2048
    AND crc_check_chance = 1.0
    AND cdc = false;
</code></pre>
",<select><cassandra><cql>,"<p>For such things, CQL has <a href=""https://docs.datastax.com/en/dse/5.1/cql/cql/cql_using/useQueryColumnsSort.html"" rel=""nofollow noreferrer"">""PER PARTITION LIMIT"" clause</a> (available in Cassandra 3.6+ IIRC). But to use on your table, you need to change table definition to <code>CLUSTERING ORDER BY (datetime DESC)</code>, and then you could write:</p>

<pre><code>select * from prospacedb.quarter_utilisation per partition limit 1;
</code></pre>

<p>and get row with latest timestamp for every partition key you have.</p>
",['table']
60237215,60237310,2020-02-15 08:50:45,Modelling Cassandra Data Struncture for both real-time and search from all?,"<p>My project serves both real-time data and past data. It works like feed, so it shows real-time data through socket and past data(if scroll down) through REST api. To get real-time data efficiently, I set date as partition key and time as clustering key. For real-time service, I think this data structure is well modeled. But I also have to get limited number of recent datas(like pagination), which should able to show whole data if requested. To serve data like recent 0~20 / 20~40 / 40~60 through REST api calls, my data-serving server has to remember what it showed before to load next 20 datas continuously, as bookmark. If it was SQL I would use IDs or page&amp;offset things but I cannot do that with Cassandra. So I tried:</p>

<pre><code>SELECT * FROM examples WHERE date&lt;='DATEMARK' AND create_at &lt; 'TIMEMARK' AND entities CONTAINS 'something' limit 20 ALLOW FILTERING;
</code></pre>

<p>But since date is the partition key, I cannot use comparison operation >, &lt;. The past data could be created very far from now.</p>

<p>Can I satisfy my real-time+past requirements with Cassandra? I wonder if I have to make another DB for accessing past data.</p>
",<database><cassandra><nosql><real-time>,"<p>Yes you can, but you must change your mindset and think like NoSQL patterns, in this scenarios you can save your data in duplicate manner and save your data in other table with another partition key and cluster column that satisfies your needs.</p>
",['table']
60258752,60259963,2020-02-17 08:49:26,Multi-DC replication after connection loss,"<p>We have 2 DC, which are connected via an instable VPN, which loses connection once every 3 hours.</p>

<p>All data is written to DC A and replicated to DC B.
After one of this ""connection loss"" events there is data missing (window of the outage affected) on DC B. </p>

<p>My assumption was, that DC B will resume replication after connection has been reestablished and get all the missing data. Is that assumption wrong?</p>

<p><a href=""https://i.stack.imgur.com/IYvph.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IYvph.png"" alt=""enter image description here""></a></p>
",<cassandra>,"<p>Cassandra has following ways for replicating data:</p>

<ul>
<li>hints - when <code>hinted_handoff_enabled</code> is set to true, the mutations are replayed when nodes are back online, but this happens only inside window defined by <code>max_hint_window_in_ms</code> (default is 3 hours), but this may also be affected by per table gc_grace_seconds setting (see <a href=""https://thelastpickle.com/blog/2018/03/21/hinted-handoff-gc-grace-demystified.html"" rel=""nofollow noreferrer"">this blog post for details</a>).  But use of hints could be disabled on the per-DC basis (via <code>hinted_handoff_disabled_datacenters</code>). Also, hints aren't replayed momentarily, as they are throttled by <code>hinted_handoff_throttle_in_kb</code> &amp; <code>max_hints_delivery_threads</code> parameters. </li>
<li>repairs - need to be executed explicitly, but could be more effective than hints, especially because you can run it on specific tables;</li>
<li>read repairs - for multi DC will work only if you're using something like QUORUM for reading the data...</li>
</ul>

<p>If the hints are enabled, and you still miss the data - check that hints are already replayed, or they are still replaying - <a href=""http://cassandra.apache.org/doc/latest/operating/metrics.html#hintedhandoff-metrics"" rel=""nofollow noreferrer"">there are metrics</a> that shows how many hints on disk, etc.</p>

<p>If the problem happens periodically, and you can detect it, then maybe explicit repairs will work faster - but in this case you'll need to disable cross-DC hints, so the nodes won't be receiving the data twice...</p>
",['table']
60299474,60300210,2020-02-19 11:35:37,what does nodetool garbagecollect is actually doing,"<p>I'm trying to free some disk space in C*.<br>
I've deleted many rows which created many tombstones.<br>
I'm running nodetool garbagecollect and was wondering what this tool is doing behind the scens. I've read that it deletes the actual data that the tombstone is shadowing but not the tombstones (which will be cleared after gc_grace_seconds).
Is that accurate? the garbagecollect tool does not have any correlation with the gc_grace_seconds parameter?
How does the garbagecollect actually releases disk space?  </p>

<p>there is not a lot of documentation on how this tool works and what it does.</p>

<p>any help will be much appreciated</p>
",<cassandra><nodetool>,"<p>Deletion of data in Cassandra is always adding more data so you need be careful with that.</p>

<p><code>nodetool garbagecollect</code> performs single-sstable compactions to remove overwritten or logically deleted data.  For each sstable, it will create a new sstable with unneeded data cleaned out. By default, garbagecollect removes rows or partitions that have been deleted or updated with newer data.  It may also remove deleted or updated cell values if the <code>-g CELL</code> option is specified, but this will require more resources (I/O CPU). This command may also remove expired tombstones (older than <code>gc_grace_seconds</code>), but not the fresh ones. Plus there are also other limitations on the removal of tombstones.</p>

<p>If the expired tombstones are still exist, then the only major compaction may help to evict them, for example, by running <code>nodetool compact -s</code> on the individual tables, but you need to make sure that you have enough space - the same size as a table itself.</p>
",['table']
60302558,60338828,2020-02-19 14:26:57,Does Cassandra need Autocompaction for completely immutable data?,"<p>I am trying to optimize performance of a Cassandra Table we have in production that is classic event data with timestamps. Going through the different settings, I've been spending some time looking at compaction strategys and wat compaction in cassandra does. </p>

<p>At first, I thought that TimeWindowCompaction was Ideal for our use case, but then I realized that we never delete or update Data. </p>

<p>Is it possible that it is better to disable compaction completely? How are SSTables formed when there is no compaction strategy at all?</p>
",<optimization><cassandra>,"<p>SSTables are written to disk when in memory storage (memtables) becomes full or is flushed. If you disable compaction on a table you will end up with a lot of very small SSTables. Regardless of whether you are going to update or delete data you need to compact the data as it is written.</p>

<p>Which compaction strategy you use is going to be depend on your access requirements. <a href=""https://docs.datastax.com/en/dse/6.7/dse-dev/datastax_enterprise/config/configChooseCompactStrategy.html"" rel=""nofollow noreferrer"">This</a> is a good basic guide to choosing a compaction strategy and <a href=""http://cassandra.apache.org/doc/latest/operating/compaction.html"" rel=""nofollow noreferrer"">this</a> is a more detailed guide to compaction in cassandra.</p>
",['table']
60495409,60497089,2020-03-02 19:27:28,Cassandra query table without partition key,"<p>I am trying to extract data from a table as part of a migration job.</p>

<p>The schema is as follows:</p>

<pre><code>CREATE TABLE IF NOT EXISTS ${keyspace}.entries (
    username text,

    entry_type int,

    entry_id text,

    PRIMARY KEY ((username, entry_type), entry_id)
);
</code></pre>

<p>In order to query the table we need the partition keys, the first part of the primary key.
Hence, if we know the <code>username</code> and the <code>entry_type</code>, we can query the table.</p>

<p>In this case the <code>username</code> can be whatever, but the <code>entry_type</code> is an integer in the range 0-9.</p>

<p>When doning the extraction we iterate the table 10 times for every username to make sure we try all versions of <code>entry_type</code>.</p>

<p>We can no longer find any entries as we have depleted our list of usernames. But our <code>nodetool tablestats</code> report that there is still data left in the table, gigabytes even. Hence we assume the table is not empty. </p>

<p>But I cannot find a way to inspect the table to figure out what usernames remains in the table. If I could inspect it I could add the usernames left in the table to our extraction job and eventually we could deplete the table. But I cannot simply query the table as such:</p>

<pre><code>SELECT * FROM ${keyspace}.entries LIMIT 1
</code></pre>

<p>as cassandra requires the partition keys to make meaningful queries.</p>

<p>What can I do to figure out what is left in our table?</p>
",<cassandra><cql>,"<p>To answer your first question, I would like to put more light on <strong><em>gc_grace_seconds</em></strong> property.</p>

<p>In Cassandra, data isn’t deleted in the same way it is in RDBMSs. Cassandra is designed for high write throughput, and avoids reads-before-writes. So in Cassandra, a delete is actually an update, and updates are actually inserts. A <strong>“tombstone”</strong> marker is written to indicate that the data is now (logically) deleted  (also known as soft delete). Records marked tombstoned must be removed to claim back the storage space. Which is done by a process called <strong>Compaction</strong>. But remember that tombstones are eligible for physical deletion / garbage collection only after a specific number of seconds known as gc_grace_seconds. This is a very good blog to read more in detail : <a href=""https://thelastpickle.com/blog/2016/07/27/about-deletes-and-tombstones.html"" rel=""nofollow noreferrer"">https://thelastpickle.com/blog/2016/07/27/about-deletes-and-tombstones.html</a></p>

<p>Now possibly you are looking into table size before gc_grace_seconds and data is still there. </p>

<p>Coming to your second issue where you want to fetch some samples from the table without providing partition keys. You can analyze your table content using Spark. The Spark Cassandra Connector allows you to create Java applications that use Spark to analyze database data.  You can follow the articles / documentation to write a quick handy spark application to analyze Cassandra data.</p>

<p><a href=""https://www.instaclustr.com/support/documentation/cassandra-add-ons/apache-spark/using-spark-to-sample-data-from-one-cassandra-cluster-and-write-to-another/"" rel=""nofollow noreferrer"">https://www.instaclustr.com/support/documentation/cassandra-add-ons/apache-spark/using-spark-to-sample-data-from-one-cassandra-cluster-and-write-to-another/</a></p>

<p><a href=""https://docs.datastax.com/en/dse/6.0/dse-dev/datastax_enterprise/spark/sparkJavaApi.html"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/dse/6.0/dse-dev/datastax_enterprise/spark/sparkJavaApi.html</a></p>

<p>I would recommend not to delete records  while you do the migration. Rather first complete the migration and post that do a quick validation / verification to ensure all records are migrated successfully (this use can easily do using Spark buy comparing dataframes from old and new tables). Post successful verification truncate the old table as truncate does not create tombstones and hence more efficient. Note that huge no of tombstone is not good for cluster health. </p>
",['table']
60554422,60637332,2020-03-05 21:46:19,Best Cassandra data model for maintaining bounded lists per user,"<p>I have Kafka streams containing interactions of users with a website, so every event has a timestamp and information about the event. For each user I want to store the last K events in Cassandra (e.g. 100 events).</p>

<p>Our website is constantly experiencing bot / heavy users that is why we want to cap events, just to consider ""normal"" users.</p>

<p>I currently have the current data model in Cassandra:</p>

<pre><code> user_id, event_type, timestamp, event_blob 
</code></pre>

<p>where </p>

<pre><code> &lt;user_id, event_type&gt; = partition key,   timestamp = clustering key
</code></pre>

<p>For now we write a new record in Cassandra as soon as a new event happens and later on we go and clean up ""heavier"" partitions (i.e. count of events > 100).
This doesn't happen in real time and until we don't clean up the heavy partitions we sometimes get bad latencies when reading.</p>

<p>Do you have any suggestions of a better table design for such case? 
Is there a way to tell Cassandra to store only at most K elements for partition and expire the old ones in a FIFO way? Or is there a better table design that I can opt for?</p>
",<cassandra><cql><key-value-store>,"<blockquote>
  <p>Do you have any suggestions of a better table design for such case?</p>
</blockquote>

<p>When data modeling for scenarios like this, I recommend a pattern that makes use of three things:</p>

<ul>
<li>Default TTL set on the table.</li>
<li>Clustering on a time component in descending order.</li>
<li>Adjust query to use a range on the timestamp, never querying data past the TTL.</li>
</ul>

<p><strong>TTL:</strong></p>

<blockquote>
  <p>later on we go and clean up ""heavier"" partitions</p>
</blockquote>

<p>How long (on average) before the cleanup happens?  One thing I would do, is to use a TTL on that table set to somewhere around the maximum amount of time before your team usually has to clean them up.</p>

<p><strong>Clustering Key, Descending Order:</strong></p>

<p>So your PRIMARY KEY definition looks like this:</p>

<pre><code>PRIMARY KEY ((user_id,event_type),timestamp)
</code></pre>

<p>Make sure that you're also clustering in a descending order on timestamp.</p>

<pre><code>WITH CLUSTERING ORDER BY (timestamp DESC)
</code></pre>

<p>This is important to use in conjunction with your TTL.  Here, your tombstones are on the ""bottom"" of the partition (when sorting on <code>timestamp</code> descinding) and the recent data (the data you care about) is at the ""top"" of the partition.</p>

<p><strong>Range Query:</strong></p>

<p>Finally, make sure your query has a range component on the <code>timestamp</code>.  </p>

<p>For example: if today is the 11th, and my TTL is 5 days, I can then query the last 4 days of data without pulling back tombstones:</p>

<pre><code>SELECT * FROM events
WHERE user_id = 11111 AND event_type = 'B'
AND timestamp &gt; '2020-03-07 00:00:00';
</code></pre>
",['table']
60569288,60579372,2020-03-06 17:55:09,saveAll() is too slow. Cassandra Database with Spring boot. why?,"<p>I am trying to insert in batches (Objects are stored in an arraylist and as soon as count is divisible by 10000, I insert all these objects into my table. But it takes more than 4 minutes to do so. Is there any approach which is faster?</p>

<pre><code>arr.add(new Car(name, count, type));
if(count%10000==0){
repository.saveAll(arr);
arr.clear();
}
</code></pre>
",<java><spring-boot><cassandra>,"<p>So here is what is happening.  I am most curious to see the table definition inside Cassandra.  But given your <code>Car</code> constructor,</p>

<pre><code>new Car(name, count, type)
</code></pre>

<p>Given those column names, I'm guessing that <code>name</code> is the partition key.</p>

<p>The reason that is significant, is because the <em>hash</em> of the partition key column is what Cassandra uses to figure out which node (token range) the data should be written to.</p>

<p>When you <code>saveAll</code> on 10000 <code>Cars</code> at once, there is <em>no way</em> you can guarantee that all 10000 of those are going to the same node.  To deal with this, Spring Data Cassandra must be using a <code>BATCH</code> (or something like it) behind the scenes.  If it is a <code>BATCH</code>, that essentially puts one Cassandra node (designated as a ""coordinator"") to route writes to the required nodes.  Due to Cassandra's distributed nature, <strong>that is never going to be fast.</strong></p>

<p>If you really need to store 10000 of them, the best way would be send one write at a time <em>asynchronously</em>.  Of course, you won't want 10000 threads all writing concurrently, so you'll want to throttle-down (limit) the number of active threads in your code.  DataStax's Ryan Svihla has written a couple of articles detailing how to do this.  I recommend this one- <a href=""https://medium.com/@foundev/cassandra-batch-loading-without-the-batch-the-nuanced-edition-dd78d61e9885"" rel=""nofollow noreferrer"">Cassandra: Batch Loading Without the Batch - The Nuanced Edition</a>.</p>

<p><strong>tl;dr;</strong></p>

<p>Spring Data Cassandra's <code>saveAll</code> really shouldn't be used to persist several thousand writes.  If I were using Spring Data Cassandra, I wouldn't even go beyond double-digits with <code>saveAll</code>, TBH.</p>

<p><strong>Edit</strong></p>

<p>Check out this answer for details on how to use Spring Boot/Data with Cassandra asyncrhonously: <a href=""https://stackoverflow.com/questions/52711538/asynccassandraoperations-examples"">AsyncCassandraOperations examples</a></p>
",['table']
60635798,60638898,2020-03-11 12:13:39,Issues with new node bootstrap,"<p>We are using Cassandra 3.11.2 and when trying to bootstrap a new node, the streaming takes a lot of time. The cluster is a three node one, and we are in the process of adding the fourth one. The data available on the other three nodes is close to 190GB, and the instance size is 5 core, 5GB running on spinning drives.</p>

<p><code>nodetool netstats</code> on the new node says streaming files, and of 106 files, 15 received from node A.  But same <code>netstats</code> on node A claims all 106 files have been sent.</p>

<p>Also, we were running into some keep alive related issues and we did increase the same on the new node. This is our second attempt, and in the first attempt, the bootstrap kept failing, and we either resume it or restart the Cassandra on the new node, and the data grew close to 500GB, and then compaction happened and came down to 236GB.</p>

<p>But then bootstrap kept failing. So we discarded it and started fresh again. This time, as suggested in the hardware choices doc, we went with a different physical disk for commit log and data to see if iops was the issue.</p>

<p>And the process never ends. Meaning, it fails in-between with connection reset by peer or IO exception and we have been struggling with this for almost a week now.</p>

<p>How much do you think it ideally takes for bootstrapping a node with data close to 190GB? Any advice/suggestions would be of great help. 
The new node is started with auto_bootstrap flag set to true.</p>
",<cassandra>,"<blockquote>
  <p>How much do you think it ideally takes for bootstrapping a node with data close to 190GB?</p>
</blockquote>

<p>Unfortunately, there's no easy way to answer this.  A lot of factors go into determining how quickly new nodes will bootstrap, essentially being very specific to the underlying infra.</p>

<blockquote>
  <p>We are using Cassandra 3.11.2 </p>
</blockquote>

<p>I recommend upgrading (at least) to 3.11.4.  It's a simple binary upgrade that does not require running a <code>nodetool upgradesstables</code>.  The reason, is that 3.11.4 has a feature which allows failed bootstrapping to resume where it left off.  At least then, you won't have to completely start over each time.</p>

<blockquote>
  <p>data grew close to 500GB, and then compaction happened and came down to 236GB.</p>
</blockquote>

<p>So there are some reasons that this can happen.  Are the rack definitions (cassandra-rackdc.properties) the same or different?  If you're bootstrapping the node as a new logical rack, you might see the one new node being responsible for owning 100% of available token ranges.  Whereas, if you join a new node with the same logical rack as the others, the ownership percentage (and disk footprint) will go down.</p>

<blockquote>
  <p>Any advice/suggestions would be of great help.</p>
</blockquote>

<p>I've encountered issues like this as well, when bootstrapping nodes into new physical data centers. One thing that I've had success with, was setting <code>auto_bootstrap: false</code> and running a <code>nodetool rebuild</code> to stream from the remote DC.  Of course, if you don't have another DC to stream from, that's not going to work.</p>

<p>You could also start the node without bootstrapping enabled, and run a <code>nodetool repair</code> once it comes up.  This has some drawbacks in that the new node will still try to serve client requests, regardless of whether or not it actually has the data.  But it would let you at least get the node joined, and stream the data over on a more gradual basis.</p>

<p>That's why <strong>upgrading to 3.11.4</strong> is probably your best option.  Then you can restart the node when the streams fail, it'll pick up where it left off, and it won't take client requests until data streaming completes.</p>
",['rack']
60752902,60753188,2020-03-19 07:50:59,Joining tables in cassandra and sql,"<p>I have a query as follows : </p>

<pre><code>SELECT distinct(T1.USER_ID)
FROM   T1
LEFT OUTER JOIN T2
ON (T1.USER_ID = T2.USERID)
WHERE T2.USERID IS NULL
AND T1.enrolled_date &lt; some_timestamp;
</code></pre>

<p>I need to understand how is this not the same as :</p>

<pre><code>SELECT distinct(T1.USER_ID)
FROM   T1
WHERE T1.USER_ID IS NULL
AND T1.enrolled_date &lt; some_timestamp;
</code></pre>

<p>since we are doing a left join on T1 with T2 on the condition where both user ids are same and checking if t2.user_id is null, can't we just check if t1.user_id is null or not?</p>

<p>I need to implement this on Cassandra (using DataStax), hence I was wondering if I could avoid the join.</p>
",<sql><database><join><cassandra><cql>,"<p>this query is saying as "" get all of the unique user_id from t1 which do not have a present in t2 (t2.userid is null) that have a value of enrolled_date less than the some_timestamp value</p>

<pre><code>SELECT distinct(T1.USER_ID)
  FROM   T1
LEFT OUTER JOIN T2
    ON (T1.USER_ID = T2.USERID)
 WHERE T2.USERID IS NULL
   AND T1.enrolled_date &lt; some_timestamp;
</code></pre>

<p>The re-written query is not equivalent to the one above.
It says, get all unique user_id (NULL) which is null from the single table t1 whose enrolled_date is less than the some_timestamp </p>

<pre><code>SELECT distinct(T1.USER_ID)
--I guess you missed the FROM T1 portion???
WHERE T1.USER_ID IS NULL
  AND T1.enrolled_date &lt; some_timestamp;
</code></pre>
",['table']
60911776,60912497,2020-03-29 08:30:08,Cassandra hard disk requirement with SizeTieredCompactionStrategy,"<p>I was going through Cassandra's SizeTieredCompactionStrategy and found out that it can sometimes double the size of the dataset's largest table during the compaction process. But I didn't get any information regarding when this can happen? Does anyone know about this?</p>
",<cassandra><data-modeling><cassandra-3.0>,"<p>This requirement arises from the fact that compaction process should have enough space to take all SSTables that should be compacted, read data from them, and write new SSTable to the same disk. In the <strong>worst case</strong>, if you have table consisting of all SSTables that should be compacted, their total size is 50% of available disk space, and no data will be thrown away - in this case, compaction process will write a single SSTable that is equal to size of input data. And if you have input data occupying more than 50% of disk space, compaction won't have enough space for writing a new version.</p>

<p>In real situation, you need to have enough space to compact biggest SSTables in your biggest table performed by N compaction threads at the same time. If you have many tables of similar size, then this restriction is not so strong...</p>
",['table']
60956700,60957356,2020-03-31 17:42:01,Achieve Mysql or Psql relational table (Foreign key constraint) feature in Apache Cassandra,"<p>Please help.
I want to know how to query like Mysql relational tables (which mapped to another table by using foreign key constraint) select query or other queries in Apache Cassandra? 
Is there is any way to achieve a foreign key constraint feature in Apache Cassandra?</p>
",<mysql><cassandra><nosql><foreign-keys><datastax>,"<p>No. There is no such thing in Cassandra. All joins &amp; checks should be done on the application side. Usually, when people need information from multiple tables, then they are building an aggregating table so it could be queried as one object.  And all data modeling in Cassandra is going from queries, not from the logical database schema.</p>

<p>I really recommend to take <a href=""https://academy.datastax.com/resources/ds220"" rel=""nofollow noreferrer"">DS220: Data modeling in Cassandra</a> course on the DataStax Academy (please take DS201 prior to it, to understand why Cassandra works this way).</p>
",['table']
60968338,60970294,2020-04-01 10:03:38,Cassandra data modeling when columns are dynamic,"<p>I am struggling with data modeling in cassandra where i have different attributes for different organizations. As there would be any number of attributes i am unable to model a dynamic number of columns in schema. Secondly, when i use map for this, i am unable to query against those attributes or index them etc. Am i missing something or this is a limitation in cassandra?</p>
<hr />
<h1>Scenario</h1>
<p>one organization selects specific attributes to collect data for and they can change those attributes anytime. When they change, number of attributes and name of attributes changes. If previously we were collecting data for <em><strong><code>att1,attr2,attr3</code></strong></em>, now we are collecting <em><strong><code>attr4,attr5,attr6,attr7,attr8,attr9</code></strong></em>. And this can be changed at anytime for any organization. Furthermore, organization will be searching massively on those attributes.</p>
<ol>
<li>How can we model such scenario in cassandra.</li>
<li>if it's a limitation, what could be the alternatives of cassandra where we have read/write
(mostly <strong>write</strong> and often <strong>read</strong>. Not <strong>update/delete</strong>)
proficiency.</li>
<li>Do we have to combine any other framework with cassandra? like lucene etc</li>
</ol>
<p>Thanks in advance.</p>
",<cassandra><nosql><cql><spring-data-cassandra>,"<p>This case really requires more information about queries that are executed, etc.</p>

<p>in simplest case, just put the attribute name as a clustering column in addition to existing, like this:</p>

<pre><code>create table tbl (
  id int,
  collected timestamp,
  attr_name text,
  attr_value int,
  primary key(id, collected, attr_name);
</code></pre>

<p>in this case you can select either individual attribute if you do</p>

<pre><code>select * from tbl where id = ... and collected = ... and attr_name = 'attrX';
</code></pre>

<p>or you can select all attributes by just omitting the <code>attr_name</code>:</p>

<pre><code>select * from tbl where id = ... and collected = ...;
</code></pre>

<p>but it will work only when all attribute values have the same data type. If they could be different, then you may need to add more fields for every data type.</p>
",['table']
61005902,61006876,2020-04-03 06:00:19,Cassandra query with consistency failure,"<p>I have this following table:</p>

<pre><code>CREATE TABLE mydb.customer_data (
    field1 int,
    field2 int,
    field3 int,
    field4 text,
    field5 text,
    data_time timestamp,
    PRIMARY KEY ((field1, field2, field3, field4, field5), data_time)
) WITH CLUSTERING ORDER BY (utilisation_time ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';

</code></pre>

<p>I have script to collect the daily data as follows:</p>

<pre><code>SELECT * FROM mydb.customer_data WHERE field1 = 21 AND 
  data_time &gt;= '2020-03-26 16:00:00' AND 
  data_time &lt;= '2020-03-27 15:59:00' ALLOW FILTERING
</code></pre>

<p>Almost all days can queried without problems, except one specific day, which I believe has very large data than the other.</p>

<p>the client side (in java) get this error message:</p>

<pre><code>Cassandra failure during read query at consistency LOCAL_ONE (1 responses were required but only 0 replica responded, 1 failed)
</code></pre>

<p>In the system.log, i capture this log when the client app hit the query:</p>

<pre><code>INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,029 MessagingService.java:1236 - READ messages were dropped in last 5000 ms: 1 internal and 0 cross node. Mean internal dropped latency: 5960 ms and Mean cross-node dropped latency: 0 ms
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,029 StatusLogger.java:47 - Pool Name                    Active   Pending      Completed   Blocked  All Time Blocked
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,033 StatusLogger.java:51 - MutationStage                     0         0     2273404011         0                 0

INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,033 StatusLogger.java:51 - ViewMutationStage                 0         0              0         0                 0

INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,033 StatusLogger.java:51 - ReadStage                         0         0       55205177         0                 0

INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,033 StatusLogger.java:51 - RequestResponseStage              0         0     3551921449         0                 0

INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,033 StatusLogger.java:51 - ReadRepairStage                   0         0        1016339         0                 0

INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,034 StatusLogger.java:51 - CounterMutationStage              0         0              0         0                 0

INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,034 StatusLogger.java:51 - MiscStage                         0         0              0         0                 0

INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,034 StatusLogger.java:51 - CompactionExecutor                0         0       19952206         0                 0

INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,034 StatusLogger.java:51 - MemtableReclaimMemory             0         0          22014         0                 0

INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,034 StatusLogger.java:51 - PendingRangeCalculator            0         0              8         0                 0

INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,035 StatusLogger.java:51 - GossipStage                       0         0       75343472         0                 0

INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,035 StatusLogger.java:51 - SecondaryIndexManagement          0         0              0         0                 0

INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,035 StatusLogger.java:51 - HintsDispatcher                   0         0            681         0                 0

INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,035 StatusLogger.java:51 - MigrationStage                    0         0            108         0                 0

INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,035 StatusLogger.java:51 - MemtablePostFlush                 0         0          22759         0                 0

INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,035 StatusLogger.java:51 - PerDiskMemtableFlushWriter_0         0         0          22006         0                 0

INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,036 StatusLogger.java:51 - ValidationExecutor                0         0            392         0                 0

INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,036 StatusLogger.java:51 - Sampler                           0         0              0         0                 0

INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,036 StatusLogger.java:51 - MemtableFlushWriter               0         0          22014         0                 0

INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,036 StatusLogger.java:51 - InternalResponseStage             0         0         421680         0                 0

INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,036 StatusLogger.java:51 - AntiEntropyStage                  0         0           1104         0                 0

INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,036 StatusLogger.java:51 - CacheCleanupExecutor              0         0              0         0                 0

INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,037 StatusLogger.java:51 - Native-Transport-Requests         0         0     1619401305         0             19514

INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,037 StatusLogger.java:61 - CompactionManager                 0         0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,037 StatusLogger.java:73 - MessagingService                n/a       0/0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,037 StatusLogger.java:83 - Cache Type                     Size                 Capacity               KeysToSave
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,037 StatusLogger.java:85 - KeyCache                   99614696                 99614720                      all
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,037 StatusLogger.java:91 - RowCache                          0                        0                      all
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,037 StatusLogger.java:98 - Table                                     Memtable ops,data
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,037 StatusLogger.java:101 - system_distributed.parent_repair_history  0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,037 StatusLogger.java:101 - system_distributed.repair_history         0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,037 StatusLogger.java:101 - system_distributed.view_build_status      0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,037 StatusLogger.java:101 - system.compaction_history                 0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,037 StatusLogger.java:101 - system.schema_aggregates                  0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,038 StatusLogger.java:101 - system.schema_triggers                    0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,038 StatusLogger.java:101 - system.size_estimates                     0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,038 StatusLogger.java:101 - system.paxos                              0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,038 StatusLogger.java:101 - system.views_builds_in_progress           0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,038 StatusLogger.java:101 - system.batches                            0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,038 StatusLogger.java:101 - system.schema_keyspaces                   0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,038 StatusLogger.java:101 - system.sstable_activity                   39,2699
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,038 StatusLogger.java:101 - system.batchlog                           0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,038 StatusLogger.java:101 - system.schema_columns                     0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,038 StatusLogger.java:101 - system.hints                              0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,038 StatusLogger.java:101 - system.IndexInfo                          0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,038 StatusLogger.java:101 - system.schema_columnfamilies              0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,038 StatusLogger.java:101 - system.schema_functions                   0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,038 StatusLogger.java:101 - system.built_views                        0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,038 StatusLogger.java:101 - system.peer_events                        0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,039 StatusLogger.java:101 - system.range_xfers                        0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,039 StatusLogger.java:101 - system.peers                              0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,039 StatusLogger.java:101 - system.transferred_ranges                 0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,039 StatusLogger.java:101 - system.schema_usertypes                   0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,039 StatusLogger.java:101 - system.local                              0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,039 StatusLogger.java:101 - system.available_ranges                   0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,039 StatusLogger.java:101 - system.prepared_statements                0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,040 StatusLogger.java:101 - mydb.customer_data                        30776,2714968
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,039 StatusLogger.java:101 - mydb.table1                               296603,12546709
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,039 StatusLogger.java:101 - mydb.table2                               26825,4419610
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,039 StatusLogger.java:101 - mydb.table3                               0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,039 StatusLogger.java:101 - mydb.table4                               3,236
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,039 StatusLogger.java:101 - mydb.table5                               63,4990
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,039 StatusLogger.java:101 - mydb.table6                               0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,039 StatusLogger.java:101 - mydb.table7                               3,363
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,039 StatusLogger.java:101 - mydb.table8                               0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,040 StatusLogger.java:101 - mydb.table9                               4,108
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,040 StatusLogger.java:101 - mydb.table10                              2,70
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,040 StatusLogger.java:101 - mydb.table11                              0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,040 StatusLogger.java:101 - mydb.table12                              35,1583
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,040 StatusLogger.java:101 - mydb.table13                              7920,790982
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,040 StatusLogger.java:101 - mydb.table14                              123,7473
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,040 StatusLogger.java:101 - mydb.table15                              0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,040 StatusLogger.java:101 - mydb.table16                              0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,040 StatusLogger.java:101 - mydb.table17                              18638,3336186
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,040 StatusLogger.java:101 - mydb.table18                              9254,1443624
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,040 StatusLogger.java:101 - mydb.table19                              3,69
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,040 StatusLogger.java:101 - system_schema.columns                     0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,040 StatusLogger.java:101 - system_schema.types                       0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,040 StatusLogger.java:101 - system_schema.indexes                     0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,041 StatusLogger.java:101 - system_schema.keyspaces                   0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,041 StatusLogger.java:101 - system_schema.dropped_columns                 0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,041 StatusLogger.java:101 - system_schema.aggregates                  0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,041 StatusLogger.java:101 - system_schema.triggers                    0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,041 StatusLogger.java:101 - system_schema.tables                      0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,041 StatusLogger.java:101 - system_schema.views                       0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,041 StatusLogger.java:101 - system_schema.functions                   0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,041 StatusLogger.java:101 - system_auth.roles                         0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,041 StatusLogger.java:101 - system_auth.role_members                  0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,041 StatusLogger.java:101 - system_auth.resource_role_permissons_index                 0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,041 StatusLogger.java:101 - system_auth.role_permissions                 0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,041 StatusLogger.java:101 - system_traces.sessions                    0,0
INFO  [ScheduledTasks:1] 2020-04-03 05:07:59,041 StatusLogger.java:101 - system_traces.events                      0,0
DEBUG [ScheduledTasks:1] 2020-04-03 05:07:59,041 MonitoringTask.java:152 - 1 operations timed out in the last 5017 msecs:
&lt;SELECT * FROM mydb.customer_data WHERE field1 = 21 AND data_time &gt;= 2020-03-26 16:00Z AND data_time &lt;= 2020-03-27 15:59Z LIMIT 5000&gt;, total time 5004 msec, timeout 5000 msec
</code></pre>

<p>This is <code>nodetool tablestats</code> of table <code>mydb.customer_data</code> in one of the node:</p>

<pre><code>ubuntu@ip-172-31-44-227:/usr/local/cassandra/conf$ /usr/local/cassandra/bin/nodetool tablestats prospacedb.quarter_utilisation
Total number of tables: 56
----------------
Keyspace : mydb
    Read Count: 5282126
    Read Latency: 2.48710102428454 ms
    Write Count: 2164892217
    Write Latency: 0.016614636464832373 ms
    Pending Flushes: 0
        Table: customer_data
        SSTable count: 15
        Space used (live): 2455268488
        Space used (total): 2455268488
        Space used by snapshots (total): 3241990632
        Off heap memory used (total): 1019317
        SSTable Compression Ratio: 0.3615507660944151
        Number of partitions (estimate): 14732
        Memtable cell count: 86273
        Memtable data size: 7612904
        Memtable off heap memory used: 0
        Memtable switch count: 1664
        Local read count: 592956
        Local read latency: NaN ms
        Local write count: 278088230
        Local write latency: NaN ms
        Pending flushes: 0
        Percent repaired: 76.49
        Bloom filter false positives: 0
        Bloom filter false ratio: 0.00000
        Bloom filter space used: 145840
        Bloom filter off heap memory used: 145720
        Index summary off heap memory used: 47685
        Compression metadata off heap memory used: 825912
        Compacted partition minimum bytes: 87
        Compacted partition maximum bytes: 1131752
        Compacted partition mean bytes: 63353
        Average live cells per slice (last five minutes): NaN
        Maximum live cells per slice (last five minutes): 0
        Average tombstones per slice (last five minutes): NaN
        Maximum tombstones per slice (last five minutes): 0
        Dropped Mutations: 0
</code></pre>

<p>What is the potential root cause that makes this problem? Is there any quick solution to make the query running?</p>
",<cassandra>,"<p>The major reason why this query fails is that it's incorrect - Cassandra works fast only when you have full partition key, and then you can do the range query inside that partition.  In your case, you have partition key consisting of 5 columns, but you're providing only one in the query, and Cassandra needs to perform scanning of the whole data to find where the corresponding rows are located.  I really wondering that it worked before...</p>

<p>To solve your problem you need to change the table structure to have partition/primary key matching your queries - all data modeling for Cassandra starts with queries that should be executed. I recommend to take <a href=""https://academy.datastax.com/resources/ds220"" rel=""nofollow noreferrer"">DS220 course at DataStax Academy on data modelling</a>.</p>
",['table']
61017684,61025915,2020-04-03 17:51:53,How to decide for clustering columns in Cassandra primary key?,"<p>I am following book 'Definitive Cassandra'. A hotel application is used as an example in it. THere is a table available_rooms_by_hotel_date. It is to support use case when user wants to know about room availability in a given hotel from a given date. Data model is defined as:</p>

<pre><code>hotel_id
date
room_number
is_available
</code></pre>

<p>hotel_id is partition key, while date and room_number are clustering columns.</p>

<p>Looking at the table, one can say that it supports use case when user wants to know availability of room from a given date for a given hotel.</p>

<p>I understand that order of clustering columns is also critical in Cassandra. So if I change order for date and room_number, how would it impact? Functionality wise I think use case is still supported. But does it impact performance or any other aspects like storage, node allocation etc?</p>
",<cassandra>,"<p>In Cassandra you can query a table by:</p>
<ol>
<li>the full primary key (<code>hotel_id</code>, <code>date</code>, <code>room_number</code>) - in this case you fetch only one row</li>
<li>the partition key (<code>hotel_id</code>) - in that case you get all rows inside given partition - it's a minimal requirement for <code>SELECT</code> query;</li>
<li>a partial primary key - partition key + some of the clustering columns, from left to right (<strong>any preceding clustering columns defined in the primary key should be specified</strong>). In the given example, you can specify only <code>hotel_id</code> and <code>date</code>, and it will return all rows for given date (or dates, if you do <code>date IN (...)</code>.  Another useful feature of the clustering column is that you can do a range query on it (<em>but only on the last specified clustering column</em>!).  For example, if I want to find all the rooms in a given date range, I can do <code>... WHERE hotel_id = ... AND date &gt;= '2020-04-05' AND date &lt;= '2020-04-10'</code>.</li>
</ol>
<p>If you change the order of the <code>room_number</code> and <code>date</code>, then you could only ask for availability of the specific room(s) on date (or overall), not all rooms on specific dates - because you need to specify all preceding clustering columns, but you have only <code>hotel_id</code> and <code>date</code>, but not <code>room_number</code>...</p>
",['table']
61104590,61105919,2020-04-08 15:40:55,How to load csv couple of lines per couple of lines,"<p>I'm connecting Spark to Cassandra and I was able to print the lines of my CSV using the conventional COPY method. However, if the CSV was very large as it usually happens in Big Data, how could one load the CSV file couple of lines per couple of lines in order to avoid freezing related issues etc. ?</p>

<pre><code>import org.apache.spark.SparkContext
import org.apache.spark.SparkConf
import com.datastax.spark.connector._

object SparkCassandra {

  def main(args: Array[String]): Unit = {

      val conf = new SparkConf().setAppName(""SparkCassandra"").setMaster(""local"").set(""spark.cassandra.connection.host"", ""localhost"")
      val sc = new SparkContext(conf)
      val my_rdd = sc.cassandraTable(""my_keyspace"", ""my_csv"")
      my_rdd.take(20).foreach(println)
      sc.stop()
  }
}
</code></pre>

<p>Should one use a time variable or something of that nature?</p>
",<scala><apache-spark><cassandra><bigdata><spark-cassandra-connector>,"<p>If you want just to load data into Cassandra, or unload data from Cassandra using the command-line, I would recommend to look to the <a href=""https://docs.datastax.com/en/dsbulk/doc/index.html"" rel=""nofollow noreferrer"">DataStax Bulk Loader (DSBulk)</a> - it's heavily optimized for loading data to/from Cassandra/DSE. It works with both open source Cassandra and DSE.  </p>

<p>In simplest case loading into &amp; unloading from table will look as (default format is CSV):</p>

<pre class=""lang-sh prettyprint-override""><code>dsbulk load -k keyspace -t table -url my_file.csv
dsbulk unload -k keyspace -t table -url my_file.csv
</code></pre>

<p>For more complex cases you may need to provide more options. You can find more information in <a href=""https://www.datastax.com/blog/2019/03/datastax-bulk-loader-introduction-and-loading"" rel=""nofollow noreferrer"">following series of the blog posts</a>.</p>

<p>If you want to do this with Spark, then I recommend to use Dataframe API instead of RDDs.  In this case you'll just use standard <code>read</code> &amp; <code>write</code> functions.</p>

<p>to export data from Cassandra to CSV:</p>

<pre class=""lang-scala prettyprint-override""><code>import org.apache.spark.sql.cassandra._
val data = spark.read.cassandraFormat(""tbl"", ""ks"").load()
data.write.format(""csv"").save(""my_file.csv"")
</code></pre>

<p>or read from CSV and store in Cassandra:</p>

<pre class=""lang-scala prettyprint-override""><code>import org.apache.spark.sql.cassandra._
import org.apache.spark.sql.SaveMode
val data = spark.read.format(""csv"").save(""my_file.csv"")
data.cassandraFormat(""tbl"", ""ks"").mode(SaveMode.Append).save()
</code></pre>
",['table']
61107289,61107747,2020-04-08 18:13:35,Static column in Cassandra,"<p>Can someone explain in simple terms what is the static column in Cassandra, and its use?
I came across this link <a href=""https://www.datastax.com/blog/2014/02/new-cql-features-coming"" rel=""nofollow noreferrer"">Static column</a>, but wasn't able to understand it much.</p>
",<cassandra>,"<p>Static column is a way to associate data with the whole partition, so it will be shared between all rows inside that partition.  There are legitimate cases, when all rows need to have the same data, and when data is updated, we won't need to update every row.</p>

<p>One example that comes in mind is e-commerce. For example, you're selling something, and you're selling in different countries with different currency &amp; different prices. But some things are common between them, like, description, sizes, etc.  In this case we can model it as following:</p>

<pre><code>create table articles (
  sku text,
  description text static,
  country text,
  currency text,
  price decimal,
  primary key (sku, country)
);
</code></pre>

<p>in this case, when you do <code>select * from articles where sku = ... and country = ...</code> then you get description anyway, and you can update description only with <code>update articles set description = '...' where sku = ...</code>, and next select will pull updated description.</p>

<p>Also, static columns may exist in partition without any rows.  One of the use cases that I've seen is collection of the aggregated information, where detailed data were stored as individual rows with some TTL, and there was a job that aggregated data into static column, so when rows are expired, then this partition still stays only with aggregated data. </p>
",['table']
61135105,61136781,2020-04-10 06:29:27,Cassandra light weight transaction confusion,"<p>I am a bit confused here in terms of terminology for light weight transactions. I am not sure why in most of the cassandra literature it says that it works only for a single partition.</p>

<p>Like when I use IF NOT EXISTS, IF EXISTS, it should apply to the whole primary key not just partition key as it says in this post as well <a href=""https://stackoverflow.com/questions/35428976/how-the-lwt-light-weight-transaction-is-working-when-we-use-if-not-exist"">How the LWT- Light Weight Transaction is working when we use IF NOT EXIST?</a></p>

<p>However, in the book Cassandra, the Definition Guide, I see this example </p>

<pre><code>INSERT INTO reservation.reservations_by_confirmation 
(confirm_number,
hotel_id, start_date, end_date, room_number, guest_id) VALUES (
'RS2G0Z', 'NY456', '2020-06-08', '2020-06-10', 111, 1b4d86f4-ccff- 
4256-a63d-45c905df2677) IF NOT EXISTS;
</code></pre>

<p>This command checks to see if there is a record with the partition key, which for this table consists of the confirm_number. So let’s find out what happens when you execute this command a second time:</p>

<pre><code>INSERT INTO reservation.reservations_by_confirmation 
(confirm_number,
hotel_id, start_date, end_date, room_number, guest_id) VALUES (
'RS2G0Z', 'NY456', '2020-06-08', '2020-06-10', 111, 1b4d86f4-ccff- 
4256-a63d-45c905df2677) IF NOT EXISTS;
</code></pre>

<p>In this case, the transaction fails, because there is already a reservation with the number “RS2G0Z,” and cqlsh helpfully echoes back a row containing a failure indication and the values you tried to enter.</p>

<p><a href=""https://i.stack.imgur.com/RypiR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RypiR.png"" alt=""enter image description here""></a></p>

<p>Now my question is if I run, another query </p>

<pre><code>INSERT INTO reservation.reservations_by_confirmation 
(confirm_number,
hotel_id, start_date, end_date, room_number, guest_id) VALUES (
'RS2G0Z', 'NY466', '2020-06-08', '2020-06-10', 111, 1b4d86f4-ccff- 
4256-a63d-45c905df2677) IF NOT EXISTS;
</code></pre>

<p>which is a different primary key but with the same partition key, it should succeed</p>

<p>So when the book says </p>

<pre><code>This command checks to see if there is a record with the partition key
</code></pre>

<p>Isn't this a wrong statement? Please let me know if I am misinterpreting something</p>
",<cassandra>,"<p>It's a error in the book, although diagram shows complex primary key, the table <code>reservation.reservations_by_confirmation</code> has very simple primary key - <code>confirm_number</code>, so in this case queries work as described in the text, and it doesn't allow to insert duplicate primary key.</p>

<p>When you see mentioning of the partition key in context of the LWT, this usually means that coordination happens between nodes that have replica of given partition...</p>
",['table']
61161378,61166526,2020-04-11 17:59:54,Create Columnfamily problem with Cassandra from CLI,"<p>I'm a beginner in Cassandra and noSQL data base. For my exam i have to correct some instruction that the professor gave me. I have corrected some of these, but now i'm not able to fix this:</p>

<pre><code>cqlsh&gt; create COLUMNFAMILY post
   ... with comparator = UTF8Type
   ... and read_repair_chance = 0.1
   ... and keys_cached = 100
   ... and gc_grace = 0
   ... and min_compaction_threshold = 5
   ... and max_compaction_threshold = 31
</code></pre>

<p>I receive this error from CLI:</p>

<pre><code>SyntaxException: line 2:0 mismatched input 'with' expecting '(' (create COLUMNFAMILY post[with] comparator...)
</code></pre>

<p>I would be really grateful if somebody can help me.</p>

<p>I hope it is understandable because my English isn't very good</p>
",<database><cassandra><nosql><cql><cqlsh>,"<p>Your columnfamily definition, or table definition (which is the more recent term for Cassandra tables) is missing the actual columns and a primary key, and it seems you have mixed your column definition (comparator = UTF8Type) with your table options, such as read_repair_chance and gc_grace_period.</p>

<p>All Cassandra tables must have at least one primary key column.</p>

<p>To create a minimal table <code>post</code> in the keyspace <code>ks</code> with default table options, you can do this:</p>

<pre><code>CREATE TABLE ks.post (comparator text PRIMARY KEY);
</code></pre>

<p>To inspect the table definition and the full syntax for setting the table options:</p>

<pre><code>describe table ks.post;

CREATE TABLE ks.post (
    comparator text PRIMARY KEY
) WITH bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND speculative_retry = '99PERCENTILE';
</code></pre>

<p>Note: if you are using default table options, you do not have to specify them in your table creation command.</p>

<p>For documentation about table creation, check here for example:
<a href=""https://docs.datastax.com/en/dse/5.1/cql/cql/cql_reference/cql_commands/cqlCreateTable.html"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/dse/5.1/cql/cql/cql_reference/cql_commands/cqlCreateTable.html</a></p>

<p>or here:
<a href=""http://cassandra.apache.org/doc/latest/cql/ddl.html#create-table"" rel=""nofollow noreferrer"">http://cassandra.apache.org/doc/latest/cql/ddl.html#create-table</a></p>
",['table']
61178735,61182424,2020-04-12 21:57:20,What is the difference between Consistent Hashing and Partitioner in Cassandra,"<p>I'm new to Cassandra, I got confused between <code>consistent hashing</code> and <code>partitioner</code>.
Are they both same ?</p>
<p>Please find the definitions from Datastax documentation:</p>
<blockquote>
<p>A partitioner determines how data is distributed across the nodes in the cluster (including replicas). Basically, a partitioner is a function for deriving a token representing a row from its partition key, typically by hashing. Each row of data is then distributed across the cluster by the value of the token.</p>
<p>Consistent hashing allows distribution of data across a cluster to minimize reorganization when nodes are added or removed. Consistent hashing partitions data based on the partition key. (For an explanation of partition keys and primary keys, see the Data modeling example in CQL for Cassandra 2.2 and later.)</p>
</blockquote>
",<cassandra>,"<p>With consistent hashing, the buckets are arranged in a ring with a predefined range; the exact range depends on the partitioner being used. Keys are then hashed to produce a value that lies somewhere along the ring.</p>

<p>I think you have already got the definition that is correct but for other ways you can understand the things from below.
<a href=""https://dzone.com/articles/introduction-apache-cassandras"" rel=""nofollow noreferrer"">https://dzone.com/articles/introduction-apache-cassandras</a>. There is good explanation about both. </p>
",['partitioner']
61250478,61250880,2020-04-16 12:40:40,"Cassandra, Delete if a set contains value","<p>I'm a beginner in Cassandra and I have a table like this:</p>

<pre><code>CREATE TABLE Books(
Title text PRIMARY KEY,
Authors set&lt;text&gt;,
Family set &lt;text&gt;,
Publisher text,
Price decimal
);
</code></pre>

<p>(the other options are missing because it's only an example)</p>

<p>now I would like to execute this query: </p>

<p><code>DELETE Price FROM Books WHERE Authors CONTAINS 'J.K. Rowling' IF EXISTS;</code></p>

<p>But it doesn't work. I searched on Google but found nothing.</p>

<p>Hope somebody can help me and sorry if my english is not very good.</p>
",<cassandra><cql><cassandra-3.0><cqlsh><cql3>,"<blockquote>
  <p>but it doesn't work.</p>
</blockquote>

<p>That doesn't really give us enough information to help you.  Usually, you'll want to provide an error message.  I built your table locally, inserted data, and tried your approach.  This is the error that I see:</p>

<pre><code>InvalidRequest: Error from server: code=2200 [Invalid query]
   message=""Some partition key parts are missing: title""
</code></pre>

<p><code>DELETE</code> requires that the appropriate PRIMARY KEY components be specified in the <code>WHERE</code> clause.  In your case, <code>Authors</code> is <em>not</em> part of the PRIMARY KEY definition.  Given the error message returned (and the table definition) specifying <code>title</code> is the only way to delete rows from this table.</p>

<pre><code>aploetz@cqlsh:stackoverflow&gt; DELETE FROM Books 
    WHERE title = 'Harry Potter and the Chamber of Secrets'
    IF EXISTS;

 [applied]
-----------
      True
</code></pre>

<blockquote>
  <p>Can I do a query like this? <code>UPDATE Books SET Family = Family + {'Fantasy'} WHERE Authors CONTAINS 'J.K. Rowling';</code></p>
</blockquote>

<p>No.  This fails for the same reason.  Writes in Cassandra (INSERTs, UPDATEs, DELETEs are all writes) require the primary key (specifically, the partition key) in the <code>WHERE</code> clause.  Without that, Cassandra can't figure out which node holds the data, and it needs that to perform the write.</p>
",['table']
61343332,61344679,2020-04-21 12:14:43,Insert data from pyspark dataframe to another cassandra table using pyspark,"<p>I have a <strong>cassandra</strong> table - <strong>test</strong>:</p>

<pre><code>+----+---------+---------+
| id | country | counter |
+====+=========+=========+
|  A |      RU |       1 |
+----+---------+---------+
|  B |      EN |       2 |
+----+---------+---------+
|  C |      IQ |       1 |
+----+---------+---------+
|  D |      RU |       3 |
+----+---------+---------+
</code></pre>

<p>Also I have a table <strong>main</strong> in the same space with column ""country_main"" and ""main_id"".
In column main_id I have same ids as in test table, and also I have some unique ids. country_main has empty values and the same as in test. For ex:</p>

<pre><code>+---------+--------------+---------+
| main_id | country_main |      ...|
+=========+==============+=========+
|  A      |              |      ...|
+---------+--------------+---------+
|  B      |      EN      |      ...|
+---------+--------------+---------+
|  Y      |      IQ      |      ...|
+---------+--------------+---------+
|  Z      |      RU      |      ...|
+---------+--------------+---------+
</code></pre>

<p>How to insert data from test table to main using pyspark to fill empty values in country_main according to ids?</p>
",<apache-spark><pyspark><cassandra><spark-cassandra-connector>,"<p>Having following schema &amp; data:</p>

<pre><code>create table test.ct1 (
  id text primary key,
  country text,
  cnt int);

insert into test.ct1(id, country, cnt) values('A', 'RU', 1);
insert into test.ct1(id, country, cnt) values('B', 'EN', 2);
insert into test.ct1(id, country, cnt) values('C', 'IQ', 1);
insert into test.ct1(id, country, cnt) values('D', 'RU', 3);


create table test.ct2 (
  main_id text primary key,
  country_main text,
  cnt int);

insert into test.ct2(main_id, cnt) values('A', 1);
insert into test.ct2(main_id, country_main, cnt) values('B', 'EN', 2);
insert into test.ct2(main_id, country_main, cnt) values('C', 'IQ', 1);
insert into test.ct2(main_id, country_main, cnt) values('D', 'RU', 3);
</code></pre>

<p>It should be something like this:</p>

<pre class=""lang-py prettyprint-override""><code>from pyspark.sql.functions import *

ct1 = spark.read.format(""org.apache.spark.sql.cassandra"")\
   .option(""table"", ""ct1"").option(""keyspace"", ""test"").load()

ct2 = spark.read.format(""org.apache.spark.sql.cassandra"")\
  .option(""table"", ""ct2"").option(""keyspace"", ""test"").load()\
  .where(col(""country_main"").isNull())

res = ct1.join(ct2, ct1.id == ct2.main_id).select(col(""main_id""), 
  col(""country"").alias(""country_main""))
res.write.format(""org.apache.spark.sql.cassandra"")\
   .option(""table"", ""ct2"").option(""keyspace"", ""test"")\
   .mode(""append"").save()
</code></pre>

<p>What code does:</p>

<ol>
<li>selects all rows from <code>ct2</code> (corresponds to your <code>main</code> table) where <code>country_main</code> is <code>null</code>;</li>
<li>performs join with <code>ct1</code> (corresponds to your <code>test</code> table) to get value of country from it (optimization could be to select only necessary columns from both tables). Also, please note that join is done by Spark, not on Cassandra level - Cassandra-level joins will be supported only in upcoming version of Spark Cassandra Connector (3.0, but alpha versions already published);</li>
<li>renames columns to match structure of <code>ct2</code> table;</li>
<li>write data back.</li>
</ol>

<p>Result:</p>

<pre><code>cqlsh&gt; select * from test.ct2;

 main_id | cnt | country_main
---------+-----+--------------
       C |   1 |           IQ
       B |   2 |           EN
       A |   1 |           RU
       D |   3 |           RU
</code></pre>

<p>for source data:</p>

<pre><code>cqlsh&gt; select * from test.ct2;
main_id | cnt | country_main
---------+-----+--------------                                       
       C |   1 |           IQ                                  
       B |   2 |           EN                                                                                         
       A |   1 |         null                                      
       D |   3 |           RU
</code></pre>
",['table']
61361140,61395343,2020-04-22 08:59:40,scylla sstableloader giving error on copying sstables from cassandra to scylla?,"<p>I am using Cassandra 3.0.15v and want to move my data to scylla db 3.3.0v using the sstableloader utility provided by scylla and i have tried different approaches but i am not able to do it.</p>

<p>Table schema is,</p>

<pre><code>CREATE TABLE events.test (
    ""Id1"" text,
    ""Id2"" text,
    ""event"" set&lt;text&gt;,
    PRIMARY KEY (""Id1"", ""Id2"")
) 
WITH CLUSTERING ORDER BY (""Id2"" ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'ALL'}
    AND comment = ''
    AND compaction = {'class': 'SizeTieredCompactionStrategy'}
    AND compression = {'sstable_compression': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99.0PERCENTILE';
</code></pre>

<p>The steps that i followed were,</p>

<ol>
<li>took the snapshot of the table.</li>
<li>copied it on the scylla server.</li>
<li>Run command: sstableloader --nodes serverIP  keySpace/tableName </li>
</ol>

<p>On running above command the response it get is: </p>

<pre><code>com.datastax.driver.core.exceptions.SyntaxError: line 1:75  : missing elements...

</code></pre>

<p>I am not able to understand what am i doing wrong here. So any help would be highly appreciated.</p>

<p>Just found these logs using journalctl _COMM=scylla</p>

<pre><code>cql_server - exception while processing connection: std::system_error (error system:32, sendmsg: Broken pipe)
Apr 22 14:54:02 ip-1-0-4-100 scylla[1371]:  [shard 0] cql_server - exception while processing connection: std::system_error (error system:32, sendmsg: Broken pipe)
Apr 22 14:54:02 ip-1-0-4-100 scylla[1371]:  [shard 2] cql_server - exception while processing connection: std::system_error (error system:32, sendmsg: Broken pipe)
Apr 22 15:35:33 ip-1-0-4-100 scylla[1371]:  [shard 3] cql_server - exception while processing connection: std::system_error (error system:32, sendmsg: Broken pipe)
lines 3484-3523/3523 (END)
</code></pre>
",<cassandra><scylla>,"<p>There are some minor differences in the table attributes between Cassandra and Scylla. That could be the cause for the error. It requires Minor changes in your Schema</p>

<p>Have you tried following the steps from this Cassandra to Scylla Migration doc?
<a href=""https://docs.scylladb.com/operating-scylla/procedures/cassandra_to_scylla_migration_process/"" rel=""noreferrer"">https://docs.scylladb.com/operating-scylla/procedures/cassandra_to_scylla_migration_process/</a></p>

<p>Specifically note the schema diff at the bottom of the doc. And also the limitations and known issues.</p>

<p>Another good source is this blog about migration strategies (if you are willing to consider other methods than <code>sstableloader</code>), there's also a link there to a webinar on the topic.
<a href=""https://www.scylladb.com/2019/04/02/spark-file-transfer-and-more-strategies-for-migrating-data-to-and-from-a-cassandra-or-scylla-cluster/"" rel=""noreferrer"">https://www.scylladb.com/2019/04/02/spark-file-transfer-and-more-strategies-for-migrating-data-to-and-from-a-cassandra-or-scylla-cluster/</a></p>

<p>Good luck!</p>
",['table']
61412231,61454379,2020-04-24 15:44:14,com.datastax.driver.core.exceptions.InvalidQueryException: unconfigured table peers_v2,"<p>I am developing an application with Spring Boot that connects to cassandra</p>

<p>This is my connection settings</p>

<pre><code>spring.data.cassandra.contact-points=localhost
spring.data.cassandra.keyspace-name=sa_tas_db_cassandra
spring.data.cassandra.schema-action=create_if_not_exists
spring.data.cassandra.username=cassandra
</code></pre>

<p>I'm just using a table, which I create by means of a model. Which is the following</p>

<pre><code>import org.springframework.data.cassandra.core.mapping.PrimaryKey
import org.springframework.data.cassandra.core.mapping.Table
import java.time.LocalTime
import java.util.*

@Table
data class CallLog(
    @PrimaryKey
    val callId:UUID,
    val CustomerUsername: String? = null,
    val callDirection: String? = null,
    val callingPartyName:String? = null,
    val callingPartyNumber:String? = null,
    val calledPartyNumber:String? = null,
    val typeOfCall:String? = null,
    val startTime: Date? = null,
    val answerTime: LocalTime? = null,
    val disconnectTime:Date? = null,
    val timeCallForwarded: Date? = null,
    val voicemailSystemAccessNumber:String? = null
)
</code></pre>

<p>That's all my cassandra setup</p>

<p>I am also monitoring my application with new relic, and it is generating the following problem</p>

<pre><code>….netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
…hannel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:377)
…hannel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:363)
….channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:355)
  io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
…hannel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:377)
…hannel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:363)
….channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:355)
…etty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
…hannel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:377)
…hannel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:363)
….channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:355)
…tty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:321)
…o.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:295)
…hannel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:377)
…hannel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:363)
….channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:355)
…netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:93)
…hannel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:377)
…hannel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:363)
….channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:355)
…channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
…hannel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:377)
…hannel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:363)
…o.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
….channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
   io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
….channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)
  io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)
                  io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
….netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
         io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
   io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
</code></pre>

<p>This error does not affect the operation of the application, it only affects the performance</p>

<p>Thanks</p>
",<java><cassandra><newrelic><datastax-java-driver><spring-data-cassandra>,"<p>This is a part of compatibility test for supporting Cassandra 4.0 that stores information about nodes in the cluster in a table with different name, so driver first assumes that it runs against newer version, and checks this table, and if it receives an error, then it uses the old <code>peers</code> table. It's not possible to reliably detect Cassandra node's features only from version number, that's why it's better to check for table explicitly.</p>

<p>The corresponding code is <a href=""https://github.com/datastax/java-driver/blob/3.x/driver-core/src/main/java/com/datastax/driver/core/ControlConnection.java#L92"" rel=""nofollow noreferrer"">here</a>.</p>
",['table']
61469356,61476080,2020-04-27 22:14:15,Does adding a column to a cassandra table complete instantly?,"<p>We plan to add a column of type list to an existing cassandra table which data file size is about 350 GB.  We can temporarily halt all the read/write for a few minutes while applying the schema change.</p>

<p>Our understanding is that cassandra does not lock a table when applying schema changes, but to be sure our DBA wants to do an experiment on a table with datafile at 25 GB in size. However it will take 3-4 weeks to grow in such size on a small server where a non-production cassandra server is running (having more concurrent inserts starts to cause time out issues).</p>

<p>Does anyone know that adding a column to an existing cassandra table returns promptly regardless the underlying data file size?</p>

<p>Thanks </p>
",<cassandra><schema>,"<p>Adding a column in Cassandra is just an addition of the column's meta-information to internal table that keeps schema information. No modification of existing data happens when this change is done - Cassandra will simply put null instead of the column value when there is no data for it on the disk (for any column, not only what was added) - this happens when the data is returned to caller, not by adding null to the files.</p>

<p>Similarly, deletion of the column doesn't modify the existing data - instead a new entry is added to <code>system_schema.dropped_columns</code> table, and corresponding data is filtered out after they are read from the disk.</p>
",['table']
61486045,61640311,2020-04-28 17:17:01,NoNodeAvailableException after some insert request to cassandra,"<p>I am trying to insert data into Cassandra local cluster using async execution and version 4 of the driver (as same as my Cassandra instance)</p>

<p>I have instantiated the cql session in this way:</p>

<pre class=""lang-java prettyprint-override""><code>CqlSession cqlSession = CqlSession.builder()
  .addContactEndPoint(new DefaultEndPoint(
    InetSocketAddress.createUnresolved(""localhost"",9042))).build();
</code></pre>

<p>Then I create a statement in an async way:</p>

<pre class=""lang-java prettyprint-override""><code>return session.prepareAsync(
       ""insert into table (p1,p2,p3, p4) values (?, ?,?, ?)"")
          .thenComposeAsync(
              (ps) -&gt; {
                 CompletableFuture&lt;AsyncResultSet&gt;[] result = data.stream().map(
                     (d) -&gt; session.executeAsync(
                          ps.bind(d.p1,d.p2,d.p3,d.p4)
                       )
                  ).toCompletableFuture()
              ).toArray(CompletableFuture[]::new);
          return CompletableFuture.allOf(result);
      }
);
</code></pre>

<p><code>data</code> is a dynamic list filled with user data.</p>

<p>When I exec the code I get the following exception:</p>

<pre><code>Caused by: com.datastax.oss.driver.api.core.NoNodeAvailableException: No node was available to execute the query

    at com.datastax.oss.driver.api.core.AllNodesFailedException.fromErrors(AllNodesFailedException.java:53)
    at com.datastax.oss.driver.internal.core.cql.CqlPrepareHandler.sendRequest(CqlPrepareHandler.java:210)
    at com.datastax.oss.driver.internal.core.cql.CqlPrepareHandler.onThrottleReady(CqlPrepareHandler.java:167)
    at com.datastax.oss.driver.internal.core.session.throttling.PassThroughRequestThrottler.register(PassThroughRequestThrottler.java:52)
    at com.datastax.oss.driver.internal.core.cql.CqlPrepareHandler.&lt;init&gt;(CqlPrepareHandler.java:153)
    at com.datastax.oss.driver.internal.core.cql.CqlPrepareAsyncProcessor.process(CqlPrepareAsyncProcessor.java:66)
    at com.datastax.oss.driver.internal.core.cql.CqlPrepareAsyncProcessor.process(CqlPrepareAsyncProcessor.java:33)
    at com.datastax.oss.driver.internal.core.session.DefaultSession.execute(DefaultSession.java:210)
    at com.datastax.oss.driver.api.core.cql.AsyncCqlSession.prepareAsync(AsyncCqlSession.java:90)

</code></pre>

<p>The node is active and some data are inserted before the exception rise. I have also tried to set up a data center name on the session builder without any result.</p>

<p>Why this exception rise if the node is up and running? Actually I have only one local node and that could be a problem?</p>
",<cassandra><insert>,"<p>Finally, I have found a solution using <code>BatchStatement</code> and a little custom code to create a chucked list.</p>

<pre class=""lang-java prettyprint-override""><code>    int chunks = 0;
    if (data.size() % 100 == 0) {
      chunks = data.size() / 100;
    } else {
      chunks = (data.size() / 100) + 1;
    }

    final int finalChunks = chunks;

    return session.prepareAsync(
           ""insert into table (p1,p2,p3, p4) values (?, ?,?, ?)"")
            .thenComposeAsync(
                    (ps) -&gt; {


                      AtomicInteger counter = new AtomicInteger();

                      final List&lt;CompletionStage&lt;AsyncResultSet&gt;&gt; batchInsert = data.stream()
                              .map(
                                      (d) -&gt; ps.bind(d.p1,d.p2,d.p3,d.p4)

                              )
                              .collect(Collectors.groupingBy(it -&gt; counter.getAndIncrement() / finalChunks))
                              .values().stream().map(
                                      boundedStatements -&gt; BatchStatement.newInstance(BatchType.LOGGED, boundedStatements.toArray(new BatchableStatement[0]))
                              ).map(
                                      session::executeAsync
                              ).collect(Collectors.toList());

                      return CompletableFutures.allSuccessful(batchInsert);
                    }
            );
</code></pre>
",['table']
61593149,61594460,2020-05-04 13:17:40,IN Query in Cassandra Where clause,"<p>I have a Scylla cluster with 3 Nodes and 1 Table created with the below Query</p>

<pre><code>CREATE TABLE id_features (
    id int PRIMARY KEY,
    id_feature_1 int,
    id_feature_2 int,

)
</code></pre>

<p>I am issuing below query from the application
<code>SELECT * FROM  id_features where id in (1,2,3,4...120);</code>
The query can have a maximum of 120 ids.</p>

<p>Will this Query contact all 3 nodes based on the token value of id`s to fetch data for 120 ids in the worst case? 
Or only 1 node will be contacted to fetch the data for all the ids and multiple nodes are used only for high availability</p>

<p>Do the replication factor, consistency level, and load balancing policy will play any role in deciding the node?</p>
",<cassandra><cql><cassandra-3.0><scylla><cqlengine>,"<blockquote>
<p>Will this Query contact all 3 nodes based on the token value of <code>id</code>s to fetch data</p>
<p>Do the replication factor, consistency level, and load balancing policy will play any role in deciding the node?</p>
</blockquote>
<p>It very much depends on things like replication factor (RF), query consistency, and load balancing policy.  Specifically, if RF &lt; number of nodes, then multiple nodes will be contacted, based on the hashed token value of <code>id</code> and the nodes primarily assigned to those token ranges.</p>
<p>But, given this statement:</p>
<blockquote>
<p>Or only 1 node will be contacted to fetch the data for all the ids and multiple nodes are used only for high availability</p>
</blockquote>
<p>...I get the sense that RF=3 in this case.</p>
<p>If the app is configured to use the (default) <code>TokenAwarePolicy</code> then yes, for single-key queries only, requests can be sent to the individual nodes.</p>
<p>But in this case, the query is using the <code>IN</code> operator.  Based on the 120 potential entries, the query <em>cannot</em> determine a single node to send the query.  In that case, the <code>TokenAwarePolicy</code> simply acts as a pass-through for its child policy (<code>DCAwareRoundRobinPolicy</code>), and it will pick a node at <code>LOCAL</code> distance to be the &quot;coordinator.&quot;  The coordinator node will then take on the additional tasks of routing replica requests and compiling the result set.</p>
<p>As to whether or not non-primary replicas are utilized in query plans, the answer is again &quot;it depends.&quot;  While the load balancing policies differ in implementation, in general all of them compute <a href=""https://docs.datastax.com/en/developer/java-driver/3.6/manual/load_balancing/#query-plan"" rel=""nofollow noreferrer"">query plans</a> which:</p>
<ul>
<li>are different for each query, in order to balance the load across the cluster;</li>
<li>only contain hosts that are known to be able to process queries, i.e. neither ignored nor down;</li>
<li>favor local hosts over remote ones.</li>
</ul>
<p><em>Taken from: <a href=""https://docs.datastax.com/en/developer/java-driver/3.6/manual/load_balancing/#query-plan"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/developer/java-driver/3.6/manual/load_balancing/#query-plan</a></em></p>
<p>So in a scenario where RF = number of nodes, a single node <em>sometimes</em> may be used to return all requested replicas.</p>
<p><strong>Pro-tip</strong>:</p>
<p>Try not to use the <code>IN</code> operator with a list of 120 partition key entries.  That is forcing Cassandra to perform <em>random</em> reads, where it really excels at <em>sequential reads</em>.  If that's a query the app really needs to do, try:</p>
<ul>
<li>Building a new table to better support that query pattern.</li>
<li>Not exceed double-digits of entries for <code>IN</code>.</li>
</ul>
",['table']
61681364,61723703,2020-05-08 14:24:26,saving dataset to cassandra using java spark,"<p>I'm trying to save a dataset to cassandra db using java spark.
I'm able to read data into dataset successfully using the below code</p>

<pre><code>Dataset&lt;Row&gt; readdf = sparkSession.read().format(""org.apache.spark.sql.cassandra"")
.option(""keyspace"",""dbname"")
.option(""table"",""tablename"")
.load();
</code></pre>

<p>But when I try to write dataset I'm getting <strong>IOException: Could not load or find table, found similar tables in keyspace</strong></p>

<pre><code>Dataset&lt;Row&gt; dfwrite= readdf.write().format(""org.apache.spark.sql.cassandra"")
.option(""keyspace"",""dbname"")
.option(""table"",""tablename"")
.save();
</code></pre>

<p>I'm setting host and port in sparksession
The thing is I'm able to write in overwrite and append modes but not able to create table</p>

<p>Versions which I'm using are below:
spark java 2.0
spark cassandra connector 2.3</p>

<p>Tried with different jar versions but nothing worked
I have also gone through different stack overflow and github links </p>

<p>Any help is greatly appreciated.</p>
",<java><apache-spark><cassandra><spark-cassandra-connector>,"<p>The <code>write</code> operation in Spark doesn't have a mode that will automatically create a table for you - there are multiple reasons for that. One of them is that you need to define a primary key for your table, otherwise, you may just overwrite data if you set incorrect primary key.  Because of this, <a href=""https://github.com/datastax/spark-cassandra-connector/blob/b2.5/doc/14_data_frames.md#creating-a-new-cassandra-table-from-a-dataset-schema"" rel=""nofollow noreferrer"">Spark Cassandra Connector provides a separate method to create a table based on your dataframe structure</a>, but you need to provide a list of partition &amp; clustering key columns.  In Java it will look as following (full code is <a href=""https://github.com/alexott/dse-playground/blob/master/spark-oss/src/main/java/com/datastax/alexott/demos/spark/TableCreate.java"" rel=""nofollow noreferrer"">here</a>):</p>

<pre class=""lang-java prettyprint-override""><code>DataFrameFunctions dfFunctions = new DataFrameFunctions(dataset);
Option&lt;Seq&lt;String&gt;&gt; partitionSeqlist = new Some&lt;&gt;(JavaConversions.asScalaBuffer(
          Arrays.asList(""part"")).seq());
Option&lt;Seq&lt;String&gt;&gt; clusteringSeqlist = new Some&lt;&gt;(JavaConversions.asScalaBuffer(
          Arrays.asList(""clust"", ""col2"")).seq());
CassandraConnector connector = new CassandraConnector(
          CassandraConnectorConf.apply(spark.sparkContext().getConf()));
dfFunctions.createCassandraTable(""test"", ""widerows6"",
          partitionSeqlist, clusteringSeqlist, connector);
</code></pre>

<p>and then you can write data as usual:</p>

<pre class=""lang-java prettyprint-override""><code>dataset.write()
   .format(""org.apache.spark.sql.cassandra"")
   .options(ImmutableMap.of(""table"", ""widerows6"", ""keyspace"", ""test""))
   .save();
</code></pre>
",['table']
61768228,61768812,2020-05-13 06:58:37,Does Cassandra Insert JSON creates Tombstones?,"<p>I am using <a href=""https://docs.datastax.com/en/cql-oss/3.3/cql/cql_using/useInsertJSON.html"" rel=""nofollow noreferrer"">Cassandra's insert JSON feature</a> to insert data into the table. In the JSON that I create, I do not include the <code>null</code> values. Meaning, ignore the fields that have <code>null</code> values in it. </p>

<p>Does Cassandra create tombstones in such cases, if I am inserting the record say, for the first time? </p>

<p>I assume for subsequent upsert for the same <code>key</code> will create tombstone. Is that right?</p>
",<cassandra><tombstone>,"<p>Cell tombstone should still get created for the values which are not included in the json you are trying to insert.</p>

<p>Consider a table called cyclist having id, first name and last name. We will insert a row using a json string which contains only last name.</p>

<pre><code>CREATE KEYSPACE IF NOT EXISTS cycling WITH REPLICATION = { 'class' : 'NetworkTopologyStrategy', 'datacenter1' : 3 };

CREATE TABLE cycling.cyclist ( id UUID PRIMARY KEY, first_name text, last_name text );

CREATE TABLE cycling.cyclist ( id UUID PRIMARY KEY, first_name text, last_name text );

INSERT INTO cycling.cyclist JSON '{""id"" : ""829aa84a-4bba-411f-a4fb-38167a987cda"", ""last_name"" : ""MYLASTNAME"" }';
</code></pre>

<p>Now if we look at the data structure within sstable, it looks like below.</p>

<pre><code>[
  {
    ""partition"" : {
      ""key"" : [ ""829aa84a-4bba-411f-a4fb-38167a987cda"" ],
      ""position"" : 0
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 30,
        ""liveness_info"" : { ""tstamp"" : ""2020-05-13T07:10:59.298374Z"" },
        ""cells"" : [
          { ""name"" : ""first_name"", ""deletion_info"" : { ""local_delete_time"" : ""2020-05-13T07:10:59Z"" }
          },
          { ""name"" : ""last_name"", ""value"" : ""MYLASTNAME"" }
        ]
      }
    ]
  }
]
</code></pre>

<p>Observe the cell tombstone created for first_name.</p>

<p>This is different than the sstable structure when we insert the data using selective fields.</p>

<pre><code>INSERT INTO cycling.cyclist(id, first_name) VALUES ( 'c49d1614-e841-4bd4-993b-02d49ae7414c', 'MYFIRSTNAME');
</code></pre>

<p>Now look at the sstable structure</p>

<pre><code>[
  {
    ""partition"" : {
      ""key"" : [ ""829aa84a-4bba-411f-a4fb-38167a987cda"" ],
      ""position"" : 0
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 30,
        ""liveness_info"" : { ""tstamp"" : ""2020-05-13T07:10:59.298374Z"" },
        ""cells"" : [
          { ""name"" : ""first_name"", ""deletion_info"" : { ""local_delete_time"" : ""2020-05-13T07:10:59Z"" }
          },
          { ""name"" : ""last_name"", ""value"" : ""MYLASTNAME"" }
        ]
      }
    ]
  },
  {
    ""partition"" : {
      ""key"" : [ ""c49d1614-e841-4bd4-993b-02d49ae7414c"" ],
      ""position"" : 47
    },
    ""rows"" : [
      {
        ""type"" : ""row"",
        ""position"" : 77,
        ""liveness_info"" : { ""tstamp"" : ""2020-05-13T07:23:42.964609Z"" },
        ""cells"" : [
          { ""name"" : ""first_name"", ""value"" : ""MYFIRSTNAME"" }
        ]
      }
    ]
  }
]
</code></pre>
",['table']
61941402,61957544,2020-05-21 18:32:44,Cassandra follows which partitioning technique?,"<p>I am new to Cassandra and while reading about partitioning a database - vertical and horizontal, I got confused and would like to know whether Cassandra follows Horizontal partitioning (sharding) OR vertical partitioning technique?</p>

<p>Moreover, according to my understanding, as Cassandra is column oriented DB, it should follow Vertical partitioning technique. If this is not the case then can anyone please explain it in detail? </p>
",<database><cassandra><partitioning><sharding><column-oriented>,"<blockquote>
  <p>as Cassandra is column oriented DB</p>
</blockquote>

<p>This point has been discussed ad-nauseam on Stack Overflow, <a href=""https://stackoverflow.com/questions/25441921/is-cassandra-a-column-oriented-or-columnar-database/25447422#25447422"">specifically in this answer</a>.  Cassandra is <strong>NOT</strong> a column oriented database.  It is a <em>partitioned row store</em>.  Data is organized and presented in ""rows,"" similar to a relational database.</p>

<blockquote>
  <p>whether Cassandra follows Horizontal partitioning (sharding)</p>
</blockquote>

<p>Technically, Cassandra is what you would call a ""sharded"" database, but it's almost never referred to in this way.  Essentially, each node is responsible for a specific range of partitions.  These partitions (tokens) are a numeric value, and with the <a href=""https://docs.datastax.com/en/cassandra-oss/3.0/cassandra/architecture/archPartitionerM3P.html"" rel=""nofollow noreferrer"">Murmur3Partitioner</a> range from -2^63 to +2^63-1.</p>

<p>In fact, in a scenario where a node is simplified to hold a <em>single</em> token range, you can compute the ranges based on the number of nodes in the cluster (data center) like this:</p>

<pre><code>python -c 'print [str(((2**64 / 6) * i) - 2**63) for i in range(6)]'

['-9223372036854775808', '-6148914691236517206', '-3074457345618258604',
 '-2', '3074457345618258600', '6148914691236517202']
</code></pre>

<p>Of course with <a href=""https://docs.datastax.com/en/dse/6.8/dse-arch/datastax_enterprise/dbArch/archDataDistributeVnodesUsing.html"" rel=""nofollow noreferrer"">vNodes</a>, a node is almost always responsible for multiple token ranges.</p>

<p>At operation time, the partition key is hashed into a token.  This token tells Cassandra which node the data resides on.  Consider this table:</p>

<pre><code>SELECT token(studentid),studentid,fname,lname FROM student ;

 system.token(studentid) | studentid | fname | lname
-------------------------+-----------+-------+----------
    -5626264886876159064 | janderson | Jordy | Anderson
    -1472930629430174260 |   aploetz | Avery |   Ploetz
     8993000853088610283 |      mgin | Micah |      Gin

(3 rows)
</code></pre>

<p>As this table has a simple primary key definition of <code>studentid</code>, that is used as the partition key.  The results of the <code>token(studentid)</code> function above indicate which partitions contain the data.</p>

<p>If there was another table which also used <code>studentid</code> as its partition key, that table's data would be stored on the same nodes as the <code>student</code> table.</p>

<p>In any case, this is a simplified version of what happens.  Feel free to read up on vNodes (link above) as well as <a href=""https://rads.stackoverflow.com/amzn/click/com/B01HY3TC5E"" rel=""nofollow noreferrer"" rel=""nofollow noreferrer"">Cassandra: High Availability</a> by Robbie Strickland.  He has written (IMO) the <em>best</em> description of Cassandra's hashing and partition distribution process.</p>
",['table']
61944845,61952150,2020-05-21 22:29:28,Why secondary indexes are less efficient in Cassandra?,"<p>I read in Cassandra documentation that creating secondary index is less efficient as because in worst case it need to touch all nodes in order to find out the data of that non-key column. </p>

<p>But my doubt is even if we do not create secondary index, then also it will have to touch all nodes (in worst case) and find out where that particular row with this non-key column value resides.</p>

<p>Note: Yeah, I understand that it is possible that if the cardinality is high then the secondary index will contain(store) index for mostly all rows and in this way it is bad in terms of storage. But I want to know how not creating secondary index is efficient than creating secondary index?</p>
",<cassandra><nosql><distributed-database><secondary-indexes>,"<p>Secondary indexes should be used only in specific cases, like, when you use them together with condition on partition key column, you have correct cardinality for data, etc.</p>

<p>For example, if we have following table:</p>

<pre><code>create table test.test (
  pk int,
  c1 int,
  val1 int,
  val2 int,
  primary key(pk, c1));
</code></pre>

<p>and you created a secondary index on the column <code>val2</code>, then following query will be very effective:</p>

<pre><code>select * from test.test where pk = 123 and val2 = 10
</code></pre>

<p>because you restricted the execution of query only to the nodes that are replicas for <code>pk</code> with value <code>123</code>.  </p>

<p>But if you do</p>

<pre><code>select * from test.test where val2 = 10
</code></pre>

<p>then Cassandra will need to go to the every node, and ask for data there - it will be much slower, and put a pressure to coordinating node.</p>

<p>Standard secondary indexes have other limitations, such as, search only for specific values, problems when column has very low or very high cardinality, etc.  SASI indexes are better from design standpoint, although they are still experimental, and have problems with implementation.  </p>

<p>You can find technical details about implementation of secondary indexes in the following <a href=""http://www.doanduyhai.com/blog/?p=13191"" rel=""nofollow noreferrer"">blog post</a>.</p>

<p>DataStax has other implementations in the commercial offering:</p>

<ul>
<li>DSE Search that is based on the Apache Solr, so you get a lot of flexibility (full text search, range queries, etc.)</li>
<li>new implementation called SSTable Attached Indexes (SAI) - they are currently marked as beta, but they provide more flexibility than standard secondary indexes, with less overhead than DSE Search</li>
</ul>
",['table']
61946840,61950609,2020-05-22 02:24:21,"Insert Spark Dataset[(String, Map[String, String])] to Cassandra Table","<p>I have a Spark Dataset of type Dataset[(String, Map[String, String])].</p>

<p>I have to insert the same into a Cassandra table.</p>

<p>Here, key in the Dataset[(<strong>String</strong>, Map[String, String])] will become my primary key of the row in Cassandra.</p>

<p>The Map in the Dataset[(String, <strong>Map[String, String]</strong>)] will go in the same row in a column <strong>ColumnNameValueMap</strong>.</p>

<p>The Dataset can have millions of rows.</p>

<p>I also want to do it in optimum way (e.g. batch insert Etc.)</p>

<p>My Cassandra table structure is:</p>

<pre><code>CREATE TABLE SampleKeyspace.CassandraTable (
  RowKey text PRIMARY KEY,
  ColumnNameValueMap map&lt;text,text&gt;
);
</code></pre>

<p>Please suggest how to do the same.</p>
",<scala><apache-spark><cassandra><nosql><spark-cassandra-connector>,"<p>Everything that you need is just to use <a href=""https://github.com/datastax/spark-cassandra-connector/tree/master/doc"" rel=""nofollow noreferrer"">Spark Cassandra Connector</a> (better to take version 2.5.0 that was just released). It provides read &amp; write functions for datasets, so in your case it will be just </p>

<pre class=""lang-scala prettyprint-override""><code>import org.apache.spark.sql.cassandra._
your_data.write.cassandraFormat(""CassandraTable"", ""SampleKeyspace"").mode(""append"").save()
</code></pre>

<p>If your table don't exist yet, then <a href=""https://github.com/datastax/spark-cassandra-connector/blob/master/doc/14_data_frames.md#creating-a-new-cassandra-table-from-a-dataset-schema"" rel=""nofollow noreferrer"">you can create it base don the structure of the dataset itself</a> - there are 2 functions: <code>createCassandraTable</code> &amp; <code>createCassandraTableEx</code> - it's better to use 2nd, as it provides more control over table creation. </p>

<p>P.S. You can find more about 2.5.0 release in the following <a href=""https://www.datastax.com/blog/2020/05/advanced-apache-cassandra-analytics-now-open-all"" rel=""nofollow noreferrer"">blog post</a>.</p>
",['table']
61982652,61983299,2020-05-24 07:00:45,Cassandra - What is guaranteed with respect to the tables,"<p>I had two following tables ( taken from Cassandra Definitve Guide , <a href=""https://gist.github.com/jeffreyscarpenter/761ddcd1c125dfb194dc02d753d31733"" rel=""nofollow noreferrer"">https://gist.github.com/jeffreyscarpenter/761ddcd1c125dfb194dc02d753d31733</a> } - What is guaranteed with respect to the folloowing tables assuming they had the same partition key ?</p>

<ol>
<li>Can we safely assume the data for both the tables present in the same node as long as the partition key is same ? as both tables contain same partition key.</li>
<li>Ok , and as tables are different from each other , will they be stored in different partitions or same partition in the ""same"" node</li>
</ol>

<p><a href=""https://gist.github.com/jeffreyscarpenter/761ddcd1c125dfb194dc02d753d31733"" rel=""nofollow noreferrer"">https://gist.github.com/jeffreyscarpenter/761ddcd1c125dfb194dc02d753d31733</a></p>

<pre><code>CREATE TABLE hotel.pois_by_hotel (
    poi_name text,
    hotel_id text,
    description text,
    PRIMARY KEY ((hotel_id), poi_name)
) WITH comment = 'Q3. Find pois near a hotel';

CREATE TABLE hotel.available_rooms_by_hotel_date (
    hotel_id text,
    date date,
    room_number smallint,
    is_available boolean,
    PRIMARY KEY ((hotel_id), date, room_number)
) WITH comment = 'Q4. Find available rooms by hotel / date';
</code></pre>
",<cassandra>,"<ol>
<li>if both tables have the same partition key, then the same value will be mapped into the same token. If tables are in the same keyspace, then yes - they will be on the same node(s). If they are in the different keyspaces, then there could be a partial overlap - if replication factor is different, for example, one keyspace has higher RF (like, KS1 has RF=2, and KS2 has RF=3, then 2 nodes will have replicas for both keyspaces, and 3rd node will have only for KS2).</li>
<li>Each table will have its own set of the files on disk, so although they have the same ""logical partitions"", on disk they are in different files. You can always look into data files, something like, <code>/var/lib/cassandra/data/&lt;keyspace&gt;/&lt;table&gt;-&lt;table-uuid&gt;/</code></li>
</ol>
",['table']
62037315,62042091,2020-05-27 07:08:02,Cassandra create duplicate table with different primary key,"<p>I'm new to Apache Cassandra and have the following issue:</p>

<p>I have a table with <code>PRIMARY KEY (userid, countrycode, carid)</code>. As described in many tutorials this table can be queried by using following filter criteria:</p>

<ul>
<li>userid = x</li>
<li>userid = x and countrycode = y</li>
<li>userid = x and countrycode = y and carid = z</li>
</ul>

<p>This is fine for most cases, but now I need to query the table by filtering only on</p>

<ul>
<li>userid = x and carid = z</li>
</ul>

<p>Here, the documentation sais that is the best solution to create another table with a modified primary key, in this case <code>PRIMARY KEY (userid, carid, countrycode)</code>.</p>

<p>The question here is, how to copy the data from the ""original"" table to the new one with different index?</p>

<ul>
<li>On small tables</li>
<li>On huge tables</li>
</ul>

<p>And another important question concerning the duplication of a huge table: What about the storage needed to save both tables instead of only one?</p>
",<cassandra><cql><cqlsh>,"<p>You can use COPY command to export from one table and import into other table.</p>

<p>From your example - I created 2 tables. user_country and user_car with respective primary keys.</p>

<pre><code>CREATE KEYSPACE user WITH REPLICATION = { 'class' : 'NetworkTopologyStrategy',  'datacenter1' : 2 } ;
CREATE TABLE user.user_country ( user_id text, country_code text, car_id text, PRIMARY KEY (user_id, country_code, car_id));
CREATE TABLE user.user_car ( user_id text, country_code text, car_id text, PRIMARY KEY (user_id, car_id, country_code));
</code></pre>

<p>Let's insert some dummy data into one table.</p>

<pre><code>cqlsh&gt; INSERT INTO user.user_country (user_id, country_code, car_id) VALUES ('1', 'IN', 'CAR1');
cqlsh&gt; INSERT INTO user.user_country (user_id, country_code, car_id) VALUES ('2', 'IN', 'CAR2');
cqlsh&gt; INSERT INTO user.user_country (user_id, country_code, car_id) VALUES ('3', 'IN', 'CAR3');
cqlsh&gt; select * from user.user_country ;

 user_id | country_code | car_id
---------+--------------+--------
       3 |           IN |   CAR3
       2 |           IN |   CAR2
       1 |           IN |   CAR1

(3 rows)
</code></pre>

<p>Now we will export the data into a CSV. <strong>Observe the sequence of columns mentioned.</strong></p>

<pre><code>cqlsh&gt; COPY user.user_country (user_id,car_id, country_code) TO 'export.csv';
Using 1 child processes

Starting copy of user.user_country with columns [user_id, car_id, country_code].
Processed: 3 rows; Rate:       4 rows/s; Avg. rate:       4 rows/s
3 rows exported to 1 files in 0.824 seconds.
</code></pre>

<p>export.csv can now be directly inserted into other table.</p>

<pre><code>cqlsh&gt; COPY user.user_car(user_id,car_id, country_code) FROM 'export.csv';
Using 1 child processes

Starting copy of user.user_car with columns [user_id, car_id, country_code].
Processed: 3 rows; Rate:       6 rows/s; Avg. rate:       8 rows/s
3 rows imported from 1 files in 0.359 seconds (0 skipped).
cqlsh&gt;
cqlsh&gt;
cqlsh&gt; select * from user.user_car ;

 user_id | car_id | country_code
---------+--------+--------------
       3 |   CAR3 |           IN
       2 |   CAR2 |           IN
       1 |   CAR1 |           IN

(3 rows)
cqlsh&gt;
</code></pre>

<p>About your other question - yes the data will be duplicated, but that's how cassandra is used. </p>
",['table']
62086888,62103361,2020-05-29 13:16:13,How should I set the replication factor in Cassandra to account for node failure?,"<p>Lets say we have a cassandra deployment with a replication factor of 2. By this I mean that we can tolerate the total loss of one node of persistent storage without overall data loss. I understand this to mean that each of the values are stored on at least two different nodes at any given time. Therefore the total storage required is at least the total data of the values x 2. Ie, if we need to store 100TB in the cluster, we would need at least 200TB persistent storage across the nodes. </p>

<p>However, as the node count increases, so does the likelyhood of more than 1 node failing. Therefore, do we need to increase the replication factor as the number of nodes increases?</p>

<p>For example:</p>

<p>Lets assume that all components are 100% reliable, except for my nodes local storage controllers, which for time to time completely corrupt all local storage with no possibility for restoration (ie, data loss is total). All rack equipment, switches, power, cooling etc are all perfect. I know this is not realistic.</p>

<p>Lets also assume that any data loss is really, really bad for this application. </p>

<p>Lets say my nodes have 1TB each of storage. For 100TB of values, I would need 200 machines to achieve a replication factor of 2 (ie, I can lose any one node and still retain data). However, if I believe that the simultaneous failure of 2 nodes in that set of 200 is likely I will need to raise the replication factor to 3. Therefore now I need three copies of each value (on three different nodes) and now I need 300 nodes. I now feel that the simultaneous loss of 3 or more nodes is likely, so I have to add more nodes again, etc...</p>

<p>Surely this isn't actually how this scales? What is wrong with my logic?</p>
",<cassandra><replication><distributed>,"<p>There are several types of failures that you need to take into account:</p>

<ol>
<li>Individual node failure (hardware/os/...) - your node is failed, either completely (data is lost), or partially (for example, power adapter has failed)</li>
<li>Rack/data center failure - when nodes in specific part of data center, or data center completely failed, or not available over network</li>
</ol>

<p>Replication helps to avoid complete data unavailability, but it may also depend on the deployment strategy.</p>

<p>For example, if all your servers in one data center, if it's not available, you'll lose access to the data. Or if you didn't setup cluster to have rack-aware data placement, replicas could be put into the same rack, and if it's going down, you lose your replica.</p>

<p>Typically, it's recommended to use replication factor 3, and if you're planning big deployment, definitely use rack-aware data placement - but you should be careful, so number of racks should match RF (in cloud deployments, usually the rack is mapped to the availability zone). </p>

<p>Availability is also depends on your business requirements - in simplest case, if you use  consistency levels <code>ONE</code> or <code>LOCAL_ONE</code>, your data is available even only one replica is available, but if your business logic requires stronger consistency, you need to have more replicas available. And replication factor also affects the consistency levels - if you use RF=2, and require CL=QUORUM, you can't tolerate single node failure, while it's possible to achieve that CL with RF=3 and one node failed.</p>
",['rack']
62131860,62135449,2020-06-01 12:09:54,Apache NiFi : How to fetch data from Cassandra table whenever table record is getting modified,"<p>I have <strong>Cassandra</strong> as the database, We are using ""<strong>QueryCassandra</strong>"" processor to fetch values from Cassandra table to an output port,
Which uses a select query to fetch the records. I have a use case mentioned below.</p>

<p>1) First time all the records need to be <strong>fetched from Cassandra and transferred to the output port</strong>, that's happening now. (i.e  All data is frequently fetched from the table at particular time interval as we mention in Run Schedule)</p>

<p>2) Later whenever the Cassandra table is modified (Insert New Record or Row Updated or Row delete) then only the records need to be sent to the output port, 
is there any way we can achieve this instead of fetching every time intervals?</p>

<p><a href=""https://i.stack.imgur.com/Zogre.png"" rel=""nofollow noreferrer"">Sample Nifi Template</a></p>
",<java><cassandra><apache-flink><apache-nifi>,"<p>This isn't currently possible with NiFi (1.11.4 at the time of this writing), we'd need either a Cassandra version of QueryDatabaseTable (where you provide a column that only increases, like timestamp) or a CaptureChangeCassandra processor where we use a <a href=""https://cassandra.apache.org/doc/latest/operating/cdc.html"" rel=""nofollow noreferrer"">CommitLogReader</a> to read the commit log rather than querying the table itself.</p>

<p>Please feel free to write a New Feature <a href=""https://issues.apache.org/jira/browse/NIFI"" rel=""nofollow noreferrer"">Jira case</a> to add CDC capabilities for Cassandra.</p>
",['table']
62374126,62374887,2020-06-14 14:50:21,spark cassandra connector problem using catalogs,"<p>I am following the instructions <a href=""https://github.com/datastax/spark-cassandra-connector/blob/master/doc/1_connecting.md"" rel=""nofollow noreferrer"">found here</a> to connect my spark program to read data from Cassandra. Here is how I have configured spark:</p>

<pre><code>val configBuilder = SparkSession.builder
  .config(""spark.sql.extensions"", ""com.datastax.spark.connector.CassandraSparkExtensions"")
  .config(""spark.cassandra.connection.host"", cassandraUrl)
  .config(""spark.cassandra.connection.port"", 9042)
  .config(""spark.sql.catalog.myCatalogName"", ""com.datastax.spark.connector.datasource.CassandraCatalog"")
</code></pre>

<p>According to the documentation, once this is done I should be able to query Cassandra like this:</p>

<p><code>spark.sql(""select * from myCatalogName.myKeyspace.myTable where myPartitionKey = something"")</code> </p>

<p>however when I do so I get the following error message: </p>

<pre><code>mismatched input '.' expecting &lt;EOF&gt;(line 1, pos 43)

== SQL ==
select * from myCatalog.myKeyspace.myTable where myPartitionKey = something
----------------------------------^^^
</code></pre>

<p>When I try in the following format I am successful at retrieving entries from Cassandra:</p>

<pre><code>val frame = spark
  .read
  .format(""org.apache.spark.sql.cassandra"")
  .options(Map(""keyspace"" -&gt; ""myKeyspace"", ""table"" -&gt; ""myTable""))
  .load()
  .filter(col(""timestamp"") &gt; startDate &amp;&amp; col(""timestamp"") &lt; endDate)
</code></pre>

<p>However this query requires a full table scan to be performed. The table contains a few million entries and I would prefer to avail myself of the predicate Pushdown functionality, which it would seem is only available via the SQL API.</p>

<p>I am using spark-core_2.11:2.4.3, spark-cassandra-connector_2.11:2.5.0 and Cassandra 3.11.6</p>

<p>Thanks!</p>
",<apache-spark><cassandra><spark-cassandra-connector>,"<p>The Catalogs API is available only in SCC version 3.0 that is not released yet. It will be released with Spark 3.0 release, so it isn't available in the SCC 2.5.0.  So for 2.5.0 you need to register your table explicitly, with <code>create or replace temporary view...</code>, as <a href=""https://github.com/datastax/spark-cassandra-connector/blob/b2.5/doc/14_data_frames.md#example-creating-a-source-using-spark-sql"" rel=""nofollow noreferrer"">described in docs</a>:</p>

<pre class=""lang-scala prettyprint-override""><code>spark.sql(""""""CREATE TEMPORARY VIEW myTable
     USING org.apache.spark.sql.cassandra
     OPTIONS (
     table ""myTable"",
     keyspace ""myKeyspace"",
     pushdown ""true"")"""""")
</code></pre>

<p>Regarding the pushdowns (they work the same for all Dataframe APIs, SQL, Scala, Python, ...) - such filtering will happen when your <code>timestamp</code> is the first clustering column.  And even in that case, the typical problem is that you may specify <code>startDate</code> and <code>endDate</code> as strings, not timestamp.  You can check by executing <code>frame.explain</code>, and checking that predicate is pushed down - it should have <code>*</code> marker near predicate name.</p>

<p>For example, </p>

<pre class=""lang-scala prettyprint-override""><code>val data = spark.read.cassandraFormat(""sdtest"", ""test"").load()
val filtered = data.filter(""ts &gt;= cast('2019-03-10T14:41:34.373+0000' as timestamp) AND ts &lt;= cast('2019-03-10T19:01:56.316+0000' as timestamp)"")
val not_filtered = data.filter(""ts &gt;= '2019-03-10T14:41:34.373+0000' AND ts &lt;= '2019-03-10T19:01:56.316+0000'"")
</code></pre>

<p>the first <code>filter</code> expression will push predicate down, while 2nd (<code>not_filtered</code>) will require a full scan.</p>
",['table']
62419610,62426638,2020-06-17 00:30:21,Read from Hive tables and Write to Cassandra tables,"<p>I have some external tables in Hive in Cloudera cluster partitioned by <code>daily_date</code> column.</p>

<p>I also have DataStax Enterprise Cassandra cluster where I have created tables same as Hive tables structure.</p>

<p>Question: I want to export/write the tables data from Hive tables to the corresponding Cassandra tables.</p>

<p>Is there any Hive to Cassandra connector available? Or do I need to do this in Spark, if yes How? What would be the best practice/solution here?</p>

<p>I have tried to google a lot different keywords, but have not come across any correct/recommended solution.</p>

<p>Please guide.</p>
",<apache-spark><hive><cassandra><cloudera><spark-cassandra-connector>,"<p>Just use Spark with <a href=""https://github.com/datastax/spark-cassandra-connector/tree/b2.5/doc"" rel=""nofollow noreferrer"">Spark Cassandra Connector</a>, better with <a href=""https://github.com/datastax/spark-cassandra-connector/blob/b2.5/doc/14_data_frames.md"" rel=""nofollow noreferrer"">Dataframe APIs</a>.  Access data in Hive as <a href=""https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html"" rel=""nofollow noreferrer"">described in Spark docs</a>, and after getting the dataframe, write it to Cassandra. Something like this:</p>

<pre class=""lang-scala prettyprint-override""><code>// assuming that table is registered already:
val df = sql(""SELECT * from hive_table"")
df.write
  .format(""org.apache.spark.sql.cassandra"")
  .options(Map(""table"" -&gt; ""..."", ""keyspace"" -&gt; ""...""))
  .save()
</code></pre>
",['table']
62514566,62536136,2020-06-22 12:33:29,"Spring data Cassandra, Allow filtering","<p>I have the following table</p>
<pre><code>CREATE TABLE magazines.magazine_name (
    frequency smallint,
    magazine_id varchar,
    magazine_name varchar,
    PRIMARY KEY (magazine_id,magazine_name)
);
</code></pre>
<p>Should I use allow filter annotation to have the following repository method get executed</p>
<pre><code>@Query(&quot;SELECT * from magazine_name where magazine_id = ?0&quot;)
MagazineName findMagazineCQlQuery(String id);
</code></pre>
<p>because I get the folowing execption :</p>
<pre><code>org.springframework.data.cassandra.CassandraInvalidQueryException:Query; 
CQL[com.datastax.oss.driver.internal.core.cql.DefaultSimpleStatement@c78c2039]; 
Cannot execute this query as it might involve data filtering and thus may have unpredictable performance. 
If you want to execute this query despite the performance unpredictability, use ALLOW FILTERING; 

nested exception is 
 com.datastax.oss.driver.api.core.servererrors.InvalidQueryException:
Cannot execute this query as it might involve data filtering 
and thus may have unpredictable performance. 
If you want to execute this query despite the performance unpredictability, use ALLOW FILTERING
</code></pre>
<p>By the way, I know that I can use query methods or even findById method, but actually I am just experimenting with cql quires and try to learn about it.</p>
<p>--update
The domain object</p>
<pre><code>@Table(value = &quot;magazine_name&quot;)
@Data
@Builder
public class MagazineName {

    @PrimaryKeyColumn(name = &quot;magazine_id&quot;, ordinal = 0, type = PrimaryKeyType.PARTITIONED)
    private String magazineId;
    @PrimaryKeyColumn(name = &quot;magazine_name&quot;, ordinal = 1, type = PrimaryKeyType.CLUSTERED)
    private String name;
}
</code></pre>
",<cassandra><spring-data><cql><spring-data-cassandra>,"<p>I defined the table exactly like yours and here is my repository. I can query without error.</p>
<p><strong>1. My Repository</strong></p>
<pre><code>public interface IMagazineDao extends CrudRepository&lt;Magazine, String&gt; {

    @Query(&quot;SELECT * from magazine_name where magazine_id = ?0&quot;)
    Magazine findMagazineCQlQuery(String id);


}
</code></pre>
<p><strong>2. Application</strong></p>
<pre><code>@SpringBootApplication
public class Application implements CommandLineRunner {
    
    @Autowired
    private IMagazineDao magazineDao;

    @Override
    public void run(String... args) throws Exception {
        this.magazineDao.save(new Magazine(&quot;magazine1&quot;, &quot;name&quot;, (short) 1));
        this.magazineDao.findMagazineCQlQuery(&quot;magazine1&quot;);
    }

    public static void main(String[] args) {
        SpringApplication.run(Application.class, args);
    }

}
</code></pre>
<p><strong>3. Magazine class</strong></p>
<pre><code>@Table(value = &quot;magazine_name&quot;)
public class Magazine {

    @PrimaryKeyColumn(name = &quot;magazine_id&quot;, ordinal = 0, type = PrimaryKeyType.PARTITIONED)
    private String magazineId;

    @PrimaryKeyColumn(name = &quot;magazine_name&quot;, ordinal = 1, type = PrimaryKeyType.CLUSTERED)
    private String name;

    @Column
    private Short frequency;

    public Magazine() {

    }
    public Magazine(String magazineId, String name, Short frequency) {
        this.magazineId = magazineId;
        this.name = name;
        this.frequency = frequency;
    }

    public String getMagazineId() {
        return magazineId;
    }

    public void setMagazineId(String magazineId) {
        this.magazineId = magazineId;
    }

    public String getName() {
        return name;
    }

    public void setName(String name) {
        this.name = name;
    }

    public Short getFrequency() {
        return frequency;
    }

    public void setFrequency(Short frequency) {
        this.frequency = frequency;
    }
}
</code></pre>
",['table']
62634335,62635052,2020-06-29 08:43:49,Cassandra IN clause for non primary key column,"<p>I want to use the IN clause for the non-primary key column in Cassandra. Is it possible? if it is not is there any alternate or suggestion?</p>
",<cassandra><cql>,"<p><strong>Three possible solutions</strong></p>
<ul>
<li>Create a secondary index. This is not recommended due to performance problems.</li>
<li>See if you can designate that column in the existing table as part of the primary key</li>
<li>Create another denormalised table that table is optimised for your query. i.e data model by query pattern</li>
</ul>
<p><strong>Update:</strong></p>
<p>And also even after you move that to primary key, operations with IN clause can be further optimised. I found this <a href=""https://stackoverflow.com/questions/62643342/cassandra-lookup-by-list-of-primary-keys-in-java"">cassandra lookup by list of primary keys in java</a> very useful</p>
",['table']
62634423,62634853,2020-06-29 08:48:16,Where does Cassandra reside in the CAP theorem?,"<p>Datastax course says that Cassandra is <strong>availability/partition tolerance</strong>. However, according to this <a href=""https://docs.datastax.com/en/ddac/doc/datastax_enterprise/dbInternals/dbIntConfigConsistency.html"" rel=""nofollow noreferrer"">document</a> it can be tuned to be strong consistency (i.e. CP) by setting <strong>W + R &gt; RF</strong>, where <strong>W</strong> is the write consistency level, <strong>R</strong> is the read consistency level, and <strong>RF</strong> is the replication factor.</p>
<p><a href=""https://i.stack.imgur.com/AFejw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AFejw.png"" alt=""Screenshot from datastax's course"" /></a></p>
",<cassandra>,"<p><strong><code>Tuneable to strong consistency for single partition</code></strong></p>
<ul>
<li>It can be tuned to strong consistency for documents in <code>single partition</code>. So if you statements belong to different partitions (note same partition key and different table is still different partition), you cannot tune it for strong consistency. So Cassandra has its upper bound to it's strong consistency unlike in RDBMS where you can update multiple records in different tables or different rows in same table atomically.</li>
</ul>
<p><strong><code>Tuning for higher consistency makes you lose some of the </code>Availability<code>and</code>Partition Tolerance`</strong></p>
<ul>
<li>When you use <code>hinted handoff</code>, it is almost on the <code>AP</code> axis as it is always available to write even with network partitions. But as soon you start tuning for higher consistency, clients have to wait for writes or reads until it is written to enough replicas /read from enough replicas to satisfy the requested consistency. So you are losing bit of <code>availability</code> and <code>partition tolerance</code></li>
</ul>
<p><strong>Summary</strong></p>
<p>You can configure it for maximum <code>availability</code> and <code>partition tolerance</code> but you cannot configure for much stronger <code>consistency</code>. So Cassandra lies in AP axis in CAP</p>
",['table']
62663166,62738668,2020-06-30 17:38:49,Better way to define UDT's in Cassandra database,"<p>We are trying to remove 2 columns in a table with 3 types and make them as UDT instead of having those 2 as columns. So we came up with below two options. I just wanted to understand if there are any difference in these two UDT in Cassandra database?</p>
<p><strong>First option is:</strong></p>
<pre><code>CREATE TYPE test_type (
    cid int,
    type text,
    hid int
);
</code></pre>
<p>and then using like this in a table definition</p>
<pre><code>test_types set&lt;frozen&lt;test_type&gt;&gt;,
</code></pre>
<p>vs</p>
<p><strong>Second option is:</strong></p>
<pre><code>CREATE TYPE test_type (
    type text,
    hid int
);
</code></pre>
<p>and then using like this in a table definition</p>
<pre><code>test_types map&lt;int, frozen&lt;test_type&gt;
</code></pre>
<p>So I am just curious which one is a preferred option here for performance related or they both are same in general?</p>
",<cassandra><datastax-java-driver><user-defined-types>,"<p>It's really depends on how will you use it - in the first solution you won't able to select element by <code>cid</code>, because to access the <code>set</code> element you'll need to specify the full UDT value, with all fields.</p>
<p>The better solution would be following, assuming that you have only one collection column:</p>
<pre><code>CREATE TYPE test_type (
    type text,
    hid int
);

create table test (
  pk int, 
  cid int
  udt frozen&lt;test_type&gt;,
  primary key(pk, cid)
);
</code></pre>
<p>In this case:</p>
<ul>
<li>you can easily select individual element by specifying the full primary key. The ability to select individual elements from <code>map</code> is coming only in Cassandra 4.0. See the <a href=""https://issues.apache.org/jira/browse/CASSANDRA-7396"" rel=""nofollow noreferrer"">CASSANDRA-7396</a>.  Until that you'll need to get full map back, even if you need one element, and this will limit you on the size of the map</li>
<li>you can even select the range of the values, using the range query</li>
<li>you can get all values by specifying only partition key (<code>pk</code> in this example)</li>
<li>you can select multiple non-consecutive values by doing <code>select * from test where pk = ... and cid in (..., ..., ...);</code></li>
</ul>
<p>See the &quot;Check use of collection types&quot; section in the <a href=""https://docs.datastax.com/en/landing_page/doc/landing_page/dataModel.html"" rel=""nofollow noreferrer"">data model checks best practices doc</a>.</p>
",['table']
62666179,62672575,2020-06-30 21:08:55,java.lang.ClassCastException: java.util.ArrayList cannot be cast to java.util.UUID exception with cassandra?,"<p>I have a spring boot java application that talks to cassandra .
However one of my queries is failing .</p>
<pre><code> public class ParameterisedListItemRepository {
    
        private PreparedStatement findByIds;
    
        public ParameterisedListItemRepository(Session session, Validator validator, ParameterisedListMsisdnRepository parameterisedListMsisdnRepository ) {
            this.findByIds =  session.prepare(&quot;SELECT * FROM mep_parameterisedListItem WHERE id IN ( :ids )&quot;);
    
    
    }
    public List&lt;ParameterisedListItem&gt; findAll(List&lt;UUID&gt; ids){
        
        List&lt;ParameterisedListItem&gt; parameterisedListItemList = new ArrayList&lt;&gt;();

        BoundStatement stmt =this.findByIds.bind();
        stmt.setList(&quot;ids&quot;, ids);
        session.execute(stmt)
            .all()
            .stream()
            .map(parameterisedListItemMapper)
            .forEach(parameterisedListItemList::add);
        return parameterisedListItemList;
    }
    }
</code></pre>
<p>the following is the stack trace</p>
<pre><code>java.lang.ClassCastException: java.util.ArrayList cannot be cast to java.util.UUID
    at com.datastax.driver.core.TypeCodec$AbstractUUIDCodec.serialize(TypeCodec.java:1626)
    at com.datastax.driver.core.AbstractData.setList(AbstractData.java:358)
    at com.datastax.driver.core.AbstractData.setList(AbstractData.java:374)
    at com.datastax.driver.core.BoundStatement.setList(BoundStatement.java:681)
    at com.openmind.primecast.repository.ParameterisedListItemRepository.findAll(ParameterisedListItemRepository.java:128)
    at com.openmind.primecast.repository.ParameterisedListItemRepository$$FastClassBySpringCGLIB$$46ffc15e.invoke(&lt;generated&gt;)
    at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:204)
    at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:738)
    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
    at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:92)
    at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
    at org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:673)
    at com.openmind.primecast.repository.ParameterisedListItemRepository$$EnhancerBySpringCGLIB$$b2db3c41.findAll(&lt;generated&gt;)
    at com.openmind.primecast.service.impl.ParameterisedListItemServiceImpl.findByParameterisedList(ParameterisedListItemServiceImpl.java:102)
    at com.openmind.primecast.web.rest.ParameterisedListItemResource.getParameterisedListItemsByParameterisedList(ParameterisedListItemResource.java:94)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
</code></pre>
<p>Any idea what is going wrong. I know this query is the problem</p>
<pre><code>SELECT * FROM mep_parameterisedListItem WHERE id IN ( :ids )
</code></pre>
<p>any idea how I can change the findAll function to achieve the query?</p>
<p>this is the table definition</p>
<pre><code>CREATE TABLE &quot;Openmind&quot;.mep_parameterisedlistitem (
    id uuid PRIMARY KEY,
    data text,
    msisdn text,
    ordernumber int,
    parameterisedlist uuid
) WITH COMPACT STORAGE;
</code></pre>
<p>Thank you.</p>
",<cassandra><datastax><datastax-java-driver>,"<p>Without knowing the table schema, my guess is that a change was made to the table so the schema no longer match the bindings in the prepared statement.</p>
<p>A big part of the problem is your query with <code>SELECT *</code>. Our recommendation for best practice is to <em>explicitly</em> name all the columns you're retrieving from the table. By specifying the columns in your query, you avoid surprises when the table schema changes.</p>
<p>In this instance, either a new column was added or an old column was dropped. With the cached prepared statement, it was expecting one column type and got another -- the <code>ArrayList</code> doesn't match <code>UUID</code>.</p>
<p>The solution is to re-prepare the statement and name all the columns. Cheers!</p>
",['table']
62690912,62691403,2020-07-02 07:14:44,cannot insert list of maps to Apache Cassandra,"<p>This is my schema</p>
<pre><code>CREATE TABLE app_category_agg (
    category text,
    app_count int,
    sp_count int,
    subscriber_count int,
    window_revenue bigint,
    top_apps frozen &lt;list&lt;map&lt;text,int&gt;&gt;&gt;,
    PRIMARY KEY (category)
);
</code></pre>
<p>insert query generated by java is in this form</p>
<pre><code>INSERT INTO analytics_info.app_category_agg (category,app_count,sp_count,subscriber_count, window_revenue,top_apps) VALUES ('Entertainment',3,2,65,1620,[{APP_992984515=30}, {APP_991415478=23}, {APP_999095235=12}]);
</code></pre>
<blockquote>
<p>issue is with '=' sign in the map objects in the list of maps. How can
I insert it correctly? in java.</p>
</blockquote>
",<java><cassandra><datastax>,"<p>map value should be specified as <code>{key1:val1, key2:val2}</code> (see <a href=""https://docs.datastax.com/en/cql-oss/3.3/cql/cql_using/useInsertMap.html"" rel=""nofollow noreferrer"">docs</a>), in your case it should be a</p>
<pre><code>[{'APP_992984515':30}, {'APP_991415478':23}, {'APP_999095235':12}]
</code></pre>
<p>but really, you shouldn't generate literal query yourself, as it could be a complex task - you need to handle quotes correctly, etc.</p>
<p>More appropriate will be to use <a href=""https://docs.datastax.com/en/developer/java-driver/3.8/manual/statements/prepared/"" rel=""nofollow noreferrer"">prepared statements, and bind values</a> - in this case, all syntax will be handled by driver.</p>
<p>Another possibility is to use Object Mapper (doc for <a href=""https://docs.datastax.com/en/developer/java-driver/3.8/manual/object_mapper/"" rel=""nofollow noreferrer"">driver 3.x</a>, <a href=""https://docs.datastax.com/en/developer/java-driver/4.7/manual/mapper/"" rel=""nofollow noreferrer"">driver 4.x</a>) - in this case, you may work with rows in the table using the Java objects, all conversion will be handled by driver as well.</p>
",['table']
62734242,62741421,2020-07-04 20:41:51,How to properly model data in Apache Cassandra to allow querying by two different fields that are not unique,"<p>I am developing a simple API in Go using Apache Cassandra and I am wondering what is the best way to represent data that I have.</p>
<p>I have the following struct in Go.</p>
<pre><code>type Message struct {
    Id          gocql.UUID `json:&quot;id&quot;`
    Email       string     `json:&quot;email&quot;`
    Title       string     `json:&quot;title&quot;`
    Content     string     `json:&quot;content&quot;`
    Number      int64      `json:&quot;number&quot;`
    DateCreated time.Time  `json:&quot;dateCreated&quot;`
}
</code></pre>
<p>What is the best way to create a data structure in Cassandra to allow querying by Email and Number which are both <strong>not</strong> unique (Only Id is unique. There can be multiple messages that have the same email and/or number)? Should I create two separate tables to allow querying by those two fields?</p>
<p>For querying by Id I would create table like this:</p>
<pre><code>USE some_keyspace;

CREATE TABLE IF NOT EXISTS messages
(
    id           UUID,
    email        TEXT,
    title        TEXT,
    content      TEXT,
    number.      BIGINT,
    date_created TIMESTAMP,
    PRIMARY KEY (id)
);
</code></pre>
",<database><go><cassandra><nosql><data-modeling>,"<p>The <em>best</em> way?  Create a table for each query you want to serve.  Build the PRIMARY KEY definitions with <em>both</em> the column you want to query by and <code>id</code> (to ensure uniqueness):</p>
<pre><code>CREATE TABLE IF NOT EXISTS messages_by_email (
    id           UUID,
    email        TEXT,
    title        TEXT,
    content      TEXT,
    number      BIGINT,
    date_created TIMESTAMP,
    PRIMARY KEY (email,id));

CREATE TABLE IF NOT EXISTS messages_by_number (
    ....
    PRIMARY KEY (number,id));
</code></pre>
<p>When you write a row to these tables, use <code>BATCH</code> to ensure atomicity.</p>
<pre><code>BEGIN BATCH
    INSERT INTO messages (id,email,number,title,content,date_created)
        VALUES (uuid(),'aaron@dot.com',1,'Hi','Cassandra rocks!',toTimestamp(now()));
    INSERT INTO messages_by_email (id,email,number,title,content,date_created)
        VALUES (uuid(),'aaron@dot.com',1,'Hi','Cassandra rocks!',toTimestamp(now()));
    INSERT INTO messages_by_number (id,email,number,title,content,date_created)
        VALUES (uuid(),'aaron@dot.com',1,'Hi','Cassandra rocks!',toTimestamp(now()));
APPLY BATCH;
</code></pre>
",['table']
62772730,62789872,2020-07-07 10:01:34,Spark java filter isin method or something else?,"<p>I have around 2 billions of rows in my cassandra database which I filter with the isin method based on an experimentlist with 4827 Strings, as shown below. However, I noticed that after the distinct command I have only 4774 unique rows. Any ideas why 53 are missing? Does the isin method has a threshold/limitations? I have double and triple checked the experimentlist, it does have 4827 Strings, and also the other 53 strings do exist in the database as I can query them with cqlsh. Any help much appreciated!</p>
<pre><code>Dataset&lt;Row&gt; df1 = sp.read().format(&quot;org.apache.spark.sql.cassandra&quot;)
                .options(new HashMap&lt;String, String&gt;() {
                    {
                        put(&quot;keyspace&quot;, &quot;mdb&quot;);
                        put(&quot;table&quot;, &quot;experiment&quot;);
                    }
                })
                .load().select(col(&quot;experimentid&quot;)).filter(col(&quot;experimentid&quot;).isin(experimentlist.toArray()));
List&lt;String&gt; tmplist=df1.distinct().as(Encoders.STRING()).collectAsList();
   
System.out.println(&quot;tmplist &quot;+tmplist.size());
</code></pre>
",<java><apache-spark><cassandra><spark-cassandra-connector>,"<p>Regarding the actual question about &quot;missing data&quot; - there could be problems when your cluster has missing writes, and repair isn't done regularly.  Spark Cassandra Connector (SCC) reads data with consistency level <code>LOCAL_ONE</code>, and may hit nodes without all data.  You can try to set consistency level to <code>LOCAL_QUORUM</code> (via <code>--conf spark.cassandra.input.consistency.level=LOCAL_QUORUM</code>), for example, and repeat the experiment, although it's better to make sure that data is repaired.</p>
<p>Another problem that you have is that you're using the <code>.isin</code> function - it's translating into a query <code>SELECT ... FROM table WHERE partition_key IN (list)</code>.  See the execution plan:</p>
<pre><code>scala&gt; import org.apache.spark.sql.cassandra._
import org.apache.spark.sql.cassandra._
scala&gt; val data = spark.read.cassandraFormat(&quot;m1&quot;, &quot;test&quot;).load()
data: org.apache.spark.sql.DataFrame = [id: int, m: map&lt;int,string&gt;]

scala&gt; data.filter($&quot;id&quot;.isin(Seq(1,2,3,4):_*)).explain
== Physical Plan ==
*Scan org.apache.spark.sql.cassandra.CassandraSourceRelation [id#169,m#170] PushedFilters: [*In(id, [1,2,3,4])], ReadSchema: struct&lt;id:int,m:map&lt;int,string&gt;&gt;
</code></pre>
<p>This query is very inefficient, and put an additional load to the node that performs query.  In the SCC 2.5.0, there are some optimizations around that, but it's better to use so-called &quot;Direct Join&quot; that was also <a href=""https://www.datastax.com/blog/2020/05/advanced-apache-cassandra-analytics-now-open-all"" rel=""nofollow noreferrer"">introduced in the SCC 2.5.0</a>, so SCC will perform requests to specific partition keys in parallel - that's more effective and put the less load to the nodes. You can use it as following (the only difference that I have it as &quot;DSE Direct Join&quot;, while in OSS SCC it's printed as &quot;Cassandra Direct Join&quot;):</p>
<pre><code>scala&gt; val toJoin = Seq(1,2,3,4).toDF(&quot;id&quot;)
toJoin: org.apache.spark.sql.DataFrame = [id: int]

scala&gt; val joined = toJoin.join(data, data(&quot;id&quot;) === toJoin(&quot;id&quot;))
joined: org.apache.spark.sql.DataFrame = [id: int, id: int ... 1 more field]

scala&gt; joined.explain
== Physical Plan ==
DSE Direct Join [id = id#189] test.m1 - Reading (id, m) Pushed {}
+- LocalTableScan [id#189]
</code></pre>
<p>This direct join optimization needs to be explicitly enabled as <a href=""https://github.com/datastax/spark-cassandra-connector/blob/b2.5/doc/14_data_frames.md#special-cassandra-catalyst-rules-since-scc-25"" rel=""nofollow noreferrer"">described in the documentation</a>.</p>
",['table']
62817991,62837946,2020-07-09 15:08:19,How to choose my Cassandra key for correct default sorting?,"<p>My table <code>samples</code> consists of columns like this:</p>
<pre><code>id : uuid
created : timestamp
device : ascii
reading : float
</code></pre>
<p>Most of my queries will be to fetch the most recent <code>n</code> samples across all devices, so I would like this to be the default sorting:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT * FROM samples LIMIT 1024
</code></pre>
<p>I would also like to be able to efficiently fetch the most recent <code>n</code> samples for a given <em>device</em>:</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT * FROM samples WHERE device = 'abc' LIMIT 1024
</code></pre>
<p>How should I design my partition key to achieve this?</p>
",<cassandra><cql>,"<p>With Cassandra, it is recommended to take a query-based modeling approach.  To this end, it is not uncommon to have one table for each query which needs to be supported.</p>
<pre><code>SELECT * FROM samples LIMIT 1024
</code></pre>
<p>For this first query the biggest problem I see right away, is that there is no <code>WHERE</code> clause.  This will cause Cassandra to have to check every node to build a result set; definitely don't want that happening.  But it sounds like you care most about recent data, or data by a specific date.  To do that, we'll need to create a partition key or &quot;bucket&quot; based on a date/time component.</p>
<p><strong>Note, this is also required because data retrieved from multiple partitions cannot be sorted.</strong></p>
<p>For this, the cardinality of your data is important.  You're selecting the top 1024, so is it common to get that many in a single day?  Or is that over a week?  For now, I'll assume &quot;day,&quot; and add a <code>day_bucket</code> column.</p>
<pre><code>CREATE TABLE samples_by_day (
  id uuid,
  created timestamp,
  device ascii,
  reading float,
  day_bucket bigint,
  PRIMARY KEY (day_bucket,created,id)
) WITH CLUSTERING ORDER BY (created DESC, id ASC);
</code></pre>
<p>This primary key definition will partition data by day (ex: 20200710).  Within those partitions, data will be ordered by <code>created</code> in descending order (to get the most-recent on top).  The <code>id</code> column is added to ensure uniqueness.  This will support the following query:</p>
<pre><code>SELECT * FROM samples_by_day
WHERE day_bucket = 20200710 LIMIT 1024;
</code></pre>
<p>For multiple days, you could run multiple queries.  You could even &quot;bucket&quot; by week or month, assuming that doesn't push the bounds of the 2 billion cells/partition limit.</p>
<p>Supporting this query:</p>
<pre><code>SELECT * FROM samples
WHERE device = 'abc' LIMIT 1024;
</code></pre>
<p>...is <em>much</em> easier.</p>
<pre><code>CREATE TABLE samples_by_device (
  id uuid,
  created timestamp,
  device ascii,
  reading float,
  day_bucket bigint,
  PRIMARY KEY (device,created,id)
) WITH CLUSTERING ORDER BY (created DESC, id ASC);
</code></pre>
<p>This works, but will likely run into the problem of &quot;unbound row growth.&quot;  Basically, if device samples keep being added for each device, the partition size will eventually max-out.  So adding <code>day_bucket</code> (or whatever time bucket works for you) as an additional partition key is probably necessary:</p>
<pre><code>PRIMARY KEY ((device,day_bucket),created,id)
</code></pre>
<p>With this change, the query also needs to change:</p>
<pre><code>SELECT * FROM samples_by_device
WHERE device = 'abc' AND day_bucket = 20200710 LIMIT 1024;
</code></pre>
",['table']
62832261,62833775,2020-07-10 10:23:08,"Cassandra nodejs driver, how to update data correctly","<p>I am new to Cassandra and I don't quite know if my data model is correct. I have tried to create it based on the queries I want to make in my application. I want to create and update book objects and I want to find books by author and by publish date. I am using the DataStax Node.js Driver for Cassandra (using Typescript) and here is my schema so far:</p>
<pre><code>CREATE TABLE IF NOT EXISTS books_by_author (
    author_id UUID,
    book_id UUID,
    book_name TEXT,
    date_published TIMESTAMP,
    PRIMARY KEY (author_id, date_published);

CREATE TABLE IF NOT EXISTS books (
    book_id uuid PRIMARY KEY,
    book_name text,
    book_description TEXT,
    date_published TIMESTAMP,
    author_id uuid,
    author_name TEXT,
 + many more columns for book details);
</code></pre>
<p>Making author_id and date_published as primary key I was able to make queries with the nodejs driver and with help from the DataStax documentation:</p>
<pre><code>const q = cassandra.mapping.q;

const results = await this.bookMapper.find(
          {
            authorId: '1', datePublished: q.and(q.gte(start), q.lte(end)), // given timerange for publish date, works fine
          },
          docInfo,
          options);
</code></pre>
<p>The above code works well; I can get the list of books by author and by specifying a date range when publised. The bookMapper is mapping both tables (books_by_author, books) so I am using it to make all my DB queries.</p>
<p>Then I ran into issues. I created a book in my application but I gave it the wrong publish date and I would like to change that. So, to see how it could be done, I created a unit test that saves a book to the DB, then tries to use bookMapper.update to update the book's datePublished property. Here's some pseudo code on what I tried to achieve:</p>
<pre><code>const bookId = '123uuid';

const existingBook = new Book({
    id: bookId,
    name: 'The Book',
    datePublished: '2020-07-03T13:00:00.000Z',
    description: 'Book description',
    author: {
      id: '1',
      name: 'A. Author',
    }
});
... // insert existingBook to DB and read book details from DB using bookMapper.get({bookId})

const modifiedBook = new Book({
    id: bookId,
    name: 'The Book',
    datePublished: '2020-07-02T13:00:00.000Z', // modified publish date
    description: 'Modified book description', // modified the book description as well
    author: {
      id: '1',
      name: 'A. Author',
    }
});

await this.bookMapper.update(modifiedBook); // update the book

await this.bookMapper.get({bookId}); // returns the book with data from existingBook, not modifiedBook

await this.bookMapper.find(
          {
            authorId: '1', datePublished: q.and(q.gte(start), q.lte(end)),
          },
          docInfo,
          options); 
// query with author id, returns a list of 2 books, both the existingBook and modifiedBook ??
</code></pre>
<p>As you can see, the update actually created a new book row to the DB and now I have 2 books instead of 1. And I have no idea what is the correct way of updating that data. I tried to use batching:</p>
<pre><code>let changes = [];
changes.push(this.bookMapper.batching.remove(exisitingBook));
changes.push(this.bookMapper.batching.insert(modifiedBook));
await this.mapper.batch(changes);

const book = await this.bookMapper.get({bookId});
--&gt; book is null!
</code></pre>
<p>Using batching to remove and insert seems to work so that remove is the last call to DB, it doesn't matter in which order I add those statements to my changes array, and it removes the book causing my last get statement to return null.</p>
<p>I wanted to use batching to make the operation atomic. I don't want to end up in a situation where I first delete the existing book and then insert the new book in separate DB calls without batching because if some error occurs after delete but before insert, then I will have lost my book data from the DB.</p>
<p>My question: What is the correct way to update the book data when the updated property happens to be part of a primary key? Thank you.</p>
",<cassandra><datastax-node-driver>,"<p>This is a well known &quot;feature&quot; of Cassandra - in the batch the both statements are getting the same timestamp, so the <code>DELETE</code> operation wins over the <code>INSERT</code>.  The only solution to fix that is to explicitly set timestamps for every operation, with timestamp for <code>DELETE</code> lower than <code>INSERT</code>.  I'm not Node.js developer, so it how it should be looking in pseudo-code/CQL (Node.js mapper should support setting custom timestamp on statements):</p>
<pre><code>TS=currentTimestampInMicroseconds
BEGIN BATCH
DELETE FROM table USING TIMESTAMP TS-1 WHERE PK = ... US;
INSERT INTO table (....) VALUES (....) USING TIMESTAMP TS;
APPLY BATCH;
</code></pre>
",['table']
62868981,62870389,2020-07-13 03:50:01,How To Improve Multiple JOIN Performance For SQL Database,"<p>Suppose I have a Spotify-Liked app and has schema as below:</p>
<p><a href=""https://i.stack.imgur.com/bHDcO.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bHDcO.jpg"" alt=""enter image description here"" /></a></p>
<p>After user login, and click on My Songs button, a query needs to be returned all purchased song for this user.</p>
<p>According to the above schema, I need to write a SQL like:</p>
<pre><code>select s.name, al.name, ar.name, g.genres
from users u 
join purchases p on u.id = p.userid
join purchaseitem pi on p.id= pi.purchaseid
join songs s on pi.itemid = s.id
join albums al on al.id = s.albumid
join genres g on g.id = s.genreid
join artists ar on ar.id = al.artisted
</code></pre>
<p>This ugly and multiple joins query may be causing significant performance issues.</p>
<ol>
<li><p>What enhancement we can do to the query itself?</p>
</li>
<li><p>If nothing we can do on the SQL query, how the database schema can be redesign to enhance this capability?</p>
</li>
<li><p>If we are able to partition SQL database, i.e, indexing, will it help improve performance?</p>
</li>
<li><p>If performance is the only concern, will a NoSQL database such as Cassandra or MongoDB be a better choice?</p>
</li>
</ol>
",<sql-server><mongodb><join><cassandra>,"<p>You can denormalize purchase item table and keep all other data (album name, artist name, etc) in purchaseitem table. Once the purchase is done then data will not change.</p>
<p>E.g What if you change artist name after purchase. Then later your reports will have a new artist name but not the name of the artist exists in the time of purchase.</p>
<p>Dont you need to keep purchase history in the system. ?</p>
<p>Then you can simplify this quarry but y<strong>ou have to think about your overall design</strong> this may be a one quarry in your solution. When your redundancing the data, you have to make sure that you have correct controls in the system.  There is no hard and fast rule to normalize all data in a relational database.</p>
<p>You can improve read performance by denormalizing tables but it impacts the insert and updates.  You need to balance these with your requirement</p>
<p>Going for NOSQL will not be a silver bullet. You can handle millions of records in a relational database system with a proper design. Also, the microservices pattern can be used for scalability but it will complex your design and technology stack.</p>
",['table']
63015618,63191070,2020-07-21 13:29:59,Perfomance related question on spark + cassandra (JAVA code),"<p>i am using cassandra as my dumping ground on which i have multiple jobs running to process the data and update different system. below are the job related filters</p>
<p>Job 1. data filter based on active_flag and update_date_time and expiry_time and process the filtered data.</p>
<p>Job 2. data filter based on update_date_time process the data</p>
<p>Job 3. data filter based on created_date_time and active flag</p>
<p>db columns on which where condition would run are (one or many columns in one query)</p>
<ol>
<li>Active -&gt; yes/no</li>
<li>created_date -&gt; timestamp</li>
<li>expiry_time -&gt; timestamp</li>
<li>updated_date -&gt; timestamp</li>
</ol>
<p>My question on these conditions :-</p>
<ol>
<li><p>how should i form my cassandra primary key? as i dont see any way to acheive uniqueness on this (id is present but thats not required for me to process data).</p>
</li>
<li><p>do i even need the primary key if i use the filtering on spark code using table scan?</p>
</li>
</ol>
<p>considering this for millions of records processing.</p>
",<apache-spark><cassandra><spark-cassandra-connector>,"<p>Answering to your question - you need to have a primary key, even if it consists only of the partition key :-)</p>
<p>More detailed answer really depends on how often these jobs are running, how much data overall, how many nodes in the cluster, what hardware is used, etc.  Usually, we're trying to push as much filtering to Cassandra as possible, so it will return only relevant data, not everything.  The most effective this filtering happens on the first clustering column, for example, if I want to process only newly created entries, then I can use the table with following structure:</p>
<pre><code>create table test.test (
  pk int,
  tm timestamp,
  c2 int,
  v1 int,
  v2 int,
  primary key(pk, tm, c2));
</code></pre>
<p>and then I can fetch only newly created entries by using:</p>
<pre class=""lang-scala prettyprint-override""><code>import org.apache.spark.sql.cassandra._
val data = spark.read.cassandraFormat(&quot;test&quot;, &quot;test&quot;).load()
val filtered = data.filter(&quot;tm &gt;= cast('2019-03-10T14:41:34.373+0000' as timestamp)&quot;)
</code></pre>
<p>Or I can fetch entries in the given time period:</p>
<pre class=""lang-scala prettyprint-override""><code>val filtered = data.filter(&quot;&quot;&quot;ts &gt;= cast('2019-03-10T14:41:34.373+0000' as timestamp)
  AND ts &lt;= cast('2019-03-10T19:01:56.316+0000' as timestamp)&quot;&quot;&quot;)
</code></pre>
<p>The effect of the filter pushdown could be checked by executing <code>explain</code> on the dataframe, and checking the <code>PushedFilters</code> section - conditions that are marked with <code>*</code> will be executed on Cassandra side...</p>
<p>But it's not always possible to design tables to match all queries, so you'll need to design primary key for jobs that are executed most often. In your case, <code>update_date_time</code> could be a good candidate for that, but if you put it as clustering column, then you'll need to take care when updating it - you'll need to perform change as batch, something like this:</p>
<pre><code>begin batch
delete from table where pk = ... and update_date_time = old_timestamp;
insert into table (pk, update_date_time, ...) values (..., new_timestamp, ...);
apply batch;
</code></pre>
<p>or something like this.</p>
",['table']
63068006,63068254,2020-07-24 06:24:30,Cassandra configuration config by cqlsh,"<p>Cassandra version: <code>3.9</code>, CQLSH version: <code>5.0.1</code></p>
<p>Can I query Cassandra configuration (<code>cassandra.yaml</code>) using <code>cqlsh</code>?</p>
",<cassandra><cql><cassandra-3.0><cqlsh>,"<p>No, it's not possible in your version.  It's possible only starting with Cassandra 4.0 that has so-called virtual tables, and there is a special table for configurations: <code>system_views.settings</code>:</p>
<pre><code>cqlsh:test&gt; select * from system_views.settings ;
 name                                            | value
-------------------------------------------------+-------
     transparent_data_encryption_options_enabled | false
   transparent_data_encryption_options_iv_length |    16
                                   trickle_fsync | false
                    trickle_fsync_interval_in_kb | 10240
                  truncate_request_timeout_in_ms | 60000
....
</code></pre>
<p>You can find more information on the virtual tables in the <a href=""https://thelastpickle.com/blog/2019/03/08/virtual-tables-in-cassandra-4_0.html"" rel=""nofollow noreferrer"">following blog post from TLP</a>.</p>
<p>In the meantime, you can access configuration parameters via JMX.</p>
","['trickle_fsync', 'table']"
63072179,63072663,2020-07-24 10:56:15,"Adding a new cassandra node to an existing cluster, with a different snitch","<p>I have a cassandra cluster with 5 nodes that is using EC2 snitch but for the new node I want to add I want to use GossipingPropertyFileSnitch. Is it okay to have this node with a different snitch, will it cause any impact to the schema or schema versions?</p>
",<database><cassandra><datastax>,"<p>All nodes in a cluster should use the same snitch since it is critical in determining the cluster topology and position of the replicas (to avoid them all being on the same rack for example.).</p>
<p>Just as an experiment, I changed a node in a 3 node sandbox cluster to use a different snitch and while it did start up, when running nodetool status on the 2 nodes with different snitches, they reported very different topologies - as you would know, this is not a good thing at all.</p>
<p>If you wish to move the whole cluster to GossipingPropertyFileSnitch, then there is a documented process on how to change the snitch of a cluster:
<a href=""https://docs.datastax.com/en/dse/6.8/dse-admin/datastax_enterprise/operations/opsSwitchSnitch.html"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/dse/6.8/dse-admin/datastax_enterprise/operations/opsSwitchSnitch.html</a></p>
<p>You will find that there are versions of that page for different versions of DSE. You would change the snitch first via the process, then add the additional node.</p>
",['rack']
63117195,63119530,2020-07-27 14:12:57,I have declared primary using PartitionKey annotation but still getting Entity Order does not declare a primary key,"<p>I have the following POJO:</p>
<pre><code>@Entity
public class Order {    
    
    @Column(name = &quot;id&quot;)
    @PartitionKey
    private String id;

    @Column(name = &quot;customer_id&quot;)
    private String customerId;

    @Column(name = &quot;loyalty_id&quot;)
    private String loyaltyId;

    @Column(name = &quot;customer_email&quot;)
    private String customerEmail;

    public Order() {

    }
    public String getId() {
       return id;
    }

    public void setId(String id) {
        this.id = id;
    }
    ... getters and setters
}
</code></pre>
<p>Getting the exception at the following code:</p>
<pre><code>            CqlSession session = CqlSession.builder().build();
            OrderMapper mapper = new OrderMapperBuilder(session).build();
            orderDao = mapper.orderDao(CqlIdentifier.fromCql(connectionManager.getSession().getLoggedKeyspace()));
</code></pre>
<p>The definition of connectionManager is here: <a href=""https://pastebin.com/b3GKJuV6"" rel=""nofollow noreferrer"">https://pastebin.com/b3GKJuV6</a></p>
<p>The exception is as:</p>
<pre><code>    Entity Order does not declare a primary key
    com.datastax.oss.driver.api.mapper.MapperException: Entity Order does not declare a primary key
        at com.datastax.oss.driver.api.mapper.MapperException.copy(MapperException.java:44)
        at com.datastax.oss.driver.internal.core.util.concurrent.CompletableFutures.getUninterruptibly(CompletableFutures.java:149)
    ...
</code></pre>
<p>I am implementing by following the documentation here: <a href=""https://docs.datastax.com/en/developer/java-driver/4.2/manual/mapper/"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/developer/java-driver/4.2/manual/mapper/</a> . What could be the possible cause of this?</p>
<p>EDIT:- Adding the schema definition:</p>
<pre><code>    CREATE TABLE order_keyspace.order (
        id text PRIMARY KEY,
        customer_email text,
        customer_id text,
        loyalty_id text
    ) WITH bloom_filter_fp_chance = 0.01
        AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
        AND comment = ''
        AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
        AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
        AND crc_check_chance = 1.0
        AND dclocal_read_repair_chance = 0.1
        AND default_time_to_live = 0
        AND gc_grace_seconds = 864000
        AND max_index_interval = 2048
        AND memtable_flush_period_in_ms = 0
        AND min_index_interval = 128
        AND read_repair_chance = 0.0
        AND speculative_retry = '99PERCENTILE';
</code></pre>
",<java><cassandra><datastax-java-driver>,"<p>Full Edit :</p>
<p>There are a few things going on here which confuses me, and the only way I could reproduce the error was to break the getter/setter which doesn't appear to be wrong on your code, but there is some copy/paste conversions that are happening. What I can so far see as follows:</p>
<ul>
<li>The use of @Column(name = &quot;id&quot;) - which is not an annotation from com.datastax.oss.driver.api.mapper.annotations. @CqlName should be used.</li>
<li>The use of a reserved keyword for a table - the term 'order' is a reserved keyword, I am pretty sure this results in considerable confusion within the code - although I got a different error when I attempted to use the keyword as a table name with the mapper. I think you should name this table to something else and avoid the keyword.</li>
</ul>
<p>Annotation Link:
<a href=""https://docs.datastax.com/en/developer/java-driver/4.2/manual/mapper/entities/"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/developer/java-driver/4.2/manual/mapper/entities/</a> contains the rules / annotations available and you can see the use of @CqlName there to inform the mapper of the Cassandra column name if it differs from the naming convention.</p>
<p>I belive you were on the 4.2 driver in an earlier question - but if updated to 4.7 (recommended) then this is the link : <a href=""https://docs.datastax.com/en/developer/java-driver/4.7/manual/mapper/entities/"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/developer/java-driver/4.7/manual/mapper/entities/</a> )</p>
<p>The following code worked correctly:</p>
<p>Cql:</p>
<pre><code>CREATE TABLE customer_orders (
    id text PRIMARY KEY,
    customer_email text,
    customer_id text,
    loyalty_id text
);

insert into customer_orders (id, customer_email, customer_id, loyalty_id) values ('a','a@b.c.com', '1234', '5678');
</code></pre>
<p>Pojo:</p>
<pre><code>import com.datastax.oss.driver.api.mapper.annotations.CqlName;
import com.datastax.oss.driver.api.mapper.annotations.Entity;
import com.datastax.oss.driver.api.mapper.annotations.PartitionKey;

@Entity
public class CustomerOrders {
    @CqlName(&quot;id&quot;)
    @PartitionKey
    private String id;

    @CqlName(&quot;customer_id&quot;)
    private String customerId;

    @CqlName(&quot;loyalty_id&quot;)
    private String loyaltyId;

    @CqlName(&quot;customer_email&quot;)
    private String customerEmail;

    public CustomerOrders() {

    }

    public String getId() {
        return id;
    }

    public void setId(String id) {
        this.id = id;
    }

    public String getCustomerId() {
        return customerId;
    }

    public void setCustomerId(String customerId) {
        this.customerId = customerId;
    }

    public String getLoyaltyId() {
        return loyaltyId;
    }

    public void setLoyaltyId(String loyaltyId) {
        this.loyaltyId = loyaltyId;
    }

    public String getCustomerEmail() {
        return customerEmail;
    }

    public void setCustomerEmail(String customerEmail) {
        this.customerEmail = customerEmail;
    }
}
</code></pre>
<p>DaoMapper:</p>
<pre><code>import com.datastax.oss.driver.api.core.CqlIdentifier;
import com.datastax.oss.driver.api.mapper.annotations.*;

@Mapper
public interface DaoMapper {

    @DaoFactory
    OrderDao orderDao(@DaoKeyspace CqlIdentifier keyspace);
}
</code></pre>
<p>OrderDao:</p>
<pre><code>import com.datastax.oss.driver.api.mapper.annotations.Dao;
import com.datastax.oss.driver.api.mapper.annotations.Delete;
import com.datastax.oss.driver.api.mapper.annotations.Insert;
import com.datastax.oss.driver.api.mapper.annotations.Select;

@Dao
public interface OrderDao {

    @Select
    CustomerOrders findById(String id);

    @Insert
    void save(CustomerOrders order);

    @Delete
    void delete(CustomerOrders order);
}
</code></pre>
<p>I then created a simple test snippet:</p>
<pre><code> @Test
    void GetRecordViaOrderDao() {
        try (CqlSession session = CqlSession.builder().build()) {

            DaoMapper daoMapper = new DaoMapperBuilder(session).build();
            OrderDao orderDao = daoMapper.orderDao(CqlIdentifier.fromCql(&quot;killrvideo&quot;));
            CustomerOrders order = orderDao.findById(&quot;a&quot;);
            System.out.println(order.getCustomerEmail());
        }
    }
</code></pre>
<p>killrvideo keyspace in this instance because I am hitting an Astra DB and thats the keyspace I have there currently.</p>
<pre><code>Result: a@b.c.com
</code></pre>
<p>Driver version 4.7.2 was specified in my pom.xml</p>
",['table']
63234493,63239703,2020-08-03 17:50:01,Datastax Node.js Cassandra driver When to use a Mapper vs. Query,"<p>I'm working with the Datastax Node.js driver and I can't figure out when to use a mapper vs. query. Both seem to be able to perform the same CRUD operations.</p>
<p>With a query:</p>
<pre><code>const q = SELECT * FROM mykeyspace.mytable WHERE id='12345';
client.execute(q).then(result =&gt; console.log('This is the data', result);
</code></pre>
<p>With mapper:</p>
<p><code>const tableRow = await tableMapper.find({ id: '12345' });</code></p>
<p>When should I use the mapper over a query and vice versa?</p>
",<node.js><database><cassandra>,"<p>Mapper is a feature from cassandra-driver released in 2018. Using mapper, cassandra-driver can make a map from your cassandra table to an object in nodejs and you can handle in your nodejs application like a set of document.</p>
<p>Using mapper you can make selects or inserts in your database like said in this article:
<a href=""https://www.datastax.com/blog/2018/12/introducing-datastax-nodejs-mapper-apache-cassandra"" rel=""nofollow noreferrer"">https://www.datastax.com/blog/2018/12/introducing-datastax-nodejs-mapper-apache-cassandra</a></p>
<p>With query method, if you need to use or reuse any property from your json you will need to make a Json.Parse().</p>
",['table']
63266057,63591380,2020-08-05 13:17:35,Alter table add if not exists Cassandra,"<p>I have an update script I created and run before.</p>
<pre><code>ALTER TABLE &quot;facilities&quot; ADD &quot;tariff_id&quot; text
GO 
</code></pre>
<p>I want to run this query again without deleting it from the script. If exists did not work with alter. How can I do it?
I have a this exception:</p>
<blockquote>
<p>Cassandra.InvalidQueryException: 'Invalid column name tariff_id
because it conflicts with an existing column'</p>
</blockquote>
",<c#><cassandra>,"<p>I could not find an answer to this problem. The best solution was to ignore that exception code. My solution:</p>
<pre><code>try
{
    DB.Instance.Execute(script);
    script = &quot;&quot;;
}
catch (Exception e)
{
    //It helps to pass the lines that already exist. Alter table add etc.
    if ( HResult == (-2146233088))
        script = &quot;&quot;;
    else 
        throw e;
}
</code></pre>
",['table']
63276887,63283716,2020-08-06 04:32:55,Cassandra - combining multiple tables as a view,"<p>I have 20 tables in Cassandra. For one particular request, I need data from all 20 tables. How can I do this? Can I use a materialized view in Cassandra?</p>
<p>Or should I use Cassandra trigger and make an entry in different table whenever something changes in any of the 20 tables?</p>
<p>Or is there any better way?</p>
",<cassandra><triggers><cassandra-3.0><materialized-views>,"<p>The materialized view in Cassandra just maintaining a new table based on the data of the single table, so you can't use this functionality for your purpose. Triggers are also not designed for this kind of work.</p>
<p>So for you, the only the choice is to do a multiple requests to many tables (slow), or maintain a separate table with all data that are necessary for answering this query - this is recommended way in Cassandra, as all table structures are based on the queries.</p>
<p>I recommend to take DS220 course on Data modeling on <a href=""https://academy.datastax.com/"" rel=""nofollow noreferrer"">DataStax Academy</a></p>
",['table']
63295459,63334408,2020-08-07 04:55:55,Spring cassandra Batch Operation delete by partition key only,"<p>I have a requirement in which i have to perform a delete operation in cassandra by using only partiton key (delete all records with the partition key ) under batch operation using spring and springboot ,but the CassandraBatchOperations 's delete method only takes in input full entity object like</p>
<pre><code>CassandraBatchOperations delete(Object... entities);
</code></pre>
<p>i have a table say table1
and it has keys :
key1- partiton key ,
key2 -clustering key1 ,
key 3-clustering key2</p>
<pre><code>so my requirement is that in batch operation below query should run
DELETE from table1 where key1='input key';


so when i create an object like 
tableEntity recordToDelete=new Table1Entity();
recordToDelete.setKey1('input key');

and run batchOperations like 
CassandraBatchOperations batchOps=cassandraTemplate.batchOps();
batchOps.delete(recordToDelete);

then the effective query getting generated is 
DELETE from table1 where key1='input key' and key2=null and key3=null
</code></pre>
<p>then i am getting below exception</p>
<pre><code>&gt;  rg.springframework.data.cassandra.CassandraInvalidQueryException: 
&gt; Query; CQL [BEGIN BATCH DELETE FROM table1 WHERE key2=null AND
&gt; key3=null AND key1='0002';APPLY BATCH;]; Invalid null value in
&gt; condition for column key2; nested exception is
&gt; com.datastax.driver.core.exceptions.InvalidQueryException: Invalid
&gt; null value in condition for column key2
</code></pre>
<p>The problem is the query getting create is also considering the clustering key key2 and key3 which does not have values as i want to delete only by partition key .</p>
<p>I want to know how can i delete only by partiton Key ,
Getting the list of record from DB is not an option as i am also inserting the records in cassandra under same batch operation and it can happen that there is also a record getting instered in the same batch operation which has the partition key that i want to delete . so in that case if i get and delete the record the new record that is getting inserted in batch operation will not get deleted .</p>
",<java><spring><spring-boot><cassandra>,"<p>The Answer to this question was my Entity class was containing all  the pk ( as it should)
so when the Cassandra Template created the query it was picking up all the keys</p>
<p><strong>Original Entity class</strong></p>
<pre><code>@Table(CassandraDBSchemaConstants.TABLE1)
public class Table {
    @PrimaryKeyColumn (name = CassandraDBSchemaConstants.KEY1 , type = PrimaryKeyType.PARTITIONED)
    private String key1;
    
    @PrimaryKeyColumn(name = CassandraDBSchemaConstants.KEY2 , type = PrimaryKeyType.CLUSTERED)
    private String key2;
    
    @PrimaryKeyColumn(name = CassandraDBSchemaConstants.KEY3 , type = PrimaryKeyType.CLUSTERED)
    private String key3;
    
    
    ..//other columns
    
    }
</code></pre>
<p>so as fix : I created a new Entity class with same table name with only 1 key (partition key ) in it</p>
<p><strong>a new class apart from the existing entity class</strong></p>
<pre><code>@Table(CassandraDBSchemaConstants.TABLE1)
public class TableOnlyByPartitonKey {
    @PrimaryKeyColumn (name = CassandraDBSchemaConstants.KEY1 , type = PrimaryKeyType.PARTITIONED)
    private String key1;
    
    }
</code></pre>
<p>and when I had to delete by partition key only I passed the new entity class to the cassandra batch operation's delete method</p>
<p>and the query that got executed deleted the record only by the partition key which was present in new entity class</p>
",['table']
63385222,63387513,2020-08-12 21:55:48,Hard time understanding Cassandra query,"<p>In Cassandra, I understand that tables are supposed to be created according to what needs to be queried. For example, I currently have a Users and Users_By_Status table.</p>
<pre><code>##Users##
CREATE TABLE Users (
    user_id uuid,
    name text,
    password text,
    status int,
    username text,
    PRIMARY KEY (user_id) 
);
CREATE INDEX user_username_idx ON Users (username);

##Users_By_Status##
CREATE TABLE Users_By_Status (
    username text,
    status int,
    user_id uuid,
    PRIMARY KEY (username, status, user_id) 
);
</code></pre>
<p>In this case, if a user leaves, their record won't be deleted. Instead, status will be changed from 1 to 0.</p>
<p>If I insert data into the Users table, do I need to manually insert the data into Users_By_Status table too? What happens if I update the status in Users? Do I need to manually update the record in Users_By_Status table too?</p>
<p>I have a feeling I'm understanding Cassandra wrongly. Appreciate all the help I can get.</p>
",<cassandra><nosql><cql>,"<p>Shortly answer: yes, in your case you need to delete manually.
In cassandra db you need to write more code in your app layer to handle cenarios like that.</p>
<p>But we have other options like materialized view or BATCH Statements.</p>
<p>For your solution, i think that materialized view is the best option. You can create a Materialized view from your table Users. Like this:</p>
<pre><code>CREATE MATERIALIZED VIEW Users_By_Status
AS SELECT username, status, userid 
FROM Users
PRIMARY KEY(username, status, userid);
</code></pre>
<p>And yes, when you update table users, the update will happen in the Materialized View Users_By_Status too.</p>
<p>Reference: <a href=""https://docs.datastax.com/en/cql-oss/3.3/cql/cql_using/useCreateMV.html"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/cql-oss/3.3/cql/cql_using/useCreateMV.html</a></p>
",['table']
63502082,63502889,2020-08-20 09:23:01,Datastax cassandra seem to cache preparestatent,"<p>When my application runs a long time, everything works as well. But when I change type a column from int to text(Drop table and recreate), I caught a Exception:</p>
<pre><code>com.datastax.oss.driver.api.core.type.codec.CodecNotFoundException: Codec not found for requested operation: [INT &lt;-&gt; java.lang.String]
    at com.datastax.oss.driver.internal.core.type.codec.registry.CachingCodecRegistry.createCodec(CachingCodecRegistry.java:609)
    at com.datastax.oss.driver.internal.core.type.codec.registry.DefaultCodecRegistry$1.load(DefaultCodecRegistry.java:95)
    at com.datastax.oss.driver.internal.core.type.codec.registry.DefaultCodecRegistry$1.load(DefaultCodecRegistry.java:92)
    at com.datastax.oss.driver.shaded.guava.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527)
    at com.datastax.oss.driver.shaded.guava.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2276)
    at com.datastax.oss.driver.shaded.guava.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2154)
    at com.datastax.oss.driver.shaded.guava.common.cache.LocalCache$Segment.get(LocalCache.java:2044)
    at com.datastax.oss.driver.shaded.guava.common.cache.LocalCache.get(LocalCache.java:3951)
    at com.datastax.oss.driver.shaded.guava.common.cache.LocalCache.getOrLoad(LocalCache.java:3973)
    at com.datastax.oss.driver.shaded.guava.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4957)
    at com.datastax.oss.driver.shaded.guava.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4963)
    at com.datastax.oss.driver.internal.core.type.codec.registry.DefaultCodecRegistry.getCachedCodec(DefaultCodecRegistry.java:117)
    at com.datastax.oss.driver.internal.core.type.codec.registry.CachingCodecRegistry.codecFor(CachingCodecRegistry.java:215)
    at com.datastax.oss.driver.api.core.data.SettableByIndex.set(SettableByIndex.java:132)
    at com.datastax.oss.driver.api.core.data.SettableByIndex.setString(SettableByIndex.java:338)
</code></pre>
<p>This exception appears occasionally. I'm using PreparedStatement to execute the query, I think it is cached from DataStax's driver.</p>
<p>I'm using AWS Keyspaces(Cassandra version 3.11.2), DataStax driver 4.6.
Here is my application.conf:</p>
<pre><code>  basic.request {
    timeout = 5 seconds
    consistency = LOCAL_ONE
  }
  advanced.connection {
    max-requests-per-connection = 1024
    pool {
      local.size = 1
      remote.size = 1
    }
  }
  advanced.reconnect-on-init = true
  advanced.reconnection-policy {
    class = ExponentialReconnectionPolicy
    base-delay = 1 second
    max-delay = 60 seconds
  }
  advanced.retry-policy {
    class = DefaultRetryPolicy
  }
  advanced.protocol {
    version = V4
  }
  advanced.heartbeat {
   interval = 30 seconds
   timeout = 1 second
  }
  advanced.session-leak.threshold = 8
  advanced.metadata.token-map.enabled = false
}
</code></pre>
",<cassandra><datastax-java-driver>,"<p>Yes, Java driver 4.x caches prepared statement - it's a difference from the driver 3.x.  From <a href=""https://docs.datastax.com/en/developer/java-driver/4.8/manual/core/statements/prepared/"" rel=""nofollow noreferrer"">documentation</a>:</p>
<blockquote>
<p>the session has a built-in cache, it’s OK to prepare the same string twice.</p>
</blockquote>
<p>...</p>
<blockquote>
<p>Note that caching is based on: <strong>the query string exactly as you provided it</strong>: the driver does not perform any kind of trimming or sanitizing.</p>
</blockquote>
<p>I'm not sure 100% about the source code, but the relevant entries in the cache may not be cleared up on the table drop.  I suggest to open the <a href=""https://datastax-oss.atlassian.net/browse/JAVA"" rel=""nofollow noreferrer"">JIRA against Java driver</a>, although, such type changes are often not really recommended - it's better to introduce new field with new type, even if it's possible to re-create table.</p>
",['table']
63502404,63504588,2020-08-20 09:42:41,cassandra restore from incremental backup,"<p>I want to use incremental backup as main cassandra backup type in my system, but I have some miss understanding:</p>
<ol>
<li>The one way to make restore from incremental backup that worked for me - just copy from backup folder to table folder , is that the right way to make it?</li>
<li>Can I ,somehow , make backup of table\keyspace parameters , like index/replica_factor etc?</li>
</ol>
<p>Thanks.</p>
",<cassandra><cassandra-3.0>,"<ol>
<li><p>If you have not dropped the table.for restoration</p>
<p>Stop the C* node</p>
<p>Copy the backup folder to table directory. You are right on that.</p>
<p>Start the server.</p>
</li>
<li><p>If you want schema, you can check snapshot folder. There you can find schema.cql</p>
</li>
</ol>
",['table']
63538320,63540674,2020-08-22 16:10:45,Spark UDF To Look up Keys Using Cassandra Connector,"<ul>
<li><p>I'm trying use cassandra as a Key-Value Lookupstore in some of our spark
jobs.</p>
</li>
<li><p>We use Dataframes primarily and have moved away from the RDD    APIs.</p>
</li>
<li><p>Instead of joining with the tables, loading them into spark or<br />
pushing the join to cassandra and taking measures to avoid large<br />
table scans,  i thought i could just write a Spark UDF that connects
to cassandra a looks up one key</p>
</li>
<li><p>I additionally want to convert the result row into a case class
object and return the object.</p>
</li>
</ul>
<p>I got some of this information based on responses from this question below. <strong>withSessionDo</strong> reuses an underlying JVM Level Session that is available on each node
<a href=""https://stackoverflow.com/questions/26526880/spark-cassandra-connector-proper-usage"">Spark Cassandra Connector proper usage</a></p>
<pre><code>val connector = CassandraConnector(sparkConf) // I Know this is serializable.

def lookupKey(connector: CassandraConnector, keyspace: String, table: String): UserDefineFunction = udf((key: String) =&gt; {
    connector.withSessionDo(session =&gt; {
        val stmt = session.prepare(s&quot;SELECT * FROM $keyspace.$table WHERE key = ?&quot;)
        val result = session.execute( stmt.bind(key) )
        MyCaseClass(
           fieldl1 = result.getString(0),
           fieldl2 = result.getInt(1)
           ...
        )
    }
})
</code></pre>
<p>Session isn't serializable so we cannot create one outside the udf and pass it in so we can use mapping manager to convert the rows to case class instances.
An Alternative approach using Mapping Manager,</p>
<pre><code>def lookupKeyAlt(connector: CassandraConnector, keyspace: String, table: String): UserDefineFunction = udf((key: String) =&gt; {
    connector.withSessionDo(session =&gt; {
        val manager = new MappingManager(session)   // session isn't serializable, so creating one outside and passing to udf is not an option if wf we were willing to do the session management.
        val mapperClass = manager.mapper(classOf[MyCaseClass], keyspace)
        mapperClass.get(key)
    }
})
</code></pre>
<p>I am new to cassandra so please bear with me on a few questions.</p>
<ol>
<li>Are there any Gotchas in these approaches that i am not aware of ?</li>
<li>In the Second approach, i understand we are creating a new MappingManager(session) with every call of the UDF. Will this still use the jvm level session and open any more sessions ?
Is it even right to instantiate MappingManager with every call ? The session isn't serializable so i cant create it outside and pass it to the UDF.</li>
<li>What are the some other ways to convert a result Row to an object of a Case Class ?</li>
<li>Are there any better alternatives to do this kind of lookup ?</li>
</ol>
",<apache-spark><cassandra><spark-cassandra-connector>,"<p>You're trying to emulate what Spark Cassandra Connector (SCC) is doing under the hood, but your implementation will be much slower that SCC's because you're using synchronous API, and getting all data one after another, while SCC is using asynchronous API, and pull data for multiple rows in parallel.</p>
<p>The best way to achieve what you want is to use Cassandra-optimized join (often called &quot;direct join&quot;).  This kind of join was always available for RDD API, but for a long time was available for Dataframe API only in the commercial version of the connector.  But since SCC 2.5.0 (<a href=""https://www.datastax.com/blog/2020/05/advanced-apache-cassandra-analytics-now-open-all"" rel=""nofollow noreferrer"">released in May 2020th</a>), this functionality is also available in open source version, so you can use it instead of building its emulation.  The direct join is performed only when you <a href=""https://github.com/datastax/spark-cassandra-connector/blob/b2.5/doc/14_data_frames.md#special-cassandra-catalyst-rules-since-scc-25"" rel=""nofollow noreferrer"">enable special Catalyst extensions</a>, by passing the <code>spark.sql.extensions=com.datastax.spark.connector.CassandraSparkExtensions</code> when configuring SparkSession (for example via command-line).  After that, you can perform join with Cassandra table by full or partial primary key, and SCC will automatically convert join into individual requests to Cassandra that are executed very effectively. You can check that this happens by executing <code>explain</code> on the joined dataframe, so you should see something like this (look for string <strong>Cassandra Direct Join</strong>):</p>
<pre><code>scala&gt; joined.explain
== Physical Plan ==
Cassandra Direct Join [pk = id#30, c1 = cc1#32] test.jtest1 - Reading (pk, c1, c2, v) Pushed {}
+- *(1) Project [cast(id#28L as int) AS id#30, cast(id#28L as int) AS cc1#32]
   +- *(1) Range (1, 5, step=1, splits=8)
</code></pre>
<p>I recently <a href=""https://alexott.blogspot.com/2020/07/spark-effective-joins-with-cassandra.html"" rel=""nofollow noreferrer"">wrote a long blog post</a> that explains how to perform effective joins with data in Cassandra using both Dataframe &amp; RDD APIs - I don't want to repeat it here :-)</p>
",['table']
63565125,63566831,2020-08-24 16:34:47,"Writing JSON data into Cassandra using python client, issue with primary key choice","<p>So I want to write data, which is coded as a JSON string into a Cassandra table. I did the following steps:</p>
<ul>
<li>Create a Cassandra table containing columns with all the attributes of my JSON string. Here is the cql for that:</li>
</ul>
<pre><code>CREATE TABLE on_equipment (
  ChnID varchar,
  StgID varchar,
  EquipID varchar,
  SenID varchar,
  value1 float,
  value2 float,
  value3 float,
  electric_consumption float,
  timestamp float,
  measurement_location varchar,
  PRIMARY KEY ((timestamp))
) WITH comment = 'A table for the on equipment readings';
</code></pre>
<ul>
<li>Write a python Cassandra client to write the data into Cassandra from a JSON payload.
Here is the code snippet to make the INSERt query (msg.value is the json string):</li>
</ul>
<pre><code>session.execute('INSERT INTO ' + table_name + ' JSON ' + &quot;'&quot; + msg.value + &quot;';&quot;)
</code></pre>
<p>I get no writing errors when doing this.</p>
<p>However, I ran into a problem:</p>
<p>The JSON data I have is from IoT sources, and one of the attributed I have is a unix timestamp. An example of a JSON record is as follows (notice the timestamp attribute):</p>
<pre><code>{'timestamp': 1598279069.441547, 'value1': 0.36809349674042857, 'value2': 18.284579388599308, 'value3': 39.95615809003724, 'electric_consumption': 1.2468644044844224, 'SenID': '1', 'EquipID': 'MID-1', 'StgID': '1', 'ChnID': '1', 'measurement_location': 'OnEquipment'}
</code></pre>
<p>In order to insert many records, I have defined the timestamp value as the primary key of the data in the Cassandra table. The problem is that not all records are being written into Cassandra, only records who's timestamps fall into a certain group. I know this because I have produced around 100 messages and received zero write errors, yet the contents of the table only has 4 rows:</p>
<pre><code> timestamp  | chnid | electric_consumption | equipid | measurement_location | senid | stgid | value1   | value2   | value3
------------+-------+----------------------+---------+----------------------+-------+-------+----------+----------+----------
 1.5983e+09 |     1 |             0.149826 |   MID-1 |          OnEquipment |     1 |     1 | 0.702309 | 19.92813 | 21.47207
 1.5983e+09 |     1 |              1.10219 |   MID-1 |          OnEquipment |     1 |     1 | 0.141921 |  5.11319 | 78.17094
 1.5983e+09 |     1 |              1.24686 |   MID-1 |          OnEquipment |     1 |     1 | 0.368093 | 18.28458 | 39.95616
 1.5983e+09 |     1 |              1.22841 |   MID-1 |          OnEquipment |     1 |     1 | 0.318357 |  16.9013 |  71.5506
</code></pre>
<p>In other words, Cassandra is updating the values of these four rows, when it should be writing all the 100 messages.</p>
<p>My guess is that I incorrectly using the Cassandra primary key. The timestamp column is type float.</p>
<p>My questions:
Does this behaviour make sense? Can you explain it?
What can I use as the primary key to solve this?
Is there a way to make the primary key a Cassandra writing or arrival time?</p>
<p>Thank you in advance for your help!</p>
",<json><cassandra><cassandra-3.0>,"<p>You have defined the primary key as just the timestamp - if you insert data into a Cassandra table, and the data you are writing has the same primary key as data already in the table, you will overwrite it. All inserts are in effect insert/update, so when you use the same primary key value a 2nd time, it will update.</p>
<p>As to the solution - this is tricker - the primary key has to hold true to it's name - it is primary, e.g. unique - even if it was a timestamp instead of a float you should also have at least 1 other field (such as the IoT unique identifier) within the primary key so that 2 readings from two different devices made at the exact same time do not clash.</p>
<p>In Cassandra you model the data and the keys based on how you intend to access the data - without knowing that it would not be possible to know what the primary key (Partition + Clustering key) should be. You also ideally need to know something about the data cardinality and selectivity.</p>
<p>Identify and define the queries you intend to run against the data, that should guide your partition key and clustering key choices - which together make the primary key.</p>
<p>The specific issue here to add to the above is that the data is exceeding the precision that the float can be stored at - capping the value in effect and making them all identical. If you change the float to a double, it then stores the data without capping the values into the same value - which then causes the upsert instead of a new row inserted. (The JSON insert part is not relevant to the issue as it happens)</p>
<p>Recreating the issue as follows:</p>
<pre><code> CREATE TABLE on_equipment (
  ChnID varchar,
  timestamp float,
  PRIMARY KEY ((timestamp))
) ;

insert into on_equipment(timestamp, chnid) values (1598279061,'1');
insert into on_equipment(timestamp, chnid) values (1598279062,'2');
insert into on_equipment(timestamp, chnid) values (1598279063,'3');
insert into on_equipment(timestamp, chnid) values (1598279064,'4');

select count(*) from on_equipment;

1

select timestamp from on_equipment;

1.59827904E9
</code></pre>
<p>You can see the value has been rounded and capped, all 4 values capped the same, if you use smaller numbers for the timestamps it works, but isn't very useful to do so.</p>
<p>Changing it to a double:</p>
<pre><code>CREATE TABLE on_equipment (
  ChnID varchar,
  timestamp double,
  PRIMARY KEY ((timestamp))
) ;

insert into on_equipment(timestamp, chnid) values (1598279061,'1');
insert into on_equipment(timestamp, chnid) values (1598279062,'2');
insert into on_equipment(timestamp, chnid) values (1598279063,'3');
insert into on_equipment(timestamp, chnid) values (1598279064,'4');

select count(*) from on_equipment;

4
</code></pre>
",['precision']
63640908,63641325,2020-08-28 21:10:54,using Kafka Consumer in Node JS app to indicate computations have been made,"<p>So my question may involve some brainstorming based on the nature of the application.</p>
<p>I have a Node JS app that sends messages to Kafka. For example, every single time a user clicks on a page, a Kafka app runs a computation based on the visit. I then at the same instance want to retrieve this computation after triggering it through my Kafka message. So far, this computation is stored in a Cassandra database. The problem is that, if we try to read from Cassandra before the computation is complete then we will query nothing from the database(key has not been inserted yet)and won't return anything(error), or possibly the computation is stale. This is my code so far.</p>
<pre><code>router.get('/:slug', async (req, res) =&gt;{

Producer = kafka.Producer


KeyedMessage = kafka.KeyedMessage
  client = new kafka.KafkaClient()



producer = new Producer(client)



km = new KeyedMessage('key', 'message')
  kafka_message = JSON.stringify({ id: req.session.session_id.toString(), url: arbitrary_url })
  payloads = [
    { topic: 'MakeComputationTopic', messages: kafka_message}
  ]; 
const clientCass = new cassandra.Client({
contactPoints: ['127.0.0.1:9042'],
localDataCenter: 'datacenter1', // here is the change required
keyspace: 'computation_space',
authProvider: new auth.PlainTextAuthProvider('cassandra', 'cassandra')
});



const query = 'SELECT * FROM computation  WHERE id = ?';




clientCass.execute(query, [req.session.session_id],{ hints : ['int'] })
  .then(result =&gt; console.log('User with email %s', result.rows[0].computations))
  .catch((message) =&gt; {
    console.log('Could not find key')
  });


}
</code></pre>
<p>Firstly, async and await came to mind but that is ruled out since this does not stop stale computations.</p>
<p>Secondly, I looked into letting my application sleep, but it seems that this way will slow my application down.</p>
<p>I am possibly deciding on using Kafka Consumer (in my node-js) to consume a message that indicates that it's safe to now look into the Cassandra table.</p>
<p>For e.g. (using kafka-node)</p>
<pre><code>consumer.on('message', function (message) {
    clientCass.execute(query, [req.session.session_id],{ hints : ['int'] })
  .then(result =&gt; console.log('User with computation%s', result.rows[0].computations))
  .catch((message) =&gt; {
    console.log('Could not find key')
  });
}); 
</code></pre>
<p>This approach while better seems a bit off since I will have to make a consumer every time a user clicks on a page, and I only care about it being sent 1 message.</p>
<p>I was wondering how I should deal with this challenge? Am I possibly missing a scenario, or is there a way to use kafka-node to solve this problem? I was also thinking of doing a while loop that waits for the promise to succeed and that computations are not stale(compare values in the cache)</p>
",<node.js><promise><cassandra><consumer><kafka-node>,"<blockquote>
<p>This approach while better seems a bit off since I will have to make a consumer every time a user clicks on a page, and I only care about it being sent 1 message.</p>
</blockquote>
<p>I would come to the same conclusion. Cassandra is not designed for these kind of use cases. The database is eventually consistence. Your current approach maybe works at the moment, if you hack something together, but will definitely result in undefined behavior once you have a Cassandra cluster. Especially when you update the entry.</p>
<p>The id in the computation table is your partition key. This means once you have a cluster Cassandra distributes the data by the id. It looks like it only contains one row. This is a very inefficient way of modeling your Cassandra tables.</p>
<p>Your use case looks like one for a session storage or cache. <a href=""https://redis.io/"" rel=""nofollow noreferrer"">Redis</a> or <a href=""https://github.com/google/leveldb"" rel=""nofollow noreferrer"">LevelDB</a> are well suited for these kind of use cases. Any other key value storage would do the job too.</p>
<p>Why don't you write your result into another topic and have another application which reads this topic and writes the result into a database. So that you don't need to keep any state. The result will be in the topic when it is done. It would look like this:</p>
<p>incoming data -&gt; first kafka topic -&gt; computational app -&gt; second kafka topic -&gt; another app writing it into the database &lt;- another app reading regularly the data.</p>
<p>If it is there it is there and therefore not done yet.</p>
",['table']
63700532,63733938,2020-09-02 07:16:51,Apache Beam - Write BigQuery TableRow to Cassandra,"<p>I'm trying to read data from BigQuery (using TableRow) and write the output to Cassandra. How to do that?</p>
<p>Here's what I've tried. This works:</p>
<pre><code>/* Read BQ */
PCollection&lt;CxCpmMapProfile&gt; data =  p.apply(BigQueryIO.read(new SerializableFunction&lt;SchemaAndRecord, CxCpmMapProfile&gt;() {
    public CxCpmMapProfile apply(SchemaAndRecord record) {
        GenericRecord r = record.getRecord();
        return new CxCpmMapProfile((String) r.get(&quot;channel_no&quot;).toString(), (String) r.get(&quot;channel_name&quot;).toString());
    }
}).fromQuery(&quot;SELECT channel_no, channel_name FROM `dataset_name.table_name`&quot;).usingStandardSql().withoutValidation());

/* Write to Cassandra */
data.apply(CassandraIO.&lt;CxCpmMapProfile&gt;write()
    .withHosts(Arrays.asList(&quot;&lt;IP addr1&gt;&quot;, &quot;&lt;IP addr2&gt;&quot;))
    .withPort(9042)
    .withUsername(&quot;cassandra_user&quot;).withPassword(&quot;cassandra_password&quot;).withKeyspace(&quot;cassandra_keyspace&quot;)
    .withEntity(CxCpmMapProfile.class));
</code></pre>
<p>But when I changed <strong>Read BQ</strong> part using TableRow like this:</p>
<pre><code>/* Read from BQ using readTableRow */
PCollection&lt;TableRow&gt; data = p.apply(BigQueryIO.readTableRows()
    .fromQuery(&quot;SELECT channel_no, channel_name FROM `dataset_name.table_name`&quot;)
    .usingStandardSql().withoutValidation());
</code></pre>
<p>In <strong>Write to Cassandra</strong> I got the following error</p>
<p><code>The method apply(PTransform&lt;? super PCollection&lt;TableRow&gt;,OutputT&gt;) in the type PCollection&lt;TableRow&gt; is not applicable for the arguments (CassandraIO.Write&lt;CxCpmMacProfile&gt;)</code></p>
",<cassandra><google-bigquery><google-cloud-dataflow><apache-beam><dataflow>,"<p>The error is due to the input PCollection containing <code>TableRow</code> elements, while the CassandraIO read is expecting <code>CxCpmMacProfile</code> elements. You need to read the elements from BigQuery as <code>CxCpmMacProfile</code> elements. The <a href=""https://beam.apache.org/releases/javadoc/2.23.0/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.html"" rel=""nofollow noreferrer"">BigQueryIO documentation</a> has an example of reading rows from a table and parsing them into a custom type, done through the <a href=""https://beam.apache.org/releases/javadoc/2.23.0/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.html#read-org.apache.beam.sdk.transforms.SerializableFunction-"" rel=""nofollow noreferrer""><code>read(SerializableFunction)</code></a> method.</p>
",['table']
63746440,63747579,2020-09-04 18:36:01,"In Cassandra, why dropping a column from tables defined with compact storage not allowed?","<p>As per datastx documentation <a href=""https://docs.datastax.com/en/dse/5.1/cql/cql/cql_reference/cql_commands/cqlAlterTable.html"" rel=""nofollow noreferrer"">here</a>, we cannot delete column from tables defined with COMPACT STORAGE option. What is the reason for this?</p>
",<cassandra><cql3><cassandra-cli>,"<p>This goes back to the original implementation of CQL3, and changes which were made to allow it to abstract a &quot;SQL-like,&quot; wide-row structure on top of the original Thrift-based storage engine.  Ultimately, managing the schema comes down to whether or not the underlying structure is a <em>table</em> or a <em>column_family</em>.</p>
<p>As an example, I'll create two tables using an old install of Apache Cassandra (2.1.19):</p>
<pre><code>CREATE TABLE student (
  studentid TEXT PRIMARY KEY,
  fname TEXT,
  name TEXT);

CREATE TABLE studentcomp (
  studentid TEXT PRIMARY KEY,
  fname TEXT,
  name TEXT)
WITH COMPACT STORAGE;
</code></pre>
<p>I'll insert one row into each table:</p>
<pre><code>INSERT INTO student (studentid, fname, lname) VALUES ('janderson','Jordy','Anderson');
INSERT INTO studentcomp (studentid, fname, lname) VALUES ('janderson','Jordy','Anderson');
</code></pre>
<p>And then I'll look at the tables with the old cassandra-cli tool:</p>
<pre><code>[default@stackoverflow] list student;
Using default limit of 100
Using default cell limit of 100
-------------------
RowKey: janderson
=&gt; (name=, value=, timestamp=1599248215128672)
=&gt; (name=fname, value=4a6f726479, timestamp=1599248215128672)
=&gt; (name=lname, value=416e646572736f6e, timestamp=1599248215128672)

[default@stackoverflow] list studentcomp;
Using default limit of 100
Using default cell limit of 100
-------------------
RowKey: janderson
=&gt; (name=fname, value=Jordy, timestamp=1599248302715066)
=&gt; (name=lname, value=Anderson, timestamp=1599248302715066)
</code></pre>
<p>Do you see the empty/&quot;ghost&quot; column value in the first result?  That empty column value was CQL3's link between the column values and the table's meta data.  If it's not there, then CQL cannot be used to manage a table's columns.</p>
<p>The comparator used for type conversion was all that was really exposed via Thrift.  This lack of meta data control/exposure is what allowed Cassandra to be considered &quot;schemaless&quot; in the pre-CQL days.  If I run a <code>describe studentcomp</code> from within the cassandra-cli, I can see the comparators (validation class) used:</p>
<pre><code>Column Metadata:
  Column Name: lname
    Validation Class: org.apache.cassandra.db.marshal.UTF8Type
  Column Name: fname
    Validation Class: org.apache.cassandra.db.marshal.UTF8Type
</code></pre>
<p>But if I try <code>describe student</code>, I see this:</p>
<pre><code>WARNING: CQL3 tables are intentionally omitted from 'describe' output.
See https://issues.apache.org/jira/browse/CASSANDRA-4377 for details.

Sorry, no Keyspace nor (non-CQL3) ColumnFamily was found with name: student (if this is a CQL3 table, you should use cqlsh instead)
</code></pre>
<p>Bascially, tables and column families were different entities forced into the same bucket.  Adding <code>WITH COMPACT STORAGE</code> essentially made a table a column family.
With that came the lack of any schema management (adding or removing columns), outside of access to the comparators.</p>
<p><strong>Edit 20200905</strong></p>
<blockquote>
<p>Can we somehow / someway (hack) drop the columns from table?</p>
</blockquote>
<p>You <em>might</em> be able to accomplish this.  Sylvain Lebresne wrote <a href=""https://www.datastax.com/blog/2012/10/thrift-cql3-upgrade-guide"" rel=""nofollow noreferrer"">A Thrift to CQL3 Upgrade Guide</a> which will have some necessary details for you.  I also advise reading through the Jira ticket mentioned above (<a href=""https://issues.apache.org/jira/browse/CASSANDRA-4377"" rel=""nofollow noreferrer"">CASSANDRA-4377</a>), as that covers many of the in-depth technical challenges that make this difficult.</p>
",['table']
63770308,63770403,2020-09-07 01:20:53,what is the impact of limit in cassandra cql,"<p>When executing a cqlsh query like <code>select * from table limit 10</code>, would cassandra scan the entire table and just return the first 10 records, or it can precisely locate the first 10 records across whole datacenter without scanning the entire table?</p>
",<cassandra><cqlsh>,"<p>The <code>LIMIT</code> option puts an upper-bound on the maximum number of rows returned by a query but it doesn't prevent the query from performing a full table scan.</p>
<p>Cassandra has internal mechanisms such as request timeouts which prevent bad queries from causing the cluster to crash so queries are more likely to timeout rather than overloading the cluster with scans on all nodes/replicas.</p>
<p>As a side note, the <code>LIMIT</code> option is irrelevant when used with <code>SELECT COUNT()</code> since the count function returns just 1 row (by design). <code>COUNT()</code> needs to do a full table scan regardless of the limit set. I've explained it in a bit more detail in this post -- <a href=""https://community.datastax.com/questions/6897/"" rel=""noreferrer"">https://community.datastax.com/questions/6897/</a>. Cheers!</p>
",['table']
63770348,63770517,2020-09-07 01:28:46,Is recreating table/ keyspace with same name in Cassandra not good?,"<p>I was going to a blog by Datastax which says it is not recommended to recreate table with same name. That is drop the table and create with the same name. Here is the link for Datastax <a href=""https://support.datastax.com/hc/en-us/articles/204226339-FAQ-How-to-drop-and-recreate-a-table-in-Apache-Cassandra-versions-older-than-2-1"" rel=""nofollow noreferrer"">recreate table faq</a>.
It talks about jira ticket <a href=""https://issues.apache.org/jira/plugins/servlet/mobile#issue/CASSANDRA-5202"" rel=""nofollow noreferrer"">CASSANDRA-5202</a>. It was fixed in 2.1.</p>
<p>I have questions, I am on Cassandra 2.1.16</p>
<ol>
<li>Is it safe to recreate table or keyspace with same name after dropping?</li>
<li>What precautions we must take if we recreate table or keyspace with same name?</li>
</ol>
",<cassandra>,"<p>I wrote that post 6 years ago. :)</p>
<p>As it clearly states, the problem existed in older versions of Cassandra. In C* 2.1 (and newer), a table ID (time UUID) is added to the table directory name on disk to prevent the problems I outlined in that post (<a href=""https://issues.apache.org/jira/browse/CASSANDRA-5202"" rel=""noreferrer"">CASSANDRA-5202</a>). Cheers!</p>
",['table']
63839559,63839773,2020-09-11 01:18:50,"Cassandra node running, but cant connect?","<p>using Cassandra version 3.11.8, openjdk-8u242-b08</p>
<p>Prior to this crashing, I was altering a table with 50k+ columns so this might (is) a factor to all of this. I would Ideally rather lose the data in the commit (if its inserting a backlog still perpetually) so I can connect to the hosts so service can be resumed.</p>
<p>Before start of error, I started alter table commands inserting many columns into the table in calls of 1000 at a time. Eventually after it may have done about half of them i received this error for all the nodes.</p>
<blockquote>
<p>2020-09-10 15:34:29 WARNING  [control connection] Error connecting to
127.0.0.3:9042: Traceback (most recent call last):   File &quot;cassandra\cluster.py&quot;, line 3522, in
cassandra.cluster.ControlConnection._reconnect_internal   File
&quot;cassandra\cluster.py&quot;, line 3591, in
cassandra.cluster.ControlConnection._try_connect   File
&quot;cassandra\cluster.py&quot;, line 3588, in
cassandra.cluster.ControlConnection._try_connect   File
&quot;cassandra\cluster.py&quot;, line 3690, in
cassandra.cluster.ControlConnection._refresh_schema   File
&quot;cassandra\metadata.py&quot;, line 142, in
cassandra.metadata.Metadata.refresh   File &quot;cassandra\metadata.py&quot;,
line 165, in cassandra.metadata.Metadata._rebuild_all   File
&quot;cassandra\metadata.py&quot;, line 2522, in get_all_keyspaces   File
&quot;cassandra\metadata.py&quot;, line 2031, in get_all_keyspaces   File
&quot;cassandra\metadata.py&quot;, line 2719, in
cassandra.metadata.SchemaParserV3._query_all   File
&quot;cassandra\connection.py&quot;, line 985, in
cassandra.connection.Connection.wait_for_responses   File
&quot;cassandra\connection.py&quot;, line 983, in
cassandra.connection.Connection.wait_for_responses   File
&quot;cassandra\connection.py&quot;, line 1435, in
cassandra.connection.ResponseWaiter.deliver
cassandra.OperationTimedOut: errors=None, last_host=None</p>
</blockquote>
<p>I am running 8 nodes on a server. I have reset all nodes and handshakes are done. But I cannot make a connect to my cluster on any of the nodes.My system.log and debug.log have similar logs throughout once cassandra starts running. gc.log has not updated in some time so it makes me wonder what is going on? Interesting point is i only retrieve the list of columns in the table 3 times total, I have ran this code on my desktop without issues using 2 nodes (much much less resources) and have not received any of these issues.</p>
<p>Edit: just for clarity my application/connections are not running and these logs below are what is happening periodically..I tried looking at scheduled tasks and cannot find information about cassandra for this. I wonder what backlog its reading from and if I can stop it. Ideally I would like to stop this backload of operations from happening...</p>
<pre><code>-------SYSTEM.LOG-------
INFO  [GossipStage:1] 2020-09-10 17:38:52,376 StorageService.java:2400 - Node /127.0.0.9 state jump to NORMAL
WARN  [OptionalTasks:1] 2020-09-10 17:38:54,802 CassandraRoleManager.java:377 - CassandraRoleManager skipped default role setup: some nodes were not ready
INFO  [OptionalTasks:1] 2020-09-10 17:38:54,802 CassandraRoleManager.java:416 - Setup task failed with error, rescheduling
INFO  [HANDSHAKE-/127.0.0.4] 2020-09-10 17:38:56,965 OutboundTcpConnection.java:561 - Handshaking version with /127.0.0.4
INFO  [HANDSHAKE-/127.0.0.4] 2020-09-10 17:38:58,262 OutboundTcpConnection.java:561 - Handshaking version with /127.0.0.4
INFO  [GossipStage:1] 2020-09-10 17:38:59,102 Gossiper.java:1139 - Node /127.0.0.4 has restarted, now UP
INFO  [GossipStage:1] 2020-09-10 17:38:59,103 TokenMetadata.java:497 - Updating topology for /127.0.0.4
INFO  [GossipStage:1] 2020-09-10 17:38:59,103 TokenMetadata.java:497 - Updating topology for /127.0.0.4
INFO  [GossipStage:1] 2020-09-10 17:38:59,105 Gossiper.java:1103 - InetAddress /127.0.0.4 is now UP
INFO  [HANDSHAKE-/127.0.0.5] 2020-09-10 17:38:59,813 OutboundTcpConnection.java:561 - Handshaking version with /127.0.0.5
INFO  [GossipStage:1] 2020-09-10 17:39:00,104 StorageService.java:2400 - Node /127.0.0.4 state jump to NORMAL
INFO  [HANDSHAKE-/127.0.0.5] 2020-09-10 17:39:01,029 OutboundTcpConnection.java:561 - Handshaking version with /127.0.0.5
INFO  [GossipStage:1] 2020-09-10 17:39:01,266 Gossiper.java:1139 - Node /127.0.0.5 has restarted, now UP
INFO  [GossipStage:1] 2020-09-10 17:39:01,267 TokenMetadata.java:497 - Updating topology for /127.0.0.5
INFO  [GossipStage:1] 2020-09-10 17:39:01,267 TokenMetadata.java:497 - Updating topology for /127.0.0.5
INFO  [GossipStage:1] 2020-09-10 17:39:01,270 Gossiper.java:1103 - InetAddress /127.0.0.5 is now UP
INFO  [GossipStage:1] 2020-09-10 17:39:04,271 StorageService.java:2400 - Node /127.0.0.5 state jump to NORMAL
INFO  [ScheduledTasks:1] 2020-09-10 17:43:05,805 NoSpamLogger.java:91 - Some operations were slow, details available at debug level (debug.log)
INFO  [ScheduledTasks:1] 2020-09-10 17:48:40,892 NoSpamLogger.java:91 - Some operations were slow, details available at debug level (debug.log)
INFO  [ScheduledTasks:1] 2020-09-10 17:54:35,999 NoSpamLogger.java:91 - Some operations were slow, details available at debug level (debug.log)
INFO  [ScheduledTasks:1] 2020-09-10 17:59:36,083 NoSpamLogger.java:91 - Some operations were slow, details available at debug level (debug.log)
INFO  [Service Thread] 2020-09-10 18:00:24,722 GCInspector.java:285 - ParNew GC in 237ms.  CMS Old Gen: 717168160 -&gt; 887151520; Par Eden Space: 1718091776 -&gt; 0; Par Survivor Space: 12757512 -&gt; 214695936
INFO  [ScheduledTasks:1] 2020-09-10 18:04:56,160 NoSpamLogger.java:91 - Some operations were slow, details available at debug level (debug.log)

------DEBUG.LOG------
INFO  [Service Thread] 2020-09-10 18:00:24,722 GCInspector.java:285 - ParNew GC in 237ms.  CMS Old Gen: 717168160 -&gt; 887151520; Par Eden Space: 1718091776 -&gt; 0; Par Survivor Space: 12757512 -&gt; 214695936
DEBUG [ScheduledTasks:1] 2020-09-10 18:00:26,102 MonitoringTask.java:173 - 1 operations were slow in the last 4996 msecs:
&lt;SELECT * FROM system_schema.columns&gt;, was slow 2 times: avg/min/max 1256/1232/1281 msec - slow timeout 500 msec
DEBUG [ScheduledTasks:1] 2020-09-10 18:00:56,110 MonitoringTask.java:173 - 1 operations were slow in the last 5007 msecs:
&lt;SELECT * FROM system_schema.columns&gt;, time 795 msec - slow timeout 500 msec
DEBUG [ScheduledTasks:1] 2020-09-10 18:01:01,111 MonitoringTask.java:173 - 1 operations were slow in the last 5003 msecs:
&lt;SELECT * FROM system_schema.columns&gt;, time 808 msec - slow timeout 500 msec
DEBUG [ScheduledTasks:1] 2020-09-10 18:03:41,143 MonitoringTask.java:173 - 1 operations were slow in the last 5002 msecs:
&lt;SELECT * FROM system_schema.columns&gt;, time 853 msec - slow timeout 500 msec
DEBUG [ScheduledTasks:1] 2020-09-10 18:04:06,148 MonitoringTask.java:173 - 1 operations were slow in the last 4996 msecs:
&lt;SELECT * FROM system_schema.columns&gt;, time 772 msec - slow timeout 500 msec
DEBUG [ScheduledTasks:1] 2020-09-10 18:04:26,153 MonitoringTask.java:173 - 1 operations were slow in the last 4991 msecs:
&lt;SELECT * FROM system_schema.columns&gt;, time 838 msec - slow timeout 500 msec
DEBUG [ScheduledTasks:1] 2020-09-10 18:04:31,154 MonitoringTask.java:173 - 1 operations were slow in the last 5009 msecs:
&lt;SELECT * FROM system_schema.columns&gt;, time 841 msec - slow timeout 500 msec
INFO  [ScheduledTasks:1] 2020-09-10 18:04:56,160 NoSpamLogger.java:91 - Some operations were slow, details available at debug level (debug.log)
DEBUG [ScheduledTasks:1] 2020-09-10 18:04:56,160 MonitoringTask.java:173 - 1 operations were slow in the last 5004 msecs:
&lt;SELECT * FROM system_schema.columns&gt;, time 772 msec - slow timeout 500 msec
DEBUG [ScheduledTasks:1] 2020-09-10 18:05:11,165 MonitoringTask.java:173 - 1 operations were slow in the last 4994 msecs:
&lt;SELECT * FROM system_schema.columns&gt;, time 808 msec - slow timeout 500 msec
DEBUG [ScheduledTasks:1] 2020-09-10 18:05:31,171 MonitoringTask.java:173 - 1 operations were slow in the last 5004 msecs:
&lt;SELECT * FROM system_schema.columns&gt;, time 834 msec - slow timeout 500 msec
DEBUG [ScheduledTasks:1] 2020-09-10 18:05:56,176 MonitoringTask.java:173 - 1 operations were slow in the last 5010 msecs:
&lt;SELECT * FROM system_schema.columns&gt;, was slow 2 times: avg/min/max 847/837/857 msec - slow timeout 500 msec
DEBUG [ScheduledTasks:1] 2020-09-10 18:07:16,196 MonitoringTask.java:173 - 1 operations were slow in the last 5003 msecs:
&lt;SELECT * FROM system_schema.columns&gt;, time 827 msec - slow timeout 500 msec
DEBUG [ScheduledTasks:1] 2020-09-10 18:07:31,200 MonitoringTask.java:173 - 1 operations were slow in the last 5007 msecs:
&lt;SELECT * FROM system_schema.columns&gt;, time 834 msec - slow timeout 500 msec
DEBUG [ScheduledTasks:1] 2020-09-10 18:08:01,207 MonitoringTask.java:173 - 1 operations were slow in the last 5000 msecs:
&lt;SELECT * FROM system_schema.columns&gt;, time 799 msec - slow timeout 500 msec
DEBUG [ScheduledTasks:1] 2020-09-10 18:08:16,211 MonitoringTask.java:173 - 1 operations were slow in the last 4999 msecs:
&lt;SELECT * FROM system_schema.columns&gt;, time 780 msec - slow timeout 500 msec
DEBUG [ScheduledTasks:1] 2020-09-10 18:08:36,217 MonitoringTask.java:173 - 1 operations were slow in the last 5000 msecs:
&lt;SELECT * FROM system_schema.columns&gt;, time 835 msec - slow timeout 500 msec
DEBUG [ScheduledTasks:1] 2020-09-10 18:09:01,221 MonitoringTask.java:173 - 1 operations were slow in the last 5002 msecs:
&lt;SELECT * FROM system_schema.columns&gt;, time 832 msec - slow timeout 500 msec
INFO  [ScheduledTasks:1] 2020-09-10 18:09:56,231 NoSpamLogger.java:91 - Some operations were slow, details available at debug level (debug.log)
DEBUG [ScheduledTasks:1] 2020-09-10 18:09:56,231 MonitoringTask.java:173 - 1 operations were slow in the last 4995 msecs:
&lt;SELECT * FROM system_schema.columns&gt;, time 778 msec - slow timeout 500 msec
DEBUG [ScheduledTasks:1] 2020-09-10 18:10:06,233 MonitoringTask.java:173 - 1 operations were slow in the last 5009 msecs:
&lt;SELECT * FROM system_schema.columns&gt;, time 1099 msec - slow timeout 500 msec

</code></pre>
",<cassandra><garbage-collection><cassandra-3.0>,"<p>The timeout is from the driver trying to parse the schema while establishing the control connection.</p>
<p>The driver uses the control connection for admin tasks such as discovering the cluster's topology and schema during the initialisation phase. I've discussed it in a bit more detail in this post -- <a href=""https://community.datastax.com/questions/7702/"" rel=""nofollow noreferrer"">https://community.datastax.com/questions/7702/</a>.</p>
<p>In your case, the driver initialisation times out while parsing the thousands of columns in the table you mentioned. I have to admit that this is new to me. I've never worked with a cluster that had thousands of columns so I'm curious to know what your use case is and perhaps there might be a better data model for it.</p>
<p>As a workaround, you can try to bump out the default timeout to see if the driver is able to eventually initialise. However, this is going to be a band-aid solution since the driver needs to parse the schema every time a DDL takes place. Cheers!</p>
",['table']
63871489,63881311,2020-09-13 13:42:17,Database choice for saving and querying stock prices,"<p>I'm currently receiving 2000 prices per second from a stock exchange and need to save those in an appropriate database. My current choice is PostgresQL which is way too slow. I need to save those prices (ticks) in an aggregated form like OHLC. So if I want to save D1 data for instance, I need to first get the previous D1 record for the stock from the database, check if the high or low price has changed and set a new close price and then save it to the database again. This is taking forever and is not possible with Postgres. I don't want to save the OHLC data, I prefer querying (aggregating) those in real-time.</p>
<p>So my requirements are:</p>
<ul>
<li>persistance</li>
<li>fast writes (currently 2k per second, up to 10k)</li>
<li>queries, e.g. aggregating OHLC data in real-time (50-100 per second)</li>
<li>adoptable to any modern programming language without writing raw queries (SDK for Python or JS for that database)</li>
<li>deployable on AWS or GCP without hassle</li>
</ul>
<p>I was thinking about Apache Cassandra. I'm not familiar with Cassandra, are powerful queries like OHLC one possible? Are there any alternatives to Cassandra?</p>
<p>Thanks in advance!</p>
",<database><amazon-web-services><cassandra><nosql>,"<p>Given what I've understood from your question, I believe Cassandra should easily fit your use-case.</p>
<p>Regarding your requirements:</p>
<ul>
<li><strong>persistence</strong> : Cassandra will not only persist your data but also cover redundancy with minimal configuration;</li>
<li><strong>fast writes</strong> : this is what Cassandra is most optimized for and while the exact throughput depends on a lot of factors, in general Cassandra will manage writes measured in the thousands/sec/core; Also, the eventual number o writes is not really relevant as Cassandra can scale linearly with no real penalty so 5k,10k, 100k or more are all doable;</li>
<li><strong>adaptability</strong> : Cassandra has official drivers for the most common languages(Python, C family, NodeJs, Java, Ruby, PHP, Scala) as well as community developed ones for more languages (<a href=""https://cassandra.apache.org/doc/latest/getting_started/drivers.html"" rel=""nofollow noreferrer"">list of divers</a>);</li>
<li><strong>deployable</strong> : It's very easy to deploy in the cloud. You can chose to deploy it manually on independent instances or maybe use a managed Cassandra cluster (AWS has one, it's called '<em>AWS Keyspaces</em>', Datastax(the company driving most of the development behind Cassandra) has one called '<em>Astra</em>' and there are even more possible solutions. Given that Cassandra is one of the major players when it comes to big-data storage finding a place for you DB in the cloud should be easy.</li>
</ul>
<p>I have only mentioned 4 of the 5 requirements. That is because when talking about reading, things get more complex and a larger discussion is needed.</p>
<p>500-100 reads/s given the 2k+ writes per second seem to be in line with the general idea of Cassandra being optimized for write intensive tasks. In Cassandra the way you will model your tables will dictate how well things can work. For a task like you have described my first thoughts are:</p>
<ol>
<li>You bucket each stock per day =&gt; you get a partition with around 30k rows (1 update/s for 8 trading hours) and a size of under 0.2MB (30k * 4B). This would be well within the recommended values and clearly under the worst case scenario ones;</li>
<li>when you need the aggregated data you have 2 options:</li>
</ol>
<p>2a. You read the partition as is and aggregate it application side (what I would recommend);</p>
<p>2b. You implement an &quot;<em>User-Defined Aggregate</em>&quot; function on your database that will do the work (<a href=""https://cassandra.apache.org/doc/latest/cql/functions.html"" rel=""nofollow noreferrer"">docs</a>). This should be doable although I won't guarantee it. Apart from being harder to implement, the problem is that putting this kind of extra workload on the DB might not be want you want given your apparent use-case. Let me explain: I'd expect your reading load to be most active during certain times, (before, during and after trading hours) with times when the load is lighter. Depending on your architecture, you could have multiple application instances up during peak times, and then scale them back during off-peak in order to lower costs. While applications can be easily scaled up and down on cloud providers like AWS and GC. Cassanadra cannot be scaled up and down like this (5 nodes in the morning, 3 in the night and so on)(well it could but it's not designed to and would be a terrible decision). So moving as much of the non-constant workload to the application seems the best idea;</p>
<ol start=""3"">
<li>(Optional) have a worker that at the end of the day/trading day will aggregate the values for each stock and save them to another table so that when looking at historic data it will be easier. This data could even be bucketed by week, month or even year depending on how much space the aggregated data takes.</li>
</ol>
<p>You could also add Spark and Kafka in front of Casandra for a more powerful approach to the real-time aggregation but we should't deviate that much from the question at hand.</p>
<p>Cassandra is very powerful with the right modeling and the right architecture. At first glance what you need seems to be a good fit for Cassandra however as powerful as it can be, as bad as it can get if you use it in ways it wasn't designed to. I hope this answer puts you on a path into making the right decision.</p>
<p>Cheers.</p>
",['table']
63908552,63912788,2020-09-15 19:21:32,Is there a way to efficiently get the top n smallest datapoints over the cluster key in Cassandra?,"<p>I understand that for Cassandra data is sorted on the cluster key only per partition key.</p>
<p>I am wondering if Cassandra has optimizations on global scans. Lets say that the cluster key is an integer value, if I want to search over all data on a Cassandra cluster to find collections with values <code>&lt; 3</code>. The Cassandra query engine will not need to continue looking at collections in a partition after encountering a number <code>&gt;= 3</code>. Are there APIs (such as CDK) offered by Cassandra which exercise these optimizations?</p>
",<cassandra>,"<p>There isn't a native CQL optimisation available for full table scans -- they will always be bad since Cassandra is optimised for OLTP workloads.</p>
<p>There are however optimisations done by the <a href=""https://github.com/datastax/spark-cassandra-connector"" rel=""nofollow noreferrer"">spark-cassandra-connector</a> for analytics (OLAP) workloads with Spark.</p>
<p>OLTP vs OLAP are worlds apart so you have to use the right tool for the job. Cheers!</p>
",['table']
63909451,63912921,2020-09-15 20:31:48,Query limitations due to composite partition keys in cassandra?,"<p>If i have two table structures, one with:
Let this be A,</p>
<pre><code>PRIMARY KEY (measureid, statename, reportyear, countyname) 
</code></pre>
<p>and another with, (Let this be B):</p>
<pre><code>PRIMARY KEY ((measureid, statename, reportyear), countyname)
</code></pre>
<p>What are the query limitations of table structure B over A ?
In what queries having composite partition key will pose a problem?</p>
",<cassandra><cql>,"<p>In table A where:</p>
<pre><code>PRIMARY KEY (measureid, statename, reportyear, countyname)
</code></pre>
<p>You can query the table with just <code>measureid</code> and it will return rows of <code>statename</code>. Specifically:</p>
<pre><code>SELECT FROM ... WHERE measureid = ?
</code></pre>
<p>Alternatively, you can also query with:</p>
<pre><code>SELECT FROM ... WHERE measureid = ? AND statename = ?
SELECT FROM ... WHERE measureid = ? AND statename = ? AND reportyear = ?
SELECT FROM ... WHERE measureid = ? AND statename = ? AND reportyear = ? AND countyname = ?
</code></pre>
<p>In table B where:</p>
<pre><code>PRIMARY KEY ((measureid, statename, reportyear), countyname)
</code></pre>
<p>You must specify all of <code>measureid</code>, <code>statename</code>, <code>reportyear</code> to query the data. This will return all the rows of <em>one partition</em>:</p>
<pre><code>SELECT FROM ... WHERE measureid = ? AND statename = ? AND reportyear = ?
</code></pre>
<p>To retrieve <em>one specific row</em> of <em>one partition</em>:</p>
<pre><code>SELECT FROM ... WHERE measureid = ? AND statename = ? AND reportyear = ? AND countyname = ?
</code></pre>
<p>To be clear, you <em>cannot</em> query table B with the following:</p>
<pre><code>SELECT FROM ... WHERE measureid = ?
SELECT FROM ... WHERE measureid = ? AND statename = ?
</code></pre>
<p>since you must specify the 3 columns of the partition key. I've explained why in this post <a href=""https://community.datastax.com/questions/7866/"" rel=""nofollow noreferrer"">https://community.datastax.com/questions/7866/</a>. Cheers!</p>
",['table']
63922135,63936634,2020-09-16 14:23:03,PerSSTableIndexWriter.java:211 - Rejecting value for column payload (analyzed false),"<p>we have Cassandra 3.4.4.
In the system.log we have a lot message like this:</p>
<pre><code>INFO  [CompactionExecutor:2] 2020-09-16 13:42:52,916 PerSSTableIndexWriter.java:211 - Rejecting value (size 1.938KiB, maximum 1.000KiB) for column payload (analyzed false) at /opt/cassandra/data/cfd/request-c8399780225a11eaac403f5be58182da/md-609-big SSTable.
</code></pre>
<p>What are the significance of these messages?
These entries appear several hundred per second, log rotates every minute.</p>
",<cassandra>,"<p>The symptoms you described tell me that you have added a SASI index on the <code>payload</code> column of the <code>cfd.request</code> table but didn't used to.</p>
<p>Those messages are coming because Cassandra is going through the data trying to index them that the <code>payload</code> column has too much data in it. The maximum term size for SASI is 1024 bytes but in the example you posted, the term size was 1.9KB.</p>
<p>If the column only contains ASCII characters, the maximum term length is 1024 characters since each ASCII character is 1 byte. If the column has extended Unicode such as Chinese or Japanese characters, the maximum term length is shorter since each of those take up 3 bytes.</p>
<p>You don't have a SASI analyzer configured on the index (<code>analyzed false</code>) so the whole column value is taken up as a single term. If you use the standard SASI analyzer, the column value will get tokenised breaking them up into multiple terms which are shorter and you won't see those indexing failures get logged.</p>
<p>If you're interested in the detailed fix steps, see <a href=""https://community.datastax.com/questions/8370/"" rel=""nofollow noreferrer"">https://community.datastax.com/questions/8370/</a>. Cheers!</p>
",['table']
63939694,63945495,2020-09-17 13:56:40,Add Column to Cassandra db with out losing data,"<p>I am using Cassandra database integrated into a spring boot application.</p>
<p>My Question is around the schema actions. If I need to make structural changes to the DB, say add a column to a table, the database needs to be recreated, however this means all the existing data gets deleted:</p>
<pre><code>schema-action: CREATE_IF_NOT_EXISTS
</code></pre>
<p>The only way I have managed to solve this is by using the <code>RECREATE</code> scheme action, but as mentioned earlier, this results in data-loss.</p>
<p>What would be the best approach to handle this? To add structural changes such as a column name with out having to recreate the database and lose all existing data?</p>
<p>Thanks</p>
",<java><spring-boot><cassandra>,"<p>Cassandra does allow you to modify the schema of an existing table without recreating it from scratch, using the <a href=""https://docs.scylladb.com/getting-started/ddl/#alter-table-statement"" rel=""nofollow noreferrer"">ALTER TABLE</a> statement via <code>cqlsh</code>. However, as explained in that link, there are some important limitations on the kind of changes you can do. You cannot modify the primary key of the table at all, you can add or delete regular columns, and you can't change the type of a column to a non-compatible one.</p>
<p>The reason for most of these limitations is how Cassandra needs to deal with the old data that already exists in the table. For example, it doesn't make sense to say that a column A that until now contained strings - will now contain integers - how are we supposed to handle all the old values in column A which weren't integers?</p>
<p>As Aaron rightly said in a comment, it is unlikely you'll want to do these schema changes as part of your application. These are usually rare operations which are done manually, or via some management application - not your usual application.</p>
",['table']
63978857,63985914,2020-09-20 12:14:17,Cassandra user table data modelling,"<p>I am learning cassandra. I have to do basic user table modeling. Here waht I have did.</p>
<pre><code>CREATE KEYSPACE IF NOT EXISTS users 
WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 3};

CREATE TYPE IF NOT EXISTS users.date_type(
    month SMALLINT,
    day SMALLINT,
    year INT
);

CREATE TYPE IF NOT EXISTS users.phone_type(
    code INT,
    phone TEXT,
);

-- create user
CREATE TABLE IF NOT EXISTS users.user(
    userid TIMEUUID,
    name TEXT,
    dob FROZEN&lt;date_type&gt;,
    username TEXT,
    email TEXT,
    phone FROZEN&lt;phone_type&gt;,
    email_verified BOOLEAN,
    phone_verified BOOLEAN,
    created_on TIMESTAMP,
    PRIMARY KEY (userid)
);

-- get user by username
CREATE TABLE IF NOT EXISTS users.user_by_username(
    userid TIMEUUID,
    username TEXT,
    password TEXT,
    PRIMARY KEY (username)
);

-- get user by user phone number
-- can have same number for more than one account
CREATE TABLE IF NOT EXISTS users.user_by_phone(
    userid TIMEUUID,
    phone FROZEN&lt;phone_type&gt;,
    password TEXT,
    PRIMARY KEY (phone, userid)
);

-- get user by email
-- can have same email for more than one account
CREATE TABLE IF NOT EXISTS users.user_by_email(
    userid TIMEUUID,
    email TEXT,
    password TEXT,
    PRIMARY KEY (email, userid)
);

-- user history
CREATE TABLE IF NOT EXISTS users.user_history(
    userid TIMEUUID,
    name TEXT,
    dob FROZEN&lt;date_type&gt;,
    username TEXT,
    phone FROZEN&lt;phone_type&gt;,
    email TEXT,
    phone_verified BOOLEAN,
    email_verified BOOLEAN,
    action TEXT,
    date TIMESTAMP,
    PRIMARY KEY (userid, date, action)
);
</code></pre>
<ol>
<li>Is above table design valid as per cassandra data modeling?</li>
<li>Why because if I want to create an user, I have to insert in 5 tables</li>
<li>And if I want to update email, phone or password, first I have to delete entry in table then insert with updated values</li>
</ol>
",<cassandra><data-modeling><cassandra-3.0><datamodel>,"<blockquote>
<p>Is above table design valid as per cassandra data modeling?</p>
</blockquote>
<p>Yes it is valid table design as per Cassandra data modelling.</p>
<blockquote>
<p>Why because if I want to create an user, I have to insert in 5 tables</p>
</blockquote>
<p>This is how you do in Cassandra, first you have queries which you want to cater and then design your tables according to them.</p>
<blockquote>
<p>And if I want to update email, phone or password, first I have to
delete entry in table then insert with updated values</p>
</blockquote>
<p>Just update the values, you do not need to delete them first. But make sure you update corresponding entries in each table.</p>
",['table']
64002650,64002809,2020-09-22 03:23:49,How to flush data in all tables of keyspace in cassandra?,"<p>I am currently writing tests in golang and I want to get rid of all the data of tables after finishing tests. I was wondering if it is possible to flush the data of all tables in cassandra.</p>
<p>FYI: I am using 3.11 version of Cassandra.</p>
",<cassandra><cql>,"<p>The term &quot;flush&quot; is ambiguous in this case.</p>
<p>In Cassandra, &quot;flush&quot; is an operation where data is &quot;flushed&quot; from memory and written to disk as SSTables. Flushing can happen automatically based on certain triggers or can be done manually with the <code>nodetool flush</code> command.</p>
<p>However based on your description, what you want is to &quot;truncate&quot; the contents of tables. You can do this using the following CQL command:</p>
<pre><code>cqlsh&gt; TRUNCATE ks_name.table_name
</code></pre>
<p>You will need to iterate over each table in the keyspace. For more info, see the <a href=""https://docs.datastax.com/en/cql-oss/3.x/cql/cql_reference/cqlTruncate.html"" rel=""nofollow noreferrer"">CQL <code>TRUNCATE</code> command</a>. Cheers!</p>
",['table']
64031514,64033950,2020-09-23 15:38:22,Is it a bad practice to have a Cassandra table with partitions of a single row?,"<p>Let's say I have a table like this</p>
<pre><code>CREATE TABLE request(
  transaction_id text,
  request_date timestamp,
  data text, 
  PRIMARY KEY (transaction_id)
);
</code></pre>
<p>The transaction_id is unique, so as far as I understand <strong>each partition in this table would have one row only</strong> and I'm not sure if this situation causes a performance issue in the OS, maybe because Cassandra creates a file for each partition causing lots of files to manage for its hosting OS, as a note I'm not sure how Cassandra creates its files for its tables.</p>
<p>In this scenario I can find a request by its transaction_id like</p>
<p><code>select data from request where transaction_id = 'abc';</code></p>
<p>If the previous assumption is correct, a different approach could be the next one?</p>
<pre><code>CREATE TABLE request( 
  the_date date, 
  transaction_id text, 
  request_date timestamp, 
  data text, 
  PRIMARY KEY ((the_date), transaction_id)
);
</code></pre>
<p>The field <em>the_date</em> would change every next day, so the partitions in the table would be created for each day.</p>
<p>In this scenario I would have to have <em>the_date</em> data always <strong>available</strong> to the client so I can find a request using the next query</p>
<p><code>select data from request where the_date = '2020-09-23' and transaction_id = 'abc';</code></p>
<p>Thank you in advance for your kind help!</p>
",<cassandra><primary-key><partition>,"<p>Cassandra doesn't create a separate file for each partition. One SSTable file may contain multiple partitions.  Partitions that consist only of one row are often called &quot;skinny rows&quot; - they aren't very bad, but may cause some performance issues:</p>
<ul>
<li>to access such partitions you still need to read a block with compressed data (by default it's 64Kb) that needs to be decompressed to read that data. If you're doing really random access, such blocks would be discarded from file cache and needs to be re-read from disk.  In this case, it's maybe useful to decrease the block size</li>
<li>if you have a lot of such partitions per table per node - this may heavily increase the size of the bloom filter, because each partition has a separate entry in it.   I saw some customers that had tens of gigabytes of memory allocated for bloom filter only because of the skinny partitions</li>
</ul>
<p>so it's really depends on the amount of data, access patterns, etc. It could be good or bad, depends on that factors.</p>
<p>If you have date available, and want to use it as part partition key - that may also not advisable because if you're writing and reading a lot of data on that day, then only some nodes will handle that load - this is so-called &quot;hot partitions&quot;.</p>
<p>You may implement so-called bucketing, when you infer partition key from the data.  But this will depend on the data available.  For example, if you have date + transaction ID as a string, you may create partition key as date + 1st character of that string - in this case you'll have N partition keys per day, that are distributed between nodes, eliminating the hot partition problem.</p>
<p>See the <a href=""https://docs.datastax.com/en/landing_page/doc/landing_page/dataModel.html#Checktablestructure"" rel=""noreferrer"">corresponding best practices doc from DataStax</a> about that topic.</p>
",['table']
64135800,64137666,2020-09-30 10:37:23,How to delete all rows in Cassandra Keyspace,"<p>I need to delete all rows in Cassandra but with Amazon Keyspace isn't possible to execute <code>TRUNCATE tbl_name</code> because the TRUNCATE api isn't supported yet.</p>
<p>Now the few ideas that come in my mind are a little bit tricky:</p>
<h3>Solution A</h3>
<ul>
<li>select all the rows</li>
<li>cycle all the rows and delete it (one by one or in a batch)</li>
</ul>
<h3>Solution B</h3>
<ul>
<li><code>DROP TABLE</code></li>
<li><code>CREATE TABLE </code> with the structure of the old table</li>
</ul>
<p>Do you have any idea to keep the process <strong>simplest</strong>?</p>
<p>Tnx in advance</p>
",<node.js><amazon-web-services><cassandra><amazon-keyspaces>,"<p>Solution B should be fine in absence of TRUNCATE. In older versions (version prior to 2.1) of Cassandra recreating table with the same name was a problem. Refer article <a href=""https://support.datastax.com/hc/en-us/articles/204226339-FAQ-How-to-drop-and-recreate-a-table-in-Apache-Cassandra-versions-older-than-2-1"" rel=""nofollow noreferrer"">Datastax FAQ Blog</a>. But since then issue has been resolved via <a href=""https://issues.apache.org/jira/plugins/servlet/mobile#issue/CASSANDRA-5202"" rel=""nofollow noreferrer"">CASSANDRA-5202</a>.</p>
<p>If data in table is not required anymore it is better to drop the table and recreate it. Moreover it will be very tedious task if table contains big amount of data.</p>
",['table']
64161387,64331252,2020-10-01 18:38:18,"Apache Cassandra 3.11.6: clustering key error, Undefined column name in table after cass-stress write","<p>I am currently experiencing an issue where I try to SELECT or INSERT specific column data from cassandra and keep getting an undefined column name error, despite the fact that the column name is a clustering key when looking at the table. However, other columns behave normally.</p>
<pre><code>keyspace_name | table_name | column_name | clustering_order | column_name_bytes | kind          | position | type
---------------+------------+-------------+------------------+-------------------+---------------+----------+------
    keyspace1 |  standard1 |          C0 |             none |            0x4330 |        static |       -1 | blob
    keyspace1 |  standard1 |     column1 |              asc |  0x636f6c756d6e31 |    clustering |        0 | text
    keyspace1 |  standard1 |         key |             none |          0x6b6579 | partition_key |        0 | blob
    keyspace1 |  standard1 |       value |             none |      0x76616c7565 |       regular |       -1 | blob

cqlsh&gt; SELECT  &quot;column1&quot; from keyspace1.standard1;
InvalidRequest: Error from server: code=2200 [Invalid query] message=&quot;Undefined column name column1&quot;

cqlsh&gt; SELECT  &quot;C0&quot; from keyspace1.standard1;

 C0
------------------------
 0xdc9e1bf05eab897f470a
 0x5ff08459ccd892a25f91
 0x85182fdfe7f86306cd58
 0x10f1dd6febff8cbcf3ad
 0xb8e05320cd1037d6e317
</code></pre>
<p>Additionally, when inserting data, despite column1 being &quot;undefined,&quot; it is still required as a clustering key</p>
<pre><code>cqlsh&gt; insert into keyspace1.standard1 (key) VALUES (0xcccc) ;
InvalidRequest: Error from server: code=2200 [Invalid query] message=&quot;Some clustering keys are missing: column1&quot;
</code></pre>
<p>when inserting data into other columns however, behavior is normal</p>
<pre><code>cqlsh&gt; insert into keyspace1.standard1 (key, &quot;C0&quot;) VALUES (0xcccc, 0xbbbb) ;  
cqlsh&gt; SELECT  &quot;C0&quot; from keyspace1.standard1;

 C0                      key
 ----------------------- ------------------------
 0xdc9e1bf05eab897f470a 0x37373539364d4f323330
                 0xbbbb                 0xcccc
 0x5ff08459ccd892a25f91 0x4f503030314c35393330
 0x85182fdfe7f86306cd58 0x30503337373039503231
 0x10f1dd6febff8cbcf3ad 0x394e35344e4b34383631
 0xb8e05320cd1037d6e317 0x4f384c4b37394c4f3631
</code></pre>
<p>This is after I have run cassandra-stress, and I have run similar tests in 3.11.4 and it worked fine, but when queried the column1 (clustering key) entries were all null, which is something I did not think was possible.</p>
<p>Is this intended behavior that changed from 3.11.4 onwards?</p>
",<cassandra><cassandra-3.0><cqlsh><cassandra-stress>,"<p>It looks like you are dealing with a compact storage table.</p>
<p>You can verify this in cqlsh:</p>
<pre><code>describe table keyspace1.standard1;
</code></pre>
<p>If you see <code>WITH COMPACT STORAGE</code> then you have a compact storage table.</p>
<p>Once you drop the compact storage format from this table, you should be able to select the column <code>column1</code>.</p>
<p>To drop compact storage:</p>
<pre><code>ALTER TABLE keyspace1.standard1 DROP COMPACT STORAGE;
</code></pre>
<p>I am not sure how you ended up with your table definition (unless cassandra-stress created it for you). The column <code>column1</code> is normally found in tables that have been migrated from compact storage.</p>
<p>Normally the following happens when migrating the storage format (for tables that have no clustering column):</p>
<ul>
<li>Two new columns column1 text and value blob are added. These columns contain any data written outside the CQL table schema to the Thrift table.</li>
<li>column1 becomes a clustering column.</li>
<li>All regular columns become static columns.</li>
</ul>
<p>More information can be found here:
<a href=""https://docs.datastax.com/en/cql-oss/3.3/cql/cql_using/dropCompactStorage.html"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/cql-oss/3.3/cql/cql_using/dropCompactStorage.html</a></p>
",['table']
64356049,64361179,2020-10-14 14:55:18,how much disk space is required to switch from SizeTiered to level compaction strategy in Scylla db?,"<p>I have a 20 node scylla db cluster and each node is at 70% disk space usage. I want to switch to leveled compaction strategy from Size tiered strategy. Can I do that with this much disk space left? How much disk space at max will be required?</p>
",<cassandra><scylla>,"<p>As was mentioned, you don't have enough disk space to switch compaction strategies. I can think of 2 or possibly 3 ways to work around this.</p>
<ol>
<li>add more disk space - if you are in a cloud environment, it probably won't be a problem to grow your data disks and expand the file system.</li>
<li>add more nodes - however, this will take a while because the data will have to be streamed to all the new nodes and then you'll need to run <code>nodetool cleanup</code> on the cluster to reclaim the unused space.</li>
<li>change the compaction strategy on one table at a time. Start with small tables that can fit in your 30% space. this will also take a long time and there is the risk that you'll run out of space if you choose the wrong table to ALTER.</li>
</ol>
",['table']
64373999,64375142,2020-10-15 14:37:29,When to use UUID instead of millisecond timestamp in Cassandra?,"<p>I have created table in <strong>cassandra</strong>, where primary key is some column with <code>timeuuid</code> as datatype. I am able to identify each record uniquely with just millisecond precision timestamp value stored as <code>bigint</code>.</p>
<p>I have used <strong>java</strong> datastax driver to connect cassandra. Before inserting record into database I am converting millisecond timestamp into UUID for each record. Which is overhead and can be removed.</p>
<ol>
<li>Can some one explain what are the benefits of using <code>timeuuid</code> instead of <code>bigint</code> considering records are able to identified without timeuuid's uniqueness ?</li>
<li>Is there any performance impact in between <code>timeuuid</code> and <code>bigint</code> data type ?</li>
</ol>
",<java><cassandra><timestamp><datastax-java-driver>,"<p>There shouldn't be very big impact for performance if you generate timeuuid from timestamp. <code>timeuuid</code> is useful if you may have many events happening in the same millisecond, and you need sorting - with <code>timeuuid</code> you may get up to 10,000 different values inside the millisecond. Typical use case is the table with structure like this:</p>
<pre><code>create table tuuid (
  pk int,
  tuuid timeuuid, 
  ....
  ....,
  primary key (pk, tuiid));
</code></pre>
<p>In this case, you will get sorting (ascending or descending) together with uniqueness of values for <code>tuuid</code>. Of course you can come with primary key of <code>(pk, timestamp, random-value)</code>, but with <code>timeuuid</code> you don't need to have an additional column for uniqueness.  One of the drawback of <code>timeuuid</code> is integration with Spark, for example, as it doesn't have this type, and may not able to perform pushing of the filters.</p>
<p>If you don't need uniqueness, then just switch to <code>timestamp</code> - it's represented as 8-bytes long internally - the same as <code>bigint</code>, but you don't need to do conversions yourself, etc.</p>
",['table']
64442523,66286175,2020-10-20 09:42:03,Enable Json Insert on Amazon Keyspaces,"<p>I am migrating from an hosted Cassandra to Amazon Keyspace.</p>
<p>Some production processes use the <code>Cassandra Json Insert</code>. When I try to run one of this processes to store data in Amazon Keyspaces I get the following error:</p>
<pre><code>Unsupported statement: org.apache.cassandra.cql3.statements.UpdateStatement$ParsedInsertJson@7ba2351
</code></pre>
<p>I suppose that this functionality is not enabled in Amazon Keyspace. On my local Cassandra i didn't enable anything to use <code>JSON insert</code>. There is a way to enable this functionality on Amazon Keyspaces</p>
",<json><amazon-web-services><cassandra><insert><amazon-keyspaces>,"<p>Keyspaces started supporting JSON on January 22, 2021.
You can enter Inserts and Selects using the JSON API the same way you did using Cassandra.</p>
<p>You can execute insert and select statements using the JSON API against your existing tables.</p>
<p>Here is a link of Information on how Keyspaces and JSON work together: <a href=""https://aws.amazon.com/about-aws/whats-new/2021/01/amazon-keyspaces-for-apache-cassandra-now-supports-json-syntax/"" rel=""nofollow noreferrer"">https://aws.amazon.com/about-aws/whats-new/2021/01/amazon-keyspaces-for-apache-cassandra-now-supports-json-syntax/</a></p>
<p>Keyspaces is adding new features all the time. to To see supported APIs visit <a href=""https://docs.aws.amazon.com/keyspaces/latest/devguide/cassandra-apis.html"" rel=""nofollow noreferrer"">this</a>.</p>
<p>Below is an example of the JSON api. Copy the following create table statement into the Amazon Keyspaces CQL Console. Then execute the following insert and select statements after the table has been created.</p>
<p>The code bellow will create a json_keyspaces Keyspace, then it CREATES a table called shoppingcart.
Replace the keyspace name with your own. When its finished you will have a new table called shoppingcart.</p>
<p>Then the INSERT statement will insert some JSON data using the JSON API. Bellow is a SELECT statement to query the data from the table.</p>
<pre><code>
CREATE TABLE &quot;json_keyspaces”.”shoppingcart&quot;(
    &quot;user_id&quot; text,
    &quot;item_id&quot; text,
    &quot;quantity&quot; int,
    PRIMARY KEY(&quot;user_id&quot;, &quot;item_id&quot;))
WITH CUSTOM_PROPERTIES = {
    'capacity_mode':{'throughput_mode':'PAY_PER_REQUEST'}, 
    'point_in_time_recovery':{'status':'enabled'}
} AND CLUSTERING ORDER BY(&quot;item_id&quot; ASC)


INSERT INTO json_keyspaces.shoppingcart JSON '{
  &quot;user_id&quot;: &quot;id123&quot;,
  &quot;item_id&quot;: &quot;blue_shirt&quot;,
  &quot;quantity&quot; : 5
 }';
  

SELECT json user_id, item_id, quantity from json_keyspaces.shoppingcart;
</code></pre>
",['table']
64463238,64464802,2020-10-21 12:11:37,Writing Spark streaming PySpark dataframe to Cassandra overwrites table instead of appending,"<p>I'm running a 1-node cluster of Kafka, Spark and Cassandra. All locally on the same machine.</p>
<p>From a simple Python script I'm streaming some dummy data every 5 seconds into a Kafka topic. Then using Spark structured streaming, I'm reading this data stream (one row at a time) into a PySpark DataFrame with <code>startingOffset</code> = <code>latest</code>. Finally, I'm trying to append this row to an already existing Cassandra table.</p>
<p>I've been following (<a href=""https://stackoverflow.com/questions/45113538/how-to-write-streaming-dataset-to-cassandra"">How to write streaming Dataset to Cassandra?</a>) and (<a href=""https://stackoverflow.com/questions/58661075/cassandra-sink-for-pyspark-structured-streaming-from-kafka-topic"">Cassandra Sink for PySpark Structured Streaming from Kafka topic</a>).</p>
<p>One row of data is being successfully written into the Cassandra table but my problem is it's being <strong>overwritten</strong> every time <strong>rather than appended</strong> to the end of the table. What might I be doing wrong?</p>
<p>Here's my code:</p>
<p>CQL DDL for creating <code>kafkaspark</code> keyspace followed by <code>randintstream</code> table in Cassandra:</p>
<pre><code>DESCRIBE keyspaces;

CREATE KEYSPACE kafkaspark
  WITH REPLICATION = { 
   'class' : 'SimpleStrategy', 
   'replication_factor' : 1 
  };
  
USE kafkaspark; 

CREATE TABLE randIntStream (
    key int,
    value int,
    topic text,
    partition int,
    offset bigint,
    timestamp timestamp,
    timestampType int,
    PRIMARY KEY (partition, topic)
);
</code></pre>
<p>Launch PySpark shell</p>
<pre><code>./bin/pyspark --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1,com.datastax.spark:spark-cassandra-connector_2.12:3.0.0 --conf spark.cassandra.connection.host=127.0.0.1,spark.sql.extensions=com.datastax.spark.connector.CassandraSparkExtensions
</code></pre>
<p>Read latest message from Kafka topic into streaming DataFrame:</p>
<pre><code>df = spark.readStream.format(&quot;kafka&quot;).option(&quot;kafka.bootstrap.servers&quot;, &quot;localhost:9092&quot;).option(&quot;startingOffsets&quot;,&quot;latest&quot;).option(&quot;subscribe&quot;,&quot;topic1&quot;).load()
</code></pre>
<p>Some transformations and checking schema:</p>
<pre><code>df2 = df.withColumn(&quot;key&quot;, df[&quot;key&quot;].cast(&quot;string&quot;)).withColumn(&quot;value&quot;, df[&quot;value&quot;].cast(&quot;string&quot;))
df3 = df2.withColumn(&quot;key&quot;, df2[&quot;key&quot;].cast(&quot;integer&quot;)).withColumn(&quot;value&quot;, df2[&quot;value&quot;].cast(&quot;integer&quot;))
df4 = df3.withColumnRenamed(&quot;timestampType&quot;,&quot;timestamptype&quot;)
df4.printSchema()
</code></pre>
<p>Function for writing to Cassandra:</p>
<pre><code>def writeToCassandra(writeDF, epochId):
    writeDF.write \
    .format(&quot;org.apache.spark.sql.cassandra&quot;) \
    .options(table=&quot;randintstream&quot;, keyspace=&quot;kafkaspark&quot;) \
    .mode(&quot;append&quot;) \
    .save()
</code></pre>
<p>Finally, query to write to Cassandra from Spark:</p>
<pre><code>query = df4.writeStream \
.trigger(processingTime=&quot;5 seconds&quot;) \
.outputMode(&quot;update&quot;) \
.foreachBatch(writeToCassandra) \
.start()
</code></pre>
<p><code>SELECT *</code> on table in Cassandra:
<a href=""https://i.stack.imgur.com/XvdHn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XvdHn.png"" alt=""enter image description here"" /></a></p>
",<apache-spark><apache-kafka><cassandra><spark-structured-streaming><spark-cassandra-connector>,"<p>If the row is always rewritten in Cassandra, then you may have incorrect primary key in the table - you need to make sure that every row will have an unique primary key. If you're creating Cassandra table from Spark, then by default it just takes first column as partition key, and it alone may not be unique.</p>
<p>Update after schema was provided:</p>
<p>Yes, that's the case that I was referring - you have a primary key of <code>(partition, topic)</code>, but every row from specific partition that you read from that topic will have the same value for primary key, so it will overwrite previous versions. You need to make your primary key unique - for example, add the <code>offset</code> or <code>timestamp</code> columns to the primary key (although <code>timestamp</code> may not be unique if you have data produced inside the same millisecond).</p>
<p>P.S. Also, in connector 3.0.0 you don't need <code>foreachBatch</code>:</p>
<pre><code>df4.writeStream \
  .trigger(processingTime=&quot;5 seconds&quot;) \
  .format(&quot;org.apache.spark.sql.cassandra&quot;) \
  .options(table=&quot;randintstream&quot;, keyspace=&quot;kafkaspark&quot;) \
  .mode(&quot;update&quot;) \
  .start()
</code></pre>
<p>P.P.S if you just want to move data from Kafka into Cassandra, you may consider the use of the <a href=""https://docs.datastax.com/en/kafka/doc/index.html"" rel=""nofollow noreferrer"">DataStax's Kafka Connector</a> that could be much lightweight compared to the Spark.</p>
",['table']
64689423,64692845,2020-11-05 00:07:07,what's the nature ordering by cassandra java driver queryBuilder,"<p>i have cassandra table with partition key as email and clustering key as topic(String) and eventAt(timestamp), i need to query the table to get most recent N topics, the recentency is determined by eventAt,  here is the query I have:</p>
<pre><code>BuiltStatement builtStatement =
        QueryBuilder.select()
            .all()
            .from(&quot;email_table&quot;)
            .where(QueryBuilder.eq(Email.COLUMN_EMAIL, bindMarker()))
            .and(QueryBuilder.eq(Email.COLUMN_TOPIC, bindMarker()))
            .limit(bindMarker());
</code></pre>
<p>the results are sorted by eventAt column in desc order, though it's the result I expect but wondering how Cassandra handling the query result ordering? I thought it would be order by eventAt column in asc order but apparently it's not</p>
",<cassandra>,"<p>Cassandra usually returns data ordered according to the definition of the clustering columns.  So if you have table like this:</p>
<pre><code>create table tbl (
  email text,
  topic text,
  eventAt timestamp,
  ....,
  primary key (email, topic, eventAt)
) with clustering order by (topic asc, eventAt desc);
</code></pre>
<p>then when you query the data, data is returned in sorted form - as they are stored on the disk - according to the definition in the <code>clustering order by</code> (SSTables is the acronym for sorted string tables), and read sequentially:</p>
<ul>
<li>first sorted by topic, in the ascending order</li>
<li>inside the specific topic, ordered by <code>eventAt</code> in descending order</li>
</ul>
<p>You may have a limited support for changing the sorting order when querying the data, but it's just changing the order in which data is read, and if you have the multiple clustering columns, you could be surprised by results - usually it works only for last clustering column that is used in the query.</p>
<p>If you want to have N recent topics, then you need to have another table with <code>eventAt</code> as first clustering column:</p>
<pre><code>create table tbl (
  email text,
  topic text,
  eventAt timestamp,
  ....,
  primary key (email, eventAt, topic)
) with clustering order by (topic asc, eventAt desc);
</code></pre>
<p>then you can easily get last N topics by applying <code>select ... from tbl ... limit N</code>.</p>
",['table']
65186864,65199151,2020-12-07 17:56:03,Reading guarantees for full table scan while updating the table?,"<p>Given schema:</p>
<pre><code>CREATE TABLE keyspace.table (
    key text,
    ckey text,
    value text
    PRIMARY KEY (key, ckey)
) 
</code></pre>
<p>...and Spark pseudocode:</p>
<pre class=""lang-scala prettyprint-override""><code>val sc: SparkContext = ...
val connector: CassandraConnector = ...
sc.cassandraTable(&quot;keyspace&quot;, &quot;table&quot;)
  .mapPartitions { partition =&gt;
    connector.withSessionDo { session =&gt;
      partition.foreach { row =&gt;
        val key = row.getString(&quot;key&quot;)
        val ckey = Random.nextString(42)
        val value = row.getString(&quot;value&quot;)
        session.execute(s&quot;INSERT INTO keyspace.table (key, ckey, value)&quot; + 
          &quot; VALUES ($key, $ckey, $value)&quot;)
      }
    }
  }
</code></pre>
<p>Is it possible for a code like this to read an inserted value within a single application (Spark job) run? More generalized version of my question would be whether a token range scan CQL query can read newly inserted values while iterating over rows.</p>
",<apache-spark><cassandra><cql><spark-cassandra-connector><scylla>,"<p>Yes, it is possible exactly as Alex wrote
but I don't think it's possible with above code</p>
<p>So per data model the table is ordered by ckey in ascending order</p>
<p>The funny part however is the page size and how many pages are prefetched and since this is by default 1000 (spark.cassandra.input.fetch.sizeInRows), then the only problem could occur, if you wouldn't use 42, but something bigger and/or the executor didn't page yet</p>
<p>Also I think you use unnecessary nesting, so the code to achieve what you want might be simplified (after all cassandraTable will give you a data frame).</p>
<p>(I hope I understand that you want to read per partition (note a partition in your case is all rows under one primary key - &quot;key&quot;) and for every row (distinguished by ckey) in this partition generate new one (with new ckey that will just duplicate value with new ckey) - use case for such code is a mystery for me, but I hope it has some sense:-))</p>
",['table']
65218843,65256244,2020-12-09 14:33:58,Apache Cassandra stock data model design,"<p>I got a lot of data regarding stock prices and I want to try Apache Cassandra out for this purpose. But I'm not quite familiar with the primary/ partition/ clustering keys.</p>
<p>My database columns would be:</p>
<pre><code>Stock_Symbol
Price
Timestamp
</code></pre>
<p>My users will always filter for the Stock_Symbol (where stock_symbol=XX) and then they might filter for a certain time range (Greater/ Less than (equals)). There will be around 30.000 stock symbols.</p>
<p>Also, what is the big difference when using another &quot;filter&quot;, e.g. exchange_id (only two stock exchanges are available).</p>
<pre><code>Exchange_ID
Stock_Symbol
Price
Timestamp
</code></pre>
<p>So my users would first filter for the stock exchange (which is more or less a foreign key), then for the stock symbol (which is also more or less a foreign key). The data would be inserted/ written in this order as well.</p>
<p>How do I have to choose the keys?</p>
",<cassandra><nosql><primary-key><cassandra-3.0><composite-primary-key>,"<h2>The Quick Answer</h2>
<p>Based on your use-case and predicted query pattern, I would recommend one of the following for your table:</p>
<pre><code>PRIMARY KEY (Stock_Symbol, Timestamp)
</code></pre>
<p>The partition key is made of <code>Stock_Symbol</code>, and <code>Timestamp</code> is the only clustering column. This will allow <code>WHERE</code> to be used with those two fields. If either are to be filtered on, filtering on <code>Stock_Symbol</code> will be required in the query and must come as the first condition to <code>WHERE</code>.</p>
<p>Or, for the second case you listed:</p>
<pre><code>PRIMARY KEY ((Exchange_ID, Stock_Symbol), Timestamp)
</code></pre>
<p>The partition key is composed of <code>Exchange_ID</code> and <code>Stock_Symbol</code>, and <code>Timestamp</code> is the only clustering column. This will allow <code>WHERE</code> to be used with those three fields. If any of those three are to be filtered on, filtering on both <code>Exchange_ID</code> and <code>Stock_Symbol</code> will be required in the query and must come in that order as the first two conditions to <code>WHERE</code>.</p>
<p>See the last section of this answer for a few other variations that could also be applied based on your needs.</p>
<h2>Long Answer &amp; Explanation</h2>
<p><strong>Primary Keys, Partition Keys, and Clustering Columns</strong></p>
<p>Primary keys in Cassandra, similar to their role in relational databases, serve to identify records and index them in order to access them quickly. However, due to the distributed nature of records in Cassandra, they serve a secondary purpose of <em>also</em> determining which node that a given record should be stored on.</p>
<p>The primary key in a Cassandra table is further broken down into two parts - the <a href=""https://docs.datastax.com/en/dse/5.1/cql/cql/cql_using/wherePK.html"" rel=""nofollow noreferrer"">Partition Key</a>, which is mandatory and by default is the first column in the primary key, and optional <a href=""https://docs.datastax.com/en/dse/5.1/cql/cql/cql_using/whereClustering.html"" rel=""nofollow noreferrer"">clustering column(s)</a>, which are all fields that are in the primary key that are not a part of the partition key.</p>
<p>Here are some examples:</p>
<pre><code>PRIMARY KEY (Exchange_ID)
</code></pre>
<p><code>Exchange_ID</code> is the sole field in the primary key and is also the partition key. There are no additional clustering columns.</p>
<pre><code>PRIMARY KEY (Exchange_ID, Timestamp, Stock_Symbol)
</code></pre>
<p><code>Exchange_ID</code>, <code>Timestamp</code>, and <code>Stock_Symbol</code> together form a composite primary key. The partition key is <code>Exchange_ID</code> and <code>Timestamp</code> and <code>Stock_Symbol</code> are both clustering columns.</p>
<pre><code>PRIMARY KEY ((Exchange_ID, Timestamp), Stock_Symbol)
</code></pre>
<p><code>Exchange_ID</code>, <code>Timestamp</code>, and <code>Stock_Symbol</code> together form a composite primary key. The partition key is composed of both <code>Exchange_ID</code> and <code>Timestamp</code>. The extra parenthesis grouping <code>Exchange_ID</code> and <code>Timestamp</code> group them into a single composite partition key, and <code>Stock_Symbol</code> is a clustering column.</p>
<pre><code>PRIMARY KEY ((Exchange_ID, Timestamp))
</code></pre>
<p><code>Exchange_ID</code> and <code>Timestamp</code> together form a composite primary key. The partition key is composed of both <code>Exchange_ID</code> and <code>Timestamp</code>. There are no clustering columns.</p>
<p><strong>But What Do They Do?</strong></p>
<p>Internally, the partitioning key is used to calculate a token, which determines on which node a record is stored. The clustering columns are not used in determining which node to store the record on, but they are used in determining order of how records are laid out within the node - this is important when querying a range of records. Records whose clustering columns are similar in value will be stored close to each other on the same node; they &quot;cluster&quot; together.</p>
<p><strong>Filtering in Cassandra</strong></p>
<p>Due to the distributed nature of Cassandra, fields can only be filtered on if they are indexed. This can be accomplished in a few ways, usually by being a part of the primary key or by having a secondary index on the field. Secondary indexes can cause performance issues <a href=""https://docs.datastax.com/en/archived/cql/3.3/cql/cql_using/useSecondaryIndex.html"" rel=""nofollow noreferrer"">according to DataStax Documentation</a>, so it is typically recommended to capture your use-cases using the primary key if possible.</p>
<p>Any field in the primary key can have a <code>WHERE</code> clause applied to it (unlike unindexed fields which <em>cannot</em> be filtered on in the general case), but there are some stipulations:</p>
<ul>
<li><strong>Order Matters</strong> - The primary key fields in the <code>WHERE</code> clause must be in the order that they are defined; if you have a primary key of <code>(field1, field2, field3)</code>, you cannot do <code>WHERE field2 = 'value'</code>, but rather you must include the preceding fields as well: <code>WHERE field1 = 'value' AND field2 = 'value'</code>.</li>
<li><strong>The Entire Partition Key Must Be Present</strong> - If applying a <code>WHERE</code> clause to the primary key, the entire partition key must be given so that the cluster can determine what node in the cluster the requested data is located in; if you have a primary key of <code>((field1, field2), field3)</code>, you cannot do <code>WHERE field1 = 'value'</code>, but rather you must include the full partition key: <code>WHERE field1 = 'value' AND field2 = 'value'</code>.</li>
</ul>
<p><strong>Applied to Your Use-Case</strong></p>
<p>With the above info in mind, you can take the analysis of how users will query the database, as you've done, and use that information to <a href=""https://shermandigital.com/blog/designing-a-cassandra-data-model/"" rel=""nofollow noreferrer"">design your data model</a>, or more specifically in this case, the primary key of your table.</p>
<p>You mentioned that you will have about 30k unique values for <code>Stock_Symbol</code> and further that it will always be included in <code>WHERE</code> cluases. That sounds initially like a resonable candidate for a partition key, as long as queries will include only a single value that they are searching for in <code>Stock_Symbol</code> (e.g. <code>WHERE Stock_Symbol = 'value'</code> as opposed to <code>WHERE Stock_Symbol &lt; 'value'</code>). If a query is intended to return multiple records with multiple values in <code>Stock_Symbol</code>, there is a danger that the cluster will need to retrieve data from multiple nodes, which may result in performance penalties.</p>
<p>Further, if your users wish to filter on <code>Timestamp</code>, it should also be a part of the primary key, though wanting to filter on a range indicates to me that it probably shouldn't be a part of the partition key, so it would be a good candidate for a clustering column.</p>
<p>This brings me to my recommendation:</p>
<pre><code>PRIMARY KEY (Stock_Symbol, Timestamp)
</code></pre>
<p>If it were important to distribute data based on both the <code>Stock_Symbol</code> and the <code>Timestamp</code>, you could introduce a pre-calculated time-bucketed field that is based on the time but with less cardinality, such as <code>Day_Of_Week</code> or <code>Month</code> or something like that:</p>
<pre><code>PRIMARY KEY ((Stock_Symbol, Day_Of_Week), Timestamp)
</code></pre>
<p>If you wanted to introduce another field to filtering, such as <code>Exchange_ID</code>, it could be a part of the partition key, which would mandate it being included in filters, or it could be a part of the clustering column, which would mean that it wouldn't be required unless subsequent fields in the primary key needed to be filtered on. As you mentioned that users will always filter by <code>Exchange_ID</code> and then by <code>Stock_Symbol</code>, it might make sense to do:</p>
<pre><code>PRIMARY KEY ((Exchange_ID, Stock_Symbol), Timestamp)
</code></pre>
<p>Or to make it non-mandatory:</p>
<pre><code>PRIMARY KEY (Stock_Symbol, Exchange_ID, Timestamp)
</code></pre>
",['table']
65346964,65356241,2020-12-17 19:13:04,How to Design a Table for scoring system in Cassandra,"<p>I have a table for my products in Cassandra. In this table I store the product name and seller name (there may be thousands of sellers) and score (Product score of the seller) so as following</p>
<pre><code>CREATE TABLE products
(
    product_name varchar,
    seller_name varchar,
    score    int,
    primary key (product_name, seller_name)
);
</code></pre>
<p>I need to update score in my code</p>
<pre><code>UPDATE products SET score = 2 WHERE product_name = &quot;iphone-7&quot; AND seller_name = &quot;jack&quot; ;
</code></pre>
<p>Everything is fine, except for Select Query:</p>
<pre><code>SELECT * FROM products WHERE product_name = &quot;iphone-7&quot; ORDER BY SCORE DESC;
</code></pre>
<p>I need to get products by score order But as you know, it is not possible to sort without score being the primary key But if I put the score in the primary key, it will be not possible to update it what's the solution?</p>
",<database><cassandra><nosql><cassandra-3.0>,"<p>The solution is to have two tables that cover your requirements. For example the second table could be</p>
<pre><code>CREATE TABLE products_score
(
    product_name varchar,
    score    int,
    primary key (product_name, score)
);
</code></pre>
<p>so you populate both when writing and use the second one for ordering by score</p>
",['table']
65479623,65480004,2020-12-28 15:01:11,Create table in Cassandra. Order by,"<p>I am creating a table that have the columns below:</p>
<ul>
<li>Mark</li>
<li>Course Name</li>
<li>University</li>
<li>Student_ID</li>
<li>Price Paid</li>
</ul>
<p>The table should be sorted by mark.
It should be done in Cassandra and I have to implement the two queries below:</p>
<pre><code>select Student_ID, Course_Name from table where mark='A'
</code></pre>
<p>and</p>
<pre><code>select Student_ID, max(Price_Paid) from table group by student_id
</code></pre>
<p>I have tried defining the table as follows but it does not work for the second query:</p>
<pre><code>CREATE TABLE table_students(
  Course Name text,
  Price Paid int, 
  nombre_profesor text, 
  id_estudiante int, 
  Mark text, 
primary key(Mark, Price Paid, Course Name, Student_ID));
</code></pre>
<p>The first query works but when I try to run the second one a message indicating that Price Paid should not be before Student_ID appears. If I switch the order between both, the table is not properly sorted.</p>
<p>I would really appreciate your support</p>
",<cassandra><nosql>,"<p>Don't try to accomplish it with one table - the data modeling rule for Cassandra says - one query, one table.  So if you want to aggregate by <code>student_id</code>, you need to have it as partition key. Create a table with primary key something like this <code>(student_id, course_name)</code>.</p>
<p>I recommend to read first chapters of <a href=""https://www.datastax.com/resources/ebook/oreilly-cassandra-definitive-guide"" rel=""nofollow noreferrer"">this free book</a> (Cassandra. The Definitive Guide, 3rd edition)</p>
<p>P.S. Also, having <code>Mark</code> as partition key isn't very good - you have too few possible values for it, so you'll have very limited number of big partitions.  Instead, you can for example, add something to it, for example, year, or something like...</p>
",['table']
65488722,65490488,2020-12-29 07:26:09,Cassandra nodes extremely unbalanced with Vnodes token strategy,"<p>Using the Vnodes strategy with 256 tokens per node, my cluster shows info like below while executing nodetool status. Seems the load of my cluster is extremely unbalanced. I dont know what cause this. Is partition key of tables related ? Any comments would be welcome, Thanks!</p>
<pre><code>Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address        Load       Tokens       Owns    Host ID                               Rack
UN  192.168.1.190  9.78 GiB   256          ?       f3e56d8d-caf2-450a-b4f1-e5ac5163a17a  rack1
UN  192.168.1.191  77.53 MiB  256          ?       e464cda9-ca8b-400b-82eb-59256d23c94a  rack1
UN  192.168.1.192  89.31 MiB  256          ?       6feaa65f-2523-4b65-9717-f4f3cbf28ef0  rack1

</code></pre>
",<cassandra>,"<p>Yes, most probably there is a skew in the distribution of partition keys, most probably some partitions have much more rows than other.  Check <a href=""https://docs.datastax.com/en/landing_page/doc/landing_page/dataModel.html"" rel=""nofollow noreferrer"">this document</a> for recommendations, especially the sections &quot;Number of cells per partition&quot; and &quot;Big partitions&quot;.  You can use the number of tools to check the hypothesis:</p>
<ul>
<li><code>nodetool tablehistograms</code> (may need to be executed for every table separately) on each host will show you the number of cells and partition size in bytes at 50%, 75%, ..., and 100% percentiles.  You may see very big differences between 95% &amp; 100% percentiles.</li>
<li><code>nodetool tablestats</code> will show the max &amp; average size of the partition per table per host</li>
<li><a href=""https://www.datastax.com/blog/2019/07/datastax-bulk-loader-counting"" rel=""nofollow noreferrer"">DSBulk</a> has an option to show the largest partitions based on the number of rows per partition - it needs to be executed for every table in cluster, but only once, not from each host in contrast to the <code>nodetool</code>:</li>
</ul>
<pre><code>dsbulk count -k keyspace -t table --log.verbosity 0 --stats.mode partitions
</code></pre>
",['table']
65552649,65554054,2021-01-03 17:11:09,Is it possible to delete data older than 'x' in Cassandra using only timestamp PK field and w/o ALLOW FILTERING and TTL option?,"<p>Title says all. I have a table <em>timestampTEST</em></p>
<pre><code>create table timestampTEST ( timestamp timestamp, test text, PRIMARY KEY(timestamp));
</code></pre>
<p>When trying to</p>
<pre><code>select * from messagesbytimestampTEST where timestamp &gt; '2021-01-03' and timestamp &lt; '2021-01-04' ;
</code></pre>
<p>I got error</p>
<pre><code>InvalidRequest: Error from server: code=2200 [Invalid query] message=&quot;Cannot execute this query as it might involve data filtering and thus may have unpredictable performance. If you want to execute this query despite the performance unpredictability, use ALLOW FILTERING&quot;
</code></pre>
<p>What I saw here <a href=""https://docs.datastax.com/en/dse/5.1/cql/cql/cql_using/refTimeUuidFunctions.html"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/dse/5.1/cql/cql/cql_using/refTimeUuidFunctions.html</a> it this sample (but I assume it is just <em>part</em> of the cql query):</p>
<pre><code>SELECT * FROM myTable
   WHERE t &gt; maxTimeuuid('2013-01-01 00:05+0000')
   AND t &lt; minTimeuuid('2013-02-02 10:00+0000')
</code></pre>
<p>I know above is related to <em>timeuuid</em>, but I have tried it also and it yields same error.</p>
",<cassandra>,"<p>It's not possible to do in CQL without <code>ALLOW FILTERING</code>.  The primary reason is that in your table, primary key is the same as partition key, and to fulfill your query you need to scan data on all servers.  This happens because the partition key is not ordered - the value is hashed, and used to select the server on which it will be stored.  So CurrentTime-1sec will be on one server, CurrentTime-10sec - on another, etc.</p>
<p>Usually, for such queries people are using some external tools, like, DSBulk, or Spark with Spark Cassandra Connector. You can refer to following answers that I already provided on that topic:</p>
<ul>
<li><a href=""https://stackoverflow.com/questions/63121594/data-model-in-cassandra-and-proper-deletion-strategy/63128795#63128795"">Data model in Cassandra and proper deletion Strategy</a></li>
<li><a href=""https://stackoverflow.com/questions/59859771/delete-records-in-cassandra-table-based-on-time-range/59861178#59861178"">Delete records in Cassandra table based on time range</a></li>
</ul>
",['table']
65883169,65884211,2021-01-25 10:44:30,Cassandra : Update gc_grace_seconds for keyspace,"<p>Is it possible to update gc_grace_seconds for all the tables in a keyspace? Or should it had to be done per folder</p>
",<cassandra><ttl>,"<p>There is no built-in command to change table options for all tables in the keyspace, but it's easy to implement using the bash + cqlsh.  Something like this (replace <code>keyspace_name</code> and <code>new_value</code> with actual parameters):</p>
<pre class=""lang-sh prettyprint-override""><code>cqlsh -e 'DESCRIBE FULL SCHEMA;'|grep -e '^CREATE TABLE keyspace_name'|\
   sed -e 's|^CREATE TABLE \(.*\) (|ALTER TABLE \1 WITH gc_grace_seconds = new_value; |'|\
   tee schema-changes.cql
cqlsh -f schema-changes.cql
</code></pre>
",['table']
65921137,65924469,2021-01-27 14:26:36,Terraform provider to create cassandra tables,"<p>I'm looking to create cassandra DB tables through terraform, on Azure. I already have the relative keyspaces in place.</p>
<p>My deployment is leveraging <a href=""https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/cosmosdb_cassandra_keyspace"" rel=""nofollow noreferrer"">azurerm</a>, however their provisioner is lacking a cassandra-tables resources.
As of now, I can only deploy cassandra tables through Azure UI on the portal or with <a href=""https://learn.microsoft.com/en-us/azure/cosmos-db/scripts/cli/cassandra/create"" rel=""nofollow noreferrer"">Azure CLI scripting</a>, however this isn't the best solution for a variety of reasons.</p>
<p>Is there a provider that could help me with this? I'm giving a look around but it seems that there isn't much I could leverage.</p>
",<azure><cassandra><terraform><azure-cosmosdb><terraform-provider-azure>,"<p>For whatever reason it looks like hashicorp never implemented cassandra table in their provider. Their <a href=""https://github.com/terraform-providers/terraform-provider-azurerm/tree/master/azurerm/internal/services/cosmos"" rel=""nofollow noreferrer"">source code</a> is missing the implementation for it.</p>
<p>I suggest filing a new bug on their repo. You can do that <a href=""https://github.com/hashicorp/terraform/issues?q=cosmos+db"" rel=""nofollow noreferrer"">here</a></p>
",['table']
66261669,66279331,2021-02-18 14:09:31,How to find range in Cassandra Primary key?,"<p>Use case: Find maximum <code>counter</code> value in a specific <code>id</code> range</p>
<p>I want to create a table with these columns: <code>time_epoch int</code>, <code>t_counter counter</code></p>
<p>The frequent query is:</p>
<pre><code>select time_epoch, MAX t_counter where time_epoch &gt;= ... and time_epoch &lt; ...
</code></pre>
<p>This is to find the counter in specific time range. Planning to make time_epoch as primary key. I am not able to query the data. It is always asking for <code>ALLOW FILTERING</code>. Since its a very costly function, We dont want to use it.</p>
<p>How to design the table and query for the use case.</p>
",<cassandra><primary-key><cql><performancecounter>,"<p>Let's assume that we can &quot;bucket&quot; (partition) your data by day, assuming that enough write won't happen in a day to make the partitions too large.  Then, we can cluster by <code>time_epoch</code> in DESCending order.  With time based data, storing data in descending order often makes the most sense (as business reqs <em>usually</em> care more about the most-recent data).</p>
<p>Therefore, I'd build a table like this:</p>
<pre><code>CREATE TABLE event_counter (
    day bigint,
    time_epoch timestamp,
    t_counter counter,
    PRIMARY KEY(day,time_epoch))
WITH CLUSTERING ORDER BY (time_epoch DESC);
</code></pre>
<p>After inserting a few rows, the clustering order becomes evident:</p>
<pre><code>&gt; SELECT * FROM event_counter ;
    WHERE day=20210219 
      AND time_epoch&gt;='2021-02-18 18:00'
      AND time_epoch&lt;'2021-02-19 8:00';

 day      | time_epoch                      | t_counter
----------+---------------------------------+-----------
 20210219 | 2021-02-19 14:09:21.625000+0000 |         1
 20210219 | 2021-02-19 14:08:32.913000+0000 |         2
 20210219 | 2021-02-19 14:08:28.985000+0000 |         1
 20210219 | 2021-02-19 14:08:05.389000+0000 |         1

(4 rows)
</code></pre>
<p>Now SELECTing the MAX <code>t_counter</code> in that range should work:</p>
<pre><code>&gt; SELECT day,max(t_counter) as max
FROM event_counter
WHERE day=20210219
  AND time_epoch&gt;='2021-02-18 18:00'
  AND time_epoch&lt;'2021-02-19 09:00';

 day      | max
----------+-----
 20210219 |   2
</code></pre>
",['table']
66265636,66423903,2021-02-18 18:05:19,Is there a way for Cassandra to ignore clustering key when inserting?,"<p>Right now I have a table with three columns: <code>GroupId</code>,<code>ObjectId</code> and <code>Data</code>, with first two defined as partition key.</p>
<p>Currently it works as desired: if  <code>GroupId</code>and <code>ObjectId</code> match existing row, it gets overwritten.</p>
<p>I'm trying to add sorting by date, so I added third column, <code>LastModified</code> and specified it as clustering key. While it works for sorting, now I have multiple rows sharing the same <code>GroupId</code>and <code>ObjectId</code> pairs, which is not what I need.</p>
<p>How can I achieve previous behaviour?</p>
<ol>
<li>I could read table before writing and delete matching row before writing a new one.</li>
<li>I could, after reading, filter rows in my application.</li>
</ol>
<p>I dislike both solutions because they seem to be too complicated and performance is a big concern. Is there a better way?</p>
",<database><cassandra><cql>,"<p>Here is how i did this in case someone else faces same problem:</p>
<p>I have a table with <code>GroupId</code> and <code>ObjectId</code> as key. I'm not sure if it matters, but <code>ObjectId</code> is defined as clustering key.</p>
<p>Then you get desired result from following view:</p>
<pre><code>        CREATE MATERIALIZED VIEW IF NOT EXISTS objectlistbylast
        AS SELECT * FROM objectlist
        WHERE groupid IS NOT NULL AND objectid IS NOT NULL AND lastmodified IS NOT NULL
        PRIMARY KEY(groupid , lastmodified, objectid )
        WITH CLUSTERING ORDER BY(lastmodified DESC);
</code></pre>
<p><strong>Note that ordering when defining primary key matters.</strong></p>
",['table']
66518217,66519140,2021-03-07 15:35:32,Is it possible to search for a tag in cql using a set?,"<p>Is it possible to query items in a CQL table that exactly match a specific element in a set. For example, can we search for where <code>tag=&quot;music&quot;</code> on the following table?</p>
<pre><code>create table if not exists example (
        website text,
        uuid text,
        message text,
        tags set&lt;text&gt;,
        created timestamp,
        primary key ((site), uuid))
</code></pre>
",<cassandra><tags><cql>,"<p>It's possible to search for elements in the collection using the <code>CONTAINS</code> operator (see <a href=""https://docs.datastax.com/en/cql-oss/3.3/cql/cql_reference/cqlSelect.html#cqlSelect__filtering-on-collections"" rel=""nofollow noreferrer"">documentation</a>).  But it will be very slow, so index on collection may help (see <a href=""https://docs.datastax.com/en/cql-oss/3.x/cql/cql_reference/cqlCreateIndex.html"" rel=""nofollow noreferrer"">docs for syntax</a>) but it's also not necessary optimal.   Better solution would be something like Storage Attached Indexes from DataStax, but they aren't in the OSS Cassandra yet.  Another solution could be integration of real search engine, like, Solr (known as DSE Search), or Elasticsearch (as in Elassandra).</p>
<p>The main thing to take into account would be to analyze the latency requirements, in some cases, it would be better to have a separate table with tag as partition key, but in this case you may have data skew as there are not so many tags, and distribution of data is not uniform.</p>
",['table']
66562116,66563204,2021-03-10 09:46:27,How do you nest a UDT inside another UDT in Cassandra?,"<p>I have created the following user defined types (UDT) in cassandra :</p>
<pre><code>CREATE TYPE keyspace.location (
    latitude text,
    longitude text,
    accuracy text,
    address text
);

CREATE TYPE keyspace.person (
    name text,
    contact_no text,
    alternate_contact text
);
</code></pre>
<p>and I want to use these to create another UDT</p>
<pre><code>CREATE TYPE keyspace.pinpoint(
    user &lt;person&gt;,
    location &lt;location&gt;
);
</code></pre>
",<cassandra><cassandra-3.0><cassandra-2.0><spring-data-cassandra><cassandra-2.1>,"<p>You can nest UDTs by simply specifying your UDT as the type within another UDT in this manner:</p>
<pre><code>CREATE TYPE keyspace.pinpoint (
    user person,
    location location
);
</code></pre>
<p>You don't enclose them in <code>&lt;&gt;</code> brackets because those are used for collections.</p>
<p>As a side note, I personally wouldn't nest UDTs unless you have no other option. UDTs are not as flexible as native columns. Inserting or updating data in a nested UDT can get very complicated and hard to maintain.</p>
<p>Whenever possible, try to use generic table definitions. For example instead of defining the type <code>pinpoint</code>, try to use a table with native column types and clustered rows. Cheers!</p>
",['table']
66961429,66963335,2021-04-06 00:52:47,How to store Bert embeddings in cassandra,"<p>I want to use Cassandra as feature store to store precomputed Bert embedding,
Each row would consist of roughly 800 integers (ex. <code>-0.18294132</code>) Should I store all 800 in one large string column or 800 separate columns?</p>
<p>Simple read pattern, On read we would want to read every value in a row. Not sure which would be better for serialization speed.</p>
",<cassandra><embedding><bert-language-model>,"<p>Having everything as a separate column will be quite inefficient - each value will have its own metadata (writetime, for example) that will add significant overhead (at least 8 bytes per every value).  Storing data as string will be also not very efficient, and will add the complexity on the application side.</p>
<p>I would suggest to store data as <a href=""https://docs.datastax.com/en/dse/5.1/cql/cql/cql_using/refCollectionType.html"" rel=""nofollow noreferrer"">fronzen list</a> of integers/longs or doubles/floats, depending on your requirements.  Something like:</p>
<pre><code>create table ks.bert(
  rowid int primary key,
  data frozen&lt;list&lt;int&gt;&gt;
);
</code></pre>
<p>In this case, the whole list will be effectively serialized as binary blob, occupying just one cell.</p>
",['table']
67069868,68201226,2021-04-13 06:38:26,Amazon Keyspaces sync_table() doesn't see a table immediately,"<p>I am new in <code>cassandra-driver==3.25.0</code>. I've created a model from <code>Model</code> and tried to sync it. The problem:</p>
<ol>
<li>I am calling <code>sync_table</code></li>
<li>I see that table starts creating in Keyspaces and the definition is correct</li>
<li>BUT <code>sync_table</code> raises an exception <code>KeyError {__table_name__}</code> from this line of code <code>table = cluster.metadata.keyspaces[ks_name].tables[raw_cf_name]</code></li>
<li>I can call <code>sync_table</code> again and it is working fine after some time</li>
</ol>
<p>So I am assuming it can some delay from Keyspaces, so do <code>cassandra-driver</code> has some sort of wait on sync table, or can you point to the documentation, please.</p>
<pre><code>class Users(Model):
    __keyspace__ = 'aura'
    __table_name__ = 'users'
    __connection__ = 'aws_keyspace'

    user_id = columns.UUID(primary_key=True)
    tags = columns.Map(key_type=columns.Text(), value_type=columns.Text())

cluster = Cluster(...)
session = cluster.connect()
connection.register_connection('aws_keyspace', session=session, default=True)
sync_table(Users)
</code></pre>
<p>Full exception:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/usr/lib/python3.6/runpy.py&quot;, line 193, in _run_module_as_main
    &quot;__main__&quot;, mod_spec)
  File &quot;/usr/lib/python3.6/runpy.py&quot;, line 85, in _run_code
    exec(code, run_globals)
  File &quot;models.py&quot;, line 69, in &lt;module&gt;
    sync_table(AttachmentsByUsers)
  File &quot;python3.6/site-packages/cassandra/cqlengine/management.py&quot;, line 190, in sync_table
    _sync_table(m, connection=connection)
  File &quot;python3.6/site-packages/cassandra/cqlengine/management.py&quot;, line 274, in _sync_table
    table = cluster.metadata.keyspaces[ks_name].tables[raw_cf_name]
KeyError: 'users'
</code></pre>
",<python><cassandra><cassandra-driver><amazon-keyspaces>,"<p>Amazon Keyspaces creates tables in a non-blocking way. Since its asynchronous, you may not see the table immediately. This link has guidance on how to monitor system tables to check when a table is available. <a href=""https://docs.aws.amazon.com/keyspaces/latest/devguide/functional-differences.html#functional-differences.table-keyspace-management"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/keyspaces/latest/devguide/functional-differences.html#functional-differences.table-keyspace-management</a></p>
<p>The following query will show the status of your table in CQL</p>
<pre class=""lang-sql prettyprint-override""><code>SELECT keyspace_name, table_name, status FROM system_schema_mcs.tables
WHERE keyspace_name = 'mykeyspace' AND table_name = 'mytable';
</code></pre>
<p>is the table is still being created the output of the query will look like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>keyspace_name</th>
<th>table_name</th>
<th>status</th>
</tr>
</thead>
<tbody>
<tr>
<td>mykeyspace</td>
<td>mytable</td>
<td>CREATING</td>
</tr>
</tbody>
</table>
</div>
<p>If table was successfully created and is active the output of the query looks like this:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>keyspace_name</th>
<th>table_name</th>
<th>status</th>
</tr>
</thead>
<tbody>
<tr>
<td>mykeyspace</td>
<td>mytable</td>
<td>ACTIVE</td>
</tr>
</tbody>
</table>
</div>",['table']
67092670,67093660,2021-04-14 13:23:16,Is it bad design to rotate tables in Cassandra?,"<p>We're going to be using Cassandra for storing large volumes of data. This data is inserted and read but never updated or deleted. From what I've understood, UPDATE ops lead to tombstones and DELETE ops to shadows.</p>
<p>In order to design around this, intend to use monthly tables and TRUNCATE and then DROP tables after n (approx 4) months. Under the assumption that the data is evenly distributed and there's enough disk to store this - are there any other caveats to this approach.</p>
<p>On a side note, is there a technical term for this schema design? I'm happy to share more information if the question needs more details.</p>
",<cassandra>,"<p>I have seen some projects that were doing that to avoid deletions, so they just had the monthly tables, that were removed after N months by just dropping them (you don't need to do TRUNCATE before drop!).  But that knowledge about table naming required that applications knew about it.</p>
<p>But in your case <a href=""https://cassandra.apache.org/doc/latest/operating/compaction/twcs.html"" rel=""nofollow noreferrer"">Time Window Compaction Strategy</a> in combination with TTLs may work better because it drop the whole SSTables when all data in them expires.  You can look into this <a href=""https://thelastpickle.com/blog/2016/12/08/TWCS-part1.html"" rel=""nofollow noreferrer"">blog post</a> for explanations on how it works and where it should be used.</p>
",['table']
67117497,67118164,2021-04-16 00:10:20,Get sample data from a Cassandra table,"<p>I have read access to a production Cassandra cluster.  I'd like to look at a sample row to understand the data a little better.  I don't know any of the partition keys to filter on and I'd rather not run any query that would impact the performance of the cluster (ie. allow filtering etc)</p>
<p>I don't have access to nodetool.</p>
<p>Are there any cqlsh queries that would allow me to just get some sample data from a table without knowing a partition key ahead of time to filter on?  Or a query that would give me a list of the partition keys that I can then use to filter on?</p>
<p>Thanks!</p>
",<filter><cassandra><production><partition>,"<p>If you connect via cqlsh, you can use the <code>LIMIT</code> operator to limit the number of results returned with something like:</p>
<pre><code>SELECT * FROM table_name LIMIT 10;
</code></pre>
<p>This would limit the number of records so you're not doing a full table scan.</p>
<p>Ideally however is that you either get (a) some sample data from the app team, or (b) connect to a non-production environment. Cheers!</p>
",['table']
67136100,67166935,2021-04-17 08:16:05,CQL query to get count of elements in LIST DATATYPE of CassandraDB?,"<p>I have a column in my table which has list datatype. I want to get count of  items in the list using <strong>CQL</strong>
I tried but I am unable to find the correct query . Can anyone help me?</p>
",<database><cassandra><nosql><cql><cqlsh>,"<p>So there isn't a standard way in Cassandra to accomplish this.  The easiest way would be to <code>SELECT</code> the list from the table and pull a count of its items on the application side.</p>
<p>That being said, there <em>is</em> a way to accomplish this by building a <strong>user defined function</strong> (UDF).  First of all, UDFs are disabled by default with Cassandra, to prevent execution of malicious code by users of a cluster.  If you're ok with enabling them, you'll find that option in the <code>cassandra.yaml</code> file.  It's a simple boolean, so you'll want to set it to <code>true</code>:</p>
<pre><code>enable_user_defined_functions: true
</code></pre>
<p>Of course, you'll have to stop/restart the cluster for this change to take effect.</p>
<p>Next, you can create a UDF to return an integer representing the size of the list, like this:</p>
<pre><code>CREATE OR REPLACE FUNCTION countlist (input List&lt;text&gt;)
    RETURNS NULL ON NULL INPUT RETURNS int
    LANGUAGE java AS 'return input.size();';
</code></pre>
<p>Essentially what this does, is the <code>List&lt;text&gt;</code> Cassandra type gets mapped to a <code>java.util.List&lt;String&gt;</code> Java type.  From there, it simply invokes the <code>size()</code> method on the list.</p>
<p>Now, I can use that <code>countlist</code> function with CQL.  So let's say I have a table to keep track of parents' children:</p>
<pre><code>CREATE TABLE kids (
    parent text PRIMARY KEY,
    children list&lt;text&gt;);
</code></pre>
<p>For me, I can query it like this:</p>
<pre><code>SELECT parent,countlist(children) FROM kids WHERE parent='Aaron';

 parent | stackoverflow.countlist(children)
--------+-----------------------------------
  Aaron |                                 4

(1 rows)
</code></pre>
",['table']
67220374,68754395,2021-04-22 20:32:57,Migration path from Datastax 6.0 to Cassandra 3,"<p>I'm trying to find a migration path from Datastax Enterprise (DSE) 6.0.14 to Cassandra Community OSS 3. So far I'm not able to find a working migration path.</p>
<p>All keyspaces replications have been updated to <code>NetworkTopologyStrategy</code> or using <code>LocalStrategy/SimpleStrategy</code>.</p>
<p>When trying to add a Cassandra 3.11.10, schema agreement cannot be reached since gossip protocol seems not compatible and it crashes.</p>
<p>When trying to add a Cassandra 3.11.3, it's not crashing but schema does not seem to be compatible neither.</p>
<p>I'm running a 5 nodes DSE cluster and trying to replace it with 5 nodes Cassandra.</p>
<p>Cluster name is the same for all nodes while the new Cassandra node is using another DC name.</p>
",<cassandra><datastax-enterprise>,"<p>It is possible, the procedure we have found that works:</p>
<ol>
<li>Extract the schema of the current node / cluster</li>
<li>Extract the data of table system_schema.tables</li>
<li>Drain the node</li>
<li>sstable downgrade your keyspaces (does not work work for system* keyspaces)</li>
<li>Initialize the node as an empty (new) Cassandra OSS node</li>
<li>Allow it to create it's sys tem keyspaces and roles</li>
<li>Import schema from DSE (step 1)</li>
<li>Overwrite table id's in system_schema.tables to match in the id of DSE</li>
<li>Stop cassandra, move the downgraded sstables back into the relevant <code>data</code> directory and restart Cassandra to load the data</li>
<li>The node should be part of the cluster and you can continue your other nodes in the same manner. But use the already migrated OSS node to get the schema.</li>
</ol>
<p>From application side, we force the OSS node after migration the 1st node. This allows the application to see the OSS and DSE nodes and write to all nodes in the cluster.</p>
<p>After all nodes are migrated, run a full repair on the cluster.</p>
",['table']
67353944,67354375,2021-05-02 07:23:35,Will the Write-Ahead-Log become the bottleneck of Cassandra?,"<p>In a Cassandra database, a write needs to be logged in the Write Ahead Log first and then added to the memtable in memory. Since the Write Ahead Log is on disk, although it performs sequential writes（i.e., append only）, will it still be much slower than memory access, thus become the performance bottleneck for the writes？</p>
<p>If I understand it correctly, Cassandra supports the mechanism to store the Write Ahead Log in OS cache, and then flush it to disk every pre-configured amount of time(say 10 seconds). However, does it mean the data changes made within this 10 seconds could be all lost if the machine crashes?</p>
",<cassandra>,"<p>You can control if the sync of commit log using the <a href=""https://cassandra.apache.org/doc/latest/configuration/cass_yaml_file.html#commitlog-sync"" rel=""nofollow noreferrer"">commitlog-sync</a> configuration.  By default it's <code>periodic</code>, and synced to disk every 10 seconds (controlled by <code>commitlog_sync_period_in_ms</code> setting).</p>
<p>And yes, if you lose the power there is a risk that data in the commit log is lost.  But Cassandra relies on the fact that you have multiple replicas, and if you did setup correctly, each replica should be in separate rack (at least, better if you have additional data centers) with separate power, etc.</p>
",['rack']
67369760,67370949,2021-05-03 13:27:34,When doing a SELECT in Cassandra (Spark) by an SAI field. In what order are the rows returned?,"<p>If I have a table in Astra created like so:</p>
<pre><code>CREATE TABLE rayven.mytable (
a text,
b text,
c timestamp,
PRIMARY KEY (a, c)
) WITH CLUSTERING ORDER BY (c DESC)
</code></pre>
<p>I then added the SAI index:</p>
<pre><code>CREATE CUSTOM INDEX b_index ON mytable (b) USING 'StorageAttachedIndex';
</code></pre>
<p>When I query using ORDER BY:</p>
<pre><code>select * from mytable where b='x' order by c desc;
</code></pre>
<p>I see</p>
<pre><code>InvalidRequest: Error from server: code=2200 [Invalid query] message=&quot;ORDER BY with 2ndary indexes is not supported.&quot;
</code></pre>
<p>Since the original table is ordered by &quot;c&quot; descending. Can I assume that the result of the above SELECT will be in this order or there is no way of knowing or controling the order when selecting using an SAI index?</p>
",<cassandra><datastax><datastax-astra>,"<p>To help illustrate this, I have created your table and inserted some data.  I've then queried the table for a value of <code>b</code>, and included the <code>token</code> function on the partition key for this example.</p>
<p><em>Note: Not running in Astra, but on my local 4.0 rc1 instance.  The principles remain the same, however.</em></p>
<p>Basically, all result sets are sorted by the hashed token values of the partition key, and then the <code>CLUSTERING ORDER</code> takes precedence <em>within</em> each partition:</p>
<pre><code>&gt; SELECT a, token(a), c FROM mytable WHERE b='b';

 a  | system.token(a)      | c
----+----------------------+---------------------------------
 a4 | -9170418876698302957 | 2021-05-03 14:38:42.708000+0000
 a5 |  -925545907721365710 | 2021-05-03 14:39:06.849000+0000
 a3 |   -96725737913093993 | 2021-05-03 14:40:30.942000+0000
 a3 |   -96725737913093993 | 2021-05-03 14:39:18.340000+0000
 a2 |  5060052373555595560 | 2021-05-03 14:40:30.938000+0000
 a2 |  5060052373555595560 | 2021-05-03 14:39:14.914000+0000
 a1 |  5693669818594506317 | 2021-05-03 14:38:54.426000+0000
 a1 |  5693669818594506317 | 2021-05-03 14:38:52.758000+0000

(8 rows)
</code></pre>
<p>As you can see here, the result set is <em>not completely</em> sorted by <code>c</code>.  But initially sorted by the hashed token values of <code>a</code>, and <em>then</em> sorted by <code>c</code> <em>within</em> each partition (<code>a</code>).</p>
<p>So &quot;no,&quot; you cannot count on the data automatically being completely sorted by <code>c</code>.</p>
",['table']
67451827,67455808,2021-05-08 20:22:50,java.io.IOException: Failed to write statements to batch_layer.test. The latest exception was Key may not be empty,"<p>I am trying to count the number of words in the text and save result to the Cassandra database.
Producer reads the data from the file and sends it to kafka. Consumer uses spark streaming to read and process the date,and then sends the result of the calculations to the table.</p>
<p>My producer looks like this:</p>
<pre><code>object ProducerPlayground extends App {

  val topicName = &quot;test&quot;
  private def createProducer: Properties = {
    val producerProperties = new Properties()
    producerProperties.setProperty(
      ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,
      &quot;localhost:9092&quot;
    )
    producerProperties.setProperty(
      ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,
      classOf[IntegerSerializer].getName
    )
    producerProperties.setProperty(
      ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,
      classOf[StringSerializer].getName
    )
    producerProperties
  }

  val producer = new KafkaProducer[Int, String](createProducer)

  val source = Source.fromFile(&quot;G:\\text.txt&quot;, &quot;UTF-8&quot;)

  val lines = source.getLines()

  var key = 0
  for (line &lt;- lines) {
    producer.send(new ProducerRecord[Int, String](topicName, key, line))
    key += 1
  }
  source.close()
  producer.flush()

}
</code></pre>
<p>Consumer looks like this:</p>
<pre><code>object BatchLayer {
  def main(args: Array[String]) {

    val brokers = &quot;localhost:9092&quot;
    val topics = &quot;test&quot;
    val groupId = &quot;groupId-1&quot;

    val sparkConf = new SparkConf()
      .setAppName(&quot;BatchLayer&quot;)
      .setMaster(&quot;local[*]&quot;)
    val ssc = new StreamingContext(sparkConf, Seconds(3))
    val sc = ssc.sparkContext
    sc.setLogLevel(&quot;OFF&quot;)

    val topicsSet = topics.split(&quot;,&quot;).toSet
    val kafkaParams = Map[String, Object](
      ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG -&gt; brokers,
      ConsumerConfig.GROUP_ID_CONFIG -&gt; groupId,
      ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG -&gt; classOf[StringDeserializer],
      ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG -&gt; classOf[StringDeserializer],
      ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG -&gt; &quot;false&quot;
    )
    val stream =
      KafkaUtils.createDirectStream[String, String](
        ssc,
        LocationStrategies.PreferConsistent,
        ConsumerStrategies.Subscribe[String, String](topicsSet, kafkaParams)
      )

   
    val cass = CassandraConnector(sparkConf)

    cass.withSessionDo { session =&gt;
      session.execute(
        s&quot;CREATE KEYSPACE IF NOT EXISTS batch_layer WITH REPLICATION = {'class': 'SimpleStrategy', 'replication_factor': 1 }&quot;
      )
      session.execute(s&quot;CREATE TABLE IF NOT EXISTS batch_layer.test (key VARCHAR PRIMARY KEY, value INT)&quot;)
      session.execute(s&quot;TRUNCATE batch_layer.test&quot;)
    }

    stream
      .map(v =&gt; v.value())
      .flatMap(x =&gt; x.split(&quot; &quot;))
      .filter(x =&gt; !x.contains(Array('\n', '\t')))
      .map(x =&gt; (x, 1))
      .reduceByKey(_ + _)
      .saveToCassandra(&quot;batch_layer&quot;, &quot;test&quot;, SomeColumns(&quot;key&quot;, &quot;value&quot;))

    ssc.start()
    ssc.awaitTermination()
  }

}
</code></pre>
<p>After starting producer, the program stops working with this error. What did I do wrong ?</p>
",<apache-kafka><cassandra><spark-streaming><spark-cassandra-connector><spark-streaming-kafka>,"<p>It makes very little sense to use legacy streaming in 2021st - it's very cumbersome to use, and you also need to track offsets for Kafka, etc.  It's better to use <a href=""https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html"" rel=""nofollow noreferrer"">Structured Streaming instead</a> - it will track offsets for your through the checkpoints, you will work with high-level Dataset APIs, etc.</p>
<p>In your case code could look as following (didn't test, but it's adopted from <a href=""https://github.com/alexott/cassandra-dse-playground/blob/master/scc-2.5/src/main/scala/com/datastax/alexott/streaming/StructuredStreamingKafkaDSE.scala"" rel=""nofollow noreferrer"">this working example</a>):</p>
<pre class=""lang-scala prettyprint-override""><code>val streamingInputDF = spark.readStream
  .format(&quot;kafka&quot;)
  .option(&quot;kafka.bootstrap.servers&quot;, &quot;localhost:9092&quot;)
  .option(&quot;subscribe&quot;, &quot;test&quot;)
  .load()

val wordsCountsDF = streamingInputDF.selectExpr(&quot;CAST(value AS STRING) as value&quot;)
  .selectExpr(&quot;split(value, '\\w+', -1) as words&quot;)
  .selectExpr(&quot;explode(words) as word&quot;)
  .filter(&quot;word != ''&quot;)
  .groupBy($&quot;word&quot;)
  .count()
  .select($&quot;word&quot;, $&quot;count&quot;)

// create table ...

val query = wordsCountsDF.writeStream
   .outputMode(OutputMode.Update)
   .format(&quot;org.apache.spark.sql.cassandra&quot;)
   .option(&quot;checkpointLocation&quot;, &quot;path_to_checkpoint)
   .option(&quot;keyspace&quot;, &quot;test&quot;)
   .option(&quot;table&quot;, &quot;&lt;table_name&gt;&quot;)
   .start()

query.awaitTermination()
</code></pre>
<p>P.S. In your example, most probable error is that you're trying to use <code>.saveToCassandra</code> directly on DStream - it doesn't work this way.</p>
",['table']
67518199,67518645,2021-05-13 11:07:33,Cassandra - Nodejs - Issue while retrieving list type values,"<p>for example below is the table structure.</p>
<hr />
<pre><code>CREATE TABLE table_name(
 name text,
 id text PRIMARY KEY,
 details list&lt;text&gt;
)
</code></pre>
<hr />
<p>Assume
details[0]-&gt; contact number,
details[1]-&gt; Address</p>
<p>I want to write a query to extract contact number from this table.</p>
",<node.js><cassandra>,"<p>Actually, you should not store arrays of data. The best and simplest way will be to refactor your database to something like this.</p>
<pre><code>CREATE TABLE table_name(
 name text,
 id text PRIMARY KEY,
 contact number NOT NULL, 
 address text NOT NULL,
)
</code></pre>
<p>Then you could do <code>SELECT contact FROM table_name</code>. If the same address can be reused between multiple entities then you may think about adding one more table <code>Addresses</code> and then using foreign keys to relate this data.</p>
",['table']
67581235,67583842,2021-05-18 06:48:17,Is it possible to select data from Cassandra only for specific timeframe?,"<p>Let's assume I store data in Cassandra where one column has a timestamp format and it is also a key by which I can select data based on datetime ranges. My data has frequency of 1 minute, therefore retrieving rows for the entire day will get me 1440 data points. But for some applications I would like to retrieve only 24 points per day at each even hour.</p>
<p>Is it possible to make a query in CQL that will get me data points only for even hours or for every certain amount of time like 15 or 30 minutes?</p>
<p>My cassandra version:</p>
<p><code>[cqlsh 5.0.1 | Cassandra 3.11.10 | CQL spec 3.4.4 | Native protocol v4]</code></p>
<p>EDIT: I have designed the table but I can change it if needed. So far the query to create the table is:</p>
<pre><code>CREATE TABLE sensors_test 
(name text, 
value double, 
time timestamp, 
PRIMARY KEY (name, time) 
) WITH CLUSTERING ORDER BY (time DESC);
</code></pre>
",<database><cassandra><cassandra-3.0>,"<p>Your design will serve the purpose, but it will create wide row.</p>
<pre><code>CREATE TABLE sensors_test 
( name text,
  year  int,
  month int,
  day int,
  hour int,
  value double, 
  time timestamp, 
  PRIMARY KEY ((name,year,month,day,hour), time) 
 ) WITH CLUSTERING ORDER BY (time DESC);
</code></pre>
<p>This table design will solve the wide row problem. You can fetch data for any hour of the day, also you can use range query on time column.</p>
",['table']
67614729,67619564,2021-05-20 05:59:08,How cassandra handles new node ( how tokens are redistributed)?,"<p>I recently started exploring Cassandra for a new project. Here is my understanding of Cassandra as of now (based on numerous blogs available online)</p>
<ol>
<li>Cassandra (C*) is AP (CAP theorem), i.e. it is highly available and partition tolerant</li>
<li>C* is logically ring topology. Not in real networking sense (as all nodes can speak to each other) but in data replication sense (data is replicated to neighboring nodes)</li>
<li>C* uses <em>Murmur3</em> Partition algorithm to map out Partition keys to integer range ( roughly -2^63 to 2^63)</li>
<li>Every node is responsible for a few ranges (Vnodes enabled), i.e.
node1: [<em>-2^63</em> to <em>-2^61</em>],  [<em>2^31</em>  to <em>2^33</em>] ....
node2: [<em>-2^61</em> <em>- 1</em> to <em>2^59</em>], [<em>2^33 + 1</em> to <em>2^35</em>]....</li>
</ol>
<p>Now my questions are, suppose I create a cluster with 3 nodes and RF = 2, then</p>
<ol>
<li>In this case whole token range (I believe I am using the right terminology here) -2^63 to 2^63 will be distributed evenly in these 3 nodes?</li>
<li>What happens if I add another node in this running cluster? My assumption is that, C* will rebalance the ring because it uses consistent hashing and thus the range -2^63 to 2^63 will be re-distributed and corresponding data will be copied to new node. (Copied data from existing node won't be deleted till a repair happens?)</li>
<li>What happens when a node goes down? Does C* rebalanced the ring by re-distributing the tokens and moving data around?</li>
<li>Are tokens a fancy word for Murmur3hash(partition Key)? And what exactly it means by partition range? Is it like partition range1= <em>-2^63</em> to <em>-2^61</em>, partition range2 = <em>-2^61 + 1</em> to <em>-2^59</em> and so on.</li>
<li>What information is actually shared during gossip?</li>
</ol>
<p>Sorry if these questions seem very basic but I spent quite some time but could not find definitive answers.</p>
",<cassandra>,"<p>I will try to explain in simple way</p>
<p>Cassandra provides a simple way for configuration, all the configuration is done in cassandra.yaml. You can also go through <a href=""https://stackoverflow.com/questions/41587806/cassandra-sharding-and-replication/41590463#41590463"">THIS</a> to get a some picture of the partitioning in cluster.</p>
<p>Let's start with the basics, instead of using three nodes let's use only one node for now.
With the default configuration of cassandra we get below values in cassandra.yaml file</p>
<pre><code>num_tokens: 1
initial_token: 0
</code></pre>
<p>This means only one node and all the partitions will reside on this one node.
Now, the concept of virtual node is, in simple terms cassandra divides the tokens into multiple ranges, even though there are no physical nodes. Now, how to enable the virtual nodes feature in configuration file cassandra.yaml. The answer is num_token value.</p>
<pre><code>num_tokens: 128
#initial_token: 0
</code></pre>
<p>This configuration makes 128 token ranges, for example 0-10, 11-20, 20-30 and so on. Keep the value of initial_token commented, this means we want cassandra to decide value of initial token (One less thing to worry about).</p>
<p>Now lets add another node in to cluster. Below is the simple configuration of new node. Consider the first node IP as 127.0.0.1 and second node IP as 127.0.0.2 for simplicity.</p>
<pre><code>num_tokens: 128
#initial_token: 0
seed_provider:
- class_name: org.apache.cassandra.locator.SimpleSeedProvider
  parameters:
      - seeds: &quot;127.0.0.1, 127.0.0.2&quot;
</code></pre>
<p>We have just added a new node to our cluster, node1 will serve as seed node. The num_token value is 128, that means 128 token ranges. The value of initial_token is commented, that means cassandra will decide the initial token and range. Data transfer will start as soon as the new node joins cluster.</p>
<p>For third node, configuration shall be as below -</p>
<pre><code>num_tokens: 128
#initial_token: 0
seed_provider:
- class_name: org.apache.cassandra.locator.SimpleSeedProvider
  parameters:
      - seeds: &quot;127.0.0.1, 127.0.0.2, 127.0.0.3&quot;
</code></pre>
<p>So third node will share the few token ranges from node1 and few token ranges from node2.</p>
<p>I hope, we got answers of question 1 and question 2 till now. Let's move to our next tow questions.</p>
<p>When a node goes down, hinted-handoff helps Cassandra maintain consistency . Any one out of remaining 2 nodes keeps the hints of the data which supposed to be written on the node which is down. Once the node goes up, these hints will be replayed and data will be written on target node. There is no need to do repartitioning or  rebalancing kind of fancy things. Hints are stored in a directory which can be configured in cassandra.yaml file. By default 3 hours of hints will be stored, that means a defected node should come up within 3 hours. This value is also configurable in cassandra.yaml file.</p>
<pre><code>hinted_handoff_enabled: true
max_hint_window_in_ms: 10800000 # 3 hours
hints_directory: /home/ubuntu/node1/data/hints
</code></pre>
<p>Murmur3Partitioner calculates the hash by using partition key columns, let's make our peace with that. There are other practitioners as well like RandomPartitioner and ByteOrderedPartitioner.</p>
<p>Below is the sample output of gossip info -
You can go through each field in below protocol data</p>
<pre><code>        ubuntu@ds201-node1:~$ ./node1/bin/nodetool gossipinfo
    /127.0.0.1
      generation:1621506507  -- the tiem at this node is boot strapped.
      heartbeat:2323
      STATUS:28:NORMAL,-1316314773810616606 -----status of the node , NORMAL,LEFT,LEAVING,REMOVED,REMOVING.....
      LOAD:2295:110299.0                    -- Disk space usage
      SCHEMA:64:c5c6bdbd-5916-347a-ab5b-21813ab9d135  -- Changes if schema changes
      DC:37:Cassandra                       --- data center of the NODE
      RACK:18:rack1                         --- Rack of the within the datacenter 
      RELEASE_VERSION:4:4.0.0.2284
      NATIVE_TRANSPORT_ADDRESS:3:127.0.0.1
      X_11_PADDING:2307:{&quot;dse_version&quot;:&quot;6.0.0&quot;,&quot;workloads&quot;:&quot;Cassandra&quot;,&quot;workload&quot;:&quot;Cassandra&quot;,&quot;active&quot;:&quot;true&quot;,&quot;server_id&quot;:&quot;08-00-27-32-1E-DD&quot;,&quot;graph&quot;:false,&quot;health&quot;:0.3}
      NET_VERSION:1:256
      HOST_ID:2:ebd0627a-2491-40d8-ba37-e82a31a95688
      NATIVE_TRANSPORT_READY:66:true
      NATIVE_TRANSPORT_PORT:6:9041
      NATIVE_TRANSPORT_PORT_SSL:7:9041
      STORAGE_PORT:8:7000
      STORAGE_PORT_SSL:9:7001
      JMX_PORT:10:7199
      TOKENS:27:&lt;hidden&gt;
</code></pre>
<p>Gossip is the broadcast protocol spread data across the cluster. No one is a master in cassandra cluster, peers spread data among themselves which helps them to maintain latest information. Nodes communicates with each other randomly using gossip protocol (there is some criteria in this randomness).  Gossip spreads node metadata only and not the client data.</p>
<p>Hope this clears some doubts.</p>
",['initial_token']
67905554,67910371,2021-06-09 13:54:39,Cassandra data modeling understanding,"<p>I came from SQL and switched now to CQL. I don't quite understand something yet.</p>
<p>In SQL this works:</p>
<pre><code>Table product
id,
 name, 
 desc,
 price

Table cart
  id 
  item_id

SELECT
  p.name,
  p.desc,
  p.price

FROM product

INNER JOIN cart
ON c.item_id = p.id
</code></pre>
<p>But in Cassandra is it different or I am wrong?
Can I do his in Cassandra too, or is that bad?
How you would query multiple tables in Cassandra?</p>
",<cassandra><cql>,"<p>With Cassandra, you'll want to start with building a table to support your query:</p>
<pre><code>SELECT * FROM product
INNER JOIN cart
ON c.item_id = p.id
</code></pre>
<p>Of course there aren't any joins, so we'll have to build a table to store that cart-product data:</p>
<pre><code>CREATE TABLE cart_product (
    product_name TEXT,
    product_desc TEXT,
    product_price DECIMAL,
    product_id uuid,
    cart_id uuid,
    qty int,
    PRIMARY KEY (cart_id,product_id)
);
</code></pre>
<p>By defining my primary key like this, I'm ensuring that all of my rows will be stored together by <code>cart_id</code>.  As it's the first primary key, it becomes the <em>partitioning</em> key.  As I intend to have more than one product per cart, <code>product_id</code> needs to be a part of the key as a clustering column to ensure uniqueness.</p>
<p>After inserting some data, this works:</p>
<pre><code>&gt; SELECT product_name,product_price,product_desc 
  FROM cart_product
  WHERE cart_id=93aefdf3-acbf-4d3c-849c-4db9b2ef9e19;

 product_name         | product_price | product_desc
----------------------+---------------+---------------------------------------------------
            Minecraft |         29.99 | Build the future with blocks, and try not to die!
 Kerbal Space Program |         29.99 |         Blast into space.  Now with moar rockets!
       Cyberpunk 2077 |         59.99 |         Wake up Samurai.  We have a city to burn!

(3 rows)
</code></pre>
",['table']
67979591,67980861,2021-06-15 03:25:00,TokenAware policy Cassandra and several node in one query,"<p>What happens if our query contains several tokens that finally there on different nodes?
Are possible that the client runs multiple queries Sync or Async on nodes?</p>
<p><strong>sample:</strong></p>
<pre><code>//Our query
SELECT * FROM keyspace1.standard1 WHERE key = 1 or key = 2 or key = 3;

//Client change our query to multiple queries depends on the token ranges and run them sync or async.
SELECT * FROM keyspace1.standrad1 WHERE key = 1 or key = 3; //Token On node X
SELECT * FROM keyspace1.standard1 WHERE key = 3; //token On node Y
</code></pre>
<p><strong>Sample2:</strong></p>
<pre><code> //Our Query
 SELECT * FROM kspc.standard1;

 //Client Change our query to multiple queries on the token ranges and run them sync or async. 
 SELECT * FROM kspc.standard1 WHERE token(key) &gt; [start range node1] and token(key) &lt; [end range node1]; 
 SELECT * FROM kspc.standard1 WHERE token(key) &gt; [start range node2] and token(key) &lt; [end range node2]; 
 and ...
</code></pre>
",<cassandra>,"<p>As Manish mentioned, if query contains several partitions then token aware policy won't select anything and will send query to any node in the cluster (the same behaviour is for unpreparred queries and DDLs).  And in general, it's an anti-pattern as it put more load onto the nodes, so it should be avoided.  But if you really need, then you can force driver to send query to one of the nodes that owns a specific partition key. In Java driver 3.x there was a function <code>statement.setRoutingKey</code>, for Java driver 4.x should be something similar.  For other drivers there should be similar stuff, but maybe not in all.</p>
<p>For second class of queries - it's the same, by default driver can't find to which node to send the query, and routing key should be set explicitly.  But in general, full table scan could be tricky as you need to handle conditions on the lower &amp; upper bounds, and you can't expect that token range starts exactly at lower bound - it could be a situations when token range starts near upper bound &amp; ends slightly above the lower bound - and this is a typical error that I have seen regularly.   If you interested, I have an example of how to perform full table scan using the Java (it uses the same algorithm as <a href=""https://github.com/datastax/spark-cassandra-connector"" rel=""nofollow noreferrer"">Spark Cassandra Connector</a> and <a href=""https://github.com/datastax/dsbulk"" rel=""nofollow noreferrer"">DSBulk</a>) - the main part is <a href=""https://github.com/alexott/cassandra-dse-playground/blob/master/driver-1.x/src/main/java/com/datastax/alexott/demos/TokenRangesScan.java#L38"" rel=""nofollow noreferrer"">this cycle</a> over the available token ranges.  But if you're looking into writing full table scan yourself, think about using the <a href=""https://github.com/datastax/dsbulk"" rel=""nofollow noreferrer"">DSBulk</a> parts as an SDK - you need to look onto <a href=""https://github.com/datastax/dsbulk/tree/1.x/partitioner"" rel=""nofollow noreferrer""><code>partitioner</code> module</a> that was designed specifically for that.</p>
",['table']
68047353,68047587,2021-06-19 13:44:25,ConfigurationException: Invalid type int for column above_size: Cannot mix counter and non counter columns in the same table,"<p>Why I get this error message if I create this table ?</p>
<pre><code>items_by_name
    item_id uuid
    user_id uuid
    name TEXT
    image VARCHAR
    desc TEXT
    price DECIMAL
    category TEXT
    trouser_size INT
    shoe_size INT
    above_size INT
    color TEXT,
    liked_user_id INT,
    like_count counter,
    PRIMARY KEY (name, item_id)
</code></pre>
<p>...........................
..........................
...........................
.............................
.............................</p>
",<cassandra>,"<p>Tables with counters are handled specially in Cassandra, and as result, you may have non-counter types only as part of the primary key, but not as regular columns. From <a href=""https://cassandra.apache.org/doc/latest/cql/types.html?highlight=counter#counters"" rel=""nofollow noreferrer"">documentation</a>:</p>
<blockquote>
<p>A table that contains a counter can only contain counters. In other words, either all the columns of a table outside the PRIMARY KEY have the counter type, or none of them have it.</p>
</blockquote>
<p>There are also other limitations - see documentation.</p>
<p>In your case, you will need to have two tables - one for counters, and one - for the non-counter types.  Just use the same primary key for both tables.</p>
",['table']
68076191,68081700,2021-06-22 00:38:46,Cassandra insert without batch,"<p>I'm learning to use the PhantomDSL driver (Scala) for Cassandra.
There's an excellent sample here: <a href=""https://github.com/iamthiago/cassandra-phantom"" rel=""nofollow noreferrer"">https://github.com/iamthiago/cassandra-phantom</a></p>
<p>For example to add a Song to Cassandra:</p>
<pre><code> def saveOrUpdate(songs: Song): Future[ResultSet] = {
    Batch.logged
      .add(SongsModel.store(songs))
      .add(SongsByArtistsModel.store(songs))
      .future()
  }
</code></pre>
<p>To simplify I'm going to remove using <code>SongsByArtistsModel</code>:</p>
<pre><code> def saveOrUpdate(songs: Song): Future[ResultSet] = {
    Batch.logged
      .add(SongsModel.store(songs))
      .future()
  }
</code></pre>
<p>I have two questions:
1- How do I know if the operation has been successfully or has been an error?
2- I've been reading that using <code>Batch</code> isn't good in terms of performance. What's the code for inserting without using <code>Batch</code>?</p>
<p>Sorry if these are dummy questions, I'm just starting.</p>
",<scala><cassandra><phantom-dsl>,"<blockquote>
<p>1- How do I know if the operation has been successfully or has been an error?</p>
</blockquote>
<p>For this one, ideally, you would be checking if the Future returns success or failure. You can use a <code>.transform</code> on your Future in case you want to return something or <code>.onComplete</code> if you don't care about returning anything, then, pattern matching between <code>Success</code> and <code>Failure</code>, and decide what to do on each case.
See the example below.</p>
<pre><code>save(song).transform {
   case Success(value) =&gt; //handle success case
   case Failure(exception) =&gt; //handle failure case
}
</code></pre>
<blockquote>
<p>2- I've been reading that using Batch isn't good in terms of performance. What's the code for inserting without using Batch</p>
</blockquote>
<p>Batch operation in Cassandra is recommended in case you want to achieve atomicity. So you can combine multiple operations and that's exactly what I tried to do in my example because I want both tables to be consistent. Now, if you want to work with only one table you could write your method as follow:</p>
<pre><code>def save(song: Song): Future[ResultSet] = {
   SongsModel
      .store(song)
      .future()
}
</code></pre>
",['table']
68161734,68162243,2021-06-28 10:26:03,Cassandra queries without allow filtering,"<p>I'm beginner with Cassandra and I'm trying to execute queries without allow filtering clause.</p>
<p>I've created 2 tables: car parts and orders:</p>
<pre><code>CREATE TABLE Autosale.parts 
(
    part_number text, 
    part_name text,
    matching_model text,
    condition text,
    description text,
    price double,

    PRIMARY KEY (part_number, matching_model, condition, part_name)
) WITH CLUSTERING ORDER BY (matching_model ASC, condition ASC,  part_name ASC);

SELECT * 
FROM parts 
WHERE matching_model ='x'; //without allow filtering

SELECT * FROM parts WHERE condition ='x';
SELECT * FROM parts WHERE part_name ='x'; 
</code></pre>
<pre><code>CREATE TABLE Autohandel.zamowienia 
(
    nr_of_order int,
    id_customer uuid,
    price_total double,
    parts_name set&lt;text&gt;,
    collect_way text,
    if_paid boolean,
    additional_requests text,
    adress text,
    customer_name text,

    PRIMARY KEY(customer_id, customer_name, nr_of_order, if_paid)
) WITH CLUSTERING ORDER BY (customer_name ASC, nr_of_order ASC,  if_paid ASC);


SELECT * FROM parts WHERE customer_name = 'x'; 
SELECT * FROM parts WHERE nr_of_order = 'x';
SELECT * FROM parts WHERE if_paid = true;
</code></pre>
<p>I tried using WITH CLUSTERING ORDER BY and MATERIALIZED VIEW but there are another errors such as clustering order or primary key column cannot be restricted.</p>
<p>I will be glad for any help.</p>
",<cassandra>,"<p>You need to understand how Cassandra retrieves data for you. For that you need to understand how Cassandra stores data. For example in your below case</p>
<pre><code>CREATE TABLE Autosale.parts (
    part_number text, 
    part_name text,
    matching_model text,
    condition text,
    description text,
    price double,
PRIMARY KEY (part_number, matching_model, condition, part_name)
)WITH CLUSTERING ORDER BY (matching_model ASC, condition ASC,  part_name ASC);
</code></pre>
<p>You created a table parts with <code>PRIMARY KEY</code> as <code>(part_number, matching_model, condition, part_name)</code>. Now we try to understand how primary key is defined. Generally primary key is consist of <code>(Partition key, List of Clutering keys)</code>. So in your example <code>part_number</code> is the partition key and <code>matching_model, condition, part_name</code> are two clustering keys.</p>
<p>Now Cassandra uses partition key for grouping same kind of rows. For example, all parts with part_number &quot;XYZ&quot; will be grouped together. One part can have matching_model as &quot;ABC&quot; and part_name as &quot;Horn&quot; and condition as &quot;Good&quot;. Second part with same part_number can have matching_model as &quot;DEF&quot; and part_name as &quot;Mirror&quot; and Condition as  &quot;Bad&quot;.</p>
<p>Now looking at your queries</p>
<pre><code>Select * from parts where matching_model ='x'; //without allow filtering
Select * from parts where condition ='x';
Select * from parts where part_name ='x';
</code></pre>
<p>You want to read from <code>parts</code> table without using partition key <code>part_number</code>. Cassandra will complain because it needs partition key to respond key in optimized way. It says something like this</p>
<p><code>Bad Request: Cannot execute this query as it might involve data filtering and thus may have unpredictable performance. If you want to execute this query despite the performance unpredictability, use ALLOW FILTERING</code></p>
<p>So I would recommend you study the data modelling in Cassandra, understand the primary key, importance of partition key while querying. You might have to redesign your data model.</p>
",['table']
68177485,68343009,2021-06-29 11:02:55,cassandra and anchor modeling,"<p>I would like to go from an anchor model which has only( anchor, knot, tie and attributes) to physical model. i'm working on column oriented database, i'm using cassandra database.</p>
<p>Anyone has any document or any idea that could help me throught this passage ?</p>
",<cassandra><nosql><anchor><modeling>,"<p>Anchor modeling isn't compatible with data modeling in Cassandra.</p>
<p>Unlike relational databases, tables in Cassandra are denormalised. You start to model your data by identifying all the application queries then design a table for each query so that reads are very fast since there are no foreign lookups and no joins -- all the data you need to satisfy a query are contained within one table.</p>
<p>If you're interested, we have a free hands-on tutorial for Cassandra Data Modeling here -- <a href=""https://www.datastax.com/dev/modeling"" rel=""nofollow noreferrer"">https://www.datastax.com/dev/modeling</a>. Cheers!</p>
",['table']
68187175,68187551,2021-06-30 00:39:37,cassandra partion key grow limit?,"<p>what means partions my grow large ? I think cassandra can handle a very large size of it. Why they use in this example 2 partion keys ?</p>
<p>And what I do maybe both partiton keys are too large ?</p>
<p><a href=""https://i.stack.imgur.com/tv6Uc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tv6Uc.png"" alt=""enter image description here"" /></a></p>
",<cassandra>,"<p>The example which you gave is one of the ways for preventing partitions to become too large. In Cassandra <code>partition key</code> ( part of primary key) is used for grouping similar set of rows.</p>
<p>Here in left side data model, <code>user_id</code> is the partition key which means every video interaction by that user will be placed in same partition. As mentioned in example comment, if user is active and has 1000 interaction daily then in 60 days (2 months) you will have 60000 rows for that user. This may breach Cassandra permissible partition size (in terms of data size stored in single partirion).</p>
<p>So to avoid this situation there are many ways you can avoid partition size to grow too big. For example, you can do</p>
<ol>
<li><p>Make another column from that table a part of partition key. This is done in the example above. The <code>video_id</code> is made part of partition key along with <code>user_id</code>.</p>
</li>
<li><p>Bucketing - This is the strategy which is used in time series data generally where you make multiple buckets of a partition key. For example if <code>date</code> is your partition key then you can create 24 buckets as <code>date_1, date_2,.....,date_24</code>. Now you have divided your partition key into smaller partition keys and hence you divided one big partition into 24 small partitions.</p>
</li>
</ol>
<p>The main idea is to avoid your partition to grow too big in size. This is a data modeling technique which one should be aware of while creating data model for Cassandra.</p>
<p>If still you have large partition size, you need to remodel your data model  based on various data modelling techniques available. For that I would recommend understand your data, estimate rate of growth, calculate estimated size of partition and if your data model is not meeting the partition size demand then refine your data model.</p>
",['table']
68199482,68199821,2021-06-30 17:52:08,"Apache Cassandra ""no viable alternative at input 'OR' ""","<p>My table looks like :</p>
<pre><code>CREATE TABLE prod_cust (
    pid bigint,
    cid bigint,
    effective_date date,
    expiry_date date,
    PRIMARY KEY ((pid, cid))
);
</code></pre>
<p>My below query is giving no viable alternative at input 'OR' error</p>
<pre><code>SELECT * FROM prod_cust 
where 
pid=101 and cid=201 
OR 
pid=102 and cid=202;
</code></pre>
<p>Does Cassandra not support OR operator if not, Is there any alternate way to achieve my result.</p>
",<cassandra><cassandra-3.0>,"<p>CQL does not support the <code>OR</code> operator.  Sometimes you can get around that by using <code>IN</code>.  But even <code>IN</code> won't let you do what you're attempting.</p>
<p>I see two options:</p>
<ul>
<li>Submit each side of your <code>OR</code> as individual queries.</li>
<li>Restructure the table to better-suit what you're trying to do.  Doing a &quot;port-over&quot; from a RDBMS to Cassandra almost never works as intended.</li>
</ul>
",['table']
68218800,68221774,2021-07-02 02:06:25,How can I avoid a partition key from a too large size,"<p>The partition size should not be greater than 10 MB (its recommended). How can I avoid this ?</p>
<p>I can use 2 Partition keys, but I can't query them, it does not fit in my query.</p>
<p>If you have 2 partition keys, and you execute a query, you have to call both partition keys like this:</p>
<pre><code>user_id = partition key
post_id = partition key

SELECT * FROM posts WHERE user_id = 1 AND post_id = 1;
</code></pre>
<p>If you use this, you get an error because you don't call post_id</p>
<pre><code>SELECT * FROM posts WHERE user_id = 1;
</code></pre>
<p>But I don't need post_id in my query or user_id I only always need one of them but not both. So how I avoid now the partition size ?</p>
",<cassandra>,"<p>You can have table design like below -</p>
<pre><code>CREATE TABLE user_post_by_month (
userid int,
year int,
month int,
postid int, 
post text,
PRIMARY KEY ((userid,year,month),postid);
</code></pre>
<p>This design will not create a wide row. You can fetch the post by user for a particular month. If you are expecting too many post for a user in a month, you can add another bucket, for example a week.</p>
<pre><code>CREATE TABLE user_post_by_week (
userid int,
year int,
month int,
week int,
postid int, 
post text,
PRIMARY KEY ((userid,year,month,week),postid);
</code></pre>
",['table']
68325265,68327704,2021-07-10 06:23:58,Is it safe to copy cassandra snapshot files over sstable files in a running node?,"<p>Edited after reading nodetool tagged questions.</p>
<p>We take snapshots of our single node cassandra database daily. If I want to restore a snapshot either on that node, or on our staging server which is running a different instance of cassandra, my understanding is I have to:</p>
<ol>
<li><p>nodetool disablegossip</p>
</li>
<li><p>nodetool disablebinary</p>
</li>
<li><p>nodetool drain</p>
</li>
<li><p>Copy the sstable files from the snapshot directories to the sstable directories under the keyspace directory.</p>
</li>
<li><p>Run nodetool refresh on each table.</p>
</li>
<li><p>Enable binary &amp; gossip.</p>
</li>
</ol>
<p>Is this sufficient to safely bring the snapshot sstable files in without cassandra overwriting them while I'm doing the refresh?</p>
<p>What is the opposite of nodetool drain?</p>
<p>Another edit: What about sstableloader? Should I use that instead? If so, how? I looked at the &quot;documentation&quot; and am none the wiser.</p>
",<cassandra><nodetool>,"<p>The steps you outlined isn't quite right. You don't shutdown Cassandra and you shouldn't just copy the files on top of the existing SSTables.</p>
<p>At a high level, the steps to restore table snapshots on a node are:</p>
<ol>
<li><code>TRUNCATE</code> the table you want to restore (will remove the SSTables from the data directories).</li>
<li>Copy the SSTables from <code>data/ks_name/table-UUID/snapshots/snapshot_name</code> subdirectory into the &quot;live&quot; data directory <code>data/ks_name/table-UUID</code>.</li>
<li>Run <code>nodetool refresh -- ks_name table_name</code>.</li>
</ol>
<p>You will need to repeat these steps for each application table you want to restore. NOTE: Do <em>NOT</em> restore system tables, only application tables.</p>
<p>The detailed steps are documented in <a href=""https://docs.datastax.com/en/cassandra-oss/3.x/cassandra/operations/opsBackupSnapshotRestore.html"" rel=""nofollow noreferrer"">Restoring from a snapshot in Cassandra</a>.</p>
<p>To restore a snapshot into another cluster, I prefer to refer to this as &quot;cloning&quot;. The procedure for cloning snapshots to another cluster depends on whether the source and destination clusters have <strong>identical configuration</strong>.</p>
<p>If both source and destination clusters are identical, follow the steps I documented here -- <a href=""https://community.datastax.com/questions/4534/"" rel=""nofollow noreferrer"">https://community.datastax.com/questions/4534/</a>. I've explained what <strong>identical configuration</strong> means in this post.</p>
<p>If they are <em>not identical</em>, follow the steps I documented here -- <a href=""https://community.datastax.com/questions/4477/"" rel=""nofollow noreferrer"">https://community.datastax.com/questions/4477/</a>. Cheers!</p>
",['table']
68411337,68470299,2021-07-16 15:07:38,Finding the best db design for design Instagram problem,"<p>I was reading the post given in link:
<a href=""https://www.educative.io/courses/grokking-the-system-design-interview/m2yDVZnQ8lG"" rel=""nofollow noreferrer"">https://www.educative.io/courses/grokking-the-system-design-interview/m2yDVZnQ8lG</a>.
I am having alot of difficulty in understanding the section Database Schema:</p>
<p><a href=""https://www.educative.io/courses/grokking-the-system-design-interview/m2yDVZnQ8lG#div-stylecolorblack-background-colore2f4c7-border-radius5px-padding5px6-database-schema"" rel=""nofollow noreferrer"">https://www.educative.io/courses/grokking-the-system-design-interview/m2yDVZnQ8lG#div-stylecolorblack-background-colore2f4c7-border-radius5px-padding5px6-database-schema</a></p>
<p>Now, this section recomends to store metadata for photos in a nosql store like cassandra.</p>
<p>The questions are:</p>
<ul>
<li>With reference to the line as below, what exactly does it mean by that storing the list in different columns?</li>
</ul>
<blockquote>
<p>For the ‘UserPhoto’ table, the ‘key’ would be ‘UserID’, and the
‘value’ would be the list of ‘PhotoIDs’ the user owns, stored in
different columns.</p>
</blockquote>
<ul>
<li>While it recommends, using a nosql store, how exactly will this be useful over a rdbms?</li>
</ul>
",<database-design><cassandra><instagram>,"<blockquote>
<p>... what exactly does it mean by that storing the list in different
columns?</p>
</blockquote>
<p>I assume the table schema would look something like:</p>
<pre><code>CREATE TABLE user_photos
  userid text,
  photoid int,
  photopath varchar,
  ...
  PRIMARY KEY (userid, photoid)
)
</code></pre>
<p>The <code>PRIMARY KEY</code> for the table has the partition key as <code>userid</code> and <code>photoid</code> as a clustering column. It means that each record (identified by <code>userid</code>) in the table will have multiple &quot;rows&quot; of <code>photoid</code> (clustering column) since each user can have multiple photos.</p>
<p>Cassandra is referred to as a <strong>wide-column store</strong> because data is stored in &quot;wide columns&quot; meaning columns are repeated one or more times as required. To illustrate using the example above, below is a representation of how a record is stored on disk:</p>
<pre><code>+----------+-----------+-----------+-----+-----------+
| PK       | Column 1  | Column 2  | ... | Column n  |
+----------+-----------+-----------+-----+-----------+
| userid = | photoid = | photoid = | ... | photoid = |
| 'abc123' | 56789012  | 78901234  | ... | 90123456  |
+----------+-----------+-----------+-----+-----------+
</code></pre>
<p>Each record can have one column, or a hundred columns. It depends on how many photos a user has. It isn't a fixed number of columns like traditional RDBMS tables.</p>
<blockquote>
<p>While it recommends, using a nosql store, how exactly will this be useful over a rdbms?</p>
</blockquote>
<p>A lot of use cases for NoSQL databases can't be modelled in the traditional two-dimensional RDBMS tables (columns running along the top, rows running down the page).</p>
<p>Like the above example shows, Cassandra supports both the traditional 2D tables but also multi-dimensional tables.</p>
<p>But more importantly, RDBMS cannot achieve scale in the same way that databases like Cassandra does. You can have hundreds or a thousand nodes in a Cassandra cluster and you can have nodes distributed across the globe. There are lots of features and attributes in NoSQL DBs and Cassandra that cannot be achieved with RDBMS. Cheers!</p>
",['table']
68440246,68452088,2021-07-19 12:08:15,How to get statistics of secondary index (for example size on disk) in Cassandra 2.1.6?,"<p>How to  get statistic secodary index (for example size index in HDD) in cassandra DB (version C* 2.1.6 )?</p>
",<indexing><cassandra><cql>,"<p>Have a look at the output of <code>nodetool tablestats</code> (<code>cfstats</code> in earlier versions of Cassandra) specifically on the hidden index table of the table you're interested in.</p>
<p>To illustrate with an example, here's my table of community questions:</p>
<pre><code>CREATE TABLE community.questions (
    id int PRIMARY KEY,
    author text,
    title text
)
</code></pre>
<p>If I create a secondary index on the <code>author</code> column:</p>
<pre><code>CREATE INDEX ON community.questions (author)
</code></pre>
<p>a hidden table called <code>questions_author_idx</code> gets created in the background.</p>
<p>Here's an example output of the <code>cfstats</code> command on the hidden index table:</p>
<pre><code>$ nodetool cfstats community.questions.questions_author_idx
Total number of tables: 66
----------------
Keyspace : community
    Read Count: 1
    Read Latency: 5.832 ms
    Write Count: 10
    Write Latency: 8.5428 ms
    Pending Flushes: 0
        Table (index): questions.questions_author_idxquestions.questions_author_idx
        SSTable count: 1
        Space used (live): 5148
        Space used (total): 5148
        Space used by snapshots (total): 0
        Off heap memory used (total): 8
        SSTable Compression Ratio: 0.8454545454545455
        Number of partitions (estimate): 3
        ...
</code></pre>
<p>Note that the output of <code>nodetool</code> only relates to the node you're running on so you'll need to run it on all nodes. Cheers!</p>
",['table']
68451504,68451678,2021-07-20 08:03:45,One DC stopped syncronizing. Error in logs,"<p>We started seeing this error in the logs. At the same time the only node in the DC &quot;datacenter_spark&quot; stopped syncronizing to the DC &quot;datacenter-prod&quot;.</p>
<p>The columns from the error message point to a table that we have, but comparing the node on both DC it has the same columns.</p>
<p>What is causing this issue and how can it be fixed?</p>
<p><strong>Error:</strong></p>
<pre><code>2021-07-20 07:54:03,927 ERROR [ReadStage-1] AbstractLocalAwareExecutorService.java:169 run Uncaught exception on thread Thread[ReadStage-1,5,main]
java.lang.IllegalStateException: [color, icon_image_file, name, type] is not a subset of [icon_image_file name type]
        at org.apache.cassandra.db.Columns$Serializer.encodeBitmap(Columns.java:565)
        at org.apache.cassandra.db.Columns$Serializer.serializeSubset(Columns.java:497)
        at org.apache.cassandra.db.rows.UnfilteredSerializer.serializeRowBody(UnfilteredSerializer.java:230)
        at org.apache.cassandra.db.rows.UnfilteredSerializer.serialize(UnfilteredSerializer.java:205)
        at org.apache.cassandra.db.rows.UnfilteredSerializer.serialize(UnfilteredSerializer.java:137)
        at org.apache.cassandra.db.rows.UnfilteredSerializer.serialize(UnfilteredSerializer.java:125)
        at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:137)
        at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:92)
        at org.apache.cassandra.db.rows.UnfilteredRowIteratorSerializer.serialize(UnfilteredRowIteratorSerializer.java:79)
        at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$Serializer.serialize(UnfilteredPartitionIterators.java:307)
        at org.apache.cassandra.db.ReadResponse$LocalDataResponse.build(ReadResponse.java:187)
        at org.apache.cassandra.db.ReadResponse$LocalDataResponse.&lt;init&gt;(ReadResponse.java:180)
        at org.apache.cassandra.db.ReadResponse$LocalDataResponse.&lt;init&gt;(ReadResponse.java:176)
        at org.apache.cassandra.db.ReadResponse.createDataResponse(ReadResponse.java:76)
        at org.apache.cassandra.db.ReadCommand.createResponse(ReadCommand.java:353)
        at org.apache.cassandra.db.ReadCommandVerbHandler.doVerb(ReadCommandVerbHandler.java:50)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:66)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:165)
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:137)
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:113)
        at java.lang.Thread.run(Thread.java:748)
</code></pre>
<p><strong>Nodetool status:</strong></p>
<pre><code>Datacenter: datacenter-prod
===========================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address       Load       Tokens       Owns (effective)  Host ID                               Rack
UN  10.164.0.23   143.14 GiB  256          100.0%            e7e2a38a-d4f3-4758-a345-73fcffe26035  rack1
UN  10.164.0.24   146.79 GiB  256          100.0%            0c18b8e4-5ca2-4fb5-9e8c-663b74909fbb  rack1
UN  10.164.0.58   151.03 GiB  256          100.0%            547c0746-72a8-4fec-812a-8b926d2426ae  rack1
Datacenter: datacenter_spark
============================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address       Load       Tokens       Owns (effective)  Host ID                               Rack
UN  172.16.0.179  140.57 GiB  256          100.0%            790cef99-9234-4b2d-8389-c4407ed8cb9b  rack1
</code></pre>
",<cassandra>,"<p>The error indicates that there is a mismatch between the table schema and the data on disk.</p>
<p>You'll need to check for schema agreement across all nodes in the cluster. If you've recently made schema changes, check the current table definition versus what's on disk.</p>
<p>One possibility is that something went wrong when you tried to drop a column and the entry is missing from the <code>system_schema.dropped_columns</code> table. If this is the case, you will need to temporarily add the column back to the table with the same CQL data type then drop the column. Cheers!</p>
",['table']
68536805,68537872,2021-07-26 22:04:28,What are the differences between wide partition and data skew in Cassandra?,"<p>As I understood both are telling data amount in a specific partition should not be more than other partitions. So we should choose proper partition key(s) to compensate for these problems. But really what are the differences between these two idioms?</p>
",<database><cassandra><nosql>,"<p>While they can occur for the same reasons (Data Model and Partition Key Cardinality), the data imbalance between nodes can occur for others reasons.</p>
<p>If a partition key is not selective enough, there can be situations where the amount of data partition grows, with a maximum recommended amount of 100 Mb per partition, but ideally not more than even 10 Mb.</p>
<p>While having a low cardinality partition key can result in some skew, you can also get a skew in the allocation of the tokens to the ring. The RandomPartitioner has more of a habit of producing an unbalanced result compared to the MurmurPartitioner - but even Murmur can be improved by using the allocate_tokens_for_keyspace / allocate_tokens_for_local_replication_factor - the same setting has different names depending on the C* or DSE version being used, but the idea is to provide the partitioner with more information relating to the intended replication factor, so it produces more of a balanced allocation.</p>
<p>A further way in which data can be unbalanced is from the topology choices - if you create a cluster with keyspaces using NetworkTopologyStrategy (recommended that you should), and multiple racks - unless the number of nodes per rack is the same, then the data will not be balanced.
For example (to demonstrate the result, not that you would do this.)</p>
<ul>
<li>Rack 1 = 5 nodes</li>
<li>Rack 2 = 5 nodes</li>
<li>Rack 3 = 2 nodes.</li>
</ul>
<p>With an RF of 3 and 100 GB of Data, each rack will hold a replica. Nodes in rack 1 and 2 will roughly be 20Gb each, rack 3 will be 50Gb each (roughly).</p>
<p>This is why the normal advice when using racks is you will increase the node count by 3 per DC as it expands.</p>
","['allocate_tokens_for_local_replication_factor', 'partitioner', 'allocate_tokens_for_keyspace', 'rack']"
68545205,68545537,2021-07-27 12:52:50,Read queries in cql when all clustering columns are not available,"<p>In cql, when we write read queries we need to provide columns in same order as they are mentioned in primary key definition, and all of them must be present. But how to write query if we don't have values available for some of the clustering columns.</p>
<p>For example, if primary key is defined as primary key((state), city, name) for a table containing records for users with their state, city and name as part of primary key, then how to write query to find all users with a given name from a state irrespective of their city?
Something like</p>
<pre><code>Select * from USERS where state ='....', name='....';
</code></pre>
<p>won't work.</p>
",<cassandra><cql>,"<p>You have two options here.</p>
<ol>
<li><p>Create a query table alongside the original table keyed on <code>PRIMARY KEY ((state),name)</code>.</p>
</li>
<li><p>Invoke the <code>ALLOW FILTERING</code> directive on the above query, which filters on only <code>state</code> and <code>name</code>.</p>
</li>
</ol>
<p>While the use of <code>ALLOW FILTERING</code> is normally discouraged (and with good reason), it's not nearly as bad in <em>this</em> case.  By specifying <code>state</code>, you've given the query the ability to locate the node which contains the data.  Yes, you'll still be &quot;filtering&quot; through data in a non-performant way, but at least it won't be across multiple nodes.</p>
",['table']
68548556,68554207,2021-07-27 16:23:19,When does Cassandra fetches full rows when a client sends a update query,"<p>Lets take an example table:</p>
<pre><code>CREATE TABLE student (
    id int PRIMARY KEY,
    name text,
    phone text
);
</code></pre>
<p>And a clients sends a update query like: <code>update student set name='name_temp' where id in (1, 2);</code></p>
<p>My question is what gets saved into memtable, does it save the whole row for ids 1 and 2 (which means it has to fetch the whole row first) with updated value for <code>name</code> column or just the delta? When does the whole row gets fetched as I assume when it writes to SSTable it has to write the whole row with the latest 'name` column value.</p>
<p><strong>EDIT:</strong></p>
<p>For complete understanding please read the comments as part of the selected answer.</p>
",<java><cassandra><nosql><datastax><cql>,"<p>In Cassandra, <code>INSERT</code>, <code>UPDATE</code> and <code>DELETE</code> statements are all inserts under the hood. Cassandra doesn't do a read-before-write (with the exception of lightweight transactions) so your query:</p>
<pre><code>UPDATE student SET name='name_temp' WHERE id IN (1, 2);
</code></pre>
<p>does not &quot;fetch the rows&quot; before updating the 2 partitions.</p>
<p>All it does is insert 2 new records to the <code>student</code> table where only the <code>name</code> column is set -- for these 2 particular mutations, there is no value for the column <code>phone</code>.</p>
<p>Provided there are no new mutations (inserts/updates/deletes) to those 2 records, the following records get flushed from the memtable to disk:</p>
<pre><code>{ id = 1, name = 'name_temp' }
{ id = 2, name = 'name_temp' }
</code></pre>
<p>Cassandra has sparse storage meaning only the columns with values set are stored on disk. Since the mutation did not contain the <code>phone</code> column, it will not get included in the new SSTable that resulted from the memtable flush. Cheers!</p>
",['table']
68690601,68705021,2021-08-07 08:20:21,"UnixTime saved as int, how to query as Date. Cassandra CQLSH","<p>Im building the pipeline: sensorData - MQTT broker - Kafka - Cassandra. Payload is transferred as JSON and when saving in Cassandra, date was save as int. I can't get the readable date when query CQLSH.</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE sensordata.mqttsensordata (
  sensor text, 
  temperature float, 
  humidity int, 
  timestamp int, 
  battery int, 
  calibratedhumidity int, 
  datetime timestamp, 
  receiver text, 
  rssi float, 
  voltage float, 
  PRIMARY KEY (
    (sensor, temperature, humidity), 
    timestamp
 )
</code></pre>
<p>How do I get the readable timestamp when query the database like the picture below?</p>
<p><a href=""https://i.stack.imgur.com/XqzlU.jpg"" rel=""nofollow noreferrer"">Query table</a></p>
",<apache-kafka><cassandra><nosql><cqlsh>,"<p>The only time function that can operate on a UNIX timestamp is [min|max]timeuuid (<code>mintimeuuid()</code> or <code>maxtimeuuid()</code>).  You can use either of those on the timestamp column, and nest it inside the <code>toTimestamp()</code> function.</p>
<p>For example, if I have a table of sample times:</p>
<pre><code>&gt; SELECT * FROM sample_times WHERE a=1;

 a | b                               | c                                    | d
---+---------------------------------+--------------------------------------+---------------
 1 | 2021-08-08 21:42:54.131000+0000 | 96594031-f891-11eb-b7bc-e12958c8479f | 1628458974131
</code></pre>
<p>Column <code>d</code> is a <code>bigint</code> where I have stored the UNIX timestamp.  I can show that as a timestamp like this:</p>
<pre><code>&gt; SELECT totimestamp(mintimeuuid(d)) FROM sample_times WHERE a=1;

 system.totimestamp(system.mintimeuuid(d))
-------------------------------------------
           2021-08-08 21:42:54.131000+0000

(1 rows)
</code></pre>
",['table']
68782996,68783579,2021-08-14 11:45:32,Cassandra need for IN clause in consideration of a messaging application,"<p>For a messaging app i have a database structure comparatively to:</p>
<pre><code>CREATE TABLE users(
    userid text,
    name text, 
    rooms list&lt;text&gt;
    ...
    PRIMARY KEY (userid)
);

CREATE TABLE rooms(
    roomid text,
    members list&lt;text&gt;,
    createdat bigint,
    lastmessage bigint,
    ...
    PRIMARY KEY (roomid, createdat)
);

CREATE TABLE messages(
    roomid text,
    bucket int,
    messageid bigint,
    authorid text,
    ...
    PRIMARY KEY ((hash, roomid), messageid)
);
</code></pre>
<p>On startup the client requests all rooms for a given user. First I query all roomids for the given user with:</p>
<pre><code>SELECT rooms FROM users WHERE userId = 1234
</code></pre>
<p>Then i use the IN clause to gather all rooms</p>
<pre><code>SELECT * FROM rooms WHERE roomid IN ('room_1', 'room_2', ......);
</code></pre>
<p>and return the entities to the client.</p>
<p>I have researched, that the IN clause could lead to putting one node under a lot of pressure. I expect users to have up to a hundred rooms.</p>
<p>Must I split the request into single queries or is their another way like changing the data model ?</p>
<p>Why does the IN clause leads to pressure on a single node ?</p>
<p>Thanks in advance !</p>
",<cassandra><cql><cassandra-3.0><cql3>,"<p>You are correct in that you should limit the number of keys in the <code>IN()</code> operator. I generally recommend very low single-digit number of keys like 2 or 3, not much more, or the coordinator will be under a lot of pressure since it has to fire off as many separate requests.</p>
<p>You are right that you should model your data differently to get optimal performance.</p>
<p>I would highly recommend creating a new table that is partitioned by user IDs:</p>
<pre><code>CREATE TABLE rooms_by_userid (
  ...
  PRIMARY KEY (userid, roomid)
)
</code></pre>
<p>When you query the table with:</p>
<pre><code>SELECT ... FROM rooms_by_userid WHERE userid = 1234
</code></pre>
<p>you'll get rows of data clustered by room IDs. This is the best way to model your data since it is organised based on the app requirement.</p>
<p>Your current model is effectively doing a clumsy JOIN by having to query 2 tables. The way I'm proposing means that you only need to retrieve data from one table so it's really efficient. Cheers!</p>
",['table']
68786237,68788109,2021-08-14 18:58:31,Cassandra chat app: sorting rooms after last message inserted,"<p>For a messaging app I have a database structure comparatively to:</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE users(
    userid text,
    name text, 
    rooms list&lt;text&gt;
    ...
    PRIMARY KEY (userid)
);

CREATE TABLE rooms(
    roomid text,
    members list&lt;text&gt;,
    createdat bigint,
    lastmessage bigint,
    ...
    PRIMARY KEY (roomid, createdat)
);

CREATE TABLE messages(
    roomid text,
    bucket int,
    messageid bigint,
    authorid text,
    ...
    PRIMARY KEY ((hash, roomid), messageid)
);
</code></pre>
<p>On startup the client requests all rooms for a given user. I expect at some point, that a user will be member of hundreds of channels. So I only want to retrieve the last X active channels to reduce traffic.</p>
<p>Currently the room stores the last messageid (snowflake including timestamp) so I am capable to sort, after retrieving all rooms.</p>
<p>What changes are necessary to only load the last X active rooms from Cassandra? I know that I need to denormalize the structure somehow, but I do not know how.</p>
",<cassandra><cql><cassandra-3.0><cql3>,"<p>This looks like a variation of your question in <a href=""https://stackoverflow.com/questions/68782996/"">#68782996</a> where I suggested creating this table for your app query &quot;give me all rooms for a user&quot;:</p>
<pre><code>CREATE TABLE rooms_by_userid (
  ...
  PRIMARY KEY (userid, roomid)
)
</code></pre>
<p>From your description, it sounds like the app query is &quot;give me the 10 most recent rooms by a user&quot;. You also mentioned that you are determining the most recent rooms using the <code>messageid</code>. In this case, the table would look like:</p>
<pre><code>CREATE TABLE rooms_by_userid_by_messageid (
   userid text,
   messageid bigint,
   roomid text,
   ...
   PRIMARY KEY (userid, messageid)
) WITH CLUSTERING ORDER BY (messageid DESC, roomid ASC)
</code></pre>
<p>The data in this table would be partitioned by user ID and would contain rows sorted by message ID in reverse order (most recent first) where each message has an associated rooms. You would retrieve the 10 most recent rooms using <code>LIMIT 10</code> like this:</p>
<pre><code>SELECT roomid FROM rooms_by_userid_by_messageid
  WHERE userid = ?
  AND messageid = ?
  LIMIT 10;
</code></pre>
<p>The important point here is that the data is already sorted in the order you need so you don't need to do any client-side sorting when you get the results from the database. Cheers!</p>
",['table']
68814139,68814624,2021-08-17 08:34:21,How do I change default compaction strategy in cassandra/scylla?,"<p>I've read that you can set compaction strategy per table in Cassandra/Scylla, as described here <a href=""https://docs.scylladb.com/operating-scylla/procedures/config-change/change_compaction/"" rel=""nofollow noreferrer"">https://docs.scylladb.com/operating-scylla/procedures/config-change/change_compaction/</a></p>
<p>The default compaction strategy is Size-tiered compaction strategy (STCS).</p>
<p>But is there a way to change it somehow, in the settings, such that each table that's created uses another compaction strategy by default?</p>
<p>Thanks.</p>
",<cassandra><scylla>,"<p>The compaction strategy is a sub-property of the compaction configuration of each table so you will need to use the CQL <code>ALTER TABLE</code> command to choose a different compaction strategy other than the default.</p>
<p>In almost all cases, the <code>SizeTieredCompationStrategy</code> (STCS) is the right choice and so it is the default. There are very limited cases where you would choose a different compaction strategy.</p>
<p>The most common situation where you would change it is if you have a time-series use case where <code>TimeWindowCompactionStrategy</code> (TWCS) is recommended. <code>LeveledCompactionStrategy</code> (LCS) is only valid for workloads were there is very little writes and your app is almost exclusively doing reads.</p>
<p>So unless you fit into these narrow use cases, STCS should be your choice of compaction strategy. Cheers!</p>
",['table']
68814332,68814850,2021-08-17 08:47:02,How to BULK Read from Azure SQL server and BULK INSERT to Cassandra in Camel,"<p>I want to read 5+ million events from an Azure SQL DB table and perform a BULK INSERT to Cassandra.
The table has 2 columns.
I see the SQL component available for reading from Azure SQL DB. <a href=""https://camel.apache.org/components/3.7.x/sql-component.html"" rel=""nofollow noreferrer"">https://camel.apache.org/components/3.7.x/sql-component.html</a></p>
<p>Question: Consuming from Azure SQL DB</p>
<ol>
<li>Is there a better way to read all the rows and store in a map considering 5M records ?</li>
<li>Is there a possibility to read messages in batches ?</li>
</ol>
<p>There is a cql component available for Cassandra
<a href=""https://camel.apache.org/components/3.7.x/cql-component.html"" rel=""nofollow noreferrer"">https://camel.apache.org/components/3.7.x/cql-component.html</a></p>
<p>Question: Producing to Cassandra</p>
<ol>
<li>Can we INSERT in batches ?</li>
</ol>
<p>Can I use camel for this use case ?</p>
",<spring-boot><cassandra><apache-camel><spring-data-cassandra><camel-sql>,"<p>For each table in your Azure database, we recommend that you export the data into a CSV file. There are lots of tools and methods available that would allow you to do that. For example, have a look at <a href=""https://learn.microsoft.com/en-us/sql/relational-databases/import-export/overview-import-export"" rel=""nofollow noreferrer"">Import and export data from Azure SQL Database</a>.</p>
<p>Once you have exported your data to CSV, you can use the <a href=""https://docs.datastax.com/en/dsbulk/doc/dsbulk/dsbulkAbout.html"" rel=""nofollow noreferrer"">DataStax Bulk Loader</a> tool (DSBulk) to bulk load it to a Cassandra table.</p>
<p>Here are some references with examples to help you get started quickly:</p>
<ul>
<li>Blog - <a href=""https://www.datastax.com/blog/datastax-bulk-loader-introduction-and-loading"" rel=""nofollow noreferrer"">DSBulk Intro + Loading data</a></li>
<li>Blog - <a href=""https://www.datastax.com/blog/datastax-bulk-loader-more-loading"" rel=""nofollow noreferrer"">More DSBulk Loading examples</a></li>
<li>Blog - <a href=""https://www.datastax.com/blog/datastax-bulk-loader-counting"" rel=""nofollow noreferrer"">Counting records with DSBulk</a></li>
<li>Docs - <a href=""https://docs.datastax.com/en/dsbulk/doc/dsbulk/reference/dsbulkLoad.html"" rel=""nofollow noreferrer"">Loading data examples</a></li>
<li>Answered questions - <a href=""https://community.datastax.com/topics/dsbulk.html"" rel=""nofollow noreferrer"">DS Community</a></li>
</ul>
<p>DSBulk is open-source so it's free to use. Cheers!</p>
",['table']
68824710,68833675,2021-08-17 22:29:42,Configuring cassandra-rackdc and cassandra-topology,"<p>I am configuring 6 nodes Cassandra cluster on AWS EC2,3 nodes in region and 3nodes in other region:</p>
<p>eu-central-1</p>
<ul>
<li>node0   cass-db-0   10.10.37.79   eu-central-1a</li>
<li>node1   cass-db-1   10.10.38.229  eu-central-1b</li>
<li>node2   cass-db-2   10.10.36.76   eu-central-1a</li>
</ul>
<p>eu-west-1</p>
<ul>
<li>node3   cass-db-0   10.10.37.80   eu-west-1a</li>
<li>node4   cass-db-1   10.10.39.177  eu-west-1b</li>
<li>node5   cass-db-2   10.10.37.231  eu-west-1a</li>
</ul>
<p>I have completed the local configuration in cassandra.yaml.</p>
<p>Now I need to configure cassandra-rackdc.properties &amp; cassandra-topology.properties but I don't understand the network topology.</p>
<p>Please Advise.</p>
<p>Thanks</p>
",<cassandra>,"<p>Erick provides some great background here, which should be helpful for you.  In terms of getting to a simple solution, I'd recommend this:</p>
<ul>
<li>Make sure you're using the <code>GossipingPropertyFileSnitch</code> in the <code>cassandra.yaml</code>.</li>
<li>Delete <code>cassandra-topology.properties</code>.</li>
<li>Edit <code>cassandra-rackdc.properties</code> and set <code>dc=eu-west-1</code> for the 3 the west nodes; likewise <code>dc=eu-central-1</code> for the central nodes.</li>
<li>Leave the rack at the default, as you only have 3 nodes across 2 availability zones (AZs 1a and 1b).</li>
</ul>
<p>If you were using AZs 1a, 1b, and 1c I'd say to use that for the <code>rack</code> property.  Erick mentions defining your keyspaces with a RF of 3, which is solid advice.  Typically, you'll want the number of AZs to match your RF for even data distribution and availability, which is why I'd recommend leaving <code>rack</code> at the default value for all.</p>
<p>Likewise, your keyspace definitions would look something like this:</p>
<pre><code>CREATE KEYSPACE keyspace_name WITH REPLICATION = 
    {'class':'NetworkTopologyStrategy',
     'eu-west-1':'3',
     'eu-central-1':'3'};
</code></pre>
<p>The main point to consider, is that your data center names <em>must</em> match between the keyspace definition and the entries in the <code>cassandra-rackdc.properties</code> files.</p>
",['rack']
68833241,68836199,2021-08-18 13:21:13,Cassandra DB Query for System Date,"<p>I have one table <code>customer_info</code> in a Cassandra DB &amp; it contains one column as <code>billing_due_date</code>, which is date field (<code>dd-MMM-yy</code> ex. 17-AUG-21).  I need to fetch the certain fields from <code>customer_info</code> table based on <code>billing_due_date</code> where <code>billing_due_date</code> should be equal to system date +1.</p>
<p>Can anyone suggest a Cassandra DB query for this?</p>
",<cassandra><cql>,"<blockquote>
<p>fetch the certain fields from <code>customer_info</code> table based on <code>billing_due_date</code></p>
<p><code>transaction_id</code> is primarykey , It is just generated through uuid()</p>
</blockquote>
<p>Unfortunately, there really isn't going to be a good way to do this.  Right now, the data in the <code>customer_info</code> table is distributed across all nodes in the cluster based on a hash of the <code>transaction_id</code>.  Essentially, any query based on something other than <code>transaction_id</code> is going to read from multiple nodes, which is a query anti-pattern in Cassandra.</p>
<p>In Cassandra, you need to design your tables based on the queries that they need to support.  For example, choosing <code>transaction_id</code> as the sole primary key may distribute well, but it doesn't offer much in the way of query flexibility.</p>
<p>Therefore, the best way to solve for this query, is to create a query table containing the data from <code>customer_info</code> with a key definition of <code>PRIMARY KEY (billing_date,transaction_id)</code>.  Then, a query like this should work:</p>
<pre><code>&gt; SELECT * FROM customer_info_by_date
  WHERE billing_due_date = toDate(now()) + 2d;

 billing_due_date | transaction_id                       | name
------------------+--------------------------------------+---------
       2021-08-20 | 2fe82360-e314-4d5b-aa33-5deee9f03811 | Rinzler
       2021-08-20 | 92cb9ee5-dee6-47fe-b372-0829f2e384cd |     Clu

(2 rows)
</code></pre>
<p>Note that for this example, I am using the system date plus 2 days out. So in your case, you'll want to adjust the &quot;duration&quot; aspect from <code>2d</code> down to <code>1d</code>. Cassandra 4.0 allows date arithmetic, so this should work just fine if you are on that version.  If you are not, you'll have to do the &quot;system date plus one&quot; calculation on the app side.</p>
<p>Another way to go about this, would be to create a secondary index on <code>billing_due_date</code>, but I don't recommend that path as it will query multiple nodes to build the result set.</p>
",['table']
68839795,71971999,2021-08-18 21:57:57,Error to write dataframe in Cassandra table on Amazon Keyspaces,"<p>I'm trying to write a dataframe on AWS (Keyspace), but I'm getting the following messages below:</p>
<p>Stack:</p>
<pre><code>dfExploded.write.cassandraFormat(table = &quot;table&quot;, keyspace = &quot;hub&quot;).mode(SaveMode.Append).save()
21/08/18 21:45:18 WARN DefaultTokenFactoryRegistry: [s0] Unsupported partitioner 'com.amazonaws.cassandra.DefaultPartitioner', token map will be empty.
java.lang.AssertionError: assertion failed: There are no contact points in the given set of hosts
  at scala.Predef$.assert(Predef.scala:223)
  at com.datastax.spark.connector.cql.LocalNodeFirstLoadBalancingPolicy$.determineDataCenter(LocalNodeFirstLoadBalancingPolicy.scala:195)
  at com.datastax.spark.connector.cql.CassandraConnector$.$anonfun$dataCenterNodes$1(CassandraConnector.scala:192)
  at scala.Option.getOrElse(Option.scala:189)
  at com.datastax.spark.connector.cql.CassandraConnector$.dataCenterNodes(CassandraConnector.scala:192)
  at com.datastax.spark.connector.cql.CassandraConnector$.alternativeConnectionConfigs(CassandraConnector.scala:207)
  at com.datastax.spark.connector.cql.CassandraConnector$.$anonfun$sessionCache$3(CassandraConnector.scala:169)
  at com.datastax.spark.connector.cql.RefCountedCache.createNewValueAndKeys(RefCountedCache.scala:34)
  at com.datastax.spark.connector.cql.RefCountedCache.syncAcquire(RefCountedCache.scala:69)
  at com.datastax.spark.connector.cql.RefCountedCache.acquire(RefCountedCache.scala:57)
  at com.datastax.spark.connector.cql.CassandraConnector.openSession(CassandraConnector.scala:89)
  at com.datastax.spark.connector.cql.CassandraConnector.withSessionDo(CassandraConnector.scala:111)
  at com.datastax.spark.connector.datasource.CassandraCatalog$.com$datastax$spark$connector$datasource$CassandraCatalog$$getMetadata(CassandraCatalog.scala:455)
  at com.datastax.spark.connector.datasource.CassandraCatalog$.getTableMetaData(CassandraCatalog.scala:421)
  at org.apache.spark.sql.cassandra.DefaultSource.getTable(DefaultSource.scala:68)
  at org.apache.spark.sql.cassandra.DefaultSource.inferSchema(DefaultSource.scala:72)
  at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.getTableFromProvider(DataSourceV2Utils.scala:81)
  at org.apache.spark.sql.DataFrameWriter.getTable$1(DataFrameWriter.scala:339)
  at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)
</code></pre>
<p>SparkSubmit:</p>
<pre><code>spark-submit --deploy-mode cluster --master yarn  \
--conf=spark.cassandra.connection.port=&quot;9142&quot; \
--conf=spark.cassandra.connection.host=&quot;cassandra.sa-east-1.amazonaws.com&quot; \
--conf=spark.cassandra.auth.username=&quot;BUU&quot; \
--conf=spark.cassandra.auth.password=&quot;123456789&quot; \
--conf=spark.cassandra.connection.ssl.enabled=&quot;true&quot; \
--conf=spark.cassandra.connection.ssl.trustStore.path=&quot;cassandra_truststore.jks&quot;
--conf=spark.cassandra.connection.ssl.trustStore.password=&quot;123456&quot;
</code></pre>
<p>Connection by cqlsh everything ok, but in spark got this error</p>
",<scala><apache-spark><cassandra><spark-cassandra-connector>,"<p>To read and write data between Keyspaces and Apache Spark by using the open-source Spark Cassandra Connector all you have to do is update the partitioner for your Keyspaces account.</p>
<p>Docs: <a href=""https://docs.aws.amazon.com/keyspaces/latest/devguide/spark-integrating.html"" rel=""nofollow noreferrer"">https://docs.aws.amazon.com/keyspaces/latest/devguide/spark-integrating.html</a></p>
",['partitioner']
68903456,68904009,2021-08-24 07:31:30,The partition key must be defined on delete queries in django-cassandra-engine,"<pre><code>from django.utils import timezone
from cassandra.cqlengine import columns
from django_cassandra_engine.models import DjangoCassandraModel


class RequestResponse(DjangoCassandraModel):
    id = columns.UUID(primary_key=True, default=uuid.uuid4)
    sel_no = columns.Text(index=True)
    transaction_type = columns.Text()
    created_at = columns.DateTime(default=timezone.now())
    updated_at = columns.DateTime(default=timezone.now())
</code></pre>
<p>I am using django-cassandra-engine==1.6.1. When I am trying to delete my all data from this model then these errors occurred. My command line:</p>
<pre><code>RequestResponse.objects.all().count()
&gt;&gt; 10123
RequestResponse.objects.all().delete()
</code></pre>
<p>Then these errors occurred.</p>
<pre><code>    996         partition_keys = set(x.db_field_name for x in self.model._partition_keys.values())
    997         if partition_keys - set(c.field for c in self._where):
--&gt; 998             raise QueryException(&quot;The partition key must be defined on delete queries&quot;)
    999 
   1000         dq = DeleteStatement(

QueryException: The partition key must be defined on delete queries

</code></pre>
",<python><django><cassandra>,"<p>Executing the equivalent of &quot;delete all&quot; will cause a full table scan and is not efficient in Cassandra so you can only delete a specific partition by providing the partition key. The underlying CQL statement is equivalent to:</p>
<pre><code>DELETE FROM table_name WHERE pk = ?
</code></pre>
<p>But when you're trying to do a &quot;delete all&quot;, it isn't valid in Cassandra since the <code>DELETE</code> must be restricted to a partition using the <code>WHERE</code> clause so it isn't valid in Django-Cassandra either.</p>
<p>If you don't need or want the data in the table, you should instead connect to your Cassandra instance directly with cqlsh and issue a <code>TRUNCATE</code>. Cheers!</p>
",['table']
68917135,68917767,2021-08-25 05:14:22,"How to store data on topic which will be passed as a message through producer, such that it can loaded to database on respective columns","<p>I followed <a href=""https://www.confluent.io/blog/apache-kafka-spring-boot-application/?utm_medium=sem&amp;utm_source=bing&amp;utm_campaign=ch.sem_br.nonbrand_tp.rmkt_tgt.kafka_mt.xct_rgn.india_lng.eng_dv.all_con.kafka-spring&amp;utm_term=spring%20kafka&amp;creative=&amp;device=c&amp;placement=&amp;msclkid=fb574f0256621aec4fb22e2787e3e53d"" rel=""nofollow noreferrer"">this blog</a> to produce a message on to topic using kafka spring so far.</p>
<p>The message I'm going to pass is just the name and now I would like to stream through this topic and add incremental value as ID along with name and store it on <code>OutputTopic</code> and now, I would like to store data to cassandra.</p>
<p>My table structure in cassandra as follows:-</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE emp(
   emp_id int PRIMARY KEY,
   emp_name text,
   )
</code></pre>
<p>In which format the data should be on output topic so that I can easily store it on cassandra table?</p>
<p>How can I achieve the above functionality?</p>
",<apache-kafka><cassandra>,"<p>Once you've got the data published on a Kafka topic, you can just use the DataStax <a href=""https://docs.datastax.com/en/kafka/doc/kafka/kafkaIntro.html"" rel=""nofollow noreferrer"">Kafka connector</a> for Apache Cassandra, DataStax Enterprise and <a href=""https://dtsx.io/2XM1INf"" rel=""nofollow noreferrer"">Astra DB</a>.</p>
<p>The connector lets you save records from a Kafka topic to Cassandra tables. It is open-source so it's free to use.</p>
<p>There's a <a href=""https://docs.datastax.com/en/kafka/doc/kafka/kafkaTutorial.html"" rel=""nofollow noreferrer"">working example here</a> that includes a Cassandra table schema, details of how to configure the Kafka sink connector and map a topic to the CQL table. Cheers!</p>
",['table']
68979039,68979265,2021-08-30 04:47:19,Elassandra replication information and rack configuration,"<p>I recently started working with an Elassandra cluster with two data centers which have been configured using NetworkTopologyStrategy.</p>
<p>Cluster details : <code>Elassandra 6.2.3.15 = Elasticsearch 6.2.3 + Cassandra 3.11.4</code></p>
<pre><code>Datacenter: DC1
=================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address       Load       Tokens       Owns    Host ID                               Rack
UN  &lt;ip1&gt;         50 GiB  256          ?       6cab1f4c-8937-437d-b010-0a5677443dc3  rack1
UN  &lt;ip2&gt;         48 GiB  256          ?       6c9e7ad5-a642-4c0d-8b77-e78d821d904b  rack1
UN  &lt;ip3&gt;         50 GiB  256          ?       7e493bc6-c8a5-471e-8eee-3f3fe985b90a  rack1
Datacenter: DC2
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address       Load       Tokens       Owns    Host ID                               Rack
UN  &lt;ip4&gt;         47 GiB  256          ?       c49c1203-cc38-41a2-b9c8-2b42bc907c17  rack1
UN  &lt;ip5&gt;         67 GiB  256          ?       0d9f31bc-9690-49b6-9d88-4fb30c1b6c0d  rack1
UN  &lt;ip6&gt;         88 GiB  256          ?       80c4d60d-185f-457a-ae9b-2eb611735f07  rack1
</code></pre>
<p>schema info <br>
<code>CREATE KEYSPACE my_keyspace WITH replication = {'class': 'NetworkTopologyStrategy', 'DC1': '3', 'DC2': '3'}  AND durable_writes = true;</code></p>
<p>The <code>DC2</code> is kind of a Disaster Recovery site and in an ideal world, we should be able to use only that in case of a disaster.</p>
<ol>
<li>With the very limited knowledge I have, I strongly suspect that we need
to modify the rack configuration to have a 'proper' D/R cluster (So
that data in DC1 gets replicated in DC2) or am I getting this
wrong? If so, is there a standard guideline to follow?</li>
<li>When there are multiple DCs, does Cassandra automatically replicate this regardless of rack configurations? (Are racks kind of additional fail proof?)</li>
<li>DC2 has more data than DC1. Is this purely related to hash function?</li>
<li>Is there any other things that can be rectified in this cluster?</li>
</ol>
<p>Many thanks!</p>
",<cassandra><cassandra-3.0><elassandra>,"<p>These replication settings mean that the data for your keyspace is replicated in real time between the 2 DCs with each DC having 3 replicas (copies):</p>
<pre><code>CREATE KEYSPACE my_keyspace WITH replication = {
  'class': 'NetworkTopologyStrategy',
  'DC1': '3',
  'DC2': '3'
}
</code></pre>
<p>Replication in Cassandra happens in real time -- any writes sent to one DC is sent to all other DCs at the same time. Unlike traditional RDBMS or configurations with primary/secondary or active/DR, Cassandra replication is instantaneous and immediate.</p>
<p>The logical Cassandra racks are for additional redundancy mechanism. If you have C* nodes deployed in different (a) physical racks, or (b) public cloud availability zones, Cassandra will distribute the replicas to separate racks so each rack has a full copy of the data. With a replication factor of 3 in the DC, if a rack goes down for whatever reason then there's still full copies of the data in the remaining 2 racks and read/write requests with a consistency of <code>LOCAL_QUORUM</code> (or lower) will not be affected.</p>
<p>I've explained this in a bit more detail in this post -- <a href=""https://community.datastax.com/questions/1128/"" rel=""nofollow noreferrer"">https://community.datastax.com/questions/1128/</a>.</p>
<p>If you're new to Cassandra, we recommend <a href=""https://www.datastax.com/dev"" rel=""nofollow noreferrer"">https://www.datastax.com/dev</a> which has links to short hands-on tutorials where you can quickly learn the basics of Cassandra -- all free. This tutorial is a good place to start -- <a href=""https://www.datastax.com/try-it-out"" rel=""nofollow noreferrer"">https://www.datastax.com/try-it-out</a>. Cheers!</p>
",['rack']
69111355,69120991,2021-09-09 01:52:26,org.springframework.data.cassandra.CassandraUncategorizedException,"<p>I'm using a hibernate in the spring boot to call the method <code>findByIdField(UUID id)</code> in Cassandra 3.11 to return a list and this erro bellow appear in the console log.</p>
<p>I have a <em>entity</em>, <em>repository</em> and <em>service</em> with this simple method. I'm passing a UUID as parameter and I check the ID in the database and it exist.</p>
<p>Controller :</p>
<pre><code>Optional&lt;List&lt;Fields&gt;&gt; fieldList = fieldService.findByIdField(UUID.fromString(id));
</code></pre>
<p>Service :</p>
<pre><code>public Optional&lt;List&lt;Fields&gt;&gt; findByIdField(UUID id) throws SQLException {
    return repository.findByIdField(id);
}
</code></pre>
<p>Repository:</p>
<pre><code>@AllowFiltering
Optional&lt;List&lt;Fields&gt;&gt; findByIdField(UUID id);
</code></pre>
<p>Error:</p>
<blockquote>
<p>org.springframework.data.cassandra.CassandraUncategorizedException: nested exception is java.lang.NullPointerException</p>
<p>at org.springframework.data.cassandra.core.cql.CassandraExceptionTranslator.translate(CassandraExceptionTranslator.java:160) ~[spring-data-cassandra-3.1.3.jar:3.1.3]<br />
at org.springframework.data.cassandra.core.cql.CassandraExceptionTranslator.translateExceptionIfPossible(CassandraExceptionTranslator.java:72) ~[spring-data-cassandra-3.1.3.jar:3.1.3]
at org.springframework.data.cassandra.config.CqlSessionFactoryBean.translateExceptionIfPossible(CqlSessionFactoryBean.java:646) ~[spring-data-cassandra-3.1.3.jar:3.1.3]
at org.springframework.dao.support.ChainedPersistenceExceptionTranslator.translateExceptionIfPossible(ChainedPersistenceExceptionTranslator.java:61) ~[spring-tx-5.3.3.jar:5.3.3]
at org.springframework.dao.support.DataAccessUtils.translateIfNecessary(DataAccessUtils.java:242) ~[spring-tx-5.3.3.jar:5.3.3]
at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:152) ~[spring-tx-5.3.3.jar:5.3.3]
at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186) ~[spring-aop-5.3.3.jar:5.3.3]
at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215) ~[spring-aop-5.3.3.jar:5.3.3]
at com.sun.proxy.$Proxy117.findByIdSensor(Unknown Source) ~[na:na]
at br.com.genesis.service.operacao.GerarStatusSinalSasMcsScmPiService.findByIdSensor(GerarStatusSinalSasMcsScmPiService.java:44) ~[classes/:2.4.2]
at br.com.genesis.controller.operacao.OpeStatusComponeteController.mostraHistoricoFalha(OpeStatusComponeteController.java:941) ~[classes/:2.4.2]
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_152]
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_152]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_152]
at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_152]
at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:197) ~[spring-web-5.3.3.jar:5.3.3]
at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:141) ~[spring-web-5.3.3.jar:5.3.3]
at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:106) ~[spring-webmvc-5.3.3.jar:5.3.3]
at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:894) ~[spring-webmvc-5.3.3.jar:5.3.3]
at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:808) ~[spring-webmvc-5.3.3.jar:5.3.3]
at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:87) ~[spring-webmvc-5.3.3.jar:5.3.3]
at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:1060) ~[spring-webmvc-5.3.3.jar:5.3.3]
at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:962) ~[spring-webmvc-5.3.3.jar:5.3.3]
at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:1006) ~[spring-webmvc-5.3.3.jar:5.3.3]
at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:909) ~[spring-webmvc-5.3.3.jar:5.3.3]
at javax.servlet.http.HttpServlet.service(HttpServlet.java:652) ~[servlet-api.jar:4.0.FR]
at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:883) ~[spring-webmvc-5.3.3.jar:5.3.3]
at javax.servlet.http.HttpServlet.service(HttpServlet.java:733) ~[servlet-api.jar:4.0.FR]
at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:231) [catalina.jar:9.0.41]
at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [catalina.jar:9.0.41]
at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:53) ~[tomcat-websocket.jar:9.0.41]
at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) [catalina.jar:9.0.41]
at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [catalina.jar:9.0.41]
at org.springframework.web.filter.RequestContextFilter.doFilterInternal(RequestContextFilter.java:100) ~[spring-web-5.3.3.jar:5.3.3]
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:119) [spring-web-5.3.3.jar:5.3.3]
at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) [catalina.jar:9.0.41]
at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [catalina.jar:9.0.41]
at org.springframework.web.filter.FormContentFilter.doFilterInternal(FormContentFilter.java:93) ~[spring-web-5.3.3.jar:5.3.3]
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:119) [spring-web-5.3.3.jar:5.3.3]
at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) [catalina.jar:9.0.41]
at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [catalina.jar:9.0.41]
at org.springframework.boot.web.servlet.support.ErrorPageFilter.doFilter(ErrorPageFilter.java:126) [spring-boot-2.4.2.jar:2.4.2]
at org.springframework.boot.web.servlet.support.ErrorPageFilter.access$000(ErrorPageFilter.java:64) [spring-boot-2.4.2.jar:2.4.2]
at org.springframework.boot.web.servlet.support.ErrorPageFilter$1.doFilterInternal(ErrorPageFilter.java:101) [spring-boot-2.4.2.jar:2.4.2]
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:119) [spring-web-5.3.3.jar:5.3.3]
at org.springframework.boot.web.servlet.support.ErrorPageFilter.doFilter(ErrorPageFilter.java:119) [spring-boot-2.4.2.jar:2.4.2]
at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) [catalina.jar:9.0.41]
at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [catalina.jar:9.0.41]
at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:201) [spring-web-5.3.3.jar:5.3.3]
at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:119) [spring-web-5.3.3.jar:5.3.3]
at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:193) [catalina.jar:9.0.41]
at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:166) [catalina.jar:9.0.41]
at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:202) [catalina.jar:9.0.41]
at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:97) [catalina.jar:9.0.41]
at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:542) [catalina.jar:9.0.41]
at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:143) [catalina.jar:9.0.41]
at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:92) [catalina.jar:9.0.41]
at org.apache.catalina.valves.AbstractAccessLogValve.invoke(AbstractAccessLogValve.java:690) [catalina.jar:9.0.41]
at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:78) [catalina.jar:9.0.41]
at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:343) [catalina.jar:9.0.41]
at org.apache.coyote.http11.Http11Processor.service(Http11Processor.java:374) [tomcat-coyote.jar:9.0.41]
at org.apache.coyote.AbstractProcessorLight.process(AbstractProcessorLight.java:65) [tomcat-coyote.jar:9.0.41]
at org.apache.coyote.AbstractProtocol$ConnectionHandler.process(AbstractProtocol.java:888) [tomcat-coyote.jar:9.0.41]
at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1597) [tomcat-coyote.jar:9.0.41]
at org.apache.tomcat.util.net.SocketProcessorBase.run(SocketProcessorBase.java:49) [tomcat-coyote.jar:9.0.41]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_152]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_152]
at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) [tomcat-util.jar:9.0.41]
at java.lang.Thread.run(Thread.java:748) [na:1.8.0_152]
Caused by: java.lang.NullPointerException: null
at sun.invoke.util.ValueConversions.unboxLong(ValueConversions.java:121) ~[na:1.8.0_152]
at br.com.genesis.model.entity.operacao.GerarStatusSinalSasMcsScmPi_Accessor_7l4hif.setProperty(Unknown Source) ~[classes/:2.4.2]
at org.springframework.data.mapping.model.InstantiationAwarePropertyAccessor.setProperty(InstantiationAwarePropertyAccessor.java:104) ~[spring-data-commons-2.4.3.jar:2.4.3]
at org.springframework.data.mapping.model.ConvertingPropertyAccessor.setProperty(ConvertingPropertyAccessor.java:63) ~[spring-data-commons-2.4.3.jar:2.4.3]
at org.springframework.data.cassandra.core.convert.MappingCassandraConverter.readProperty(MappingCassandraConverter.java:413) ~[spring-data-cassandra-3.1.3.jar:3.1.3]
at org.springframework.data.cassandra.core.convert.MappingCassandraConverter.readProperties(MappingCassandraConverter.java:400) ~[spring-data-cassandra-3.1.3.jar:3.1.3]
at org.springframework.data.cassandra.core.convert.MappingCassandraConverter.doReadEntity(MappingCassandraConverter.java:389) ~[spring-data-cassandra-3.1.3.jar:3.1.3]
at org.springframework.data.cassandra.core.convert.MappingCassandraConverter.doReadEntity(MappingCassandraConverter.java:365) ~[spring-data-cassandra-3.1.3.jar:3.1.3]
at org.springframework.data.cassandra.core.convert.MappingCassandraConverter.readEntityFromRow(MappingCassandraConverter.java:345) ~[spring-data-cassandra-3.1.3.jar:3.1.3]
at org.springframework.data.cassandra.core.convert.MappingCassandraConverter.readRow(MappingCassandraConverter.java:341) ~[spring-data-cassandra-3.1.3.jar:3.1.3]
at org.springframework.data.cassandra.core.convert.MappingCassandraConverter.read(MappingCassandraConverter.java:298) ~[spring-data-cassandra-3.1.3.jar:3.1.3]
at org.springframework.data.cassandra.core.CassandraTemplate.lambda$getMapper$11(CassandraTemplate.java:910) ~[spring-data-cassandra-3.1.3.jar:3.1.3]
at org.springframework.data.cassandra.core.CassandraTemplate.lambda$select$0(CassandraTemplate.java:337) ~[spring-data-cassandra-3.1.3.jar:3.1.3]
at org.springframework.data.cassandra.core.cql.RowMapperResultSetExtractor.extractData(RowMapperResultSetExtractor.java:83) ~[spring-data-cassandra-3.1.3.jar:3.1.3]
at org.springframework.data.cassandra.core.cql.RowMapperResultSetExtractor.extractData(RowMapperResultSetExtractor.java:43) ~[spring-data-cassandra-3.1.3.jar:3.1.3]
at org.springframework.data.cassandra.core.cql.CqlTemplate.query(CqlTemplate.java:298) ~[spring-data-cassandra-3.1.3.jar:3.1.3]
at org.springframework.data.cassandra.core.cql.CqlTemplate.query(CqlTemplate.java:320) ~[spring-data-cassandra-3.1.3.jar:3.1.3]
at org.springframework.data.cassandra.core.CassandraTemplate.select(CassandraTemplate.java:337) ~[spring-data-cassandra-3.1.3.jar:3.1.3]
at org.springframework.data.cassandra.repository.query.CassandraQueryExecution$CollectionExecution.execute(CassandraQueryExecution.java:136) ~[spring-data-cassandra-3.1.3.jar:3.1.3]
at org.springframework.data.cassandra.repository.query.CassandraQueryExecution$ResultProcessingExecution.execute(CassandraQueryExecution.java:262) ~[spring-data-cassandra-3.1.3.jar:3.1.3]
at org.springframework.data.cassandra.repository.query.AbstractCassandraQuery.execute(AbstractCassandraQuery.java:105) ~[spring-data-cassandra-3.1.3.jar:3.1.3]
at org.springframework.data.repository.core.support.RepositoryMethodInvoker.doInvoke(RepositoryMethodInvoker.java:137) ~[spring-data-commons-2.4.3.jar:2.4.3]
at org.springframework.data.repository.core.support.RepositoryMethodInvoker.invoke(RepositoryMethodInvoker.java:121) ~[spring-data-commons-2.4.3.jar:2.4.3]
at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.doInvoke(QueryExecutorMethodInterceptor.java:152) ~[spring-data-commons-2.4.3.jar:2.4.3]
at org.springframework.data.repository.core.support.QueryExecutorMethodInterceptor.invoke(QueryExecutorMethodInterceptor.java:131) ~[spring-data-commons-2.4.3.jar:2.4.3]
at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186) ~[spring-aop-5.3.3.jar:5.3.3]
at org.springframework.data.projection.DefaultMethodInvokingMethodInterceptor.invoke(DefaultMethodInvokingMethodInterceptor.java:80) ~[spring-data-commons-2.4.3.jar:2.4.3]
at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186) ~[spring-aop-5.3.3.jar:5.3.3]
at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:97) ~[spring-aop-5.3.3.jar:5.3.3]
at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186) ~[spring-aop-5.3.3.jar:5.3.3]
at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:215) ~[spring-aop-5.3.3.jar:5.3.3]
at com.sun.proxy.$Proxy117.findByIdSensor(Unknown Source) ~[na:na]
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_152]
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_152]
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_152]
at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_152]
at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:344) ~[spring-aop-5.3.3.jar:5.3.3]
at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:198) ~[spring-aop-5.3.3.jar:5.3.3]
at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:163) ~[spring-aop-5.3.3.jar:5.3.3]
at org.springframework.dao.support.PersistenceExceptionTranslationInterceptor.invoke(PersistenceExceptionTranslationInterceptor.java:137) ~[spring-tx-5.3.3.jar:5.3.3]
... 63 common frames omitted</p>
</blockquote>
",<java><spring-boot><cassandra><cassandra-3.0>,"<p>While the error message indicates a NPE (null pointer exception), I think that's masking the root problem here.  However, the error message displayed when removing <code>ALLOW FILTERING</code>:</p>
<pre><code>Cannot execute this query as it might involve
data filtering and thus may have unpredictable
performance. If you want to execute this query
despite the performance unpredictability, use
ALLOW FILTERING;
</code></pre>
<p>...is much more revealing.</p>
<p>This tells me that the underlying table is not keyed by the UUID.  This means that Cassandra has to scan all partitions on each node (exhaustively) to build the result set.  It probably works in cqlsh because the query timeout threshold is set higher than Spring's.</p>
<p>As for how to resolve this, I'll recommend that a new table be created to support this query, with a partition key based on that UUID.  That will prevent every node from being a part of the query.</p>
<p>If that's not possible to do, you could try adding a secondary index on that column.  Secondary indexes don't typically perform well with nigh-unique cardinality (which is why building a new table is really the best option), but it would probably perform better than using the <code>ALLOW FILTERING</code> directive.</p>
",['table']
69139536,69144355,2021-09-11 02:19:42,cassandra local_quorum query is inconsistent,"<p>cassandra version: 2.1.15</p>
<p>Number of nodes: dc1: 80, dc2: 80</p>
<p>problem:</p>
<p>Our copy strategy is as follows:</p>
<pre><code>WITH REPLICATION = {'class':'NetworkTopologyStrategy','dc1': 3,'dc2': 3};
</code></pre>
<p>We encountered a problem with Cassandra, and it was inconsistent when querying with local_quorum. We will only read and write in dc1.
We also use local_quorum for writing, and then use local_quorum for queries.</p>
<p>But there is a phenomenon, use the following statement:</p>
<pre><code>select count from table where partitionKey=?
</code></pre>
<p>The results of the query were initially inconsistent and eventually consistent.</p>
<p>Assuming that the first is 10000, the second is 9998, and the third is 9997, it may remain at 10001 in the end(Maybe it was triggered to read repair, which led to the final stabilization) .
During this period, we have done a large-scale expansion. And make sure that every machine is cleaned up. And we also found that the results of using getEndpoint    on different machines are inconsistent. In the end, we found that the result of getEndpoint has 4 machines in dc1.</p>
<p>Then we executed getSstable on the corresponding 4 machines, only 3 machines showed the results, and the other machine did not show the results. At the same time, we encountered a similar problem with another partitionKey, but this partitionKey was only queried once, because we recorded the total number of partitionKey in another place, and we can confirm that the total number of partitionKey is incorrect.</p>
<p>After we restarted each machine of dc1 one by one, this problem was solved.
The total number of partitionKey is consistent with the result recorded by us, and if the same query is done multiple times, the result will not change.
Therefore, I suspect that the gossip synchronization node information is too slow, which may lead to inconsistent final results when selecting nodes for query.</p>
",<cassandra>,"<p>Sounds like there may be a few things going on here.  I'll agree that it was likely due to the expansion of the cluster.</p>
<blockquote>
<p>The results of the query were initially inconsistent and eventually consistent.</p>
</blockquote>
<p>Cassandra 2.1 still allows for each table to have a configured read repair chance, and the default value is 10%.  So given enough retries, a read repair will be triggered and inconsistent values will repair themselves.</p>
<blockquote>
<p>Then we executed getSstable on the corresponding 4 machines, only 3 machines showed the results, and the other machine did not show the results. At the same time, we encountered a similar problem with another partitionKey, but this partitionKey was only queried once, because we recorded the total number of partitionKey in another place, and we can confirm that the total number of partitionKey is incorrect.</p>
</blockquote>
<p>Not exactly sure what happened there, but it sounds like one or more new nodes had trouble joining and something may have been messed up during the token allocation process.  My guess is that one or more original nodes had token range maps (in <code>system.peers</code>) which were incorrect or inconsistent with what the other nodes could see.  Good to know that was straightened out with a restart.</p>
<p>Recommendations:</p>
<ul>
<li><strong>Upgrade</strong>.  If you can't get off of Cassandra 2.1, at least get on <a href=""https://archive.apache.org/dist/cassandra/2.2.19/"" rel=""nofollow noreferrer"">2.1.19</a>.  It was released last November, and will have 2 years of bug fixes that you don't have now.</li>
<li><strong>Build your node counts as a factor of your RF</strong>.  With Cassandra versions prior to 3.0, the token allocation was a little flakey.  To help optimize it, the number of nodes should allow the RF to evenly balance-out.  In your case, 80 nodes per DC is <em>not</em> divisible by 3 (the RF).  So I would bump each DC up to 81.  And then run <code>cleanup</code>, of course.</li>
<li><strong>Disable the dynamic snitch</strong>.  The dynamic snitch leverages gossip to try and optimize reads to better-performing servers.  But, it generates a lot of noise, which unfortunately can slow things down due to making its observations.  It <em>may</em> be a contributing factor here.  Just set <code>dynamic_snitch:false</code>.</li>
</ul>
<p>For more details on these tips and other info, I recommend two articles:</p>
<ul>
<li>Amy Tobey's <a href=""https://tobert.github.io/pages/als-cassandra-21-tuning-guide.html"" rel=""nofollow noreferrer"">Cassandra 2.1 Administration Guide</a></li>
<li>Jon Haddad's <a href=""https://thelastpickle.com/blog/2019/01/30/new-cluster-recommendations.html"" rel=""nofollow noreferrer"">14 Things To Do When Setting Up A New Cassandra Cluster</a></li>
</ul>
",['table']
69280468,69281141,2021-09-22 08:09:32,What is the suggested procedure for dropping a keyspace with 55TB of data on 66-node cluster?,"<p>There is a keyspace occupying 55tb of space, cluster has 66 nodes in it (dse 5.1.6, cassandra 3.11)
The keyspace has 3 tables in it and there are no reads/writes on the tables since last one month.</p>
<p>Want to drop the keyspace/tables to reclaim space without causing any issues in the cluster?</p>
<ol>
<li>When dropping tables and keyspace on this cluster - what might cause issues? the size of the unused keyspace (55tb) or the number of nodes (66) in the cluster to which the schema change (drop tables/keyspace) would would need to be propagated?</li>
<li>Other than dropping tables and then keyspace is there any other way to safely drop the keyspace? For example would dropping the sstables from nodes make drops quick and smoother? Would dropping sstables trigger repairs/compactions and cause any issues?</li>
<li>Is there any way to disable auto_snapshot at session level or from a driver level for specific tables or keyspace?</li>
<li>Any considerations before/after dropping the tables/keyspace? here are the steps I am going to follow -
a. nodetool describecluster
b. drop tables using cqlsh (with request-timeout=600)
c. drop keyspace using cqlsh (with request-timeout=600)
d. nodetool describecluster, check for any inconsistencies
e. From each node delete the data directory for the keyspace (data is already backed up somewhere, there is no need of autosnapshot)</li>
</ol>
",<cassandra>,"<p>The only real issue I can foresee you would run into is a schema disagreement. Due to (a) size of the data and (b) number of nodes, my approach would be:</p>
<ol>
<li>Attempt to <code>TRUNCATE</code> 1 table at a time</li>
<li>If the <code>TRUNCATE</code> times out, attempt a second time</li>
<li>When a table is truncated, <code>DROP</code> it</li>
<li>Wait at least 1 minute for schema to propagate</li>
<li>Check for schema disagreement and fix as appropriate</li>
<li>Repeat the steps above until all tables are dropped</li>
<li><code>DROP</code> the keyspace</li>
<li>Manually delete snapshots from filesystem as necessary</li>
</ol>
<p>To answer your questions directly:</p>
<ol>
<li>You can run into timeouts and schema disagreement. And yes, it's a combination of (a) data size and (b) number of nodes.</li>
<li>I'd recommend truncating the tables first as above. This wouldn't result in a schema disagreement since it doesn't change the schema. By truncating first, it would allow the <code>DROP</code> to work without issues.</li>
<li>No, you can only disable <code>auto_snapshot</code> in <code>cassandra.yaml</code> which requires a rolling restart. You don't want to do that because it's not necessary to restart all 66 nodes.</li>
<li>I've posted the procedure above.</li>
</ol>
",['table']
69397364,69399574,2021-09-30 18:39:19,"Cannot sort partitions in DjangoCassandraModel, ""ORDER BY is only supported when the partition key is restricted by an EQ or an IN.""","<pre><code>from cassandra.cqlengine import columns
from django_cassandra_engine.models import DjangoCassandraModel
from django.db import connections

    def get_tag(car_name, car_type):
        class Tag(DjangoCassandraModel):
            __table_name__ = car_name + &quot;_&quot; + car_type
    
            time = columns.Integer(primary_key=True, required=True)  # unix time in seconds  # noqa
            value = columns.Float(required=True)
    
            with connections['cassandra'].cursor() as cursor:
                cursor.execute(&quot;CREATE TABLE IF NOT EXISTS mydatabase.&quot; + __table_name__ + &quot; (time INT PRIMARY KEY, value FLOAT);&quot;)  # noqa
    
            def last():
                with connections['cassandra'].cursor() as cursor:
                    elem = cursor.execute(&quot;SELECT * FROM &quot; + Tag.__table_name__ + &quot; ORDER BY time DESC LIMIT 1;&quot;)  # noqa
                return elem
    
            def __str__(self) -&gt; str:
                return str(self.time) + &quot;,&quot; + str(self.value)
    
        return Tag
</code></pre>
<p>Usage in code:</p>
<pre><code>tag = get_tag(car_name, car_type)
last_elem = tag.last()
</code></pre>
<p>Produced error when calling <code>tag.last()</code>:</p>
<blockquote>
<p>cassandra.InvalidRequest: Error from server: code=2200 [Invalid query]
message=&quot;ORDER BY is only supported when the partition key is
restricted by an EQ or an IN.&quot;</p>
</blockquote>
<p>Hello. I have a dynamic Django model creation in Cassandra. (time, value) measurements. How to get the last element in the table based on time (primary key)? My implementation has an error.</p>
",<django><cassandra>,"<p>As the error message states, the <code>ORDER BY</code> clause only works when you are sorting the rows <strong>within</strong> a single partition, specifically when <em>&quot;the partition key is restricted&quot;</em>.</p>
<p>In your case, you are doing a full table scan (no <code>WHERE</code> clause) since the partition key is <em>not restricted</em> to a single partition:</p>
<pre><code>SELECT * FROM ... ORDER BY time DESC LIMIT 1
</code></pre>
<p>You cannot sort based on the partition key <code>time</code> since that means Cassandra has to retrieve <strong>all partitions</strong> from <strong>all nodes</strong> in order to sort them in descending order.</p>
<p>You can only sort on a clustering column. For example, if you had a table of phone numbers:</p>
<pre><code>CREATE TABLE phone_numbers_by_username (
  username text,
  phone_type text,
  phone_number text,
  PRIMARY KEY (username, phone_type)
)
</code></pre>
<p>Then you can sort by phone type (home, mobile, work) if you restrict the query to a single partition (<code>username</code>):</p>
<pre><code>SELECT * FROM phone_numbers_by_username
  WHERE username = ?
  ORDER BY phone_type DESC
</code></pre>
<p>However, this isn't possible in your case since each partition in your table only has a single row (no clustering columns in the <code>PRIMARY KEY</code>). Cheers!</p>
",['table']
69512555,69517430,2021-10-10 05:27:09,What is the default index in Cassandra?,"<p>I am using cassandra DSE 6.8. I know that there are several indexes used in Cassandra: 2nd index, SASI, SAI, search index, ...</p>
<p>When I create an index on a column:</p>
<pre><code>CREATE INDEX status_idx ON table_name (status);
</code></pre>
<p>What is the name of default index ?</p>
",<indexing><cassandra><cassandra-sasi-index>,"<p>In this case, since a name has been provided, it will be <code>status_idx</code>.  If a name was not provided, one is auto-generated based on the table and column:</p>
<pre><code>table_name + _ + column_name + _idx
</code></pre>
<p>So in this case, it would be:</p>
<pre><code>table_name_status_idx
</code></pre>
<p><strong>Edit</strong></p>
<p>Ahh…I get it now.</p>
<p><code>CREATE INDEX</code> creates a secondary index.</p>
<p><code>CREATE CUSTOM INDEX</code> creates a SASI index.</p>
",['table']
69541681,69546617,2021-10-12 13:44:16,Find rows from Cassandra table that contains IP address within a specific IP range,"<p>I have a table in Cassandra which stores IP in text format.
I want to delete rows from that Cassandra table whose IP value is a private IP.</p>
<p>i.e. IPs which lie within the below ranges:</p>
<pre><code> 172.16.0.0 - 172.31.255.255
192.168.0.0 - 192.168.255.255
  240.0.0.0 - 255.255.255.255
</code></pre>
<p>I was looking through querying using regex, but couldn't find a suitable answer. How do I go about it?</p>
",<cassandra><cql>,"<p>For starters, Cassandra does not allow filtering by regular expressions.</p>
<p>As Cassandra only stores and retrieves data by PRIMARY KEY components, this is going to be difficult to answer without knowing what the PRIMARY KEY structure is.  But, I can give you an example which may help.</p>
<p>Let's say I have a simple table for storing IP addresses, with a partition key on the IP's prefix (first two octets, in this case) and clustered on the IP address.  This results in a PK structure like:</p>
<pre><code>PRIMARY KEY (prefix, ip)
</code></pre>
<p>And that allows for a range query like this to work:</p>
<pre><code>SELECT ip FROM ips WHERE prefix='172.16'
  AND ip &gt;= '172.16.0.0'
  AND ip &lt;  '172.16.0.4';

 ip
------------
 172.16.0.0
 172.16.0.1
 172.16.0.2
 172.16.0.3

(4 rows)
</code></pre>
<p>Likewise, you can also <code>DELETE</code> with a range filter:</p>
<pre><code>DELETE FROM ips WHERE prefix='172.16'
  AND ip &gt;= '172.16.0.0'
  AND ip &lt;  '172.16.0.4';
</code></pre>
<p>Of course, this only works because I am filtering on the components of my PRIMARY KEY definition.</p>
<p>But the point of this example is to show that you can indeed run a range query on a string/TEXT type.  Cassandra also provides the INET data type for the specific purpose of storing IP addresses.</p>
",['table']
69616967,69622223,2021-10-18 13:27:30,"Why do I sometimes have 10,000+ tombstones when I don't do DELETEs?","<p>When doing a repair on a Cassandra node, I sometimes see a lot of tombstone logs. The error looks like this:</p>
<pre><code>org.apache.cassandra.db.filter.TombstoneOverwhelmingException: Scanned over 100001 tombstone rows during query 'SELECT * FROM my_keyspace.table_foo WHERE token(&lt;my params&gt;) &gt;= token(&lt;my params&gt;) AND token(&lt;my params&gt;) &lt;= 2988334221698479200 LIMIT 2147385647' (last scanned row partition key was ((&lt;my params&gt;), 7c650d21-797e-4476-93d5-b1248e187f22)); query aborted
</code></pre>
<p>I have read <a href=""https://thelastpickle.com/blog/2016/07/27/about-deletes-and-tombstones.html"" rel=""nofollow noreferrer"">here</a> that tombstones are inserted as a way to mark a record as deleted. However, I don't see any code in this project that runs a delete on this table - just a read and an insert. What am I missing - how can I prevent these TombStoneOverwhelmingExceptions?</p>
<p>Here is the table definition:</p>
<pre><code>CREATE TABLE my_keyspace.table_foo(
    foo1 text,
    year int,
    month int,
    foo2 text,
    PRIMARY KEY ((foo1, year, month), foo2)
) WITH CLUSTERING ORDER BY (foo2 ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND default_time_to_live = 6912000
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND speculative_retry = '99PERCENTILE';
</code></pre>
",<cassandra>,"<blockquote>
<p>However, I don't see any code in this project that runs a delete on this table - just a read and an insert.</p>
</blockquote>
<p>The code might not be running <code>DELETE</code>s, but the table definition tells Cassandra to delete anything &gt;= 80 days old.  TTLs create tombstones.</p>
<pre><code>AND default_time_to_live = 6912000
</code></pre>
<p>So the thought behind TTLs in a time series model, is that they are typically ordered by timestamp in descending order.  What ends up happening, is that most use cases tend to care only about recent data, and the descending order by timestamp causes the tombstones to end up on the &quot;bottom&quot; of the partition, where they are rarely (if ever) queried.</p>
<p>To create that effect, you'd need to create a new table with a definition something like this:</p>
<pre><code>PRIMARY KEY ((foo1, year, month), created_time, foo2)
) WITH CLUSTERING ORDER BY (created_time DESC, foo2 ASC)
</code></pre>
",['table']
69617363,69626950,2021-10-18 13:54:50,"Is there a way we can insert a Map<String,Object> onto the Cassandra Table using QueryBuilder for a dynamic table(Table with no model class)","<pre><code>public int save(String tableName, Map&lt;String, Object&gt; dataMap) throws IllegalAccessException {
    SimpleStatement saveStatement = QueryBuilder.insertInto(tableName).values()
            



    return 1;

}
</code></pre>
<p>I have tried other inbuilt methods of QueryBuilder to save map values as a whole but only a map of type map&lt;String,Term&gt; can be used to save data if &quot;values()&quot; method is used and I already have a map&lt;String,Object&gt;.This needs to be done for a dynamic table p.s. I am not that much familiar with Dynamic Tables. The other method I tried using with a return type integer was</p>
<pre><code>public int save(String tableName, Map&lt;String, Object&gt; dataMap) throws IllegalAccessException {



    SimpleStatement updateStatement = QueryBuilder.update(tableName)
            .set(appendSet(dataMap))
            .where(appendWhere(domainId))
            .build();
    log.info(updateStatement.getQuery());

    return 1;
}
private Iterable&lt;Assignment&gt; appendSet(Map&lt;String, Object&gt; dataMap) throws IllegalAccessException {
    List&lt;Assignment&gt; assignments = new ArrayList&lt;&gt;();
    for (Field field : dataMap.getClass().getDeclaredFields()) {
        if (!field.getName().equals(&quot;key&quot;)) {
            try {
                field.setAccessible(true);
                if (field.get(dataMap) != null) {
                    if (field.getType().equals(Long.class)) {
                        assignments.add(Assignment.setColumn(field.getName(), literal(Long.valueOf(field.get(dataMap).toString()))));
                    } else {
                        assignments.add(Assignment.setColumn(field.getName(), literal(field.get(dataMap))));
                    }
                }
            } catch (IllegalAccessException e) {
                log.catching(e);
            }
        }
    }
    return assignments;
}

private Iterable&lt;Relation&gt; appendWhere(Object key) {
    List&lt;Relation&gt; relations = new ArrayList&lt;&gt;();
    for (Field field : key.getClass().getDeclaredFields()) {
        try {
            field.setAccessible(true);
            if (field.get(key) != null) {
                if (field.getType().equals(Long.class)) {
                    relations.add(Relation.column(field.getName()).isEqualTo(literal(Long.valueOf(field.get(key).toString()))));
                } else {
                    relations.add(Relation.column(field.getName()).isEqualTo(literal(field.get(key))));
                }
            }
        } catch (IllegalAccessException e) {
            log.catching(e);
        }
    }
    return relations;
}
</code></pre>
<p>This probably didn't work out too. I need to return an integer with the save method but I just cannot figure out how to insert map values into the cassandra table itself using QueryBuilders or CassandraTemplate. I am working with Cassandra Database and my table is dynamic. Can anyone suggest me a good article or anything? I found articles but those did not help me insert map key value pairs into the table so I am quite struggling. Any help would be appreciated.</p>
",<spring-boot><cassandra><query-builder><dynamic-tables>,"<p><strong>Cassandra Data Modelling reminders</strong></p>
<p>With Cassandra there is no such things as dynamic tables, it works with a strict schema. I would argue that if you did not find any samples it is because it is an anti-pattern.</p>
<p>With a Cassandra databases you start by designing your queries and ONLY THEN define the tables because **you can only filter on fields (part of where clause) within the primary key. I mentionned this as <code>Iterable&lt;Relation&gt; appendWhere(Object key)</code> is suspicious, there is no joins nor relations in Cassandra.</p>
<p>If a new query arise on the same data, you duplicate the data in another table (yes, it is different for relational or document oriented)</p>
<p>At initialization of your application you <code>prepare</code> your (static) statements to validate syntax and share a <code>PrepareStatementID</code> with the server.</p>
<pre class=""lang-java prettyprint-override""><code>@PostConstruct
public void prepareStatements() {
   PreparedStatement  stmtCreateUser = 
   session.prepare(QueryBuilder.insertInto(USER_TABLENAME)
                .value(USER_EMAIL, QueryBuilder.bindMarker())
                .value(USER_FIRSTNAME, QueryBuilder.bindMarker())
                .value(USER_LASTNAME, QueryBuilder.bindMarker())
                .ifNotExists().build());
}
</code></pre>
<p>^ Noticed the constants used in the definition. Later when you reuse the column names for extra tables or rename the columns it is way easier to debug.</p>
<p>Given a table :</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE TABLE IF NOT EXISTS users_by_city (
    city name,
    firstname text,
    lastname text,
    email text,
    PRIMARY KEY ((city), lastname, firstname, email)
);
</code></pre>
<p><em>Legal but not recommended:</em></p>
<pre class=""lang-sql prettyprint-override""><code>// This query is legal but never forget the where clause or you do full scan cluster
SELECT [fields] FROM users_by_city;

// This query is legal as you provide the partition in the where clause but having * in the select is hazardous if columns are added later.
SELECT * FROM users_by_city WHERE city=?
</code></pre>
<p><em>Legal Queries</em></p>
<pre class=""lang-sql prettyprint-override""><code>SELECT firstname,lastname,email FROM users_by_city WHERE city=?

SELECT firstname,email FROM users_by_city WHERE city=? and lastname=?

// order is important
SELECT firstname,email FROM users_by_city WHERE city=? and lastname=? and firstname=?
</code></pre>
<p><em>Illegal Queries</em></p>
<pre class=""lang-sql prettyprint-override""><code>// city is required in PK
SELECT firstname,email FROM users_by_city WHERE lastname=? and firstname=?

// order of cluster columns are important
SELECT firstname,email FROM users_by_city WHERE a city=?  and firstname=?
</code></pre>
<p><strong>Implementation Details</strong></p>
<p>With all those reminders <em>(sorry if you knew already all this thinking of those who might come to the question later...)</em> here is some ideas.</p>
<p><code>QueryBuilder</code> is already a builder to build your queries dynamically but looking at your code you can think of</p>
<p><em>INSERT</em></p>
<pre class=""lang-java prettyprint-override""><code>public SimpleStatement insertInto(String keyspace, 
  String tableName, Map&lt;String, Object&gt; fields) {
  return QueryBuilder.insertInto(keyspace, tableName)
                     .values(fields.entrySet()
                         .stream().collect(Collectors.toMap(
                             entry -&gt; entry.getKey(), 
                             entry -&gt; QueryBuilder.literal(entry.getValue()))))
                     .build();
}
</code></pre>
<p><em>SELECT</em></p>
<pre class=""lang-java prettyprint-override""><code>public SimpleStatement selectFrom(String keyspace, String tableName, Map&lt;String, Relation&gt; fields) {
        return QueryBuilder.selectFrom(keyspace, tableName)
                           .columns(fields.keySet())
                           .where(fields.values())
                           .build();
        
    }
</code></pre>
<p><strong>Extra resources</strong></p>
<ul>
<li><a href=""https://docs.datastax.com/en/developer/java-driver/4.13/manual/query_builder/"" rel=""nofollow noreferrer"">More informations could be found in the documentation</a></li>
<li><a href=""https://github.com/KillrVideo/killrvideo-java/blob/master/killrvideo-service-videocatalog/src/main/java/com/killrvideo/service/video/dao/VideoCatalogDseDao.java"" rel=""nofollow noreferrer"">Hope this code could help you</a></li>
</ul>
",['table']
69688312,69698891,2021-10-23 12:48:12,Cassandra - How group by latest timestamp,"<p>I saw some related topics here , but still it isn't clear to me,  How group by the latest row values with cassandra 4.0.1</p>
<p>Let's say my table looks like;</p>
<pre><code>CREATE TABLE simple_search (
    engine text,
    term text,
    time bigint,
    rank bigint,
    url text,
    domain text,
    pagenum bigint,
    descr text,
    display_url text,
    title text,
    type text,
    PRIMARY KEY ((domain), term , time , engine, url , pagenum)
) WITH CLUSTERING ORDER BY (term DESC, time DESC,  engine DESC , url DESC);
</code></pre>
<p>My data looks like:</p>
<pre><code>SELECT time, rank, term  from search_by_domain_termsV2 where domain ='zerotoappstore.com' 



time ,    rank, term 
1633297772, 105,  avfoundation swift
1633315263, 112,  best ide
1633332881, 119,  best ide
1633365856, 50,   developing an app cost
1633375273, 36,   developing an app cost
</code></pre>
<p>I want to have after group by</p>
<pre><code>time ,    rank, term 
1633297772, 105,  avfoundation swift
1633332881, 119,  best ide
1633375273, 36,   developing an app cost
</code></pre>
<p>If I do</p>
<pre><code>SELECT max(time) , rank, term  from search_by_domain_termsV2 where domain ='zerotoappstore.com'  GROUP BY term;
</code></pre>
<p>it gives me the correct max time value but not correct rating,and term.</p>
<pre><code>1633297772  105 avfoundation swift
1633332881  112 best ide
1633375273  50  developing an app cost
</code></pre>
<p>Is it possible to group by term and take the max value of time  ?</p>
",<cassandra><cassandra-3.0>,"<p>@VitalyT,</p>
<p>First, if we're not specifying the <code>pagenum</code> as part of the clustering order by clause of the create table construct, it would give you an error as follows:</p>
<pre><code>InvalidRequest: Error from server: code=2200 [Invalid query] message=&quot;Clustering key columns must exactly match columns in CLUSTERING ORDER BY directive&quot;
</code></pre>
<p>so, it has to be like as follows:</p>
<pre><code>CREATE TABLE IF NOT EXISTS simple_search(
...
PRIMARY KEY (domain, term, time, engine, url, pagenum)
) WITH CLUSTERING ORDER BY (term DESC, time DESC, engine DESC, url [ASC|DESC]);
</code></pre>
<p>Next, with the give data sample of 5 rows. Note, I assumed some values for <code>engine</code>, <code>url</code>, <code>pagenum</code> columns as those values weren't provided in the original question:</p>
<pre><code>SELECT * FROM simple_search ;
 domain             | term                   | time       | engine  | url  | pagenum | descr | display_url | rank | title | type
--------------------+------------------------+------------+---------+------+---------+-------+-------------+------+-------+------
 zerotoappstore.com | developing an app cost | 1633375273 | engine5 | url5 |       5 |  null |        null |   36 |  null | null
 zerotoappstore.com | developing an app cost | 1633365856 | engine4 | url4 |       4 |  null |        null |   50 |  null | null
 zerotoappstore.com |               best ide | 1633332881 | engine3 | url3 |       3 |  null |        null |  119 |  null | null
 zerotoappstore.com |               best ide | 1633315263 | engine2 | url2 |       2 |  null |        null |  112 |  null | null
 zerotoappstore.com |     avfoundation swift | 1633297772 | engine1 | url1 |       1 |  null |        null |  105 |  null | null

(5 rows)
</code></pre>
<p>we would get the following result if we only retrieve the <code>MAX(time)</code> column (without any <code>GROUP BY</code>):</p>
<pre><code>SELECT MAX(time),rank,term FROM simple_search WHERE domain = 'zerotoappstore.com';

 system.max(time) | rank | term
------------------+------+------------------------
       1633375273 |   36 | developing an app cost

(1 rows)
</code></pre>
<p>Now, let's see what happens if we include the <code>GROUP BY term</code> clause to the same exact <code>SELECT</code> statement:</p>
<pre><code>SELECT MAX(time), rank, term FROM simple_search WHERE domain = 'zerotoappstore.com' GROUP BY term;
 system.max(time) | rank | term
------------------+------+------------------------
       1633375273 |   36 | developing an app cost
       1633332881 |  119 |               best ide
       1633297772 |  105 |     avfoundation swift

(3 rows)
</code></pre>
<p>What if we remove the <code>MAX</code> aggregate function on <code>time</code> column because we've the data already stored in the descending order for <code>time</code> column? We get the following:</p>
<pre><code>SELECT time,rank,term FROM simple_search WHERE domain = 'zerotoappstore.com' GROUP BY term;

 time       | rank | term
------------+------+------------------------
 1633375273 |   36 | developing an app cost
 1633332881 |  119 |               best ide
 1633297772 |  105 |     avfoundation swift

(3 rows)
</code></pre>
<p>Is this what you want as your result? Please also see <a href=""https://cassandra.apache.org/doc/4.0/cassandra/cql/dml.html#group-by-clause"" rel=""nofollow noreferrer"">the corresponding documentation</a> for certain conditions as it is laid out.</p>
",['table']
69809656,69823867,2021-11-02 11:28:32,Why do year and month functions result in long overflow in Spark?,"<p>I'm trying to make year and month columns from a column named logtimestamp (of type TimeStampType) in spark. The data source is cassandra. I am using sparkshell to perform these steps, here is the code I have written -</p>
<pre class=""lang-scala prettyprint-override""><code>import org.apache.spark.sql.cassandra._
import org.apache.spark.sql.types._
var logsDF = spark.read.cassandraFormat(&quot;tableName&quot;, &quot;cw&quot;).load()
var newlogs = logsDF.withColumn(&quot;year&quot;, year(col(&quot;logtimestamp&quot;)))
 .withColumn(&quot;month&quot;, month(col(&quot;logtimestamp&quot;)))
newlogs.write.cassandraFormat(&quot;tableName_v2&quot;, &quot;cw&quot;)
 .mode(&quot;Append&quot;).save()
</code></pre>
<p>But these steps do not succeed, I end up with the following error</p>
<pre><code>java.lang.ArithmeticException: long overflow
    at java.lang.Math.multiplyExact(Math.java:892)
    at org.apache.spark.sql.catalyst.util.DateTimeUtils$.millisToMicros(DateTimeUtils.scala:205)
    at org.apache.spark.sql.catalyst.util.DateTimeUtils$.fromJavaTimestamp(DateTimeUtils.scala:166)
    at org.apache.spark.sql.catalyst.CatalystTypeConverters$TimestampConverter$.toCatalystImpl(CatalystTypeConverters.scala:327)
    at org.apache.spark.sql.catalyst.CatalystTypeConverters$TimestampConverter$.toCatalystImpl(CatalystTypeConverters.scala:325)
    at org.apache.spark.sql.catalyst.CatalystTypeConverters$CatalystTypeConverter.toCatalyst(CatalystTypeConverters.scala:107)
    at org.apache.spark.sql.catalyst.CatalystTypeConverters$StructConverter.toCatalystImpl(CatalystTypeConverters.scala:252)
    at org.apache.spark.sql.catalyst.CatalystTypeConverters$StructConverter.toCatalystImpl(CatalystTypeConverters.scala:242)
    at org.apache.spark.sql.catalyst.CatalystTypeConverters$CatalystTypeConverter.toCatalyst(CatalystTypeConverters.scala:107)
    at org.apache.spark.sql.catalyst.CatalystTypeConverters$.$anonfun$createToCatalystConverter$2(CatalystTypeConverters.scala:426)
    at com.datastax.spark.connector.datasource.UnsafeRowReader.read(UnsafeRowReaderFactory.scala:34)
    at com.datastax.spark.connector.datasource.UnsafeRowReader.read(UnsafeRowReaderFactory.scala:21)
    at com.datastax.spark.connector.datasource.CassandraPartitionReaderBase.$anonfun$getIterator$2(CassandraScanPartitionReaderFactory.scala:110)
    at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)
    at scala.collection.Iterator$$anon$11.next(Iterator.scala:494)
    at com.datastax.spark.connector.datasource.CassandraPartitionReaderBase.next(CassandraScanPartitionReaderFactory.scala:66)
    at org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:79)
    at org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:112)
    at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
    at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
    at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)
    at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:413)
    at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1473)
    at org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:452)
    at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:360)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
    at org.apache.spark.scheduler.Task.run(Task.scala:131)
    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
</code></pre>
<p>I thought it was something to do with null values in the table so I ran the following</p>
<pre><code>scala&gt; logsDF.filter(&quot;logtimestamp is null&quot;).show()
</code></pre>
<p>But this too gave the same long overflow error.</p>
<p>How come there is an overflow in spark but not in cassandra when both have timestamps of 8 bytes?
What could be the issue here and how do I extract year and month from timestamp correctly?</p>
",<scala><apache-spark><cassandra><spark-cassandra-connector>,"<p>Turns out one of the cassandra table had a timestamp value that was greater than the highest value allowed by spark but not large enough to overflow in cassandra. The timestamp had been manually edited to get around the upserting that is done by default in cassandra, but this led to some large values being formed during development.
Ran a python script to find this out.</p>
",['table']
70043902,70150677,2021-11-20 07:02:06,Insert table Mutation to different Cassandra table,"<p>I have requirement to keep the old values of a row in a history table for auditing whenever we do row update. Is there any solution available in Apache Cassandra to achieve this?
I looked at the Trigger and not much mentioned in the docs. Not sure of performance issues if we use the triggers. Also if we use trigger, will it give the old value for a column when we do update?</p>
",<cassandra><cassandra-3.0>,"<p>Cassandra is best tool to keep the row history. I will try to explain it with an example. Consider the below table design -</p>
<pre><code>CREATE TABLE user_by_id (
userId text,
timestamp timestamp,
name text,
fullname text,
email text,
PRIMARY KEY (userId,timestamp)
) WITH CLUSTERING ORDER BY (timestamp DESC);
</code></pre>
<p>With this kind of table design you can keep the history of the record.
Here, userid is row partition key and timestamp as clustering key. Every insert for same user will be recorded as different row. for example  -</p>
<pre><code>insert into user_by_id (userId,timestamp ,name, fullname, email ) values ('1',&lt;newTimeStamp&gt;,'x',xyz,'x@xyz.com');
insert into user_by_id (userId,timestamp ,name, fullname, email ) values ('1',&lt;newTimeStamp&gt;,'y',xyz,'y@xyz.com');
insert into user_by_id (userId,timestamp ,name, fullname, email ) values ('1',&lt;newTimeStamp&gt;,'z',xyz,'z@xyz.com');
</code></pre>
<p>Above insert statements are actually updating values of the name and email column. But, this will be saved in three different rows because of timestamp as a clustering key, timestamp will be different for each row. If you want to get the latest value, just use LIMIT in your select query.</p>
<p>This design keeps the history of the row which can be used foe audit purpose.</p>
",['table']
70153180,70187678,2021-11-29 10:36:00,ScyllaDB count(*) return difference result,"<p>I have a question about query in scylladb. I want to count the rows in a table with:</p>
<pre><code>SELECT COUNT(*) 
FROM tabledata;
</code></pre>
<ul>
<li>First run returns a result of 5732 rows</li>
<li>Second run returns a result of 5432 rows</li>
</ul>
<p>Always different result.</p>
<p>Any suggestions on how to count rows in scylla?</p>
",<cassandra><scylla>,"<p>Consistency level?</p>
<p>(you can find on internet a very funny picture about eventual consistency)</p>
<p>IF you have RF=3</p>
<p>If you wrote all your rows with LOCAL_QUORUM</p>
<p>then I'd set CONSISTENCY LOCAL_QUORUM</p>
<p>and rerun the count</p>
<p>if you are not sure whether all your writes were properly done, use CL ALL</p>
<p>another option is to run a full repair and rerun the count</p>
<p>ALSO your table might have TTL, in such case having a different count every time is expected (and if you wrote it might be bigger, if you just read, then it will be smaller)</p>
<p>For efficient count look at <a href=""https://github.com/scylladb/scylla-code-samples/tree/master/efficient_full_table_scan_example_code"" rel=""nofollow noreferrer"">https://github.com/scylladb/scylla-code-samples/tree/master/efficient_full_table_scan_example_code</a> - but the same applies re consistency level (and of course this script will tell you with a timeout error that a token range couldn't queried and it means that node/shard was overloaded with other traffic, by default it doesn't retry, it's a simple script)</p>
",['table']
70254326,70273342,2021-12-07 02:44:36,How do I select all rows from two clustering columns in cassandra database,"<p>I have a Partion key: A</p>
<p>Clustering columns: B, C</p>
<p>I do understand I can query like this</p>
<pre><code>    Select * from table where A = ?
    Select * from table where A = ? and B = ?
    Select * from table where A = ? and B = ? and C = ?
</code></pre>
<p>Now I have a scenario where I need to fetch results from only B and C. Is there a way to this with out using Allow Filtering.</p>
",<cassandra><cassandra-3.0><spring-data-cassandra>,"<p>You cannot fetch on basis of 'B' and 'C' (<code>the clustering columns</code>) without partition key without using <code>Allow Filtering</code>. Though you can use spark and spark-cassandra-connector for filtering out the results on basis of 'B' and 'C'. Behind the scene it also used allow filtering but it has efficient mechanism to scan the table the right way.</p>
",['table']
70498083,70520734,2021-12-27 16:36:43,How can you query all the data with an empty set in cassandra db?,"<p>As the title says, I'm trying to query all the data I got with no value stored in it. I've been searching for a while, and the only operation allowed that I've found is <code>CONTAINS</code>, which doesn't fit my need.</p>
<p>consider the following table:</p>
<pre><code>CREATE TABLE environment(
  id uuid,
  name varchar,
  message text,
  public Boolean,
  participants set&lt;varchar&gt;,
  PRIMARY KEY (id)
)
</code></pre>
<p>How can I get all entries in the table with an empty set? E.g. participants = {} or null?</p>
",<cassandra><cql>,"<p>Unfortunately, you really can't.  Cassandra makes queries like this difficult by design, because there's no way it can be done without doing a full table scan (scanning each and every node).  This is why a big part of Cassandra data modeling is understanding all the ways that table will be queried, and building it to support those queries.</p>
<p>The other issue that you'll have to deal with, is that (generally speaking) Cassandra does not allow filtering by <code>null</code>s.  Again, it's a design choice...it's much easier to query for data that exists, rather than data that does not exist.  Although, when writing with <a href=""https://docs.datastax.com/en/cql-oss/3.3/cql/cql_using/useInsertLWT.html"" rel=""nofollow noreferrer"">lightweight transactions</a>, there are ways around this one (using the <code>IF</code> clause).</p>
<p>If you knew all of the <code>id</code>s ahead of time, you could write something to iterate through them, <code>SELECT</code> and check for null on the app-side.  Although that approach will be slow (but it won't stress the cluster).  Probably the better approach, is to use a distributed OLAP layer like <a href=""https://spark.apache.org/"" rel=""nofollow noreferrer"">Apache Spark</a>.  It still wouldn't be fast, but this is probably the best way to handle a situation like this.</p>
",['table']
70549104,70549598,2022-01-01 13:13:55,Cassandra chat table design,"<p>For my chat table design in cassandra I have the following scheme:</p>
<pre><code>USE zwoop_chat
CREATE TABLE IF NOT EXISTS public_messages (
    chatRoomId text,
    date timestamp,
    fromUserId text,
    fromUserNickName text,
    message text,
    PRIMARY KEY ((chatRoomId, fromUserId), date)
) WITH CLUSTERING ORDER BY (date ASC);
</code></pre>
<p>The following query:</p>
<pre><code>SELECT * FROM public_messages WHERE chatroomid=? LIMIT 20
</code></pre>
<p>Results in the typical message:</p>
<blockquote>
<p>Cannot execute this query as it might involve data filtering and thus
may have unpredictable performance. If you want to execute this query
despite the performance unpredictability, use ALLOW FILTERING;</p>
</blockquote>
<p>Obviously I'm doing something wrong with the partitioning here.<br />
I'm not experienced with Cassandra and a bit confused about online suggestions that Cassandra will make an entire table scan, which is something that I don't really get realistically. Why would I want to fetch an entire table.</p>
<p>Another suggestion I read about is to create partitioning, e.g. to fetch the latest per day. But this doesn't work for me. You don't know when the latest chat message occurred.<br />
Could be last day, last hour, or last week or month for that matter.</p>
<p>I'm pretty much used to sql or nosql like mongo, but this simple use case seems to be a problem for Cassandra. So what is the recommended approach here?</p>
<p>Edit:
It seems that it is common practise to add a bucket integer.<br />
Let's say I create a bucket per 50 messages, is there a way to auto-increment it when the bucket is full?<br />
I would prefer not having to do a fetch of MAX bucket and calculate when the bucket is full. Seems like bad performance for doing inserts.<br />
Also it seems like a bad idea to manage the buckets in Java. Things like app restarts or load balancing would require extra logic.</p>
<p>(I currently use Java Spring JPA for Cassandra).</p>
",<cassandra>,"<p>It works without bucketing using the following table design:</p>
<pre><code>USE zwoop_chat
CREATE TABLE IF NOT EXISTS public_messages (
   chatRoomId text,
   date timestamp,
   fromUserId text,
   fromUserNickName text,
   message text,
   PRIMARY KEY ((chatRoomId), date)
) WITH CLUSTERING ORDER BY (date DESC);
</code></pre>
<p>I had to remove the fromUserId from the partition key, I assume it is required to include it in the where clause to avoid the error.</p>
<p>The jpa query:</p>
<pre><code>publicMessageRepository.findFirst20ByPkChatRoomIdOrderByPkDateDesc(chatRoomId);
</code></pre>
",['table']
70619281,70619327,2022-01-07 09:39:53,How to copy datas from one table to another table with different Schemas? (Cassandra),"<p>I have a table named table1 which includes the following</p>
<pre><code>    +----------+-------+
    |date      |count  |
    +----------+-------+
    |2022-01-07|2      |
    |2022-01-06|0      |
    |2022-01-05|1      |
    +----------+-------+
</code></pre>
<p>Now I need to copy this table(table1) and paste this into a new table(table2) with a different schema. The new table should look like this</p>
<pre><code>    +----+----------+-------+
    |type|date      |count  |
    +----+----------+-------+
    |Typ1|2022-01-07|2      |
    |Typ1|2022-01-06|0      |
    |Typ1|2022-01-05|1      |
    +----+----------+-------+
</code></pre>
<p>Now the problems are:</p>
<ol>
<li>I cannot use cqlsh <strong>COPY</strong> command as the scheme of both the tables is different.</li>
<li>I cannot manually add the data to table2 because the table1 has 1000s of rows</li>
</ol>
<p>The schema of the tables are:</p>
<p>Table1:</p>
<pre><code>CREATE TABLE table1(
    date date PRIMARY KEY,
    count bigint
);
</code></pre>
<p>Table2:</p>
<pre><code>CREATE TABLE table2(
    type text,
    date date ,
    count bigint,
    PRIMARY KEY(type, date)
);
</code></pre>
",<cassandra><cqlsh>,"<p>You want to populate data of one table into another table. You can write a utility to do this. This utility will read your first table and push data into another table. If you can use spark, then you can do it pretty fast.</p>
",['table']
70676754,70677492,2022-01-12 05:32:40,cassandra node takes long to join the cluster,"<p>There are 6 nodes in the cluster of a single DC, there are only few tables in the cluster but one of the is quite big (~150gb) having a sasi index. Now when I am adding a new nodes (have to add another 6) to the cluster, the bootstrapping is taking quite long (about an hour or so). From the looks of it, until it completes bootstrap the status of the new node shows up as UJ and then becomes UN once done - is that how it works?</p>
<p>In the first 10-15mins do see streaming happening (via nodetool netstats), after which the sasi index creation gets kicked off on this huge table (checked via nodetool compactionstats). This part takes quite long to complete (~45mins). Is there any way to speed up the bootstrapping? Could sasi index creation be deferred (say by disabling compaction)? If it can be, guess nodetool rebuild_index needs to be used to rebuild the index? If it cannot be deferred, then what are the ways to speed up the index creation/bootstrapping process?</p>
",<cassandra>,"<blockquote>
<p>There are 6 nodes in the cluster of a single DC, there are only few tables in the cluster but one of the is quite big (~150gb) having a sasi index. Now when I am adding a new nodes (have to add another 6) to the cluster, the bootstrapping is taking quite long (about an hour or so). From the looks of it, until it completes bootstrap the status of the new node shows up as UJ and then becomes UN once done - is that how it works?</p>
</blockquote>
<p>Yes a new node bootstraps this way. UJ means that the node is getting data from other nodes and is not taking any client traffic. One it reaches UN state it starts taking traffic also.</p>
<blockquote>
<p>In the first 10-15mins do see streaming happening (via nodetool netstats), after which the sasi index creation gets kicked off on this huge table (checked via nodetool compactionstats). This part takes quite long to complete (~45mins). Is there any way to speed up the bootstrapping? Could sasi index creation be deferred (say by disabling compaction)? If it can be, guess nodetool rebuild_index needs to be used to rebuild the index? If it cannot be deferred, then what are the ways to speed up the index creation/bootstrapping process?</p>
</blockquote>
<p>You should not disable SASI index creation. You can enhance bootstrap speed by</p>
<ol>
<li>Increasing <code>streamingthourhput</code> on all the nodes. You can do using it  `nodetool setcompactionthroughput'</li>
<li>You can increase `concurrent compactors' on new node.</li>
</ol>
<p>1 hour for a joining node does not seem big as such.</p>
",['table']
70677109,70677366,2022-01-12 06:16:29,Can Cassandra or ScyllaDB give incomplete data while reading with PySpark if either clusters are left un-repaired forever?,"<p>I use both Cassandra and ScyllaDB 3-node clusters and use PySpark to read data. I was wondering if any of them are not repaired forever, is there any challenge while reading data from either if there are inconsistencies in nodes. Will the correct data be read and if yes, then why do we need to repair them?</p>
",<pyspark><cassandra><scylla>,"<p>Yes you can get incorrect data if reapir is not done. It also depends on with what consistency you are reading or writing. Generally in production systems writes are done with (Local_one/Local_quorum) and read with Local_quorum.</p>
<p>If you are writing with weak consistency level, then repair becomes important as some of the nodes might not have got the mutations and while reading those nodes may get selected.</p>
<p>For example if you write with consistency level <code>ONE</code> on a table <code>TABLE1</code> with a replication of 3. Now it may happen your write was written to <code>NodeA</code> only and <code>NodeB</code> and <code>NodeC</code> might have missed the mutation. Now if you are reading with Consistency level <code>LOCAL_QUORUM</code>, it may happen that <code>NodeB</code> and 'NodeC' get selected and they do not return the written data.</p>
<p>Repair is an important maintenance task for Cassandra which should be done periodically and continuously to keep data in healthy state.</p>
",['table']
70681469,70682792,2022-01-12 12:20:27,"How to apply ""not equal to"" ( != ) operator on timeuuid column on cassandra query?","<p>I have a column in cassandra table which contains timeuuid values. I want to exclude a row with a timeuuid value in a search query. if I apply '!=', I get the following error</p>
<p>Query:</p>
<p><code> select * from X where id != '0651de16-fa62-11eb-8318-dc4a3e6d5697 ' allow filtering ;</code></p>
<p>Error:</p>
<p><code>InvalidRequest: Error from server: code=2200 [Invalid query] message=&quot;Unsupported &quot;!=&quot; relation: id != '0651de16-fa62-11eb-8318-dc4a3e6d5697</code></p>
<p>I got that I can't apply '!=' on timeuuid values. In that case how to exclude a certain timeuuid value while searching?</p>
",<cassandra><cql>,"<blockquote>
<p>I got that I can't apply '!=' on timeuuid values.</p>
</blockquote>
<p>It's not just for timeUUIDs. The not equals operator (<code>!=</code>) does not exist in CQL.</p>
<p><strong>Searching vs. Querying</strong></p>
<blockquote>
<p>I want to exclude a row with a timeuuid value in a search query.</p>
</blockquote>
<p>First of all, searching and querying are <em>not</em> the same thing.  <em>Querying</em> involves pulling back a result set of a known size with specific criteria.  On the other hand, <em>searching</em> returns a result set of an unknown size using very liberal criteria.</p>
<p>It's important to understand this difference, because Cassandra is built to support querying.  And very specific querying, at that.  Unfortunately this use case requires Cassandra to perform <em>random</em> reads (instead of sequential reads).  The reality is that Cassandra was not designed to support random read patterns.</p>
<p>Due to Cassandra's distributed nature, it also doesn't do well with full table scans.  Essentially, query time becomes network time (for each partition in the result set) and that's <em>never</em> going to be fast.</p>
<p>This is why the not equals operator does not exist in CQL.  Because there's no way that Cassandra can isolate a specific partition or node with &quot;not equals&quot; criteria.</p>
<p>That being said, there are some solutions available:</p>
<ul>
<li>A distributed analytics layer like Apache Spark can run that query.</li>
<li>If it's just one or a few timeUUIDs which need to be excluded, then perhaps that's something which could be done in the application layer.</li>
<li>For a pure Cassandra solution, build your query to pull back the <code>id</code>s that you actually want.  That's much easier for Cassandra to do, as opposed to telling Cassandra what you <em>don't</em> want.</li>
</ul>
<p>Note: The <code>ALLOW FILTERING</code> directive should <em>never</em> be used on a production system.  The idea with Cassandra is to build tables to support intended, known queries.  Sometimes data may need to be duplicated into one or more &quot;query tables,&quot; and that's ok.</p>
",['table']
70683229,70704564,2022-01-12 14:23:19,"Geomesa: is there a way to create, delete indexes in geomesa without loosing data? Or I need to recreate a schema?","<p>I have a cassandra with geomesa, in there I have next schema</p>
<pre><code> ~  bin/geomesa-cassandra_2.11-3.3.0/bin/geomesa-cassandra describe-schema -P localhost:9042 -u cassandra -p cassandra -k geomesa -c gsm_events -f SignalBuilder 
INFO  Describing attributes of feature 'SignalBuilder'
geo           | Point   (Spatio-temporally indexed)
time          | Date    (Spatio-temporally indexed) (Attribute indexed)
cam           | String  (Attribute indexed) (Attribute indexed)
imei          | String  (Attribute indexed)
dir           | Double  
alt           | Double  
vlc           | Double  
sl            | Integer 
ds            | Integer 
dir_y         | Double  
poi_azimuth_x | Double  
poi_azimuth_y | Double  

User data:
  geomesa.attr.splits     | 4
  geomesa.feature.expiry  | time(30 days)
  geomesa.index.dtg       | time
  geomesa.indices         | z3:7:3:geo:time,attr:8:3:time,attr:8:3:cam,attr:8:3:cam:time,attr:8:3:imei
  geomesa.stats.enable    | true
  geomesa.table.partition | time
  geomesa.z.splits        | 4
  geomesa.z3.interval     | week
</code></pre>
<p>Is there a way to to create <code>z2</code> index additional to <code>z3</code>, and delete <code>cam</code> attribute index remain only <code>cam:time</code> attribute index without loosing data in db? Is <code>time</code> attribute index is unnecessary, if I already have <code>cam:time</code> index?</p>
<p>P.S. Why is this query using z2 index, not z3?</p>
<pre><code>~  bin/geomesa-cassandra_2.11-3.3.0/bin/geomesa-cassandra explain -P 10.200.217.24:9042 -u cassandra -p cassandra -k geomesa -c gsm_events -f SignalBuilder -q &quot;Bbox(geo,1,1,2,2) and time &gt; 123333&quot;; 
Planning 'SignalBuilder' BBOX(geo, 1.0,1.0,2.0,2.0) AND time &gt; 1970-01-01T00:02:03.333+00:00
  Original filter: BBOX(geo, 1.0,1.0,2.0,2.0) AND time &gt; 123333
  Hints: bin[false] arrow[false] density[false] stats[false] sampling[none]
  Sort: none
  Transforms: none
  Max features: none
  Strategy selection:
    Query processing took 21ms for 1 options
    Filter plan: FilterPlan[Z2Index(geo)[BBOX(geo, 1.0,1.0,2.0,2.0)][time &gt; 1970-01-01T00:02:03.333+00:00](1.2)]
    Strategy selection took 2ms for 1 options
  Strategy 1 of 1: Z2Index(geo)
    Strategy filter: Z2Index(geo)[BBOX(geo, 1.0,1.0,2.0,2.0)][time &gt; 1970-01-01T00:02:03.333+00:00](1.2)
    Geometries: FilterValues(List(POLYGON ((1 1, 2 1, 2 2, 1 2, 1 1))),true,false)
    Plan: org.locationtech.geomesa.cassandra.data.StatementPlan
      Tables: 
      Ranges (0): 
      Client-side filter: BBOX(geo, 1.0,1.0,2.0,2.0) AND time &gt; 1970-01-01T00:02:03.333+00:00
      Reduce: class:LocalTransformReducer, state:{name=SignalBuilder, tnam=, tsft=, tdef=, hint=RETURN_SFT,&quot;SignalBuilder,&quot;&quot;*geo:Point,time:Date,cam:String,imei:String,dir:Double,alt:Double,vlc:Double,sl:Integer,ds:Integer,dir_y:Double,poi_azimuth_x:Double,poi_azimuth_y:Double;geomesa.stats.enable='true',geomesa.z.splits='4',geomesa.feature.expiry='time(30 days)',geomesa.table.partition='time',geomesa.index.dtg='time',geomesa.indices='z3:7:3:geo:time,z2:5:3:geo,attr:8:3:time,attr:8:3:cam,attr:8:3:cam:time',geomesa.attr.splits='4',geomesa.z3.interval='week'&quot;&quot;&quot;, spec=*geo:Point,time:Date,cam:String,imei:String,dir:Double,alt:Double,vlc:Double,sl:Integer,ds:Integer,dir_y:Double,poi_azimuth_x:Double,poi_azimuth_y:Double;geomesa.stats.enable='true',geomesa.z.splits='4',geomesa.feature.expiry='time(30 days)',geomesa.table.partition='time',geomesa.index.dtg='time',geomesa.indices='z3:7:3:geo:time,z2:5:3:geo,attr:8:3:time,attr:8:3:cam,attr:8:3:cam:time',geomesa.attr.splits='4',geomesa.z3.interval='week', filt=BBOX(geo, 1.0,1.0,2.0,2.0) AND time &gt; 1970-01-01T00:02:03.333+00:00}
    Plan creation took 110ms
  Query planning took 294ms
</code></pre>
",<cassandra><geomesa>,"<p>This will involve several steps. You probably want to back up your data before attempting this, in case something goes wrong. First, you want to use <code>updateSchema</code> to add the new index and remove the old one, and set the existing indices to &quot;read only&quot; mode. You can use the GeoMesa CLI <code>scala-console</code> command to run the following:</p>
<pre><code>val sft = SimpleFeatureTypes.mutable(ds.getSchema(&quot;SignalBuilder&quot;))
// 5 is the latest version of the z2 index as of now
// 1 sets the indices to read only mode
sft.getUserData.put(&quot;geomesa.indices&quot;, &quot;z3:7:1:geo:time,z2:5:3:geo,attr:8:1:time,attr:8:1:cam:time,attr:8:1:imei&quot;)
ds.updateSchema(sft.getTypeName, sft)
</code></pre>
<p>After this, you will need to re-ingest your data to populate the z2 index. Once the index is populated, you'll need to update the schema again to set the indices back to read/write mode:</p>
<pre><code>val sft = SimpleFeatureTypes.mutable(ds.getSchema(&quot;SignalBuilder&quot;))
// 5 is the latest version of the z2 index as of now
// 3 sets the indices to read/write mode
sft.getUserData.put(&quot;geomesa.indices&quot;, &quot;z3:7:3:geo:time,z2:5:3:geo,attr:8:3:time,attr:8:3:cam:time,attr:8:3:imei&quot;)
ds.updateSchema(sft.getTypeName, sft)
</code></pre>
<p>Note that the old <code>cam</code> index table will still be around in Cassandra, but won't receive any further updates or be used for queries. You can drop it using standard Cassandra techniques.</p>
",['table']
70725156,70725386,2022-01-15 20:26:35,Cassandra duplicate tables for different partition keys?,"<p>I have the following table, called inbox_items:</p>
<pre><code>USE zwoop_chat
CREATE TABLE IF NOT EXISTS inbox_items (
    postId text,
    userId text,
    partnerId text,
    fromUserId text,
    fromNickName text,
    fromAvatar text,
    toUserId text,
    toNickName text,
    toAvatar text,
    unread int static,
    lastMessage text,
    lastMessageDate timestamp,
    PRIMARY KEY ((postId, userId), lastMessageDate)
) WITH CLUSTERING ORDER BY (lastMessageDate DESC);
</code></pre>
<p>The problem with this table is that I want to query it, both by postId and userId, as well as by userId only.</p>
<p>In other words, I have an inbox per post, but I have an inbox per user as well.</p>
<p>Afaik there is no good way to achieve this because:</p>
<ol>
<li>The partition key(s) uniquely determine the node where the data is stored. I.e. all partition keys corresponding the where clause should be present.</li>
<li>Secondary index is no good fit for keys with high cardinality (in this case, postId has high cardinality)</li>
</ol>
<p>The solution I currently see is to duplicate the table with different keys.<br />
This feels like such an overkill though.</p>
<p>Is there a better solution I'm missing?</p>
",<cassandra>,"<p>Assuming partitioning by userid alone would not generate partitions that are too large, you partition by userid, and have postid in the clustering key. You specified that you would query by :</p>
<blockquote>
<p>The problem with this table is that I want to query it, both by postId and userId, as well as by userId only.</p>
</blockquote>
<p>So in this instance, you do not need postid within the partition key, but within the clustering key. The only issue is if you intend to query by postid alone as well - but that was not mentioned.</p>
<p>If the partition by userid will result in partitions that are too large, there is additional bucketing techniques available.</p>
",['table']
70760173,70761055,2022-01-18 17:49:26,How to purge massive data from cassandra when node size is limited,"<p>I have a Cassandra cluster (with cassandra v3.11.11) with 3 data centers with replication factor 3. Each node has 800GB nvme but one of the data table is taking up 600GB of data. This results in below from <code>nodetool status</code>:</p>
<pre><code>DataCenter 1:
 node 1: 620GB (used) / 800GB (total)
DataCenter 2:
 node 1: 610GB (used) / 800GB (total)
DataCenter 3:
 node 1: 680GB (used) / 800GB (total)
</code></pre>
<p>I cannot install another disk to the server and the server does not have internet connection at all for security reasons. The only thing I can do is to put some small files like scripts into it.</p>
<p>ALL cassandra tables are set up with <code>SizeTieredCompactionStrategy</code> so I end up with very big files ~one single 200GB sstable (full table size 600). Since the cluster is running out of space, I need to free the data which will introduce tombstones. But I have only 120GB of free space left for compaction or garbage collection. That means I can only retain at most 120GB data, and to be safe for the Cassandra process, maybe even less.</p>
<p>I am executed <code>nodetool cleanup</code> so I am not sure if there is more room to free.</p>
<p>Is there any way to free 200GB of data from Cassandra? or I can only retain less than 120 GB data?</p>
<p>(if we remove 200GB data, we have 400GB data left, compaction/gc will need 400+680GB data which is bigger than disk space..)</p>
<p>Thank you!</p>
",<cassandra>,"<p>I personally would start with checking if the whole space is occupied by actual data, and not by snapshots - use <a href=""https://cassandra.apache.org/doc/3.11/cassandra/tools/nodetool/listsnapshots.html"" rel=""nofollow noreferrer"">nodetool listsnapshots</a> to list them, and <a href=""https://cassandra.apache.org/doc/3.11/cassandra/tools/nodetool/clearsnapshot.html"" rel=""nofollow noreferrer"">nodetool clearsnapshot</a> to remove them. Because if you did snapshot for some reason, then after compaction they are occupying the space as original files were removed.</p>
<p>The next step would be to try to cleanup tombstones &amp; deteled data from the small tables using <a href=""https://cassandra.apache.org/doc/3.11/cassandra/tools/nodetool/garbagecollect.html"" rel=""nofollow noreferrer"">nodetool garbagecollect</a>, or <code>nodetool compact</code> with <code>-s</code> option to split table into files of different size.  For big table I would try to use <a href=""https://cassandra.apache.org/doc/3.11/cassandra/tools/sstable/sstablesplit.html"" rel=""nofollow noreferrer"">nodetool compact</a> with <code>--user-defined</code> option on the individual files (assuming that there will be enough space for them).  As soon as you free &gt; 200Gb, you can <a href=""https://cassandra.apache.org/doc/3.11/cassandra/tools/sstable/sstablesplit.html"" rel=""nofollow noreferrer"">sstablesplit</a> (<strong>node should be down!</strong>) to split big SSTable into small files (~1-2Gb), so when node starts again, the data will be compacted</p>
",['table']
70771696,70773202,2022-01-19 13:43:11,How should I partition metering data in Cassandra?,"<p>I have built an application receiving metering data (for example, the current temperature of a room) from multiple devices (in this example, multiple rooms).</p>
<p>I receive metering data every 15 minutes. My application calculates the difference between the current temperature and the previous one received and sends it to another application. I store the received metering data in a Cassandra cluster. (timestamp, temperature, device_id, room, ...)</p>
<p><strong>Which field should I use for partitioning?</strong></p>
<p>If I use the timestamp as the partition key will it put all load on the same node? (without regarding replication)?</p>
<p>If I use the device_id/room, won't I get an unbounded partition? Maybe I could add a retention period?</p>
",<cassandra>,"<p>Rule for Cassandra data modeling is design your tables based on your queries. So prepare your queries first. For example if you have queries like</p>
<ol>
<li>Get readings for room.</li>
<li>Get reading for device.</li>
</ol>
<p>You can have two tables</p>
<ul>
<li>READING_BY_ROOM (parition key room id)</li>
<li>READING_BY_DEVICE (partition key device id)</li>
</ul>
<p>This is the only way you design tables in Cassandra. Dont try to create table RDBMS way.</p>
",['table']
70945161,70945276,2022-02-01 17:46:39,How to create Cassandra primary key in correct way,"<p>I have the following table structure:</p>
<pre><code>CREATE TABLE test_keyspace.persons (
    id uuid,
    country text,
    city text,
    address text,
    phone_number text,
    PRIMARY KEY (id, country, address)
);
</code></pre>
<p>My main scenario is to get person by id. But sometimes I want to get all cities inside country and all persons inside city as well.</p>
<p>I know that Cassandra must have at least one partition key and zero or more clustering keys, but I don't understand how to organize it to work most effectively (and generally work).</p>
<p>Can anybody give me advice?</p>
",<cassandra><nosql>,"<p>So it sounds like you want to be able to query by both <code>id</code> and <code>country</code>.  Typically in Cassandra, the way to build your data models is a &quot;one table == one query&quot; approach.  In that case, you would have two tables, just keyed differently:</p>
<pre><code>CREATE TABLE test_keyspace.persons_by_id (
    id uuid,
    country text,
    city text,
    address text,
    phone_number text,
    PRIMARY KEY (id));
</code></pre>
<p>TBH, you don't really to cluster on <code>country</code> and <code>address</code>, unless a person can have multiple addresses.  But a single PK is a completely legit approach.</p>
<p>For the second table:</p>
<pre><code>CREATE TABLE test_keyspace.persons_by_country (
    id uuid,
    country text,
    city text,
    address text,
    phone_number text,
    PRIMARY KEY (country,city,id));
</code></pre>
<p>This will allow you to query by <code>country</code>, with persons grouped/sorted by <code>city</code> and sorted by <code>id</code>.  In theory, you could also serve the query by <code>id</code> approach here, as long as you also had the <code>country</code> and <code>city</code>.  But that might not be possible in your scenario.</p>
<p>Duplicating data in Cassandra (NoSQL) to help queries perform better is ok.  The trick becomes keeping the tables in-sync, but you can use the <code>BATCH</code> functionality to apply writes to both tables atomically.</p>
<p>In case you haven't already, you might benefit from DataStax's (free) course on data modeling - <a href=""https://academy.datastax.com/#/online-courses/ca2e1209-510b-44a6-97de-d5219d835319"" rel=""nofollow noreferrer"">Data Modeling with Apache Cassandra and DataStax Enterprise</a>.</p>
",['table']
70962666,70981974,2022-02-02 20:48:07,Live location cassandra partition key strategy,"<p>I was watching a <a href=""https://www.youtube.com/watch?v=yQ0A7vaMJt0"" rel=""nofollow noreferrer"">talk</a> about Uber's live location storage using Cassandra and was curious about the partition key. My original line of thinking would be to have the following fields:</p>
<ul>
<li>ride_id</li>
<li>driver_id</li>
<li>timestamp</li>
<li>latitude</li>
<li>longitude</li>
</ul>
<p>For the partition key I was between the following:</p>
<ul>
<li>Composite primary key (ride_id, driver_id)</li>
<li>Primary key (ride_id)</li>
<li>Primary key (driver_id)</li>
</ul>
<p>When querying I would want to query for the location data for a given trip and potentially the location data for a given driver. Would it make sense to create a composite key? I would want each node to have ~100k rows. Could I also have two separate tables of duplicate data but different indexing so I can query depending on the index?</p>
<p>In the Uber talk, they mentioned they used the uuid (I am assuming related to the driver or ride) and minute offset of the timestamp as the partition key. Is that a better approach?</p>
",<database><cassandra><partition>,"<p>In Cassandra data modelling, the prime objective is to design a table for each app query. Another way of putting it is that tables and app queries have a one-to-one relationship: one app query maps to one table. If there are 10 app queries, you need to design 10 tables.</p>
<p><strong>[EDIT]</strong> - Updated my answer after getting additional info in the comments.</p>
<p>For this app query:</p>
<blockquote>
<p>I would want to query for the location data for a given trip</p>
</blockquote>
<p>you want the table to be partitioned by the trip so it would look like:</p>
<pre><code>CREATE TABLE location_by_trip (
    trip_id text,
    trip_timestamp timestamp,
    latitude float,
    longitude float,
    driver text,
    passenger text,
    ...
    PRIMARY KEY (trip_id, trip_timestamp)
)
</code></pre>
<p>And you would retrieve the location at a specific time with:</p>
<pre><code>SELECT latitude, longitude
FROM location_by_trip 
WHERE trip_id = ?
  AND trip_timestamp = ?
</code></pre>
<p>Then for the second app query:</p>
<blockquote>
<p>... the location data for a given driver</p>
</blockquote>
<p>The table schema would look almost identical except the table is partitioned by driver:</p>
<pre><code>CREATE TABLE location_by_driver (
    driver text,
    trip_timestamp timestamp,
    latitude float,
    longitude float,
    trip_id text,
    passenger text,
    ...
    PRIMARY KEY (driver, trip_timestamp)
)
</code></pre>
<p>and you would query the table with the driver as the filter in the WHERE clause:</p>
<pre><code>SELECT latitude, longitude
FROM location_by_driver
WHERE driver = ?
  AND trip_timestamp = ?
</code></pre>
<p>IDs can be UUIDs if you choose but that's all up to you. But just remember that you don't need to create artificial IDs to use as partition keys because it's always best to use &quot;natural keys&quot;. Example of natural keys are email addresses, URLs, fully-qualified phone numbers (includes country + area code).</p>
<p>You will only need to use composite partition keys if you need multiple columns to make the partition key unique. For example, movies can share the same titles so we generally recommend adding the release year to make it unique. If you're interested, I've explained it in a bit more detail with examples in this post -- <a href=""https://community.datastax.com/questions/6171/"" rel=""nofollow noreferrer"">https://community.datastax.com/questions/6171/</a>.</p>
<p>If you're new to Cassandra, have a look at <a href=""https://www.datastax.com/dev"" rel=""nofollow noreferrer"">datastax.com/dev</a>. It has lots of free hands-on tutorials that allows you to learn key concepts very quickly since each tutorial only lasts a few minutes.</p>
<p>The <a href=""https://www.datastax.com/learn/cassandra-fundamentals"" rel=""nofollow noreferrer"">Cassandra Fundamentals</a> course is a good place to start. The <a href=""https://www.datastax.com/dev/modeling"" rel=""nofollow noreferrer"">Data Modeling</a> tutorial is also a good one for you. The free tutorials are interactive and run inside your browser so there's nothing to install or configure. Cheers!</p>
",['table']
71112419,71325574,2022-02-14 13:08:06,CodecNotFoundException while writing to Amazon Keyspaces,"<p>I am trying to write a Spark DF to AWS Keyspaces.
Randomly some of the records are getting updated and some of the records are throwing this exception</p>
<pre><code>com.datastax.oss.driver.api.core.type.codec.CodecNotFoundException: Codec not found for requested operation: [INT &lt;-&gt; java.lang.String]
at com.datastax.oss.driver.internal.core.type.codec.registry.CachingCodecRegistry.createCodec(CachingCodecRegistry.java:609)
at com.datastax.oss.driver.internal.core.type.codec.registry.DefaultCodecRegistry$1.load(DefaultCodecRegistry.java:95)
at com.datastax.oss.driver.internal.core.type.codec.registry.DefaultCodecRegistry$1.load(DefaultCodecRegistry.java:92)
at com.datastax.oss.driver.shaded.guava.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527)
at com.datastax.oss.driver.shaded.guava.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2276)
at com.datastax.oss.driver.shaded.guava.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2154)
at com.datastax.oss.driver.shaded.guava.common.cache.LocalCache$Segment.get(LocalCache.java:2044)
at com.datastax.oss.driver.shaded.guava.common.cache.LocalCache.get(LocalCache.java:3951)
at com.datastax.oss.driver.shaded.guava.common.cache.LocalCache.getOrLoad(LocalCache.java:3973)
at com.datastax.oss.driver.shaded.guava.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4957)
at com.datastax.oss.driver.shaded.guava.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4963)
at com.datastax.oss.driver.internal.core.type.codec.registry.DefaultCodecRegistry.getCachedCodec(DefaultCodecRegistry.java:117)
at com.datastax.oss.driver.internal.core.type.codec.registry.CachingCodecRegistry.codecFor(CachingCodecRegistry.java:258)
at com.datastax.oss.driver.internal.core.data.ValuesHelper.encodePreparedValues(ValuesHelper.java:112)
at com.datastax.oss.driver.internal.core.cql.DefaultPreparedStatement.bind(DefaultPreparedStatement.java:158)
</code></pre>
<p>My Keyspace table schema is</p>
<pre><code>CREATE TABLE test_ks.test_table_ttl (
    consumer_id TEXT PRIMARY KEY,
    ttl_col map&lt;text, frozen&lt;tuple&lt;text, text&gt;&gt;&gt;
);
</code></pre>
<p>The codeblock which is throwing error is this.</p>
<pre><code>val rowKey =   // some string
val mapKey =   // some string
val mapValue = mapValueTupleType.newValue(tuple_value)
val mapData = ImmutableMap.builder().put(mapKey, mapValue).build()
batch.addStatement(prep_statement.bind(mapData, rowKey)) // &lt;--- error on this line
</code></pre>
",<scala><cassandra><datastax-java-driver><spark-cassandra-connector><amazon-keyspaces>,"<p>Currently, AWS Keyspaces doesn't allow frozen type. This was a bug in Keyspaces that allowed the table to be created with frozen type, but it throws an exception during insertion.</p>
<p>The only possible way is to store data using JSON as suggested by @MikeJPR.</p>
",['table']
71206874,71393335,2022-02-21 13:08:48,Saving information to Cassandra keeps no order,"<p>I'm working with Scala and trying to save my calendar information from Spark to Cassandra.</p>
<p>I started with creating the same schema with Cassandra:</p>
<p><code>session.execute(&quot;CREATE TABLE calendar (DateNum int, Date text, YearMonthNum int, ..., PRIMARY KEY (datenum,date))&quot;)</code></p>
<p>and then imported my data from spark to Cassandra:</p>
<pre><code>        .write
        .format(&quot;org.apache.spark.sql.cassandra&quot;)
        .options(Map(&quot;table&quot; -&gt; &quot;calendar&quot;, &quot;keyspace&quot; -&gt; &quot;ks&quot;))
        .mode(SaveMode.Append)
        .save()
</code></pre>
<p>But once I try to read the data I retrieved from Spark on Cassandra, the rows appear so mixed up together, while I want to keep the same order my calendar has.</p>
<p>An example of a row I have:</p>
<p><strong>20090111 | 1/11/2009 | 200901 |...</strong></p>
<p>Select/Order don't seem to fix the problem too.</p>
",<scala><apache-spark><cassandra><spark-cassandra-connector>,"<p>An answer to this was adding a new column with a common value over all the database (E.g: &quot;1&quot;) using Spark and making that column the partition key in the Cassandra table, that way you get one partition for the whole table and your information keeps ordered.</p>
",['table']
71217911,71218247,2022-02-22 08:14:01,Cassandra data modeling with multi-column filter,"<p>I have a Cassandra table <code>stokes</code> which has 4 columns <code>( item(text) ,market ( text ) , location( text ) , time ( timestamp ) , value( int) )</code>  item is a partition key and market , location and time is a clustering key in same order .</p>
<pre><code>item | market | location | time | value
 x   |  m1    | l1       | t1   | v1
 x   |  m1    | l2       | t2   | v2
 x   |  m1    | l3       | t3   | v3
 y   |  m1    | l1       | t4   | v4
 y   |  m1    | l2       | t5   | v5
 y   |  m1    | l3       | t6   | v6
</code></pre>
<p>Application has require to query to Cassandra table in two scenario</p>
<ol>
<li>For given item , market and location fetch the record by querying to  stock table .
for example item x, market l1 and location l1 below record will be fetched</li>
</ol>
<pre><code> x   |  m1    | l1       | t1   | v1
</code></pre>
<ol start=""2"">
<li><p>For given item , market and all location after input time fetch the record by querying to  stock table . For example item x , market m1 , all location after t1 time below records will be fetched</p>
<pre><code> x   |  m1    | l2       | t2   | v2
 x   |  m1    | l3       | t3   | v3
</code></pre>
</li>
</ol>
<p>I understand in Cassandra application requirement and conceptual modeling comes first in deformalizing the data modeling . How I cam model data to meet both of my application requirement .</p>
<p>For my second requirement I am not able to query like</p>
<pre><code>select &lt;columns&gt; from &lt;table&gt; where item =x and market= m1 and time &gt; t2; // wrong as location missing 
</code></pre>
<p>Skipping location is not allowed and locations could be multiple . How to model or query to meet both requirement .</p>
",<cassandra>,"<p>For the first app query, the partition key is <code>item</code> and both <code>market</code> and <code>location</code> are clustering columns:</p>
<pre><code>CREATE TABLE items (
    item text,
    market text,
    location text,
    time timestamp,
    value int,
    PRIMARY KEY (item, market, location)
)
</code></pre>
<p>For the second app query, it looks similar to the <code>items</code> table except the data is organised by <code>market</code> and <code>time</code>:</p>
<pre><code>CREATE TABLE locations_by_item (
    item text,
    market text,
    location text,
    time timestamp,
    value int,
    PRIMARY KEY (item, market, time)
)
</code></pre>
<p>When you retrieve data from this table with:</p>
<pre><code>SELECT * FROM locations_by_item
  WHERE item = ?
  AND market = ?
  AND time &gt; ?
</code></pre>
<p>it will return rows of <code>location</code> + <code>value</code>. Cheers!</p>
",['table']
71276247,71306851,2022-02-26 11:27:54,How does peer to peer architecture work in Cassandra?,"<p>How the peer-to-peer Cassandra architecture really works ? I mean :</p>
<p>When the request hits the Cluster, it must hit some machine based on an IP, right ?</p>
<p>So which machine it will hit first ? : one of the nodes, or something in the Cluster who is responsible to balance and redirect the request to the right node ?</p>
<p>Could you describe what it is ? And how this differ from the Master/Folowers architecture ?</p>
",<cassandra>,"<p>For the purposes of my answer, I will use <a href=""https://docs.datastax.com/en/developer/java-driver/latest/"" rel=""nofollow noreferrer"">the Java driver</a> as an example since it is the most popular.</p>
<p>When you connect to a cluster using one of the driver, you need to configure it with details of your cluster including:</p>
<ol>
<li><strong>Contact points</strong> - the entry point to your cluster which is a comma-separated list of IPs/hostnames for some of the nodes in your cluster.</li>
<li>Login credentials - username and password if authentication is enabled on your cluster.</li>
<li>SSL/TLS certificate and credentials - if encryption is enabled on your cluster.</li>
</ol>
<p>When your application starts, <a href=""https://docs.datastax.com/en/developer/java-driver/latest/manual/core/control_connection/"" rel=""nofollow noreferrer"">a <strong>control connection</strong> is established with the first available node</a> in the list of <strong>contact points</strong>. The driver uses this control connection for admin tasks such as:</p>
<ul>
<li>get topology information about the cluster including node IPs, rack placement, network/DC information, etc</li>
<li>get schema information such as keyspaces and tables</li>
<li>subscribe to metadata changes including topology and schema updates</li>
</ul>
<p>When you configure the driver with a <a href=""https://docs.datastax.com/en/developer/java-driver/latest/manual/core/load_balancing/"" rel=""nofollow noreferrer"">load-balancing policy</a> (LBP), the policy will determine which node the driver will pick as the coordinator for each and every single query. By default, the Java driver uses a load balancing policy which picks nodes in the local datacenter. If you don't specify which DC is local to the app, the driver will set the local DC to the DC of the first contact point.</p>
<p>Each time a driver executes a query, it generates a <strong>query plan</strong> or a list of nodes to contact. This list of nodes has the following characteristics:</p>
<ul>
<li>A query plan is different for each query to balance the load across nodes in the cluster.</li>
<li>A query plan only lists available nodes and does not include nodes which are down or temporarily unavailable.</li>
<li>Nodes in the local DC are listed first and if the load-balancing policy allows it, remote nodes are included last.</li>
</ul>
<p>The driver tries to contact each node in the query plan in the order they are listed. If the first node is available then the driver uses it as the coordinator. If the first node does not respond (for whatever reason), the driver tries the next node in the query plan and so on.</p>
<p>Finally, all nodes are equal in Cassandra. There is no active-passive, no leader-follower, no primary-secondary and this makes Cassandra a truly high availability (HA) cluster with no single point-of-failure. Any node can do the work of any other node and the load is distributed equally to all nodes by design.</p>
<p>If you're new to Cassandra, I recommend having a look at <a href=""https://www.datastax.com/dev"" rel=""nofollow noreferrer"">datastax.com/dev</a> which has lots of free hands-on interactive learning resources. In particular, the <a href=""https://www.datastax.com/learn/cassandra-fundamentals"" rel=""nofollow noreferrer"">Cassandra Fundamentals</a> learning series lets you learn the basic concepts quickly.</p>
<p>For what it's worth, you can also use the <a href=""https://stargate.io/"" rel=""nofollow noreferrer"">Stargate.io</a> data platform. It allows you to connect to a Cassandra cluster using APIs you're already familiar with. It is fully open-source so it's free to use. Here are links to the Stargate tutorials on datastax.com/dev: <a href=""https://www.datastax.com/dev/rest"" rel=""nofollow noreferrer"">REST API</a>, <a href=""https://www.datastax.com/dev/documents-api"" rel=""nofollow noreferrer"">Document API</a>, <a href=""https://www.datastax.com/dev/graphql"" rel=""nofollow noreferrer"">GraphQL API</a>, and more recently <a href=""https://www.datastax.com/blog/available-now-grpc-apache-cassandra"" rel=""nofollow noreferrer"">gRPC API</a>. Cheers!</p>
",['rack']
71299962,71304870,2022-02-28 19:13:57,Select row with highest timestamp,"<p>I have a table that stores events</p>
<pre><code>CREATE TABLE active_events (
    event_id VARCHAR,
    number VARCHAR,
....
    start_time TIMESTAMP,
    PRIMARY KEY (event_id, number)
);
</code></pre>
<p>Now, I want to select an event with the highest <code>start_time</code>. It is possible? I've tried to create a secondary index, but no success.</p>
<p>This is a query I've created</p>
<pre><code>select * from active_call order by start_time limit 1
</code></pre>
<p>But the error says <code>ORDER BY is only supported when the partition key is restricted by an EQ or an IN</code>.
Should I create some kind of materialized view? What should I do to execute my query?</p>
",<cassandra>,"<p>This is an anti-pattern in Cassandra. To order the data you need to read all data and find the highest value.  And this will require scanning of data on multiple nodes, and will be very long.</p>
<p>Materialized view also won't help much as order for data only exists inside an individual partition, so you will need to put all your data into a single partition that could be huge and data would be imbalanced.</p>
<p>I can only think of following workaround:</p>
<ul>
<li><p>Have an additional table that will have all columns of the original table, but with a fake partition key and no clustering columns</p>
</li>
<li><p>You do inserts into that table in parallel to normal inserts, but use a fixed value for that fake partition key, and explicitly setting <a href=""https://docs.datastax.com/en/cql-oss/3.x/cql/cql_reference/cqlInsert.html#Examples"" rel=""nofollow noreferrer"">a timestamp for a record</a> equal to <code>start_time</code> (don't forget to multiple by 1000 as timestamp uses microseconds). In this case it will guaranteed to be the value with the highest timestamp as Cassandra won't override it with other data with lower timestamp.</p>
</li>
</ul>
<p>But this doesn't solve a problem with data skew, and all traffic will be handled by fixed number of nodes equal to RF.</p>
<p>Another alternative - use another database.</p>
",['table']
71343502,71344044,2022-03-03 20:35:51,"Upgraded Cassandra 3.11 to 4.0, failed with ""node with address ... already exists""","<p>we try to upgrade apache cassandra 3.11.12 to 4.0.2, this is the first node we upgrade in this cluster (seed node).
we drain the node and stop the service before replace the version.</p>
<p>system log:</p>
<pre><code>NFO  [RMI TCP Connection(16)-IP] 2022-03-03 15:50:18,811 StorageService.java:1568 - DRAINED
....
....
INFO  [main] 2022-03-03 15:58:02,970 QueryProcessor.java:167 - Preloaded 0 prepared statements
INFO  [main] 2022-03-03 15:58:02,970 StorageService.java:735 - Cassandra version: 4.0.2
INFO  [main] 2022-03-03 15:58:02,971 StorageService.java:736 - CQL version: 3.4.5
INFO  [main] 2022-03-03 15:58:02,971 StorageService.java:737 - Native protocol supported versions: 3/v3, 4/v4, 5/v5, 6/v6-beta (default: 5/v5)
...
...
WARN  [main] 2022-03-03 15:58:03,328 SystemKeyspace.java:1130 - No host ID found, created d78ab047-f1f9-4a07-8118-2fa83f4571ef (Note: This should happen exactly once per node).
....
...
ERROR [main] 2022-03-03 15:58:04,543 CassandraDaemon.java:911 - Exception encountered during startup
java.lang.RuntimeException: A node with address /HOST_IP:7001 already exists, cancelling join. Use cassandra.replace_address if you want to replace this node.
        at org.apache.cassandra.service.StorageService.checkForEndpointCollision(StorageService.java:660)
        at org.apache.cassandra.service.StorageService.prepareToJoin(StorageService.java:935)
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:785)
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:730)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:420)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:765)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:889)
INFO  [StorageServiceShutdownHook] 2022-03-03 15:58:04,558 HintsService.java:222 - Paused hints dispatch
WARN  [StorageServiceShutdownHook] 2022-03-03 15:58:04,561 Gossiper.java:2032 - No local state, state is in silent shutdown, or node hasn't joined, not announcing shutdown
INFO  [StorageServiceShutdownHook] 2022-03-03 15:58:04,561 MessagingService.java:441 - Waiting for messaging service to quiesce
...
..
INFO  [StorageServiceShutdownHook] 2022-03-03 15:58:06,956 HintsService.java:222 - Paused hints dispatch
</code></pre>
<p>did we need to delete\rm -rf system* data directories after drain the node before we start the new cassandra version (We didn't do that)? how we can solve this problem?</p>
",<cassandra>,"<p>During startup, Cassandra tries to retrieve the host ID by querying the local system table with:</p>
<pre><code>SELECT host_id FROM system.local WHERE key = 'local'
</code></pre>
<p>But if the <code>system.local</code> table is empty or the SSTables are missing from <code>system/local-*/</code> data subdirectories, Cassandra assumes that it is a brand new node and assigns a new host ID. However in your case, Cassandra realises that another node with the same IP address is already part of the cluster when it gossips with other nodes.</p>
<p>You need to figure out why Cassandra can't access the local <code>system.local</code> table. If someone deleted <code>system/local-*/</code> from the data directory, then you won't be able to start the node again. If this was the case, you'll need to start from scratch which involves:</p>
<ul>
<li>wipe all the contents of <code>data/</code>, <code>commitlog/</code> and <code>saved_caches/</code></li>
<li>uninstall C* 4.0</li>
<li>reinstall C* 3.11</li>
</ul>
<p>You will then need to replace the node &quot;with itself&quot; using <a href=""https://docs.datastax.com/en/cassandra-oss/3.x/cassandra/operations/opsReplaceLiveNode.html#Usingnodetooltoreplacearunningnode"" rel=""nofollow noreferrer"">the <code>replace_address</code> method</a>. Cheers!</p>
",['table']
71358726,71360816,2022-03-05 01:15:04,How do I design a table in Cassandra for a TinyURL use case?,"<p>Recently I came across a well-known design problem.
<a href=""https://www.educative.io/courses/grokking-the-system-design-interview/m2ygV4E81AR?aid=5082902844932096&amp;utm_source=google&amp;utm_medium=paid&amp;utm_campaign=dynamic_core&amp;utm_term=&amp;utm_campaign=%5BDynamic%5D%20Programming%20Verticals&amp;utm_source=adwords&amp;utm_medium=ppc&amp;hsa_acc=5451446008&amp;hsa_cam=16452540641&amp;hsa_grp=136967452314&amp;hsa_ad=585253631476&amp;hsa_src=g&amp;hsa_tgt=aud-470210443636:dsa-765065488750&amp;hsa_kw=&amp;hsa_mt=&amp;hsa_net=adwords&amp;hsa_ver=3&amp;gclid=CjwKCAiAjoeRBhAJEiwAYY3nDMDzCWWj3R0yc196g_fb-u-kciO0pXlFippC23HE6dx-AZ4Ag7d6_RoCTwwQAvD_BwE"" rel=""nofollow noreferrer"">'Tiny URL' </a></p>
<p>What I found was people vouching for NoSQL DBS such as DynamoDB or Cassandra. I've been reading about Cassandra for a couple of days, and I want to design my solution around this DB for this specific problem.</p>
<ol>
<li>What would be the table definition? If I choose the following table definition:</li>
</ol>
<p>Create table UrlMap(tiny_url text PRIMARY KEY, url text);</p>
<p>Wouldn't this result in a lot of partitions? since my partition key can take on around 68B values (using 6 char base64 strings)</p>
<p>Would that somehow affect the overall read/write performance? If so, what would be a better model to define the table.</p>
",<cassandra>,"<p>The primary principle of data modelling in Cassandra is to design one table for each application query.</p>
<p>For a URL shortening service, the main application query is to retrieve the equivalent full URL for a given tiny URI. In pseudo-code, the query looks like:</p>
<pre><code>    GET long url FROM datastore WHERE uri = ?
</code></pre>
<p>Note that for the purpose of a service, we won't store the web domain name to make the app reusable for any domain. The filter (<code>WHERE</code> clause) is the URI so this is what you want as the partition key so we would design the table accordingly:</p>
<pre><code>CREATE TABLE urls_by_uri (
    uri text,
    long_url text,
    PRIMARY KEY(uri)
)
</code></pre>
<p>If we want to retrieve the URL for <code>http://tinyu.rl/abc123</code>, the CQL query is:</p>
<pre><code>    SELECT long_url FROM urls_by_uri WHERE uri = 'abc123'
</code></pre>
<p>As Phact and Andrew pointed, there is no need to worry about the number of partitions (records) you'll be storing in the table because you can store as many as 2^128 partitions in a Cassandra table which for practical purposes is limitless.</p>
<p>In Cassandra, each partition gets hashed into a token value using the Murmur3 hash algorithm (default partitioner). This implementation distributes each partition randomly across all nodes in the cluster. The same hash algorithm is used to determine which node &quot;owns&quot; the partition making retrieval (reads) very fast in Cassandra.</p>
<p>As long as you limit the <code>SELECT</code> queries to a single partition, retrieving the data is extremely fast. In fact, I work with hundreds of companies who have an SLA on reads of 95% between 6-9 milliseconds. This is achievable in Cassandra when you model your data correctly and size your cluster correctly. Cheers!</p>
",['table']
71366492,71366792,2022-03-05 22:46:50,How does partition range repair work in Cassandra?,"<ol>
<li>Will partition range repair (-pr) repairs only the primary token rages a node is responsible for or also the non-primary tokens a node is holding data?</li>
<li>If only primary tokens, then is it mandatory to run partition range repair on all nodes so that non-primary tokens also get repaired?</li>
<li>How do I find non-primary tokens a node is responsible for? The token ranges returned by nodetool ring, does it only show primary token ranges or both primary and non-primary token ranges a node is responsible for?</li>
</ol>
",<cassandra>,"<p>The partitioner range repair (<code>--partitioner-range</code> or <code>-pr</code>)only repair token ranges on a node where the node is the primary replica meaning it is the primary owner of the tokens (see <a href=""https://docs.datastax.com/en/cassandra-oss/3.x/cassandra/operations/opsRepairNodesManualRepair.html#ParallelvsSequentialrepair"" rel=""nofollow noreferrer"">Manual repair in Cassandra</a>).</p>
<p>Since this repair option only repairs the primary range(s) on a node, it needs to be run on all nodes in all DCs otherwise not all token ranges will get repaired.</p>
<p>You can find the token range ownership with <code>nodetool ring</code>. It doesn't list token ranges where the node is a secondary replica.</p>
<p>Partitioner range repairs (also referred to as &quot;primary range repairs&quot;) are designed to be really efficient since it doesn't repair ranges which have already been repaired on other nodes.</p>
<p>Jeremiah Jordan explains this in great detail in his blog post <a href=""https://www.datastax.com/blog/repair-cassandra"" rel=""nofollow noreferrer"">Apache Cassandra Maintenance and Repair</a>.</p>
<p>Patrick McFadin also explains <a href=""https://www.youtube.com/watch?v=5V5rGDTHs20&amp;list=PL2g2h-wyI4SrHMlHBJVe_or_Ryek2THgQ&amp;index=18"" rel=""nofollow noreferrer"">how repairs work and the different types of repairs in this video</a> extracted from the DS210 Cassandra Operations course at <a href=""https://academy.datastax.com/"" rel=""nofollow noreferrer"">DataStax Academy</a>. Cheers!</p>
",['partitioner']
71813025,71950480,2022-04-10 01:00:58,Partition Keys & Data Modeling in ScyllaDB,"<p>In Scylla, data is stored by partition key. If I query a large table with many partition keys, is it equivalent to executing multiple queries against the table? For example, suppose I have the following table:</p>
<pre><code>key1 : val1
key2 : val2
key3 : val3
</code></pre>
<p>Where each of the 3 keys (<code>key1..3</code>) is a different partition key.</p>
<p>If I execute the following query against the table:</p>
<pre><code>SELECT * from table.
</code></pre>
<p>Scylla, will presumably need to execute this query 3 times - on 3 different partitions since each row is stored on a different partition. It seems inefficient, as it means the query will be executed once per partition. Suppose the data was partitioned into 100 partitions (100 keys), will the query need to be executed 100 times to complete? (and by extension, the query will only be as fast as the slowest server?)</p>
<p>If this is true, then querying 1 row from 3 separate tables (e.g, where each row has a different partition key), should have identical performance as when querying 3 rows from one table where each of 3 three rows has a different partition key? In other words, whether the data is modeled as part of one table or multiple tables, doesn't really matter. What matters is whether two or more rows share the same partition key?</p>
<p>What happens when we query 3 different tables were each row has the same partition key, is this as efficient as querying 3 rows from one table where all of the rows have the same partition key?</p>
<p>Any guidance in evaluating performance expectations in the 3 scenarios described above would be very helpful.</p>
<p>Thanks!</p>
",<database><cassandra><nosql><distributed><scylla>,"<p>As you noted, the query <code>SELECT * FROM table</code> is not a query in an individual partition, but rather a <strong>whole-table scan</strong>. A whole-table scan is &quot;costly&quot; in the sense that it will need to read all the data in the table (if you run it to completion), but it is not as inefficient as you thought it might be:</p>
<p>Scylla or Cassandra do <strong>not</strong> begin such a query by looking for the list of extant partition keys - and then querying each of those individually. Instead, Scylla and Cassandra have a deterministic order for the partition keys, so-called &quot;token&quot; order (you can think of the partition key's &quot;token&quot; as a hash function). Individual server nodes hold contiguous ranges of these tokens, so scanning the entire table is achieved by scanning each of these contiguous token ranges (also called &quot;vnodes&quot;) - each of which is implemented efficiently by an individual node efficiently reading data sequentially from its own disk. So you can have a million or even a billion partitions, and <code>SELECT * FROM table</code> for reading the entire table will still involve mostly-sequential reads from disk - not a million or billion seeks to individual partitions.</p>
<p>Another comment I feel compelled to make is that if you are thinking about having just 3 partitions, and worrying about increasing the number to 100, you are misunderstanding data modeling in Scylla (and Cassandra). In fact, having 100 partitions is still too few. You should have a lot more than 100 partitions. The more, the better. The reason is that if you have only a few huge partitions, the data will not be evently distributed between nodes and shards (CPUs). If you have just 3 partitions and 100 CPUs, since each partition is owned by one CPU (in Cassandra, one server), you'll only have 3 out of the 100 CPUs working, which is certainly not a good idea. Having a million partitions is much better than having just 3.</p>
",['table']
71905516,71933595,2022-04-17 21:16:59,Cassandra + SpringBoot: Configure Table to automatically add INSERT timestamp,"<p>Small question regarding Cassandra, with the context of a SpringBoot application please.</p>
<p>I am interested in adding onto the table the timestamp of when a row gets inserted onto the table.</p>
<p>Therefore, when creating the table, I do this:</p>
<pre><code>create table mytable (someprimarykey text PRIMARY KEY, timestamp long, someotherfield text);
</code></pre>
<p>I can see a column timestamp, happy.</p>
<p>Then, the web application:</p>
<pre><code>@Table
public class MyTable {

    @PrimaryKey
    private final String somePrimaryKey;

    private final long timestamp;

    private final String someOtherField;

//constructor + getters + setters
</code></pre>
<p>And when I insert, I will do the usual:</p>
<pre><code>MyTable myTable = new MyTable(somePK, System.currentTimeMillis(), &quot;foo&quot;);
myTableRepository.save(myTable);
</code></pre>
<p>This works fine, I can see in the table my record, with the time of the insert, happy.</p>
<p>Problem:
Now, for the hundreds of POJOs I am interested to insert into Cassandra, all of them are carrying this <code>timestamp long</code> field. Somehow, on the web application layer, I am dealing with a database concept, the timestamp of the write.</p>
<p>Question:
May I ask if it is possible to delegate this back to the database? Some kind of:</p>
<pre><code>create table mytable (someprimarykey text PRIMARY KEY, hey-cassandra-please-automatically-add-the-time-when-this-row-is-written long, someotherfield text);
or
create table mytable (someprimarykey text PRIMARY KEY, someotherfield text WITH default timestamp-insert-time-column);
</code></pre>
<p>And the web app can have the abstraction creating and inserting POJOs without carrying this timestamp field?</p>
<p>Thank you</p>
",<spring-boot><cassandra>,"<p>It isn't necessary to store the insert time of each row separately since Cassandra already stores this for all writes in the metadata.</p>
<p>There is a built-in CQL function <code>WRITETIME()</code> which returns the date/time (encoded in microseconds) when a column was written to the database.</p>
<p>In your case, you can query your table with:</p>
<pre><code>SELECT WRITETIME(someotherfield) FROM mytable WHERE someprimarykey = ?
</code></pre>
<p>For details, see the <a href=""https://docs.datastax.com/en/cql-oss/3.x/cql/cql_reference/cqlSelect.html#cqlSelect__retrieving-the-datetime-a-write-occurred"" rel=""nofollow noreferrer"">CQL doc on retrieving the write time</a>. Cheers!</p>
",['table']
72086689,72087032,2022-05-02 12:26:07,Why am I getting this error when I run the query?,"<p>When attempting to perform this query:</p>
<pre><code>select race_name from sport_app.month_category_runner where race_type = 'URBAN RACE 10K' and club = 'CORNELLA ATLETIC';
</code></pre>
<p>I get the following error:</p>
<pre><code>Cannot execute this query as it might involve data filtering and thus may have unpredictable performance. If you want to execute this query despite the performance unpredictability, use ALLOW FILTERING
</code></pre>
<p>It is an exercise, so I am not allowed to use ALLOW FILTERING.</p>
<p>So I have created two indexes in this way:</p>
<pre><code>create index raceTypeIndex ON sport_app.month_category_runner(race_type);
create index clubIndex ON sport_app.month_category_runner(club);
</code></pre>
<p>But I keep getting the same error, <strong>am I missing something, or is there an alternative?</strong></p>
<p>Table Structure:</p>
<pre><code>CREATE TABLE month_category_runner (month text,

                            category text,

                            runner_id text,

                            club text,

                            race_name text,

                            race_type text,

                            race_date timestamp,

                            total_runners int,

                            net_time time,

                            PRIMARY KEY (month, category, runner_id, race_name, net_time));
</code></pre>
",<cassandra><cql>,"<p>Note if you add the &quot;ALLOW FILTERING&quot; the query will run on all the nodes of Cassandra cluster and can have a large impact on all nodes.</p>
<p>The recommendation is to add the partition as condition of your query, to allow the query to be executed on needed nodes only.</p>
<p>Example:</p>
<p>select race_name from month_category_runner where month = 'may' and club = 'CORNELLA ATLETIC';</p>
<p>select race_name from month_category_runner where month = 'may' and race_type = 'URBAN RACE 10K';</p>
<p>select race_name from month_category_runner where month = 'may' and race_type = 'URBAN RACE 10K' and club = 'CORNELLA ATLETIC' ALLOW FILTERING;</p>
<p>Your primary key  is composed by (month, category, runner_id, race_name, net_time) and the column month is the partition, so this column must be on your query filter as i showed in example.</p>
<p>The query that you want to do using two columns that are not in primary key despite the index column exist, you need to use the ALLOW FILTERING that can have performance impact;</p>
<p>The other option is create a new table where the primary key contains theses columns.</p>
",['table']
72174498,72216632,2022-05-09 15:27:14,Problem to write on keyspace with new versions spark 3.x,"<p>I'm trying to write on aws keyspace, but the following message appears:</p>
<p><a href=""https://i.stack.imgur.com/6ylJu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6ylJu.png"" alt=""enter image description here"" /></a></p>
<p>Spark version: 3.0.1 <br>
Connector: 3.0 <br>
Java: 1.8 <br>
Scala: 2.12</p>
<p>Respecting by the version on github:
<a href=""https://i.stack.imgur.com/mEJYO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mEJYO.png"" alt=""enter image description here"" /></a></p>
<p>In other previus version like Connector = 2.5.2 and spark = 2.4.6 works fine.</p>
",<apache-spark><cassandra><spark-cassandra-connector><amazon-keyspaces>,"<p>You should be able to connect using spark 3 and connector 3. Here are some steps to validate you setup connection accordingly and you have the right permissions.</p>
<ul>
<li>Make sure you have permissions to read the system tables.</li>
<li>If you have setup the VPCE endpoint ensure you have <a href=""https://docs.aws.amazon.com/keyspaces/latest/devguide/vpc-endpoints.html#:%7E:text=and%20updating%20rules.-,Populating%20system.peers%20table%20entries%20with%20interface%20VPC%20endpoint%20information,-Apache%20Cassandra%20drivers"" rel=""nofollow noreferrer"">permissions for describe VPC endpoints.</a></li>
<li>In you configuration make sure that host-validation set to false in ssl config.</li>
</ul>
<p>You should be able to execute the following query against your system.peers table and retrieve the ips from the endpoint public/private.  If you have 1 or no peers you need to take the steps above.  Remember the AWS console is not in your vpc and will contact the public endpoint similar to s3.</p>
<pre><code>SELECT * FROM system.peers
</code></pre>
<p>Sample Policy. You need to provide access to resource /keyspace/system* and ec2:DescribeNetworkInterfaces&quot; and &quot;ec2:DescribeVpcEndpoints&quot; on your vpc.</p>
<pre><code>    {
   &quot;Version&quot;:&quot;2012-10-17&quot;,
   &quot;Statement&quot;:[
      {
         &quot;Effect&quot;:&quot;Allow&quot;,
         &quot;Action&quot;:[
            &quot;cassandra:Select&quot;,
            &quot;cassandra:Modify&quot;
         ],
         &quot;Resource&quot;:[
            &quot;arn:aws:cassandra:us-east-1:111122223333:/keyspace/mykeyspace/table/mytable&quot;,
            &quot;arn:aws:cassandra:us-east-1:111122223333:/keyspace/system*&quot;
         ]
      },
      {
         &quot;Sid&quot;:&quot;ListVPCEndpoints&quot;,
         &quot;Effect&quot;:&quot;Allow&quot;,
         &quot;Action&quot;:[
            &quot;ec2:DescribeNetworkInterfaces&quot;,
            &quot;ec2:DescribeVpcEndpoints&quot;
         ],
         &quot;Resource&quot;:&quot;*&quot;
      }
   ]
}
</code></pre>
<p>Setup the connection by referencing the external config.</p>
<pre><code>-conf&quot;:&quot;spark.cassandra.connection.config.profile.path=application.conf&quot;
</code></pre>
<p>Sample driver config.</p>
<pre><code>datastax-java-driver {
  basic.request.consistency = &quot;LOCAL_QUORUM&quot;
  basic.contact-points = [ &quot;cassandra.us-east-1.amazonaws.com:9142&quot;]

  advanced.reconnect-on-init = true

   basic.load-balancing-policy {
        local-datacenter = &quot;us-east-1&quot;
     }

   advanced.auth-provider = {
       class = PlainTextAuthProvider
       username = &quot;user-at-sample&quot;
       password = &quot;S@MPLE=PASSWORD=&quot;
    }

    advanced.throttler = {
       class = ConcurrencyLimitingRequestThrottler
       max-concurrent-requests = 30
       max-queue-size = 2000
    }



   advanced.ssl-engine-factory {
      class = DefaultSslEngineFactory
      hostname-validation = false
    }

    advanced.connection.pool.local.size = 1


}
</code></pre>
",['table']
72180923,72183132,2022-05-10 04:56:20,How to form dynamic insert query for cassandra golang,"<p>I have been trying to create a dynamic query in golang for cassandra using gocql driver ,this is what I tried so far</p>
<pre><code>func WriteRecord(session gocql.Session, insertstring string, table string, fields []string, values ...interface{}) error {
    var placeholder []string

    for range fields {
        placeholder = append(placeholder, &quot;?&quot;)
    }
    querystring := fmt.Sprintf(insertstring, table, strings.Join(fields, &quot;, &quot;), strings.Join(placeholder, &quot;, &quot;))
    fmt.Println(querystring)
    return session.Query(querystring, values...).Exec()
}
</code></pre>
<p>And calling this method in this</p>
<pre><code>func writeData(session gocql.Session) {
    fields := []string{
        &quot;id&quot;,
        &quot;message&quot;,
    }

    for i := 1; i &lt;= 10; i++ {
        /*
            if err := session.Query(
                &quot;INSERT INTO example_keyspace.example_go (id, message) VALUES (?, ?)&quot;, i, &quot;Hello from golang!&quot;,
            ).Exec(); err != nil {
                log.Fatal(err)
            }
        */
        insertString := &quot;INSERT INTO example_keyspace.%s(%s,%s) VALUES (%s,%s)&quot;
        err := WriteRecord(session, insertString, &quot;kafka&quot;, fields, i, &quot;hey kafka&quot;)
        if err != nil {
            log.Fatal(err)
        }

    }
}
</code></pre>
<p>its giving me this output</p>
<blockquote>
<p>INSERT INTO example_keyspace.kafka(id, message,?, ?) VALUES
(%!s(MISSING),%!s(MISSING))</p>
</blockquote>
<p>How to fix this problem ,I am not sure where I am doing wrong</p>
",<go><cassandra><gocql>,"<p>You are almost right just small modifications in your formatted insertstring ,see below</p>
<pre><code>func WriteRecord(session gocql.Session, insertstring string, table string, fields []string, values ...interface{}) error {
    var placeholder []string

    for range values {
        placeholder = append(placeholder, &quot;?&quot;)
    }
    querystring := fmt.Sprintf(insertstring, table, strings.Join(fields, &quot;, &quot;), strings.Join(placeholder, &quot;, &quot;))
    fmt.Println(querystring)
    return session.Query(querystring, values...).Exec()
}



func writeData(session gocql.Session) {
    fields := []string{
        &quot;id&quot;,
        &quot;message&quot;,
    }

    for i := 1; i &lt;= 10; i++ {
        /*
            if err := session.Query(
                &quot;INSERT INTO example_keyspace.example_go (id, message) VALUES (?, ?)&quot;, i, &quot;Hello from golang!&quot;,
            ).Exec(); err != nil {
                log.Fatal(err)
            }
        */
        insertString := &quot;INSERT INTO example_keyspace.%s(%s) VALUES (%s)&quot;
        err := WriteRecord(session, insertString, &quot;kafka&quot;, fields, i, &quot;hey kafka&quot;) // Just remove extra %s as you are joining the string 
        if err != nil {
            log.Fatal(err)
        }

    }
}
</code></pre>
<p>the final insertstring output you would get is</p>
<pre><code>INSERT INTO example_keyspace.kafka(id, message) VALUES (?, ?)
</code></pre>
<p>And as per this line</p>
<pre><code>return session.Query(querystring, values...).Exec() // the  values will be passed
</code></pre>
<p>Hope it helps</p>
",['table']
72235115,72310421,2022-05-13 20:28:06,Problem to read and write aws keypace with spark connector,"<p>I'm trying to write read and write some data on aws keyspace, but the following message appears.</p>
<p><a href=""https://i.stack.imgur.com/HOxe7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HOxe7.png"" alt=""enter image description here"" /></a></p>
<p>Versions:
Spark: 2.4.6
Cassandra connector: 2.5.2
Scala: 2.11.10</p>
<p>New and old version problems occurs too.</p>
",<apache-spark><cassandra><spark-cassandra-connector><amazon-keyspaces>,"<p>This error is do to not being able to see system.peers table. Spark requires the peers table info to get the token information.</p>
<ol>
<li>check if they have access to read the system tables. If you are using a public endpoint you should have 9 and if you are using a VPCE you should have one for each availability zone.<br />
<code>SELECT * FROM system.peers</code></li>
</ol>
<p>If you are using a vpc endpoint check to see if you have setup <a href=""https://docs.aws.amazon.com/keyspaces/latest/devguide/vpc-endpoints.html#:%7E:text=network%20interface%20information.-,Important,-Populating%20the%20system"" rel=""nofollow noreferrer"">the right permissions</a>.</p>
<pre><code> {
         &quot;Sid&quot;:&quot;ListVPCEndpoints&quot;,
         &quot;Effect&quot;:&quot;Allow&quot;,
         &quot;Action&quot;:[
            &quot;ec2:DescribeNetworkInterfaces&quot;,
            &quot;ec2:DescribeVpcEndpoints&quot;
         ],
         &quot;Resource&quot;:&quot;*&quot;
      }
</code></pre>
<p>The following example is how to using Spark/Glue to export Keyspaces data to S3.
<a href=""https://github.com/aws-samples/amazon-keyspaces-examples/tree/main/scala/datastax-v4/aws-glue/export-to-s3"" rel=""nofollow noreferrer"">https://github.com/aws-samples/amazon-keyspaces-examples/tree/main/scala/datastax-v4/aws-glue/export-to-s3</a></p>
",['table']
72343566,72373132,2022-05-23 05:23:49,Is Cassandra suitable for Aggregate Queries?,"<p>I have read that Columnar databases are apt for Aggregate Queries and Cassandra is a columnar database. I am trying to use count( values 'between' or '&gt;=' for a specific partition) in Cassandra. Is this performance intensive?</p>
",<database><cassandra><aggregate>,"<p>It's a common misconception that Cassandra is a columnar database. I think it comes from the old terminology &quot;column family&quot; for tables. Data is stored in rows containing columns of key-value pairs which is why the tables used to be called column families.</p>
<p>A major difference compared to traditional relational databases is that Cassandra tables can be 2-dimensional (each record contains exactly one row) or multi-dimensional (each record can contain ONE OR MORE rows).</p>
<p>On the other hand, columnar databases flips a 2-dimensional table such that data is stored in columns instead of rows, specifically optimised for analytics-type queries such as aggregations -- this is NOT Cassandra.</p>
<p>Going back to your question, counting the rows within a single partition is ok to do for most data models. The key is to restrict the query to just one partition like:</p>
<pre><code>    SELECT COUNT(some_column) FROM table_name
        WHERE pk = ?
</code></pre>
<p>It's also OK to count the rows in a range query as long as they're restricted to one partition like:</p>
<pre><code>    SELECT COUNT(some_column) FROM table_name
        WHERE pk = ?
        AND clustering_col &gt;= ?
        AND clustering_col &lt;= ?
</code></pre>
<p>If you don't restrict the query to a single partition, it might work for (a) very small datasets and (b) clusters with a very low number of nodes but it doesn't scale as (c) the dataset grows, and (d) the number of nodes increases. I've explained why performing aggregates such as <code>COUNT()</code> is bad in Cassandra in this post -- <a href=""https://community.datastax.com/questions/6897/"" rel=""nofollow noreferrer"">https://community.datastax.com/questions/6897/</a>.</p>
<p>This is not to say that Cassandra isn't a good fit. Cassandra is a good choice if your primary use case is for storing real-time data for OLTP workloads. For analytics queries, you just need to use other software like Apache Spark since the <a href=""https://github.com/datastax/spark-cassandra-connector"" rel=""nofollow noreferrer"">spark-cassandra-connector</a> will optimise the queries to Cassandra. Cheers!</p>
",['table']
72441207,72441308,2022-05-31 00:46:39,Can I delete data/system directory in Cassandra?,"<p>I don`t know whether delete or leave this folder in Cassandra.
My cassandra system has a data folder. In data folder has a lot of folder.
especially, system folder use a much space in disk.(about 20% of entire disk)
I think if this folder is unusable so i delete this folder to have a free space in disk.</p>
<p>this system folder location is this.
/cassandra_installation_location/data/system</p>
<p>Can I delete this folder without any problem or trouble?</p>
<p>thank you.</p>
<p>** update</p>
<p>Thank you @Erick Ramirez, I appreciate your answer to my question.
As you say, I attach some images include directory structure and size of each folders in directories.</p>
<p>I am sorry because I haven`t 10 reputation in this site, so I cannot embed images my article.
I have to upload and add to link include image in my article.</p>
<p>This image show the volume of entire system.
<a href=""https://i.stack.imgur.com/7jsjg.png"" rel=""nofollow noreferrer"">This image is a volume of entire system.</a></p>
<p>And this image show the size of data folder.
<a href=""https://i.stack.imgur.com/gcGeu.png"" rel=""nofollow noreferrer"">This image is a size of data folder</a></p>
<p>And this image show the sizes of subdirectories in my data folder.
<a href=""https://i.stack.imgur.com/n2eh2.png"" rel=""nofollow noreferrer"">this image show the sizes of subdirectories in my data folder.</a></p>
<p>Last, this image show the sizes of subdirectories in data/system folder.
<a href=""https://i.stack.imgur.com/Purqb.png"" rel=""nofollow noreferrer"">this image show the sizes of subdirectories in data/system folder</a></p>
<p>Thank you for your answer.</p>
",<cassandra>,"<p>The quick answer is no, you should <strong>never</strong> manually delete any files or subdirectories on a node's <code>data/</code> directory or you risk (a) losing data, or (b) corrupt the node preventing it from working.</p>
<p>The <code>data/system/</code> directory in particular contains all the metadata that is essential for a Cassandra node's normal operation. The metadata is local to the node and is typically not replicated to other nodes so you risk losing it forever if you delete it.</p>
<p>The only exception for cleaning up the <code>data/</code> directory is <a href=""https://cassandra.apache.org/doc/latest/cassandra/tools/nodetool/clearsnapshot.html"" rel=""nofollow noreferrer"">removing snapshots</a> (backups) you no longer require.</p>
<p>You need to first determine which table in <code>data/system</code> is taking up the most space before you can take the necessary action. If you update your original post with details of which (1) table subdirectory is using a lot of disk space, and (2) the contents of the subdirectory, I'd be happy to review them and update my answer. Cheers!</p>
",['table']
72524765,72526038,2022-06-07 00:42:05,Cannot organize Cassandra table structure,"<p>I have a table</p>
<pre><code>CREATE TABLE test_keyspace.persons (
    id uuid,
    name text,
    birth_date timestamp,
    PRIMARY KEY (id, birth_date)
);
</code></pre>
<p>And I have two use-cases:</p>
<ol>
<li>Find persons by <code>id</code></li>
<li>Find persons by <code>birth_date</code> (like <code>WHERE birth_date &gt;= 1985/03/12 AND birth_date &lt;= 1985/03/30</code>) + sort in ASC/DESC order</li>
</ol>
<p>With current table example I'm not able to retrieve by dates between. Also I'm not able if I make <code>birth_date</code> as a partitioned key - I get an exception:</p>
<pre><code>Cannot execute this query as it might involve data filtering and thus may have unpredictable performance. If you want to execute this query despite the performance unpredictability, use ALLOW FILTERING
</code></pre>
<p>Do I need to change primary keys? Do I need separate table for my purpose? Please help me to understand how to design my data model most effectively.</p>
",<cassandra><nosql>,"<p>In Cassandra you design your tables based on your queries. Now you have two queries</p>
<ol>
<li>Find persons by id</li>
<li>Find persons by birth_date (like WHERE birth_date &gt;= 2022/03/12 AND birth_date &lt;= 2022/03/30) + sort in ASC/DESC order</li>
</ol>
<p>Your current table will be able to answer your first query since you are asking Cassandra to give persons by id which is your partition key.</p>
<p>For second query you can have another table. But since you are doing filtering based on date, you might need to create a partition which consist of a dummy partition key  and and date of birth as clustering key. Something like this</p>
<pre><code>CREATE TABLE test_keyspace.persons_by_year (
id uuid,
name text,
birth_date timestamp,
year text,
PRIMARY KEY (year, birth_date)
</code></pre>
<p>);</p>
<p>Then you can query like <code>where year = '1985' and  birth_date &gt;= 1985/03/12 AND birth_date &lt;= 1985/03/30</code></p>
<p>Another option is you can look out for spark-cassandra combo which can filter data in memory.</p>
",['table']
72590979,72592674,2022-06-12 09:33:50,"Cassandra: delete of data with TTL, ROW with ""null"" value instead of being removed","<p>Small question regarding Cassandra please.</p>
<p>I have created a table as follow:</p>
<pre><code>CREATE TABLE contract ( contractidentifier text, name text, telephone text, idnumber text, companyid text, company text, startdate timestamp, hiringdate timestamp, interviewdate timestamp,
                      PRIMARY KEY (contractidentifier, company, name)) WITH default_time_to_live = 2628000;
</code></pre>
<p>And the goal is very straightforward, the web application is just going to write some data about some short term contracts which only last for one month.</p>
<p>Since the employment is only a month long, what I would like to achieve from the table point of view is: &quot;keep only the data for one month only. After that, it should be deleted&quot;.</p>
<p>With this requirement in mind, I simply used the TTL feature of Cassandra (see query, WITH default_time_to_live = 2628000).</p>
<p>Now, I come back after once month, expecting the data to be deleted. However, I can see the data is still there, <strong>with some null values</strong>:</p>
<pre><code>       C102403845 | null |      null | SMITH | null |  null | null |  null | DELL |     null |    null | null |        null | null |  null

</code></pre>
<p>Questions:</p>
<p>What is the issue here please? Did I misunderstood the purpose of the TTL? (i.e. My understanding of the TTL is that the row will be entirely deleted after one month, not: the row is still after one month, with only some of the values being null)</p>
<p>If my understanding is correct, did I misconfigured something?</p>
<p>Finally, if the TTL is actually not the solution, what else could I use please?</p>
<p>Thank you</p>
",<cassandra>,"<p>You would have inserted into table with updated ttl using the <code>USING TTL</code> construct. Otherwise it is not possible that table will have values after TTL time has passed. You can check remaining ttl for the columns for which the value is shown using following construct.</p>
<pre><code>select ttl(column_name) from tablename where key= value;
</code></pre>
",['table']
72612301,73033332,2022-06-14 06:19:04,How do range queries work for clustering keys in Cassandra?,"<p>According to the official doc:
Clustering columns order data within a partition. When a table has multiple clustering columns the data is stored in nested sort order.</p>
<p>Suppose we have simple timeseries table:</p>
<pre><code>CREATE TABLE alerts_by_year(
  year int,
  ts timestamp,
  alert text,
  PRIMARY KEY ((year), ts)
);
</code></pre>
<p>A simple query that get events for some range:</p>
<pre><code>SELECT * FROM alerts_by_year
  WHERE year=2022
  AND ts &gt;'2022-06-24 03:11:00'
  AND ts &lt;'2022-06-24 04:11:00'
</code></pre>
<p>What is algorithm complexity to find this range through the &quot;ts&quot; clustering keys?
Is it constant time or O(n) time?
Does it depends on the type of storage used: memtable or sstable?</p>
<p>How does it work then? Are we simply iterating through &quot;ts&quot; clustering keys until we find the required range?</p>
",<cassandra>,"<p>Clustering columns are stored in sorted order. If you don't explicitly specify the order when you create a table, the clustering columns will be sorted in ascending order.</p>
<p>In your case, the following table option is automatically added to your table's definition:</p>
<pre><code>WITH CLUSTERING ORDER BY (ts ASC)
</code></pre>
<p>In your case, the table schema looks like:</p>
<pre><code>CREATE TABLE alerts_by_year(
  year int,
  ts timestamp,
  alert text,
  PRIMARY KEY ((year), ts)
) WITH CLUSTERING ORDER BY (ts ASC)
</code></pre>
<p>Since the rows in each partition is sorted in chronological order from oldest to newest timestamp, a range query on the <code>ts</code> column is done sequentially, iterating one row at a time until the condition is satisfied.</p>
<p>Note that the drivers will automatically page through the results. For example, <a href=""https://docs.datastax.com/en/developer/java-driver/latest/manual/core/paging/"" rel=""nofollow noreferrer"">the Java driver will return the first 5000 rows by default</a>. You app will then need to retrieve the &quot;next page&quot; to get the next set of rows. Cheers!</p>
",['table']
72697692,72703199,2022-06-21 08:40:07,search within a cassandra column,"<p>I'm working with the movielens dataset and I have a column called 'genres' which has entries such as 'Action|War', 'Action|Adventure|Comedy|Sci-Fi'. I wish to count the number of rows that have the text 'Comedy' in them.</p>
<pre><code>SELECT COUNT(*) FROM movielens.data_movies WHERE genres = 'Comedy' ALLOW FILTERING
</code></pre>
<p>But this counts only the exact instances of 'Comedy'. It does not count 'Action|Adventure|Comedy|Sci-Fi' which I want it to do. So I tried,</p>
<pre><code>SELECT COUNT(*) FROM movielens.data_movies WHERE genres CONTAINS 'Comedy' ALLOW FILTERING 
</code></pre>
<p>However, that gives me the error</p>
<pre><code>Cannot use CONTAINS on non-collection column genres
</code></pre>
<p>From <a href=""https://stackoverflow.com/questions/24858141/cassandra-full-text-search"">this</a> it seems that there is no easy way to do what I'm asking. Does anyone know of a simpler solution?</p>
",<cassandra><cql>,"<p>So what you can do, is to create a <code>CUSTOM</code> index on <code>genres</code>.</p>
<pre><code>CREATE CUSTOM INDEX ON movielens.data_movies(genres)
  USING 'org.apache.cassandra.index.sasi.SASIIndex'
  WITH OPTIONS={'mode':'CONTAINS'};
</code></pre>
<p>Then this query should work:</p>
<pre><code>SELECT COUNT(*) FROM movies
WHERE genres LIKE '%Comedy%';
</code></pre>
<p>However, if you're running a query across millions of rows over multiple nodes, this query will likely timeout.  This is because Cassandra has to poll multiple partitions and nodes to build the result set.  Queries like this don't really work well in Cassandra.</p>
<p>The best way to solve for this, is to create a table <em>partitioned</em> by <code>genre</code>, like this:</p>
<pre><code>CREATE TABLE movies_by_genre (
    id int,
    title TEXT,
    genre TEXT,
    PRIMARY KEY(genre,title,id));
</code></pre>
<p>This is of course also assuming that <code>genres</code> is split-out by each individual genre.  But then this query would work:</p>
<pre><code>SELECT COUNT(*) FROM movies_by_genre
WHERE genre = 'Comedy';
</code></pre>
",['table']
72735204,72767435,2022-06-23 18:46:56,How do I find out right data design and right tools/database/query for below requirement,"<p>I have a kind of requirement but not able to figure out how can I solve it. I have datasets in below format</p>
<pre><code>id, atime, grade
123, time1, A
241, time2, B
123, time3, C
</code></pre>
<p>or if I put in list format:</p>
<pre><code>[[123,time1,A],[124,timeb,C],[123,timec,C],[143,timed,D],[423,timee,P].......]
</code></pre>
<p>Now my use-case is to perform comparison, aggregation and queries over multiple row like</p>
<ol>
<li>time difference between last 2 rows where id=123</li>
<li>time difference between last 2 rows where id=123&amp;GradeA</li>
<li>Time difference between first, 3rd, 5th and latest one</li>
<li>all data (or last 10 records for particular id) should be easily accessible.</li>
</ol>
<p>Also need to further do compute. <strong>What format should I chose for dataset
and what database/tools should I use?</strong>
I don't Relational Database is useful here. I am not able to solve it with Solr/Elastic if you have any ideas, please give a brief.Or any other tool Spark, hadoop, cassandra any heads?
I am trying out things but any help is appreciated.</p>
",<apache-spark><elasticsearch><cassandra><nosql><bigdata>,"<p>Choosing the right technology is highly dependent on things related to your SLA. things like how much can your query have latency? what are your query types? is your data categorized as big data or not? Is data updateable? Do we expect late events? Do we need historical data in the future or we can use techniques like rollup? and things like that. To clarify my answer, probably by using window functions you can solve your problems. For example, you can store your data on any of the tools you mentioned and by using the Presto SQL engine you can query and get your desired result. But not all of them are optimal. Furthermore, usually, these kinds of problems can not be solved with a single tool. A set of tools can cover all requirements.</p>
<p><strong>tl;dr. In the below text we don't find a solution. It introduces a way to think about data modeling and choosing tools.</strong></p>
<p>Let me take try to model the problem to choose a single tool. I assume your data is not updatable, you need a low latency response time, we don't expect any late event and we face a large volume data stream that must be saved as raw data.</p>
<ul>
<li>Based on the first and second requirements, it's crucial to have random access (it seems you wanna query on a particular ID), so solutions like parquet or ORC files are not a good choice.</li>
<li>Based on the last requirement, data must be partitioned based on the ID. Both the first and second requirements and the last requirement, count on ID as an identifier part and it seems there is nothing like join and global ordering based on other fields like time. So we can choose ID as the partitioner (physical or logical) and <code>atime</code> as the cluster part; For each ID, events are ordered based on the time.</li>
<li>The third requirement is a bit vague. You wanna result on all data? or for each ID?</li>
<li>For computing the first three conditions, we need a tool that supports window functions.</li>
</ul>
<p>Based on the mentioned notes, it seems we should choose a tool that has good support for random access queries. Tools like Cassandra, Postgres, Druid, MongoDB, and ElasticSearch are things that currently I can remember them. Let's check them:</p>
<ul>
<li>Cassandra: It's great on response time on random access queries, can handle a huge amount of data easily, and does not have a single point of failure. But sadly it does not support window functions. Also, you should carefully design your data model and it seems it's not a good tool that we can choose (because of future need for raw data). We can bypass some of these limitations by using Spark alongside Cassandra, but for now, we prefer to avoid adding a new tool to our stack.</li>
<li>Postgres: It's great on random access queries and indexed columns. It supports window functions. We can shard data (horizontal partitioning) across multiple servers (and by choosing ID as the shard key, we can have data locality on computations). But there is a problem: ID is not unique; so we can not choose ID as the primary key and we face some problems with random access (We can choose the ID and <code>atime</code> columns (as a timestamp column) as a compound primary key, but it does not save us).</li>
<li>Druid: It's a great OLAP tool. Based on the storing manner (segment files) that Druid follows, by choosing the right data model, you can have analytic queries on a huge volume of data in sub-seconds. It does not support window functions, but with rollup and some other functions (like <code>EARLIEST</code>), we can answer our questions. But by using rollup, we lose raw data and we need them.</li>
<li>MongoDB: It supports random access queries and sharding. Also, we can have some type of window function on its computing framework and we can define some sort of pipelines for doing aggregations. It supports capped collections and we can use it to store the last 10 events for each ID if the cardinality of the ID column is not high. It seems this tool can cover all of our requirements.</li>
<li>ElasticSearch: It's great on random access, maybe the greatest. With some kind of filter aggregations, we can have a type of window function. It can handle a large amount of data with sharding. But its query language is hard. I can imagine we can answer the first and second questions with ES, but for now, I can't make a query in my mind. It takes time to find the right solution with it.</li>
</ul>
<p>So it seems MongoDB and ElasticSearch can answer our requirements, but there is a lot of 'if's on the way. I think we can't find a straightforward solution with a single tool. Maybe we should choose multiple tools and use techniques like duplicating data to find an optimal solution.</p>
",['partitioner']
72805088,72824007,2022-06-29 16:47:53,Is it hacky to do RF=ALL + CL=TWO for a small frequently used Cassandra table?,"<p>I plan to enhance the search for our retail service, which is managed by DataStax. We have data of about 500KB in raw from our wheels and tires and could be compressed and encrypted to about 20KB. This table is frequently used and changes about every day. We send the data to the frontend, which will be processed with Next.js later. Now we want to store this data in a single row table in a separate keyspace with a consistency level of TWO and RF equal to all nodes, replicating the table to all of the nodes.
Now the question: Is this solution hacky or abnormal? Is any solution rather this that fits best in this situation?</p>
",<cassandra><nosql><datastax-enterprise>,"<p>The quick answer to your question is yes, it is a hacky solution to do <code>RF=ALL</code>.</p>
<p>The table is very small so there is no benefit to replicating it to all nodes in the cluster. In practice, the tables are so small that the data will be cached anyway.</p>
<p>Since you are running with <a href=""https://www.datastax.com/products/datastax-enterprise"" rel=""nofollow noreferrer"">DataStax Enterprise</a> (DSE), you might as well take advantage of the <a href=""https://docs.datastax.com/en/dse/6.8/dse-admin/datastax_enterprise/inMemory/inmemTOC.html"" rel=""nofollow noreferrer"">DSE In-Memory</a> feature which allows you to keep data in RAM to save from disk seeks. Since your table can easily fit in RAM, it is a perfect use case for DSE In-Memory.</p>
<p>To configure the table to run In-Memory, set the table's compaction strategy to <code>MemoryOnlyStrategy</code>:</p>
<pre><code>CREATE TABLE inmemorytable (
    ...
    PRIMARY KEY ( ... )
) WITH compaction= {'class': 'MemoryOnlyStrategy'}
     AND caching = {'keys':'NONE', 'rows_per_partition':'NONE'};
</code></pre>
<p>To alter the configuration of an existing table:</p>
<pre><code>ALTER TABLE inmemorytable
    WITH compaction= {'class': 'MemoryOnlyStrategy'}
    AND caching = {'keys':'NONE', 'rows_per_partition':'NONE'};
</code></pre>
<p>Note that tables configured with DSE In-Memory are still persisted to disk so you won't lose any data in the event of a power outage or service disruption. In-Memory tables operate the same as regular tables so the same backup and restore processes still apply with the only difference being that a copy of the data is kept in memory for faster read performance.</p>
<p>For details, see <a href=""https://docs.datastax.com/en/dse/6.8/dse-admin/datastax_enterprise/inMemory/inmemTOC.html"" rel=""nofollow noreferrer"">DataStax Enterprise In-Memory</a>. Cheers!</p>
",['table']
72840838,72843509,2022-07-02 16:51:10,Does a Cassandra node get assigned a new token every time it restarts?,"<p>From my limited knowledge, Cassandra assigns a random token for every new node in the ring. The ring position is important because data is replicated in the SimpleStrategy according to the position. So what happens when the node restarts and wants to join the ring again, will it be assigned a new token? If that's the case then all the data in that node needs to be sent to their correct place every time it restarts.</p>
",<cassandra><consistent-hashing>,"<p>Cassandra nodes only get assigned a token when they join a cluster for the very first time.</p>
<p>When a node has bootstrapped successfully, it's allocated token is stored in the <code>system.local</code> table so it knows which token range(s) it owns when it is restarted. All the nodes also keep track of each other's token assignments in memory and are propagated via gossip (see <code>nodetool gossipinfo</code>).</p>
<p>Note that the same applies for nodes with multiple tokens (virtual nodes configuration). Cheers!</p>
",['table']
72951401,72968041,2022-07-12 11:13:10,How do I find out the data size of write request per day or per second in my Cassandra cluster?,"<p>I have Cassandra cluster with 10 nodes and 5 tables.
I want to know that how many bytes of data are stored on a specific table by write request per day (or per sec).</p>
<p>Is there any way to get it roughly using jmx or something?</p>
<p>I tried to use nodetool tablestats, but the output does not have any related informations.</p>
",<cassandra>,"<p>metrics reporting on cassandra uses Dropwizard metrics and there are a set of default counters. In the past , I've had a custom set up with Cassandra running on k8s where these metrics can be exported (and we sent that to Prometheus). JMX queries are permitted on these metrics. We had a metrics exporter that exported these exposed metrics to Prometheus</p>
<p><a href=""https://github.com/nabto/cassandra-prometheus"" rel=""nofollow noreferrer"">https://github.com/nabto/cassandra-prometheus</a></p>
<p>There are other deployment types which use other agents to export these metrics
<a href=""https://docs.datastax.com/en/opscenter/6.7/opsc/LCM/opscLCMconfigMetricsCollector.html"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/opscenter/6.7/opsc/LCM/opscLCMconfigMetricsCollector.html</a></p>
<p>the default metrics are <a href=""https://cassandra.apache.org/doc/latest/cassandra/operating/metrics.html"" rel=""nofollow noreferrer"">https://cassandra.apache.org/doc/latest/cassandra/operating/metrics.html</a>. Within this LiveDiskSpaceUsed (Disk space used by SSTables belonging to this table (in bytes)) may be of interest. You can store it on a time series database and then query this over time to get the delta</p>
",['table']
73045950,73046132,2022-07-20 04:18:59,How do I retrieve the ranking of a user from a materialized view?,"<p>I am using Cassandra for storing contest data.</p>
<p>Currently I have a contest table like this (table contest_score):</p>
<p><a href=""https://i.stack.imgur.com/bnSbS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bnSbS.png"" alt=""enter image description here"" /></a></p>
<p>And I created a materialized views for ranking users in a contest (table contest_ranking):</p>
<p><a href=""https://i.stack.imgur.com/BetIH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BetIH.png"" alt=""enter image description here"" /></a></p>
<p>For get top 10 users of a contest I can simple query select top 10 from contest_ranking;</p>
<p>But how can I get ranking of specific user. For example: user_id = 4 will have rank 2.</p>
",<cassandra><materialized-views>,"<p>The principal philosophy of data modelling in Cassandra is that you need to design a CQL table for each application query. It is a one-to-one mapping between app queries and CQL tables.</p>
<p>Since you have a completely different application query, you need to create a separate table for it. Here's an example schema:</p>
<pre><code>CREATE TABLE rank_by_userid (
    user_id int,
    rank int,
    PRIMARY KEY(user_id)
)
</code></pre>
<p>You can then get the rank of a user with this query:</p>
<pre><code>SELECT rank FROM rank_by_userid WHERE user_id = ?
</code></pre>
<p>You have to manually create and maintain this new table because you won't be able to populate it with materialized views. Cheers!</p>
",['table']
73052073,73052474,2022-07-20 12:58:00,Cassandra table data modeling,"<p>I just started to learn about Cassandra and I have a pretty specific question.
After looking at some videos and tutorials in my understanding there is 1 primary that includes partition keys and clustering keys.</p>
<p>So my question is:
If I have some data that looks like this:</p>
<pre><code>Unit: (unit_id, unit_name, unit_description)
</code></pre>
<p>and I want to save units working hours that look like:</p>
<pre><code>Working hours: (unit_id, day, start_time, end_time)
</code></pre>
<p>What would you suggest the data modeling should look like?
In addition to that, if I have to get all units that work on Sunday or all units that start at 10:00, what would you suggest to do?</p>
",<sql><database><cassandra><nosql>,"<p>You're actually doing it backwards by starting with how you want to store the data. It's a common mistake for those coming from a traditional relational background.</p>
<p>In Cassandra data modelling, we always start with the application query. For each app query, we would design a CQL table for it.</p>
<p>If your app query is &quot;get units where day is X&quot;, we would design a table that looks like this:</p>
<pre><code>CREATE TABLE units_by_day (
    day text,
    unit_id text,
    ...
    PRIMARY KEY (day, unit_id)
)
</code></pre>
<p>For each day, there are one or more rows of units (clustered by <code>unit_id</code>). To retrieve the units that worked on Sunday:</p>
<pre><code>SELECT unit_id FROM units_by_day WHERE day = 'Sunday'
</code></pre>
<p>If your app query is &quot;get units that worked on day X and started at Y&quot;:</p>
<pre><code>CREATE TABLE units_by_day_starttime (
    day text,
    start_time time,
    unit_id text,
    ...
    PRIMARY KEY (day, start_time, unit_id)
)
</code></pre>
<p>And the CQL query would look like:</p>
<pre><code>SELECT unit_id FROM units_by_day_starttime
  WHERE day = ?
  AND start_time = ?
</code></pre>
<p>Note that you can also do range queries on <code>start_time</code> like:</p>
<pre><code>SELECT unit_id FROM units_by_day_starttime
  WHERE day = ?
  AND start_time &gt;= ?
  AND start_time &lt; ?
</code></pre>
<p>Cheers!</p>
",['table']
73060026,73060202,2022-07-21 02:26:45,Is adding a column to a CQL table a constant time or linear time operation?,"<p>When altering a table to add a new column, the Cassandra <a href=""https://cassandra.apache.org/doc/latest/cassandra/cql/ddl.html#alter-table-statement"" rel=""nofollow noreferrer"">documentation</a> states that</p>
<pre><code>Adding a column is a constant-time operation based on the amount of data in the table.
</code></pre>
<p>Though, this may be a bit ambiguous. Does it mean it is linear time as the execution time depends on the amount of data in the table?</p>
",<cassandra>,"<p>Adding a column to a table takes the same amount of time regardless of how much data is in the table.</p>
<p>If you recall, SSTables (Cassandra data files) are <strong>immutable</strong> -- once they've been written to disk, they <em>never change</em>.</p>
<p>When you add a column to a table, nothing actually happens to the existing SSTables -- Cassandra doesn't alter/update/modify the existing files. But if your app writes data to the new column, the newly flushed SSTables will contain the new column.</p>
<p>Similarly, the newly-merged SSTables from the next cycle of compactions will contain the new column. What this means is that adding a new column takes exactly the same amount of time whether a table is empty or contains terrabytes of data.</p>
<p>The thing that is relevant is the cluster topology. Schema changes are propagated via gossip, it doesn't follow the normal write path. The bigger a cluster is, the longer it will take for a schema change to be gossiped to all nodes in a large cluster. Cheers!</p>
",['table']
73104866,73123639,2022-07-25 06:33:44,k8ssandra Cassandra configuration problem,"<p>I deployed k8ssandra followed officail instruction on site <a href=""https://docs-v2.k8ssandra.io/install/local/single-cluster-helm/"" rel=""nofollow noreferrer"">https://docs-v2.k8ssandra.io/install/local/single-cluster-helm/</a>.</p>
<p>After the k8ssandra deployment, I create a big table on cassandra DB. While I'm query the row nubmer by &quot;select count(*) from TABLENAME&quot;, cqlsh always got <strong>ERROR</strong> like &quot;Coordinator node timed out waiting for replica nodes' responses&quot;. So I wanna change some cassandra configurations. Could you tell me how could I do it or where the cassandra yaml file is for k8ssandra cluster?</p>
",<cassandra><k8ssandra>,"<p>This isn't a K8ssandra issue. The problem with your query is that an unbounded <code>COUNT()</code> does a full table scan.</p>
<p>This means that Cassandra has to read ALL the partitions from ALL the nodes in order to count them. There is no amount of tuning that will get around that. I've explained why in this post -- <a href=""https://dba.stackexchange.com/questions/314567/"">Why is COUNT() bad in Cassandra?</a>.</p>
<p>As a friendly note, a reminder that Stack Overflow is for getting help with coding, algorithm, or programming language problems. For future reference, you should post DB admin/ops questions on <a href=""https://dba.stackexchange.com/questions/ask?tags=cassandra"">DBA Stack Exchange</a>. Cheers!</p>
",['table']
73192124,73193373,2022-08-01 10:09:20,Cassandra(Amazon keyspace) Query Error on clustered columns,"<p>I am trying execute query on clustering columns on amazon keyspace, since I don't want to use ALLOW FILTERING with my native query I have created 4-5 clustering columns for better performance.</p>
<p>But while trying to filter it based on &gt;= and &lt;= with on 2 clustering columns, I am getting error with below message</p>
<p><strong>message=&quot;Clustering column &quot;start_date&quot; cannot be restricted (preceding column &quot;segment_id&quot; is restricted by a non-EQ relation)&quot;</strong></p>
<p>I had also tried with multiple columns query but I am getting not supported error
<strong>message=&quot;MultiColumn relation is not yet supported.&quot;</strong></p>
<p>Query for the reference</p>
<p>select * from table_name where  shard_id = 568 and division = '10' and customer_id = 568113 and (segment_id, start_date,end_date)&gt;= (-1, '2022-05-16','2017-03-28') and flag = 1;</p>
<p>or</p>
<p>select * from table_name where  shard_id = 568 and division = '10' and customer_id = 568113 and segment_id &gt; -1 and  start_date &gt;='2022-05-16';</p>
",<cassandra><cql><amazon-keyspaces>,"<p>I am assuming that the your table has the following primary key:</p>
<pre><code>CREATE TABLE table_name (
    ...
    PRIMARY KEY(shard_id, division, customer_id, segment_id, start_date, end_date)
)
</code></pre>
<p>In any case, your CQL query is invalid because you can only apply an inequality operator on the last clustering column in your query. For example, these are valid queries based on your table schema:</p>
<pre><code>SELECT * FROM table_name
    WHERE shard_id = ? AND division = ?
    AND customer_id &lt;= ?

SELECT SELECT * FROM table_name \
    WHERE shard_id = ? AND division = ? \
    AND customer_id = ? AND segment_id &gt; ?

SELECT SELECT * FROM table_name \
    WHERE shard_id = ? AND division = ? \
    AND customer_id = ? AND segment_id = ? AND start_date &gt;= ?
</code></pre>
<p>All preceding columns must be filtered by an equality operator except for the very last clustering column in your query.</p>
<p>If you require a complex predicate for your queries, you will need to index your Cassandra data with tools such as Elasticsearch or Apache Solr. They will allow you to run complex search parameters to retrieve data from your database. Cheers!</p>
",['table']
73533183,73533917,2022-08-29 18:28:51,Can you change the partition key of a particular row in a Cassandra table?,"<p>I am unsure if you can change the partition key value of a particular row in a table. For example, if you had a table such that the PRIMARY KEY(name, title) and have some entry where name = &quot;John&quot;, title = &quot;New&quot;. Would we be able to run:</p>
<pre><code>UPDATE table
SET name=&quot;Ron&quot;
WHERE name=&quot;John&quot; AND title=&quot;New&quot;;
</code></pre>
<p>I understand the general concept of the partition key such that it uniquely identifies rows in a partition while also identifying what nodes hold said partitions. So this leads to me to believe this would not run.</p>
",<cassandra><cql><datastax-enterprise>,"<blockquote>
<p>So this leads to me to believe this would not run.</p>
</blockquote>
<p>You are correct.  With Cassandra it is correct to say that <code>UPDATE</code> and <code>INSERT</code> typically do the same thing (under the hood).  However in this case, that is not true.  Running that <code>UPDATE</code> statement yields this message:</p>
<pre><code>&gt; UPDATE table SET name='Ron' WHERE name='John' AND title='New';
InvalidRequest: Error from server: code=2200 [Invalid query] message=&quot;PRIMARY KEY part name found in SET part&quot;
</code></pre>
<p>This is because the <code>UPDATE</code> has additional checks around the <code>WHERE</code> clause.  The best way to handle a situation like this, is to rewrite the row with the new key and then <code>DELETE</code> the old row.</p>
<p>And it's probably best to wrap those two statements in a <code>BATCH</code>, to ensure that entry doesn't end up in a weird state due to one of them failing.</p>
<pre><code>BEGIN BATCH
    INSERT INTO table (name,title) VALUES ('Ron','New');
    DELETE FROM table WHERE name='John' AND title='New';
APPLY BATCH;
</code></pre>
",['table']
73533833,73536750,2022-08-29 19:34:35,Contradicting information on CQL counter type in docs,"<p>I have been looking for some information on counters and it seems like there is some rather contradicting info regarding to what you can do with them.</p>
<p>According to the official DataStax Documentation (<a href=""https://docs.datastax.com/en/cql-oss/3.x/cql/cql_reference/counter_type.html"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/cql-oss/3.x/cql/cql_reference/counter_type.html</a>) &quot;You cannot set the value of a counter, which supports two operations: increment and decrement.&quot;.</p>
<p>However, if we look into the BATCH CQL documentation (<a href=""https://docs.datastax.com/en/dse/6.0/cql/cql/cql_reference/cql_commands/cqlBatch.html#cqlBatch__batch-updates"" rel=""nofollow noreferrer"">https://docs.datastax.com/en/dse/6.0/cql/cql/cql_reference/cql_commands/cqlBatch.html#cqlBatch__batch-updates</a>), the bottom page example includes setting, adding, and subtracting a counter variable within a batch.</p>
<p>This example is likely also breaking the rule that having a counter in a table should only have counters for the rest of the table.</p>
<p>So what really are the limitations / usability for counters in cassandra DataStax? There does not seem to be a clear definition.</p>
",<cassandra><datastax-enterprise>,"<p>I think it's just a misunderstanding. Those pages do not contradict each other.</p>
<p>The CQL <a href=""https://docs.datastax.com/en/cql-oss/3.x/cql/cql_reference/counter_type.html"" rel=""nofollow noreferrer"">Counter type</a> page correctly states that it is not possible to set the value of a <code>counter</code> column. For example, this is NOT valid:</p>
<pre><code>UPDATE ks.counter_table
    SET count = 10
    WHERE pk = ?
</code></pre>
<p>The only valid operations on a <code>counter</code> column are increment and decrement. Here are some examples:</p>
<pre><code>UPDATE ks.counter_table
  SET count = count + 1
  WHERE pk = ?
</code></pre>
<pre><code>UPDATE ks.counter_table
  SET count = count - 1
  WHERE pk = ?
</code></pre>
<p>In the <a href=""https://docs.datastax.com/en/dse/6.0/cql/cql/cql_reference/cql_commands/cqlBatch.html#cqlBatch__batch-updates"" rel=""nofollow noreferrer"">BATCH command page</a>, the first 2 examples are increment operations:</p>
<pre><code>UPDATE cycling.popular_count
  SET popularity = popularity + 1
  WHERE id = 6ab09bec-e68e-48d9-a5f8-97e6fb4c9b47;
</code></pre>
<pre><code>UPDATE cycling.popular_count
  SET popularity = popularity + 125
  WHERE id = 6ab09bec-e68e-48d9-a5f8-97e6fb4c9b47;
</code></pre>
<p>The last example is a decrement operation:</p>
<pre><code>UPDATE cycling.popular_count
  SET popularity = popularity - 64
  WHERE id = 6ab09bec-e68e-48d9-a5f8-97e6fb4c9b47;
</code></pre>
<p>I can't see how the examples would break the rule about only having counters in a counter table. From the examples, I would infer that the table schema is:</p>
<pre><code>CREATE TABLE cycling.popular_count
    id uuid,
    popularity counter,
    PRIMARY KEY(id)
)
</code></pre>
<p>You can have as many non-<code>counter</code> columns in the table as long as they are part of the <code>PRIMARY KEY</code>.</p>
<p>As a side note, it is not correct to refer to the software as either &quot;Cassandra DataStax&quot; or &quot;DataStax Cassandra&quot; so I've updated the title accordingly. DataStax (the company) does not own Cassandra. The more appropriate reference is &quot;Apache Cassandra&quot; or just plain &quot;Cassandra&quot;. Cheers!</p>
",['table']
73653401,73656576,2022-09-08 18:08:14,How do I efficiently partition an index-only table in Cassandra?,"<p>I need to create an append only table which should only store a pair of values <code>(foreign_id, some_string)</code>. There will be a limited number of <code>foreign_id</code> values (let's say 100 - 10 000) and 10s of millions of <code>some_string</code> values (they may not be evenly distributed between <code>foreign_ids</code>)</p>
<p>I am only interested whether a given <code>(foreign_id, some_string)</code> pair exists in the table.</p>
<p>What would be the most efficient way (when it comes to query response time) of partitioning this table?</p>
<p>I am pretty sure that creating a primary key <code>PRIMARY KEY ((foreign_id), some_string)</code> is a bad idea, because a single partition could easily grow beyond <code>100 MB</code> which is not recommended AFAIK.</p>
<p>Should I simply partition the table by both <code>foreign_id</code> and <code>some_string</code> like this <code>PRIMARY KEY ((foreign_id, some_string))</code> or is there some issue with this approach?</p>
",<cassandra>,"<p>The primary philosophy of data modelling in Cassandra is -- for each application query, design a table that is optimised for that query. It is the complete opposite of data modelling in traditional relational databases.</p>
<p>Don't get hung up on how you will store the data in the table but focus on what query your application requires because the app query is the crucial aspect that determines how the table will be optimised for reads.</p>
<p>Looking at this statement from you:</p>
<blockquote>
<p>I am only interested whether a given <code>(foreign_id, some_string)</code> pair exists in the table.</p>
</blockquote>
<p>My understanding is that your app query is something along the lines of:</p>
<blockquote>
<p>&quot;Does ID X and string Y exist?&quot;</p>
</blockquote>
<p>which means that you should partition the table by both ID and string:</p>
<pre><code>CREATE TABLE tbl_by_id_string (
    foreign_id text,
    some_string text,
    exists boolean,
    PRIMARY KEY ((foreign_id, some_string))
)
</code></pre>
<p>The equivalent CQL query to your app query is:</p>
<pre><code>SELECT exists FROM tbl_by_id_string WHERE foreign_id = ? AND some_string = ?
</code></pre>
<p>This design is optimised for your app query and completely eliminates your concern around having large partitions because each partition in the table will only ever have ONE row and will never get any bigger than that.</p>
<p>Also, you can have billions and billions of combinations of ID + string and they will be distributed evenly across the nodes in the cluster with this design. Cheers!</p>
",['table']
73676111,73679520,2022-09-11 00:45:27,How do I group by date in Cassandra?,"<p>I'm trying to find a query in Cassandra cql to group by date. I have &quot;date&quot; datatype where the date is like: &quot;mm-dd-yyyy&quot;. I'm just trying to extract the year and then group by. How to achieve that?</p>
<pre><code>SELECT sum(amount) FROM data WHERE date = 'yyyy'
</code></pre>
",<cassandra><cql>,"<p>You cannot do a partial filter with just the year on a column of type <code>date</code>. It is an invalid query in Cassandra.</p>
<p>The <a href=""https://cassandra.apache.org/doc/latest/cassandra/cql/types.html#dates"" rel=""nofollow noreferrer"">CQL <code>date</code> type</a> is encoded as a 32-bit integer that represents the days since epoch (Jan 1, 1970).</p>
<p>If you need to filter based on year the you will need to add a column to your table like in this example:</p>
<pre><code>CREATE TABLE movies (
    movie_title text,
    release_year int,
    ...
    PRIMARY KEY ((movie_title, release_year))
)
</code></pre>
<p>Here's an example for retrieving information about a movie:</p>
<pre><code>SELECT ... FROM movies WHERE movie_title = ? AND release_year = ?
</code></pre>
",['table']
73686804,73689358,2022-09-12 08:59:53,What happens with Spark partitions when using Spark-Cassandra-Connector,"<p>So, I have a 16 node cluster where every node has Spark and Cassandra installed with a replication factor of 3 and spark.sql.shuffle.partitions of 96. I am using the Spark-Cassandra Connector 3.0.0 for doing a <code>repartitionByCassandraReplica.JoinWithCassandraTable</code> and then some SparkML analysis takes place. My question is what happens eventually with the spark partitions?</p>
<p><strong>1st scenario</strong></p>
<p>My <code>PartitionsPerHost</code> parameter of <code>repartitionByCassandraReplica</code> is numberofSelectedCassandraPartitionkeys which means if I choose 4 partition keys I get 4 partitions per Host. This gives me <strong>64 spark partitions</strong> because I have 16 hosts.</p>
<p><strong>2nd scenario</strong></p>
<p>But, according to the <a href=""https://github.com/datastax/spark-cassandra-connector"" rel=""nofollow noreferrer"">Spark Cassandra connector</a> documentation, information from <code>system.size_estimates</code> table should be used in order to calculate the spark partitions. For example from my <code>system.size_estimates</code>:</p>
<pre><code>estimated_table_size = mean_partition_size x number_of_partitions
                 = (24416287.87/1000000) MB x 332
                 = 8106.2 MB

spark_partitions = estimated_table_size / input.split.size_in_mb
             = 8106.2 MB / 64 MB
             = 126.6593 partitions
</code></pre>
<p>so, when does the 1st scenario takes place and when the second? Am I calculating something wrong? Is there specific cases where the 1st scenario happens and other cases the 2nd?</p>
",<apache-spark><cassandra><spark-cassandra-connector>,"<p>Those are two completely different paths by which the number of Spark partitions are calculated.</p>
<p>If you're calling <code>repartitionByCassandraReplica()</code>, the number of Spark partitions are determined by both <code>partitionsPerHost</code> and the number of Cassandra nodes in the local DC.</p>
<p>Otherwise, the connector will use <code>input.split.size_in_mb</code> to determine the number of Spark partitions based on the estimated table size. Cheers!</p>
",['table']
73698329,73698844,2022-09-13 06:22:52,SUM() of large number of Cassandra rows returns negative value,"<p>I'm trying to get sum of column in cassandra but it return negetive value although there is no negetive row in my column.</p>
<pre><code>cqlsh:samt&gt; select cost from items where cost&lt;0 allow filtering ;

 cost
------

(0 rows)
</code></pre>
<p>But when i try to query sum of column it return :</p>
<pre><code>select sum(cost) from items;
system.sum(cost)
------------------
        -18485190
</code></pre>
<p>the number of rows I'm trying to aggregate in is more than a million, is this the cause?</p>
",<cassandra>,"<p>Welcome to Stack Overflow! You haven't provided much detail so I can only guess that the column values are so large that when they are all added together, it results in an <a href=""https://en.wikipedia.org/wiki/Integer_overflow"" rel=""nofollow noreferrer"">integer overflow</a>.</p>
<p>In any case, unbounded queries are expensive since they require a full table scan and can affect the performance of the cluster especially if run in production. If you need to run full table aggregation, you should run them in Spark.</p>
<p>Finally for future reference, you need to provide sufficient background information when asking questions. The general guidance is that you (a) provide a good summary of the problem that includes software/component versions, the full error message + full stack trace; (b) describe what you've tried to fix the problem, details of investigation you've done; and (c) minimal sample code that replicates the problem. Cheers!</p>
",['table']
73699651,73700124,2022-09-13 08:15:58,Should I denormalize a Cassandra table with 1B rows according to the queries being used?,"<p>I know that in a Cassandra table, inserts with the same <strong>partition key</strong> will overwrite the previous value. So, if we also insert 10 records with the same <strong>primary key</strong> it will do the same, meaning overwrite and store the 10th value only. Right?</p>
<p>So, I have the below table in my Cassandra database which has ~1 billion rows with ~4800 partition keys:</p>
<pre><code>CREATE TABLE tb(
parkey varchar, //this is a UUID converted to String.
pk1 text,
pk2 float,
pk3 float,
pk4 float,
pk5 text,
pk6 text,
pk7 text,
PRIMARY KEY ((parkey),pk1, pk2, pk3, pk4, pk5, pk6, pk7));
</code></pre>
<p>This means I have ~1 billion primary keys!! I have such a big primary key because every record is unique only if it has all the values. However, I have a feeling this might not be the best table schema, as it also takes 5 minutes for spark to query all these data while it also hangs for another 10 minutes just before unpersisting a table from memory for which I do not know why!</p>
<p>Should I break down and denormalize the table somehow according to the queries being used? Will that improve the query times? <strong>My thought is</strong>, that even if I break down the table, I will still have ~1 billion primary keys for each denormalized table that will be created. Would that be efficient? Will it not take again 15 minutes to query the newly created tables?</p>
<p><strong>Edit 1</strong></p>
<p>I am always using 1 query that selects partition keys. Hence one table. Would this improve times?</p>
<pre><code>CREATE TABLE tb(
parkey varchar, //this is a UUID converted to String.
pk1 varchar, //also a UUID but completely unique for every record
c1 text,
c2 float,
c3 float,
c4 float,
c5 text,
c6 text,
c7 text,
PRIMARY KEY ((parkey),pk1));
</code></pre>
",<apache-spark><cassandra>,"<p>The quick answer is YES, you should denormalise the data and always start with the app queries. Those who come from a relational DB background tend to focus on how the data is stored (table schema) instead of listing all the app queries first.</p>
<p>By focusing on the app queries first THEN designing a table for each of the queries, the table is optimised for reads. If you try to adapt an app query to an existing table then the table will never be optimised and the queries will almost always be slow.</p>
<p>As a side note, the long answer is that 1B rows <code>!=</code> 1B partitions in the schema you posted. The table definition does not have a 1:1 mapping between rows and partitions. Each of the partitions in your table can have ONE OR MORE rows. Cheers!</p>
",['table']
73817755,73819532,2022-09-22 16:09:20,How to scan properly a cassandra table page by page for ranged primary key?,"<p>How to scan a table if i have table like this in Cassandra 3.11:</p>
<pre><code>CREATE TABLE versions (
    root text,
    subroot text,
    key text,
    ts timeuuid,
    size bigint,
    PRIMARY KEY ((root, subroot, key), ts)
) WITH CLUSTERING ORDER BY (ts DESC)
</code></pre>
<p>how can I scan properly per 1000 only for root='a', subroot='b', key&gt;='c000000' and key&lt;'c000001' (I need to scan everything started with <code>c000000*</code>, for example <code>c000000-aaaaaa</code>, <code>c000000something</code>, etc)</p>
<p>Because if I do this using sum, it got timedout</p>
<pre><code>SELECT sum(size) 
FROM versions 
WHERE root='a' 
  AND subroot='b' 
  AND key&gt;='c00000' AND key&lt;'c000001' 
ALLOW FILTERING;
</code></pre>
<p>Is there a way to fetch everything without <code>ALLOW FILTERING</code> (I can sum using golang code or other language)?</p>
",<cassandra>,"<p>Yeah, you still need to do a full table scan with such partitioning. Because you have <code>key</code> column as a part of the partition key, the hash of the values could be distributed to different nodes &amp; belong to the different token ranges.  You can't do that efficiently using CQL only, so you need to write your own code or use tools like DSBulk (as I remember, you can use <a href=""https://docs.datastax.com/en/dsbulk/docs/reference/dsbulkCmd.html"" rel=""nofollow noreferrer"">DSBulk</a> as Java library as well) or Spark + <a href=""https://github.com/datastax/spark-cassandra-connector/"" rel=""nofollow noreferrer"">Spark Cassandra Connector</a> - both of these tools are heavily optimized for efficient full table scans.</p>
<p>In case if you want to implement it yourself, you need to write a code that will do following:</p>
<ul>
<li>Pull a list of token ranges</li>
<li>Create a CQL query that will include your condition + subquery <code>token(root, subroot, key) &gt; + rangeStart  AND token(root, subroot, key) &lt;=  rangeEnd</code></li>
<li>Send that query to one of the nodes owning the specific token range (I don't know if this functionality exists in Go driver)</li>
</ul>
<p>Please note that you need to correctly handle ranges - they aren't starting at <code>RANGE_MIN</code>, some ranges could include the <code>RANGE_MIN</code>.</p>
<p>You can look to <a href=""https://github.com/alexott/cassandra-dse-playground/blob/master/driver-1.x/src/main/java/com/datastax/alexott/demos/TokenRangesScan.java"" rel=""nofollow noreferrer"">this Java example</a> - it uses the same algorithm as Spark Cassandra Connector and DSBulk.</p>
",['table']
73853100,73862240,2022-09-26 10:52:55,How is the input size calculated for repartitionByCassandraReplica.JoinWIthCassandraTable() vs DirectJoin=AlwaysOn?,"<p>So I noticed that when calling repartitionByCassandraReplica().JoinWIthCassandraTable() gives me a different Input size in the Stages tab of SparkUI comparing to the one I get when the DirectJoin is always On. I know that these two follow different strategies of determining the Spark partitions:</p>
<p>When calling repartitionByCassandraReplica(), the number of Spark partitions is determined by partitionsPerHost. Otherwise, the connector will use the estimated table size. Nevertheless, as per documentation, both use DirectJoin and do not perform a full scan of a Cassandra table.</p>
<p><strong>In my case:</strong></p>
<p>With DirectJoin always On I get <strong>36.9Gb</strong> size in the Input column and it takes <strong>4.5 minutes</strong> for a Join and count. However, with repartitionByCassandraReplica().JoinWIthCassandraTable() on the same data I get <strong>68.9Gb</strong> (almost double) in <strong>3.4 minutes</strong>.</p>
<p><strong>Question 1</strong></p>
<p>How is the Input Column of Stages Tab calculated for each of these two Join strategies? Does the DirectJoinAlwaysOn uses the size of the <code>estimated table size</code> for input column and the repartitionByCassandraReplica.JoinWIthCassandraTable() the actual/precise size of the table?</p>
<p><strong>Question 2</strong></p>
<p>Why does repartitionByCassandraReplica.JoinWIthCassandraTable() take less time even if it has a bigger Input size? Is it just because of data locality?</p>
<p><strong>Question 3</strong></p>
<p>Finally, is the repartitionByCassandraReplica().JoinWIthCassandraTable() eventually affected by the size of the Cassandra table? Is the DirectJoin in these two different strategies a bit different (other than the how are the Spark partitions calculated) ?</p>
",<apache-spark><cassandra><spark-cassandra-connector>,"<p>The input size is a derivative of the previous stage.</p>
<p>To answer your first question, the Direct Join setting has no bearing on how the Spark partitions are computed. What matters is whether you call <code>repartitionByCassandraReplica()</code> or not.</p>
<p>I've explained in your previous question (<a href=""https://stackoverflow.com/questions/73686804/"">What happens with Spark partitions when using Spark-Cassandra-Connector</a>) that the Spark partitions are computed differently by the Spark Cassandra connector depending on the APIs you're using. To summarise:</p>
<ul>
<li>IF <code>repartitionByCassandraReplica()</code> gets called, the number of Spark partitions are determined by both <code>partitionsPerHost</code> and the number of Cassandra nodes in the local DC.</li>
<li>ELSE the Spark Cassandra connector uses <code>input.split.size_in_mb</code> to determine the number of Spark partitions based on the estimated table size.</li>
</ul>
<p>Given that the number of Spark partitions widely differ between these two schemes, the resulting output size (data read) will widely differ too because the Cassandra token range(s) which get mapped to each Spark partition is also going to be different -- it's not an apples-for-apples comparison.</p>
<p>As a side note, I'd like to make a friendly request that you should limit to one question per post, particularly since your second and third questions are different from the original question. Cheers!</p>
",['table']
73976564,74147238,2022-10-06 15:37:55,Does taking advantage of dynamic columns in Cassandra require duplicated data in each row?,"<p>I've been trying to understand how one would model time series data in Cassandra, like shown in the below image from a popular System Design Interview video, where counts of views are stored hourly. <a href=""https://i.stack.imgur.com/HBVcy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HBVcy.png"" alt=""enter image description here"" /></a></p>
<p>While I would think the schema for this time series data would be something like the below, I don't believe this would lead to data actually being stored in the way the screenshot shows.</p>
<pre><code>CREATE table views_data {
    video_id uuid
    channel_name varchar
    video_name varchar
    viewed_at timestamp
    count int
    PRIMARY_KEY (video_id, viewed_at)
};
</code></pre>
<p>Instead, I'm assuming it would lead to something like this (inspired by <a href=""https://www.datastax.com/blog/does-cql-support-dynamic-columns-wide-rows"" rel=""nofollow noreferrer"">datastax</a>), where technically there is a single row for each <code>video_id</code>, but the other columns seem like they would all be duplicated, such as <code>channel_name</code>, <code>video_name</code>, etc.. within the row for each unique <code>viewed_at</code>.</p>
<pre><code>[cassandra-cli]

list views_data;
RowKey: A
=&gt; (channel_name='System Design Interview', video_name='Distributed Cache', count=2, viewed_at=1370463146717000)
=&gt; (channel_name='System Design Interview', video_name='Distributed Cache', count=3, viewed_at=1370463282090000)
=&gt; (channel_name='System Design Interview', video_name='Distributed Cache', count=8, viewed_at=1370463282093000)
-------------------
RowKey: B 
=&gt; (channel_name='Some other channel', video_name='Some video', count=4, viewed_at=1370463282093000)
</code></pre>
<p>I assume this is still considered dynamic wide row, as we're able to expand the row for each unique <code>(video_id, viewed_at)</code> combination. But it seems less than ideal that we need to duplicate the extra information such as <code>channel_name</code> and <code>video_name</code>.</p>
<p><strong>Is the screenshot of modeling time series data misleading or is it actually possible to have dynamic columns where certain columns in the row do not need to be duplicated?
If I was upserting time series data to this row, I wouldn't want to have to provide the <code>channel_name</code> and <code>video_name</code> for every single upsert, I would just want to provide the <code>count</code>.</strong></p>
",<database><cassandra><nosql><data-modeling>,"<p>No, it is not necessary to duplicate the values of columns within the rows of a partition. It is possible to model your table to accomodate your use case.</p>
<p>In Cassandra, there is a concept of &quot;static columns&quot; -- columns which have the same value for all rows within a partition.</p>
<p>Here's the schema of an example table that contains two static columns, <code>colour</code> and <code>item</code>:</p>
<pre><code>CREATE TABLE statictbl (
    pk int,
    ck text,
    c int,
    colour text static,
    item text static,
    PRIMARY KEY (pk, ck)
)
</code></pre>
<p>In this table, each partition share the same <code>colour</code> and <code>item</code> for all rows of the same partition. For example, partition <code>pk=1</code> has the same <code>colour='red'</code> and <code>item='apple'</code> for all rows:</p>
<pre><code> pk | ck | colour | item   | c
----+----+--------+--------+----
  1 |  a |    red |  apple | 12
  1 |  b |    red |  apple | 23
  1 |  c |    red |  apple | 34
</code></pre>
<p>If I insert a new partition <code>pk=2</code>:</p>
<pre><code>INSERT INTO statictbl (pk, ck, colour, item, c) VALUES (2, 'd', 'yellow', 'banana', 45)
</code></pre>
<p>we get:</p>
<pre><code> pk | ck | colour | item   | c
----+----+--------+--------+----
  2 |  d | yellow | banana | 45
</code></pre>
<p>If I then insert another row withOUT specifying a <code>colour</code> and <code>item</code>:</p>
<pre><code>INSERT INTO statictbl (pk, ck, c) VALUES (2, 'e', 56)
</code></pre>
<p>the new row with <code>ck='e'</code> still has the <code>colour</code> and <code>item</code> populated even though I didn't insert a value for them:</p>
<pre><code> pk | ck | colour | item   | c
----+----+--------+--------+----
  2 |  d | yellow | banana | 45
  2 |  e | yellow | banana | 56
</code></pre>
<p>In your case, both the channel and video names will share the same value for all rows in a given partition if you declare them as <code>static</code> and you only ever need to insert them once. Note that when you update the value of static columns, ALL the rows for that partition will reflect the updated value.</p>
<p>For details, see <a href=""https://docs.datastax.com/en/cql-oss/3.x/cql/cql_using/refStaticCol.html"" rel=""nofollow noreferrer"">Sharing a static column in Cassandra</a>. Cheers!</p>
",['table']
74054972,74189853,2022-10-13 11:26:12,How do I find the partition key of Cassandra partitions with size greater than 100MB?,"<p>I want to get a list of partition with size greater than 100 MB for analysis. How do I achieve this ?</p>
",<cassandra><nosql><cassandra-3.0>,"<p>Cassandra logs a <code>WARN</code> with details of partitions getting compacted when the partition size is larger than <code>compaction_large_partition_warning_threshold</code>. The default in <code>cassandra.yaml</code> is 100MB:</p>
<pre><code># Log a warning when compacting partitions larger than this value
compaction_large_partition_warning_threshold: 100MiB
</code></pre>
<p>You can parse the <code>system.log</code> on the Cassandra nodes and look for log entries which contain the string <code>Writing large partition</code>. It looks something like:</p>
<pre><code>WARN  [CompactionExecutor:#] BigTableWriter.java:258 maybeLogLargePartitionWarning \
  Writing large partition ks_name/tbl_name:pk (###.###MiB) to sstable /path/to/.../...-big-Data.db
</code></pre>
<p>It should be easy enough to write a shell script that would extract the table name and partition key from the logs. Cheers!</p>
",['table']
74106113,74180256,2022-10-18 05:31:46,Cassandra CLUSTERING ORDER does not order data properly,"<p>I created a table that has timestamps in it but when I try to Cluster Order By the timestamp variable, it is not ordered properly.</p>
<p>To create the table I wrote:</p>
<pre><code>CREATE TABLE videos_by_tag ( 
tag text, 
video_id uuid, 
added_date timestamp, 
title text, 
PRIMARY KEY ((tag), added_date, video_id)) 
WITH CLUSTERING ORDER BY (added_date ASC);
</code></pre>
<p>And the output I got when doing a <code>SELECT * FROM videos_by_tag</code> is:</p>
<pre><code>
 tag       | added_date                      | video_id                             | title
-----------+---------------------------------+--------------------------------------+------------------------------
  datastax | 2013-04-16 00:00:00.000000+0000 | 5645f8bd-14bd-11e5-af1a-8638355b8e3a | What is DataStax Enterprise?
  datastax | 2013-10-16 00:00:00.000000+0000 | 4845ed97-14bd-11e5-8a40-8338255b7e33 |              DataStax Studio
 cassandra | 2012-04-03 00:00:00.000000+0000 | 245e8024-14bd-11e5-9743-8238356b7e32 |             Cassandra &amp; SSDs
 cassandra | 2013-03-17 00:00:00.000000+0000 | 3452f7de-14bd-11e5-855e-8738355b7e3a |              Cassandra Intro
 cassandra | 2014-01-29 00:00:00.000000+0000 | 1645ea59-14bd-11e5-a993-8138354b7e31 |            Cassandra History

(5 rows)
</code></pre>
<p>As you can see the dates are out of order. There is a 2012 year value in the middle of the output.</p>
",<cassandra>,"<p>This is a very common misconception in Cassandra. The data is in fact ordered correctly in the sample data you posted.</p>
<p>The <code>CLUSTERING ORDER</code> applies to the sort order of the rows within a partition -- NOT across ALL partitions.</p>
<p>Using the example you posted, the clustering column <code>added_date</code> is correctly sorted in ascending order for the partition <code>tag = 'datastax'</code>:</p>
<pre><code> tag       | added_date                      
-----------+---------------------------------
  datastax | 2013-04-16 00:00:00.000000+0000 
  datastax | 2013-10-16 00:00:00.000000+0000
</code></pre>
<p>Similarly, <code>added_date</code> is sorted in ascending order for <code>tag = 'cassandra'</code>:</p>
<pre><code> tag       | added_date                      
-----------+---------------------------------
 cassandra | 2012-04-03 00:00:00.000000+0000
 cassandra | 2013-03-17 00:00:00.000000+0000
 cassandra | 2014-01-29 00:00:00.000000+0000
</code></pre>
<p>Like I said, the sort order only applies to rows within a partition.</p>
<p>It would be impossible to sort all rows in all partitions because such task does not scale. Imagine if you had billions of partitions in the table across hundreds of nodes. Every time you inserted a new row to any partition, Cassandra has to do a full table scan to sort the data and it just wouldn't make sense to do so. Cheers!</p>
",['table']
74123289,74179815,2022-10-19 09:50:26,Can I set a column to the value of another column in CQL?,"<p>I'm new to cassandra and I'm trying to create a new column witch content is based on another column.</p>
<p>More precisely what I want to achive is, starting with following table:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">ColumnA</th>
<th style=""text-align: left;"">ColumnB</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">1</td>
<td style=""text-align: left;"">text</td>
</tr>
</tbody>
</table>
</div>
<p>I want to update the table obtaining :</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;"">ColumnA</th>
<th style=""text-align: left;"">ColumnB</th>
<th style=""text-align: left;"">ColumnC</th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">1</td>
<td style=""text-align: left;"">text</td>
<td style=""text-align: left;"">ColumnA value +1</td>
</tr>
</tbody>
</table>
</div>
<p>I'm tryng something like this, after adding the columnC, <code> update table set columnC = columnA+1 ;</code> but this gave me the error</p>
<p><em>Only expressions of the form X = X - are supported.</em></p>
<p>So I tryed
<code> update table set columnC = columnA ;</code> but even this gave me the error</p>
<p><em>no viable alternative at input ';' (update table set columnC=[columnA];)</em></p>
",<cassandra><cql>,"<p>The CQL grammar does not support this type of operation because it does not scale.</p>
<p>Imagine if you had billions and billions of partitions across hundreds of nodes. That kind of operation will require a full table scan. And if you had millions of concurrent users, it would be difficult to make the updates idempotent since the partitions/rows are not locked between the time that you read the value in column A and write it to column C.</p>
<p>You will need to write an ETL app for it, preferably in Spark so it can efficiently iterate over the partitions/rows. Cheers!</p>
",['table']
74123297,74178885,2022-10-19 09:50:55,"Cassandra read returns AssertionError: ""Lower bound [INCL_END_BOUND() ] is bigger than first returned value""","<p>My Cassandra server had died and I tried to restore it on the another computer. According this article <a href=""https://community.datastax.com/questions/4818/backup-and-restore-cassandra-keyspace.html"" rel=""nofollow noreferrer"">https://community.datastax.com/questions/4818/backup-and-restore-cassandra-keyspace.html</a> I moved the <code>data</code> folder to the new server. All the tables has been restored properly except one of them.
When I try read data from it I get the exception:</p>
<pre><code>ERROR [ReadStage-2] 2022-10-19 07:47:55,026 AbstractLocalAwareExecutorService.java:166 - Uncaught exception on thread Thread[ReadStage-2,10,main]
java.lang.AssertionError: Lower bound [INCL_END_BOUND(2022-10-15 15:23Z) ]is bigger than first returned value [Row: utcdate=2022-10-15 11:07Z | data={t:0.880347,g:0.530729,a:180.0,v:11.7,d:5.896}] for sstable /var/lib/cassandra/data/Telematics_Energo/devicecoordinate-1005f1704edc11ed81fb63e346a603ff/mc-2158-big-Data.db
    at org.apache.cassandra.db.rows.UnfilteredRowIteratorWithLowerBound.computeNext(UnfilteredRowIteratorWithLowerBound.java:127) ~[apache-cassandra-3.11.11.jar:3.11.11]
    at org.apache.cassandra.db.rows.UnfilteredRowIteratorWithLowerBound.computeNext(UnfilteredRowIteratorWithLowerBound.java:48) ~[apache-cassandra-3.11.11.jar:3.11.11]
    at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47) ~[apache-cassandra-3.11.11.jar:3.11.11]
    at org.apache.cassandra.utils.MergeIterator$Candidate.advance(MergeIterator.java:374) ~[apache-cassandra-3.11.11.jar:3.11.11]

&lt; ... cut ... &gt;
</code></pre>
<p>My query is like this:</p>
<pre><code>SELECT 
  utcdate, data 
FROM 
  devicecoordinate 
WHERE 
  id = 00a8efb3-7815-e911-a830-00155d03c802 
  and period = 1663 

  and utcdate &gt;= '2022-10-09 08:55:00+0000' 
  and utcdate &lt;= '2022-10-09 08:56:00+0000' 

order by 
  utcdate
;
</code></pre>
<p>For others periods the query can execute without exception as well as without <code>order by</code> clause.</p>
<p>The table structure is:</p>
<pre><code>CREATE TABLE devicecoordinate (
    id uuid,
    period int,
    utcdate timestamp,
    data text,
    PRIMARY KEY (( id, period ), utcdate)
) WITH CLUSTERING ORDER BY ( utcdate DESC )
AND bloom_filter_fp_chance = 0.01
AND comment = ''
AND crc_check_chance = 1.0
AND dclocal_read_repair_chance = 0.1
AND default_time_to_live = 0
AND gc_grace_seconds = 864000
AND max_index_interval = 2048
AND memtable_flush_period_in_ms = 0
AND min_index_interval = 128
AND read_repair_chance = 0.0
AND speculative_retry = '99.0PERCENTILE'
AND caching = {
    'keys' : 'ALL',
    'rows_per_partition' : 'NONE'
}
AND compression = {
    'chunk_length_in_kb' : 64,
    'class' : 'LZ4Compressor',
    'crc_check_chance' : 1.0,
    'enabled' : true
}
AND compaction = {
    'base_time_seconds' : 14400,
    'class' : 'DateTieredCompactionStrategy',
    'enabled' : true,
    'max_sstable_age_days' : 5,
    'max_threshold' : 32,
    'min_threshold' : 4,
    'timestamp_resolution' : 'MICROSECONDS',
    'tombstone_compaction_interval' : 86400,
    'tombstone_threshold' : 0.2,
    'unchecked_tombstone_compaction' : false
};
</code></pre>
<p>Cassadra version: <code>3.11.11</code></p>
<p>cql_version: <code>3.4.4</code></p>
<p>native_protocol_version: 4</p>
<p>How can I fixed the exception?</p>
",<cassandra><cassandra-3.0>,"<p>This <code>AssertionError</code> is thrown by <code>UnfilteredRowIteratorWithLowerBound.computeNext()</code> due to the clustering column value being &quot;larger&quot; than the previous value meaning the clustering rows are out-of-sequence:</p>
<pre><code>ERROR [ReadStage-2] 2022-10-19 07:47:55,026 AbstractLocalAwareExecutorService.java:166 - Uncaught exception on thread Thread[ReadStage-2,10,main]
java.lang.AssertionError: Lower bound [INCL_END_BOUND(2022-10-15 15:23Z) ]is bigger than first returned value [Row: utcdate=2022-10-15 11:07Z | data={t:0.880347,g:0.530729,a:180.0,v:11.7,d:5.896}] for sstable /var/lib/cassandra/data/Telematics_Energo/devicecoordinate-1005f1704edc11ed81fb63e346a603ff/mc-2158-big-Data.db
    at org.apache.cassandra.db.rows.UnfilteredRowIteratorWithLowerBound.computeNext(UnfilteredRowIteratorWithLowerBound.java:127) ~[apache-cassandra-3.11.11.jar:3.11.11]
    ...
</code></pre>
<p>In your case, <code>2022-10-15 15:23Z</code> is larger than <code>utcdate = 2022-10-15 11:07Z</code>.</p>
<p>The last time I've come across this problem is when a DBA cloned a table to another cluster but didn't create the schema correctly.</p>
<p>In the schema you posted, the clustering order for <code>utcdate</code> should be in descending (<code>DESC</code>) order:</p>
<pre><code>CREATE TABLE devicecoordinate (
    ...
) WITH CLUSTERING ORDER BY (utcdate DESC)
</code></pre>
<p>but there's a good chance the table was created withOUT specifying the clustering order so it defaulted to ascending (<code>ASC</code>) order:</p>
<pre><code>WITH CLUSTERING ORDER BY (utcdate ASC)
</code></pre>
<p>When the schema does not match the data in the SSTables, Cassandra will not be able to read the data because the rows are out-of-sequence.</p>
<p>To fix it, you will need to:</p>
<ol>
<li>Drop the table with <code>DROP TABLE devicecoordinate</code>.</li>
<li>Recreate the table.</li>
<li>Restore the SSTables to the new table.</li>
</ol>
<p>This should fix your problem. Cheers!</p>
",['table']
74490155,74490739,1668776858,"Spring-boot app with ShedLock on Cassandra, getting &quot;NullPointerException: table can not be null&quot;","<p>I am trying to integrate shedlock in my spring boot project using a cassandra db.</p>
<p>But I am getting the below error on application startup.</p>
<pre><code>2022-11-18 17:35:29,162 [main] ERRR o.s.boot.SpringApplication - Application run failed
org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'lockProvider'

Caused by: java.lang.NullPointerException: table can not be null
    at java.base/java.util.Objects.requireNonNull(Objects.java:233)
    at net.javacrumbs.shedlock.provider.cassandra.CassandraLockProvider$Configuration.&lt;init&gt;(CassandraLockProvider.java:69)
    at net.javacrumbs.shedlock.provider.cassandra.CassandraLockProvider$Configuration$Builder.build(CassandraLockProvider.java:157)
</code></pre>
<p>application.yml file</p>
<pre class=""lang-yaml prettyprint-override""><code>server:
  port: 8080
  data:
    cassandra:
      port: ${CASSANDRA_PORT:9042}
      username: ${CASSANDRA_USERNAME:cassandra}
      password: ${CASSANDRA_PASSWORD:cassandra}
      keyspace-name: ${CASSANDRA_KEYSPACE_NAME:my_keyspace}
      schema-action: recreate
      local-datacenter: ${CASSANDRA_LOCAL_DATACENTER:datacenter123}
      contact-points: ${CASSANDRA_HOST:localhost}

</code></pre>
<p>Configuration class</p>
<pre class=""lang-java prettyprint-override""><code>import com.datastax.oss.driver.api.core.CqlSession;
import net.javacrumbs.shedlock.provider.cassandra.CassandraLockProvider;
import net.javacrumbs.shedlock.provider.cassandra.CassandraLockProvider.Configuration;
import net.javacrumbs.shedlock.spring.annotation.EnableSchedulerLock;
import org.springframework.context.annotation.Bean;
import org.springframework.scheduling.annotation.EnableScheduling;

@org.springframework.context.annotation.Configuration

@EnableScheduling
@EnableSchedulerLock(defaultLockAtMostFor = &quot;10m&quot;)

public class MyAppSchedulerConfiguration {

  @Bean
  public CassandraLockProvider lockProvider(CqlSession cqlSession) {
    return new CassandraLockProvider(Configuration.builder().withCqlSession(cqlSession).build());
  }
  
}

</code></pre>
<p>pom.xml file</p>
<pre class=""lang-xml prettyprint-override""><code>
&lt;parent&gt;
  &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
  &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;
  &lt;version&gt;2.3.7.RELEASE&lt;/version&gt;
  &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;
&lt;/parent&gt;


&lt;shedlock.version&gt;4.42.0&lt;/shedlock.version&gt;

&lt;dependency&gt;
  &lt;groupId&gt;net.javacrumbs.shedlock&lt;/groupId&gt;
  &lt;artifactId&gt;shedlock-spring&lt;/artifactId&gt;
  &lt;version&gt;${shedlock.version}&lt;/version&gt;
&lt;/dependency&gt;

&lt;dependency&gt;
  &lt;groupId&gt;net.javacrumbs.shedlock&lt;/groupId&gt;
  &lt;artifactId&gt;shedlock-provider-cassandra&lt;/artifactId&gt;
  &lt;version&gt;${shedlock.version}&lt;/version&gt;
&lt;/dependency&gt;

</code></pre>
<p>Cassandra script executed</p>
<pre class=""lang-sql prettyprint-override""><code>CREATE KEYSPACE shedlock with replication={'class':'SimpleStrategy', 'replication_factor':1} and durable_writes=true;
CREATE TABLE shedlock.lock (name text PRIMARY KEY, lockUntil timestamp, lockedAt timestamp, lockedBy text);
</code></pre>
<p>I have verified via cqlsh that the lock table is created in cassandra.</p>
<p>The problem in my opinion so far is that the CqlSession bean is not available due to some reason and should have been.</p>
<p>I have tried the configurations as mentioned in the <a href=""https://github.com/lukas-krecan/ShedLock/blob/master/README.md#cassandra"" rel=""nofollow noreferrer"">official documentation page</a>.</p>
<p>I have tried downgrading versions of springboot and shedlock but of no use.
**Springboot **version change
2.5.x (same error)
2.2.13.RELEASE (same error)
2.3.x (same error)</p>
","['java', 'spring-boot', 'dependency-injection', 'cassandra', 'shedlock']","<p>I think you have to specify tableName like this</p>
<pre><code>  @Bean
  public CassandraLockProvider lockProvider(CqlSession cqlSession) {
    return new CassandraLockProvider(Configuration.builder().withCqlSession(cqlSession).withTableName(&quot;lock&quot;).build());
  }
</code></pre>
<p>It seems the default table name is not set when using the Configuration builder.</p>
",['table']
74474143,74523448,1668682192,Storing a single compressed JSON column than multiple columns in Cassandra?,"<p>Purpose of the table is to maintain Audits.
Expected Behaviour: Huge writes with infrequent reads and no edits.</p>
<p><strong>Table Definition:</strong></p>
<pre><code>PartitionKey(multiple columns), ClusteringKey(time-uuid), json BLOB;
</code></pre>
<p><strong>json</strong> will hold Snappy.compressed bytes[]. I am trying to store high frequent large data in this table  with least partition size.</p>
<p>What do you think about a single BLOB column that holds compressed JSON or multiple columns in Cassandra?</p>
","['cassandra', 'data-modeling', 'cassandra-3.0', 'datamodel', 'snappy']","<p>I think it depends on your usage pattern.  Once written, will the JSON data change at all?  If it will, you'll end up having to rewrite the entire column value (which will be expensive in terms of temporary disk space used).</p>
<p>In that case, using individual columns might be the better option.  Remember that Cassandra does allow you to read and write while representing the table structure as JSON.  So if you end up going the individual column route, that feature allows you to abstract CRUD operations with JSON.</p>
<p>But if you're just writing once and updates are rare, then a JSON blob should be fine.</p>
<blockquote>
<p>Does Cassandra compress cell values?</p>
</blockquote>
<p>Yes.  By default, all created tables will use the <code>LZ4Compressor</code>.</p>
<blockquote>
<p>Is the data stored as a String or some compressed byte[] in the sstables?</p>
</blockquote>
<p>All data in Cassandra is stored (on disk) as a hex byte array.</p>
<blockquote>
<p>Asking this to understand if is worth compressing JSON in app server and storing in Cassandra as BLOB.</p>
</blockquote>
<p>I would not.  Basically, you'd compress in the app layer only to have it compressed again at write-time.  You should be fine just letting Cassandra handle the compression.</p>
",['table']
